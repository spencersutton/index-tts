# mypy: allow-untyped-defs

from collections import namedtuple
from collections.abc import Callable, Iterator, Mapping
from typing import Any, TypeAlias, TypeVar, overload

from torch import Tensor, device, dtype
from torch._prims_common import DeviceLikeType
from torch.nn.parameter import Parameter
from torch.utils.hooks import RemovableHandle
from typing_extensions import Self

__all__ = [
    "Module",
    "register_module_backward_hook",
    "register_module_buffer_registration_hook",
    "register_module_forward_hook",
    "register_module_forward_pre_hook",
    "register_module_full_backward_hook",
    "register_module_full_backward_pre_hook",
    "register_module_module_registration_hook",
    "register_module_parameter_registration_hook",
]

_grad_t: TypeAlias = tuple[Tensor, ...] | Tensor
# See https://mypy.readthedocs.io/en/latest/generics.html#generic-methods-and-generic-self for the use
# of `T` to annotate `self`. Many methods of `Module` return `self` and we want those return values to be
# the type of the subclass, not the looser type of `Module`.
T = TypeVar("T", bound=Module)

class _IncompatibleKeys(
    namedtuple("IncompatibleKeys", ["missing_keys", "unexpected_keys"]),
):
    __slots__ = ()

    __str__ = __repr__

def _addindent(s_, numSpaces): ...

r"""This tracks hooks common to all modules that are executed immediately before
.registering the buffer/module/parameter"""
_global_buffer_registration_hooks: dict[int, Callable] = ...
_global_module_registration_hooks: dict[int, Callable] = ...
_global_parameter_registration_hooks: dict[int, Callable] = ...

class _WrappedHook:
    def __init__(self, hook: Callable, module: Module | None = None) -> None: ...
    def __call__(self, *args: Any, **kwargs: Any) -> Any: ...
    def __getstate__(self) -> dict: ...
    def __setstate__(self, state: dict): ...

r"""This tracks hooks common to all modules that are executed before/after
calling forward and backward. This is global state used for debugging/profiling
purposes"""
_global_backward_pre_hooks: dict[int, Callable] = ...
_global_backward_hooks: dict[int, Callable] = ...
_global_is_full_backward_hook: bool | None = None
_global_forward_pre_hooks: dict[int, Callable] = ...
_global_forward_hooks: dict[int, Callable] = ...
_global_forward_hooks_always_called: dict[int, bool] = ...
_global_forward_hooks_with_kwargs: dict[int, bool] = ...

def _has_any_global_hook(): ...

_EXTRA_STATE_KEY_SUFFIX = "_extra_state"

def register_module_buffer_registration_hook(
    hook: Callable[..., None],
) -> RemovableHandle: ...
def register_module_module_registration_hook(
    hook: Callable[..., None],
) -> RemovableHandle: ...
def register_module_parameter_registration_hook(
    hook: Callable[..., None],
) -> RemovableHandle: ...
def register_module_forward_pre_hook(hook: Callable[..., None]) -> RemovableHandle: ...
def register_module_forward_hook(
    hook: Callable[..., None],
    *,
    with_kwargs: bool = False,
    always_call: bool = False,
) -> RemovableHandle: ...
def register_module_backward_hook(
    hook: Callable[[Module, _grad_t, _grad_t], None | _grad_t],
) -> RemovableHandle: ...
def register_module_full_backward_pre_hook(
    hook: Callable[[Module, _grad_t], None | _grad_t],
) -> RemovableHandle: ...
def register_module_full_backward_hook(
    hook: Callable[[Module, _grad_t, _grad_t], None | _grad_t],
) -> RemovableHandle: ...

# forward as a value, rather than a function.  See also
# https://github.com/python/mypy/issues/8795
def _forward_unimplemented(self, *input: Any) -> None: ...

class Module:
    dump_patches: bool = False

    _version: int = 1
    r"""This allows better BC support for :meth:`load_state_dict`. In
    :meth:`state_dict`, the version number will be saved as in the attribute
    `_metadata` of the returned state dict, and thus pickled. `_metadata` is a
    dictionary with keys that follow the naming convention of state dict. See
    ``_load_from_state_dict`` on how to use this information in loading.

    If new parameters/buffers are added/removed from a module, this number shall
    be bumped, and the module's `_load_from_state_dict` method can compare the
    version number and do appropriate changes if the state dict is from before
    the change."""

    training: bool
    _parameters: dict[str, Parameter | None]
    _buffers: dict[str, Tensor | None]
    _non_persistent_buffers_set: set[str]
    _backward_pre_hooks: dict[int, Callable]
    _backward_hooks: dict[int, Callable]
    _is_full_backward_hook: bool | None
    _forward_hooks: dict[int, Callable]
    # Marks whether the corresponding _forward_hooks accept kwargs or not.
    # As JIT does not support set[int], this dict is used as a set, where all
    # hooks represented in this dict accept kwargs.
    _forward_hooks_with_kwargs: dict[int, bool]
    # forward hooks that should always be called even if an exception is raised
    _forward_hooks_always_called: dict[int, bool]
    _forward_pre_hooks: dict[int, Callable]
    # Marks whether the corresponding _forward_hooks accept kwargs or not.
    # As JIT does not support set[int], this dict is used as a set, where all
    # hooks represented in this dict accept kwargs.
    _forward_pre_hooks_with_kwargs: dict[int, bool]
    _state_dict_hooks: dict[int, Callable]
    _load_state_dict_pre_hooks: dict[int, Callable]
    _state_dict_pre_hooks: dict[int, Callable]
    _load_state_dict_post_hooks: dict[int, Callable]
    _modules: dict[str, Module | None]
    call_super_init: bool = False
    _compiled_call_impl: Callable | None = None

    def __init__(self, *args, **kwargs) -> None: ...

    forward: Callable[..., Any] = ...

    def register_buffer(self, name: str, tensor: Tensor | None, persistent: bool = True) -> None: ...
    def register_parameter(self, name: str, param: Parameter | None) -> None: ...
    def add_module(self, name: str, module: Module | None) -> None: ...
    def register_module(self, name: str, module: Module | None) -> None: ...
    def get_submodule(self, target: str) -> Module: ...
    def set_submodule(self, target: str, module: Module, strict: bool = False) -> None: ...
    def get_parameter(self, target: str) -> Parameter: ...
    def get_buffer(self, target: str) -> Tensor: ...
    def get_extra_state(self) -> Any: ...
    def set_extra_state(self, state: Any) -> None: ...
    def _apply(self, fn, recurse=True): ...
    def apply(self, fn: Callable[[Module], None]) -> Self: ...
    def cuda(self, device: int | device | None = None) -> Self: ...
    def ipu(self, device: int | device | None = None) -> Self: ...
    def xpu(self, device: int | device | None = None) -> Self: ...
    def mtia(self, device: int | device | None = None) -> Self: ...
    def cpu(self) -> Self: ...
    def type(self, dst_type: dtype | str) -> Self: ...
    def float(self) -> Self: ...
    def double(self) -> Self: ...
    def half(self) -> Self: ...
    def bfloat16(self) -> Self: ...
    def to_empty(self, *, device: DeviceLikeType | None, recurse: bool = True) -> Self: ...
    @overload
    def to() -> Self: ...
    @overload
    def to(self, dtype: dtype, non_blocking: bool = ...) -> Self: ...
    @overload
    def to(self, tensor: Tensor, non_blocking: bool = ...) -> Self: ...
    def to(self, *args, **kwargs): ...
    def register_full_backward_pre_hook() -> RemovableHandle: ...
    def register_backward_hook(self, hook: Callable[[Module, _grad_t, _grad_t], None | _grad_t]) -> RemovableHandle: ...
    def register_full_backward_hook() -> RemovableHandle: ...
    def _get_backward_hooks(self): ...
    def _get_backward_pre_hooks(self): ...
    def _maybe_warn_non_full_backward_hook(self, inputs, result, grad_fn): ...
    def register_forward_pre_hook() -> RemovableHandle: ...
    def register_forward_hook() -> RemovableHandle: ...
    def _slow_forward(self, *input, **kwargs): ...
    def _wrapped_call_impl(self, *args, **kwargs): ...

    # torchrec tests the code consistency with the following code
    # fmt: off
    def _call_impl(self, *args, **kwargs):...

    # fmt: on

    __call__: Callable[..., Any] = ...

    def __getstate__(self): ...
    def __setstate__(self, state): ...

    # It is crucial that the return type is not annotated as `Any`, otherwise type checking
    # on `torch.nn.Module` and all its subclasses is largely disabled as a result. See:
    # https://github.com/pytorch/pytorch/pull/115074
    # def __getattr__(self, name: str) -> Tensor | Module: ...
    # def __setattr__(self, name: str, value: Any) -> None: ...
    def __delattr__(self, name) -> None: ...
    def _register_state_dict_hook(self, hook): ...
    def register_state_dict_post_hook(self, hook): ...
    def register_state_dict_pre_hook(self, hook): ...
    def _save_to_state_dict(self, destination, prefix, keep_vars): ...

    # The user can pass an optional arbitrary mappable object to `state_dict`, in which case `state_dict` returns
    # back that same object. But if they pass nothing, an `OrderedDict` is created and returned.
    T_destination = TypeVar("T_destination", bound=dict[str, Any])

    @overload
    def state_dict() -> T_destination: ...
    @overload
    def state_dict() -> dict[str, Any]: ...

    # TODO: Change `*args` to `*` and remove the corresponding warning in docs when BC allows.
    # Also remove the logic for arg parsing together.
    def state_dict(self, *args, destination=None, prefix="", keep_vars=False): ...
    def _register_load_state_dict_pre_hook(self, hook, with_module=False): ...
    def register_load_state_dict_pre_hook(self, hook): ...
    def register_load_state_dict_post_hook(self, hook): ...
    def _load_from_state_dict(): ...
    def load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False): ...
    def _named_members(self, get_members_fn, prefix="", recurse=True, remove_duplicate: bool = True): ...
    def parameters(self, recurse: bool = True) -> Iterator[Parameter]: ...
    def named_parameters() -> Iterator[tuple[str, Parameter]]: ...
    def buffers(self, recurse: bool = True) -> Iterator[Tensor]: ...
    def named_buffers() -> Iterator[tuple[str, Tensor]]: ...
    def children(self) -> Iterator[Module]: ...
    def named_children(self) -> Iterator[tuple[str, Module]]: ...
    def modules(self) -> Iterator[Module]: ...
    def named_modules(): ...
    def train(self, mode: bool = True) -> Self: ...
    def eval(self) -> Self: ...
    def requires_grad_(self, requires_grad: bool = True) -> Self: ...
    def zero_grad(self, set_to_none: bool = True) -> None: ...
    def share_memory(self) -> Self: ...
    def _get_name(self): ...
    def extra_repr(self) -> str: ...
    def __dir__(self): ...
    def _replicate_for_data_parallel(self): ...
    def compile(self, *args, **kwargs): ...
