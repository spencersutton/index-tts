# pyright: reportAny=false, reportExplicitAny=false, reportUnknownParameterType=false, reportMissingParameterType=false
"""
This type stub file was generated by pyright.
"""

import functools
import typing
from typing import Any

import torch
import torch.nn as nn
from numpy import NDArray, float64
from torch import Tensor

class MultiScaleMelSpectrogramLoss(nn.Module):
    """Compute distance between mel spectrograms. Can be used
    in a multi-scale way.

    Parameters
    ----------
    n_mels : List[int]
        Number of mels per STFT, by default [5, 10, 20, 40, 80, 160, 320],
    window_lengths : List[int], optional
        Length of each window of each STFT, by default [32, 64, 128, 256, 512, 1024, 2048]
    loss_fn : typing.Callable, optional
        How to compare each loss, by default nn.L1Loss()
    clamp_eps : float, optional
        Clamp on the log magnitude, below, by default 1e-5
    mag_weight : float, optional
        Weight of raw magnitude portion of loss, by default 0.0 (no ampliciation on mag part)
    log_weight : float, optional
        Weight of log magnitude portion of loss, by default 1.0
    pow : float, optional
        Power to raise magnitude to before taking log, by default 1.0
    weight : float, optional
        Weight of this loss, by default 1.0
    match_stride : bool, optional
        Whether to match the stride of convolutional layers, by default False

    Implementation copied from: https://github.com/descriptinc/lyrebird-audiotools/blob/961786aa1a9d628cca0c0486e5885a457fe70c1a/audiotools/metrics/spectral.py
    Additional code copied and modified from https://github.com/descriptinc/audiotools/blob/master/audiotools/core/audio_signal.py
    """
    def __init__(
        self,
        sampling_rate: int,
        n_mels: list[int] = ...,
        window_lengths: list[int] = ...,
        loss_fn: typing.Callable = ...,
        clamp_eps: float = ...,
        mag_weight: float = ...,
        log_weight: float = ...,
        pow: float = ...,
        weight: float = ...,
        match_stride: bool = ...,
        mel_fmin: list[float] = ...,
        mel_fmax: list[float] = ...,
        window_type: str = ...,
    ) -> None: ...
    @staticmethod
    @functools.lru_cache(None)
    def get_window(window_type, window_length) -> NDArray[float64] | Any: ...
    @staticmethod
    @functools.lru_cache(None)
    def get_mel_filters(sr, n_fft, n_mels, fmin, fmax) -> NDArray[Any, Any]: ...
    def mel_spectrogram(self, wav, n_mels, fmin, fmax, window_length, hop_length, match_stride, window_type) -> Tensor:
        """
        Mirrors AudioSignal.mel_spectrogram used by BigVGAN-v2 training from:
        https://github.com/descriptinc/audiotools/blob/master/audiotools/core/audio_signal.py
        """

    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        """Computes mel loss between an estimate and a reference
        signal.

        Parameters
        ----------
        x : torch.Tensor
            Estimate signal
        y : torch.Tensor
            Reference signal

        Returns
        -------
        torch.Tensor
            Mel loss.
        """

def feature_loss(fmap_r: list[list[torch.Tensor]], fmap_g: list[list[torch.Tensor]]) -> torch.Tensor: ...
def discriminator_loss(
    disc_real_outputs: list[torch.Tensor], disc_generated_outputs: list[torch.Tensor]
) -> tuple[torch.Tensor, list[torch.Tensor], list[torch.Tensor]]: ...
def generator_loss(disc_outputs: list[torch.Tensor]) -> tuple[torch.Tensor, list[torch.Tensor]]: ...
