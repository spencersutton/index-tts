import torch
from audiotools.ml import BaseModel
from torch import nn

from .base import CodecMixin

def init_weights(m):  # -> None:
    ...

class ResidualUnit(nn.Module):
    def __init__(self, dim: int = ..., dilation: int = ...) -> None: ...
    def forward(self, x): ...

class EncoderBlock(nn.Module):
    def __init__(self, dim: int = ..., stride: int = ...) -> None: ...
    def forward(self, x):  # -> Any:
        ...

class Encoder(nn.Module):
    def __init__(self, d_model: int = ..., strides: list = ..., d_latent: int = ...) -> None: ...
    def forward(self, x):  # -> Any:
        ...

class DecoderBlock(nn.Module):
    def __init__(self, input_dim: int = ..., output_dim: int = ..., stride: int = ...) -> None: ...
    def forward(self, x):  # -> Any:
        ...

class Decoder(nn.Module):
    def __init__(self, input_channel, channels, rates, d_out: int = ...) -> None: ...
    def forward(self, x):  # -> Any:
        ...

class DAC(BaseModel, CodecMixin):
    def __init__(
        self,
        encoder_dim: int = ...,
        encoder_rates: list[int] = ...,
        latent_dim: int = ...,
        decoder_dim: int = ...,
        decoder_rates: list[int] = ...,
        n_codebooks: int = ...,
        codebook_size: int = ...,
        codebook_dim: int | list = ...,
        quantizer_dropout: bool = ...,
        sample_rate: int = ...,
    ) -> None: ...
    def preprocess(self, audio_data, sample_rate):  # -> Tensor:
        ...
    def encode(self, audio_data: torch.Tensor, n_quantizers: int = ...):  # -> tuple[Any, Any, Any, Any, Any]:
        ...
    def decode(self, z: torch.Tensor):  # -> Any:
        ...
    def forward(
        self,
        audio_data: torch.Tensor,
        sample_rate: int = ...,
        n_quantizers: int = ...,
    ):  # -> dict[str, Any]:
        ...

if __name__ == "__main__":
    model = ...
    length = ...
    x = ...
    out = ...
    grad = ...
    gradmap = ...
    gradmap = ...
    rf = ...
    x = ...
