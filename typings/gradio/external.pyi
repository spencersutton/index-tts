"""
This type stub file was generated by pyright.
"""

from collections.abc import Callable
from typing import Literal, TYPE_CHECKING
from gradio_client.documentation import document
from gradio.components.multimodal_textbox import MultimodalValue
from huggingface_hub.inference._providers import PROVIDER_T
from gradio.blocks import Blocks
from gradio.chat_interface import ChatInterface
from gradio.components.chatbot import NormalizedMessageDict
from gradio.components.login_button import LoginButton
from gradio.interface import Interface

"""This module should not be used directly as its API is subject to change. Instead,
use the `gr.Blocks.load()` or `gr.load()` functions."""
if TYPE_CHECKING: ...

@document()
def load(
    name: str,
    src: Callable[[str, str | None], Blocks] | Literal["models", "spaces", "huggingface"] | None = ...,
    token: str | None = ...,
    accept_token: bool | LoginButton = ...,
    provider: PROVIDER_T | None = ...,
    **kwargs,
) -> Blocks:
    """
    Constructs a Gradio app automatically from a Hugging Face model/Space repo name or a 3rd-party API provider. Note that if a Space repo is loaded, certain high-level attributes of the Blocks (e.g. custom `css`, `js`, and `head` attributes) will not be loaded.
    Parameters:
        name: the name of the model (e.g. "google/vit-base-patch16-224") or Space (e.g. "flax-community/spanish-gpt2"). This is the first parameter passed into the `src` function. Can also be formatted as {src}/{repo name} (e.g. "models/google/vit-base-patch16-224") if `src` is not provided.
        src: function that accepts a string model `name` and a string or None `token` and returns a Gradio app. Alternatively, this parameter takes one of two strings for convenience: "models" (for loading a Hugging Face model through the Inference API) or "spaces" (for loading a Hugging Face Space). If None, uses the prefix of the `name` parameter to determine `src`.
        token: optional token that is passed as the second parameter to the `src` function. If not explicitly provided, will use the HF_TOKEN environment variable or fallback to the locally-saved HF token when loading models but not Spaces (when loading Spaces, only provide a token if you are loading a trusted private Space as the token can be read by the Space you are loading). Find your HF tokens here: https://huggingface.co/settings/tokens.
        accept_token: if True, a Textbox component is first rendered to allow the user to provide a token, which will be used instead of the `token` parameter when calling the loaded model or Space. Can also provide an instance of a gr.LoginButton in the same Blocks scope, which allows the user to login with a Hugging Face account whose token will be used instead of the `token` parameter when calling the loaded model or Space.
        kwargs: additional keyword parameters to pass into the `src` function. If `src` is "models" or "Spaces", these parameters are passed into the `gr.Interface` or `gr.ChatInterface` constructor.
        provider: the name of the third-party (non-Hugging Face) providers to use for model inference (e.g. "replicate", "sambanova", "fal-ai", etc). Should be one of the providers supported by `huggingface_hub.InferenceClient`. This parameter is only used when `src` is "models"
    Returns:
        a Gradio Blocks app for the given model
    Example:
        import gradio as gr
        demo = gr.load("gradio/question-answering", src="spaces")
        demo.launch()
    """
    ...

def load_blocks_from_huggingface(
    name: str, src: str, token: str | None = ..., alias: str | None = ..., provider: PROVIDER_T | None = ..., **kwargs
) -> Blocks:
    """Creates and returns a Blocks instance from a Hugging Face model or Space repo."""
    ...

def from_model(
    model_name: str, token: str | None, alias: str | None, provider: PROVIDER_T | None = ..., **kwargs
) -> Blocks: ...
def from_spaces(
    space_name: str, token: str | None, alias: str | None, provider: PROVIDER_T | None = ..., **kwargs
) -> Blocks: ...
def make_event_data_fn(client, endpoint):  # -> Callable[..., Any]:
    """Create a function that accepts EventData.
    The event_data_fn has to be created in this closure so that the value of endpoint
    is correctly captured."""
    ...

def from_spaces_blocks(space: str, token: str | None) -> Blocks: ...
def from_spaces_interface(
    model_name: str, config: dict, alias: str | None, token: str | None, iframe_url: str, **kwargs
) -> Interface: ...

TEXT_FILE_EXTENSIONS = ...
IMAGE_FILE_EXTENSIONS = ...

def format_conversation(history: list[NormalizedMessageDict], new_message: str | MultimodalValue) -> list[dict]: ...
@document()
def load_chat(
    base_url: str,
    model: str,
    token: str | None = ...,
    *,
    file_types: Literal["text_encoded", "image"] | list[Literal["text_encoded", "image"]] | None = ...,
    system_message: str | None = ...,
    streaming: bool = ...,
    **kwargs,
) -> ChatInterface:
    """
    Load a chat interface from an OpenAI API chat compatible endpoint.
    Parameters:
        base_url: The base URL of the endpoint, e.g. "http://localhost:11434/v1/"
        model: The name of the model you are loading, e.g. "llama3.2"
        token: The API token or a placeholder string if you are using a local model, e.g. "ollama"
        file_types: The file types allowed to be uploaded by the user. "text_encoded" allows uploading any text-encoded file (which is simply appended to the prompt), and "image" adds image upload support. Set to None to disable file uploads.
        system_message: The system message to use for the conversation, if any.
        streaming: Whether the response should be streamed.
        kwargs: Additional keyword arguments to pass into ChatInterface for customization.
    Example:
        import gradio as gr
        gr.load_chat(
            "http://localhost:11434/v1/",
            model="qwen2.5",
            token="***",
            file_types=["text_encoded", "image"],
            system_message="You are a silly assistant.",
        ).launch()
    """
    ...

@document()
def load_openapi(
    openapi_spec: str | dict,
    base_url: str,
    *,
    paths: list[str] | None = ...,
    exclude_paths: list[str] | None = ...,
    methods: list[Literal["get", "post", "put", "patch", "delete"]] | None = ...,
    auth_token: str | None = ...,
    **interface_kwargs,
) -> Blocks:
    """
    Load a Gradio app from an OpenAPI v3 specification.

    Parameters:
        openapi_spec: URL, file path, or dictionary containing the OpenAPI specification (v3, JSON format only)
        base_url: Base URL for the API endpoints, e.g. "https://api.example.com/v1". This is used to construct the full URL for each endpoint.
        paths: Optional list of specific API paths to create Gradio endpoints from. Supports regex patterns, e.g. ["/api/v1/books", ".*user.*"]. If None, all paths in the OpenAPI spec will be included.
        exclude_paths: Optional list of API paths to exclude from the Gradio endpoints. Supports regex patterns and takes precedence over `paths`. For example, [".*internal.*"] will exclude all paths containing "internal".
        methods: Optional list of HTTP methods to include in the Gradio endpoints. If None, all methods will be included.
        auth_token: Optional authentication token to be sent as a Bearer token in the Authorization header for all API requests.
        interface_kwargs: Additional keyword arguments to pass to each generated gr.Interface instance (e.g., title, description, article, examples_per_page, etc.)
    Returns:
        A Gradio Blocks app with endpoints generated from the OpenAPI spec
    """
    ...
