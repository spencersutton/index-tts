from dataclasses import dataclass
from datetime import datetime
from enum import StrEnum

from .hf_api import HfApi
from .inference._client import InferenceClient
from .inference._generated._async_client import AsyncInferenceClient

logger = ...

class InferenceEndpointStatus(StrEnum):
    PENDING = ...
    INITIALIZING = ...
    UPDATING = ...
    UPDATE_FAILED = ...
    RUNNING = ...
    PAUSED = ...
    FAILED = ...
    SCALED_TO_ZERO = ...

class InferenceEndpointType(StrEnum):
    PUBlIC = ...
    PROTECTED = ...
    PRIVATE = ...

@dataclass
class InferenceEndpoint:
    name: str = ...
    namespace: str
    repository: str = ...
    status: InferenceEndpointStatus = ...
    health_route: str = ...
    url: str | None = ...
    framework: str = ...
    revision: str = ...
    task: str = ...
    created_at: datetime = ...
    updated_at: datetime = ...
    type: InferenceEndpointType = ...
    raw: dict = ...
    _token: str | bool | None = ...
    _api: HfApi = ...
    @classmethod
    def from_raw(
        cls, raw: dict, namespace: str, token: str | bool | None = ..., api: HfApi | None = ...
    ) -> InferenceEndpoint: ...
    def __post_init__(self) -> None: ...
    @property
    def client(self) -> InferenceClient: ...
    @property
    def async_client(self) -> AsyncInferenceClient: ...
    def wait(self, timeout: int | None = ..., refresh_every: int = ...) -> InferenceEndpoint: ...
    def fetch(self) -> InferenceEndpoint: ...
    def update(
        self,
        *,
        accelerator: str | None = ...,
        instance_size: str | None = ...,
        instance_type: str | None = ...,
        min_replica: int | None = ...,
        max_replica: int | None = ...,
        scale_to_zero_timeout: int | None = ...,
        repository: str | None = ...,
        framework: str | None = ...,
        revision: str | None = ...,
        task: str | None = ...,
        custom_image: dict | None = ...,
        secrets: dict[str, str] | None = ...,
    ) -> InferenceEndpoint: ...
    def pause(self) -> InferenceEndpoint: ...
    def resume(self, running_ok: bool = ...) -> InferenceEndpoint: ...
    def scale_to_zero(self) -> InferenceEndpoint: ...
    def delete(self) -> None: ...
