from typing import Any, Literal

from .base import BaseInferenceType, dataclass_with_extra

type TypeEnum = Literal["json", "regex", "json_schema"]

@dataclass_with_extra
class TextGenerationInputGrammarType(BaseInferenceType):
    type: TypeEnum
    value: Any

@dataclass_with_extra
class TextGenerationInputGenerateParameters(BaseInferenceType):
    adapter_id: str | None = ...
    best_of: int | None = ...
    decoder_input_details: bool | None = ...
    details: bool | None = ...
    do_sample: bool | None = ...
    frequency_penalty: float | None = ...
    grammar: TextGenerationInputGrammarType | None = ...
    max_new_tokens: int | None = ...
    repetition_penalty: float | None = ...
    return_full_text: bool | None = ...
    seed: int | None = ...
    stop: list[str] | None = ...
    temperature: float | None = ...
    top_k: int | None = ...
    top_n_tokens: int | None = ...
    top_p: float | None = ...
    truncate: int | None = ...
    typical_p: float | None = ...
    watermark: bool | None = ...

@dataclass_with_extra
class TextGenerationInput(BaseInferenceType):
    inputs: str
    parameters: TextGenerationInputGenerateParameters | None = ...
    stream: bool | None = ...

type TextGenerationOutputFinishReason = Literal["length", "eos_token", "stop_sequence"]

@dataclass_with_extra
class TextGenerationOutputPrefillToken(BaseInferenceType):
    id: int
    logprob: float
    text: str

@dataclass_with_extra
class TextGenerationOutputToken(BaseInferenceType):
    id: int
    logprob: float
    special: bool
    text: str

@dataclass_with_extra
class TextGenerationOutputBestOfSequence(BaseInferenceType):
    finish_reason: TextGenerationOutputFinishReason
    generated_text: str
    generated_tokens: int
    prefill: list[TextGenerationOutputPrefillToken]
    tokens: list[TextGenerationOutputToken]
    seed: int | None = ...
    top_tokens: list[list[TextGenerationOutputToken]] | None = ...

@dataclass_with_extra
class TextGenerationOutputDetails(BaseInferenceType):
    finish_reason: TextGenerationOutputFinishReason
    generated_tokens: int
    prefill: list[TextGenerationOutputPrefillToken]
    tokens: list[TextGenerationOutputToken]
    best_of_sequences: list[TextGenerationOutputBestOfSequence] | None = ...
    seed: int | None = ...
    top_tokens: list[list[TextGenerationOutputToken]] | None = ...

@dataclass_with_extra
class TextGenerationOutput(BaseInferenceType):
    generated_text: str
    details: TextGenerationOutputDetails | None = ...

@dataclass_with_extra
class TextGenerationStreamOutputStreamDetails(BaseInferenceType):
    finish_reason: TextGenerationOutputFinishReason
    generated_tokens: int
    input_length: int
    seed: int | None = ...

@dataclass_with_extra
class TextGenerationStreamOutputToken(BaseInferenceType):
    id: int
    logprob: float
    special: bool
    text: str

@dataclass_with_extra
class TextGenerationStreamOutput(BaseInferenceType):
    index: int
    token: TextGenerationStreamOutputToken
    details: TextGenerationStreamOutputStreamDetails | None = ...
    generated_text: str | None = ...
    top_tokens: list[TextGenerationStreamOutputToken] | None = ...
