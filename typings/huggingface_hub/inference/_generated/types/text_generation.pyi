"""
This type stub file was generated by pyright.
"""

from typing import Any, List, Literal, Optional

from .base import BaseInferenceType, dataclass_with_extra

TypeEnum = Literal["json", "regex", "json_schema"]

@dataclass_with_extra
class TextGenerationInputGrammarType(BaseInferenceType):
    type: TypeEnum
    value: Any
    ...

@dataclass_with_extra
class TextGenerationInputGenerateParameters(BaseInferenceType):
    adapter_id: str | None = ...
    best_of: int | None = ...
    decoder_input_details: bool | None = ...
    details: bool | None = ...
    do_sample: bool | None = ...
    frequency_penalty: float | None = ...
    grammar: TextGenerationInputGrammarType | None = ...
    max_new_tokens: int | None = ...
    repetition_penalty: float | None = ...
    return_full_text: bool | None = ...
    seed: int | None = ...
    stop: list[str] | None = ...
    temperature: float | None = ...
    top_k: int | None = ...
    top_n_tokens: int | None = ...
    top_p: float | None = ...
    truncate: int | None = ...
    typical_p: float | None = ...
    watermark: bool | None = ...

@dataclass_with_extra
class TextGenerationInput(BaseInferenceType):
    """Text Generation Input.
    Auto-generated from TGI specs.
    For more details, check out
    https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/scripts/inference-tgi-import.ts.
    """

    inputs: str
    parameters: TextGenerationInputGenerateParameters | None = ...
    stream: bool | None = ...

TextGenerationOutputFinishReason = Literal["length", "eos_token", "stop_sequence"]

@dataclass_with_extra
class TextGenerationOutputPrefillToken(BaseInferenceType):
    id: int
    logprob: float
    text: str
    ...

@dataclass_with_extra
class TextGenerationOutputToken(BaseInferenceType):
    id: int
    logprob: float
    special: bool
    text: str
    ...

@dataclass_with_extra
class TextGenerationOutputBestOfSequence(BaseInferenceType):
    finish_reason: TextGenerationOutputFinishReason
    generated_text: str
    generated_tokens: int
    prefill: list[TextGenerationOutputPrefillToken]
    tokens: list[TextGenerationOutputToken]
    seed: int | None = ...
    top_tokens: list[list[TextGenerationOutputToken]] | None = ...

@dataclass_with_extra
class TextGenerationOutputDetails(BaseInferenceType):
    finish_reason: TextGenerationOutputFinishReason
    generated_tokens: int
    prefill: list[TextGenerationOutputPrefillToken]
    tokens: list[TextGenerationOutputToken]
    best_of_sequences: list[TextGenerationOutputBestOfSequence] | None = ...
    seed: int | None = ...
    top_tokens: list[list[TextGenerationOutputToken]] | None = ...

@dataclass_with_extra
class TextGenerationOutput(BaseInferenceType):
    """Text Generation Output.
    Auto-generated from TGI specs.
    For more details, check out
    https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/scripts/inference-tgi-import.ts.
    """

    generated_text: str
    details: TextGenerationOutputDetails | None = ...

@dataclass_with_extra
class TextGenerationStreamOutputStreamDetails(BaseInferenceType):
    finish_reason: TextGenerationOutputFinishReason
    generated_tokens: int
    input_length: int
    seed: int | None = ...

@dataclass_with_extra
class TextGenerationStreamOutputToken(BaseInferenceType):
    id: int
    logprob: float
    special: bool
    text: str
    ...

@dataclass_with_extra
class TextGenerationStreamOutput(BaseInferenceType):
    """Text Generation Stream Output.
    Auto-generated from TGI specs.
    For more details, check out
    https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/scripts/inference-tgi-import.ts.
    """

    index: int
    token: TextGenerationStreamOutputToken
    details: TextGenerationStreamOutputStreamDetails | None = ...
    generated_text: str | None = ...
    top_tokens: list[TextGenerationStreamOutputToken] | None = ...
