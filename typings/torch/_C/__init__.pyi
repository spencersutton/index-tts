from builtins import bool as _bool, complex as _complex, float as _float, int as _int, str as _str
from collections.abc import Callable, Iterable, Iterator, Sequence
from enum import Enum
from pathlib import Path
from types import EllipsisType
from typing import IO, Any, Literal, ParamSpec, Protocol, Self, SupportsIndex, TypeVar, overload, runtime_checkable

import numpy as np
import torch
from torch import SymInt, Tensor
from torch._prims_common import DeviceLikeType
from torch.autograd.graph import Node as _Node
from torch.cuda import _POOL_HANDLE
from torch.fx.node import Node as FxNode
from torch.storage import TypedStorage, UntypedStorage
from torch.types import (
    IntLikeType,
    Number,
    Storage,
    _device,
    _dispatchkey,
    _dtype,
    _layout,
    _qscheme,
    _size,
    _symsize,
)

K = TypeVar("K")
T = TypeVar("T")
S = TypeVar("S", bound=torch.Tensor)
P = ParamSpec("P")
R = TypeVar("R", covariant=True)
T_co = TypeVar("T_co", covariant=True)

@runtime_checkable
class _NestedSequence(Protocol[T_co]):
    def __len__(self, /) -> _int: ...
    def __getitem__(self, index: _int, /) -> T_co | _NestedSequence[T_co]: ...
    def __contains__(self, x: object, /) -> _bool: ...
    def __iter__(self, /) -> Iterator[T_co | _NestedSequence[T_co]]: ...
    def __reversed__(self, /) -> Iterator[T_co | _NestedSequence[T_co]]: ...
    def count(self, value: Any, /) -> _int: ...
    def index(self, value: Any, /) -> _int: ...

class device:
    type: str
    index: _int
    def __get__(self, instance, owner=...) -> device: ...
    @overload
    def __init__(self, device: DeviceLikeType) -> None: ...
    @overload
    def __init__(self, type: str, index: _int) -> None: ...
    def __enter__(self) -> Self: ...
    def __exit__(self, exc_type, exc_val, exc_tb) -> None: ...
    def __reduce__(self) -> tuple[Any, ...]: ...

class Stream:
    stream_id: _int
    device_index: _int
    device_type: _int
    device: _device
    @overload
    def __new__(cls, device: DeviceLikeType | None = ..., *, priority: _int = ...) -> Self: ...
    @overload
    def __new__(cls, stream_id: _int, device_index: _int, device_type: _int, *, priority: _int = ...) -> Self: ...
    def query(self) -> _bool: ...
    def synchronize(self) -> None: ...
    def wait_event(self, event: Event) -> None: ...
    def wait_stream(self, other: Stream) -> None: ...
    def record_event(self, event: Event | None = ...) -> Event: ...
    def __hash__(self) -> _int: ...
    def __eq__(self, other: object) -> _bool: ...
    def __enter__(self) -> Self: ...
    def __exit__(self, exc_type, exc_val, exc_tb) -> None: ...

class Event:
    device: _device
    event_id: _int
    def __new__(
        cls,
        device: DeviceLikeType | None = ...,
        *,
        enable_timing: _bool = ...,
        blocking: _bool = ...,
        interprocess: _bool = ...,
    ) -> Self: ...
    @classmethod
    def from_ipc_handle(cls, device: _device, ipc_handle: bytes) -> Event: ...
    def record(self, stream: Stream | None = ...) -> None: ...
    def wait(self, stream: Stream | None = ...) -> None: ...
    def query(self) -> _bool: ...
    def elapsed_time(self, other: Event) -> _float: ...
    def synchronize(self) -> None: ...
    def ipc_handle(self) -> bytes: ...

class Size(tuple[_int, ...]):
    @overload
    def __getitem__(self: Size, key: SupportsIndex, /) -> _int: ...
    @overload
    def __getitem__(self: Size, key: slice, /) -> Size: ...
    def __add__(self, other: tuple[_int, ...], /) -> Size: ...
    def __radd__(self: Size, other: tuple[_int, ...], /) -> Size: ...
    def __mul__(self, other: SupportsIndex, /) -> Size: ...
    def __rmul__(self, other: SupportsIndex, /) -> Size: ...
    def numel(self: Size, /) -> _int: ...

class dtype:
    is_floating_point: _bool
    is_complex: _bool
    is_signed: _bool
    itemsize: _int
    def to_real(self) -> dtype: ...
    def to_complex(self) -> dtype: ...

class iinfo:
    bits: _int
    min: _int
    max: _int
    dtype: str
    def __init__(self, dtype: _dtype) -> None: ...

class finfo:
    bits: _int
    min: _float
    max: _float
    eps: _float
    tiny: _float
    smallest_normal: _float
    resolution: _float
    dtype: str
    @overload
    def __init__(self, dtype: _dtype) -> None: ...
    @overload
    def __init__(self) -> None: ...

float32: dtype = ...
float: dtype = ...
float64: dtype = ...
double: dtype = ...
float16: dtype = ...
bfloat16: dtype = ...
float8_e4m3fn: dtype = ...
float8_e4m3fnuz: dtype = ...
float8_e5m2: dtype = ...
float8_e5m2fnuz: dtype = ...
float8_e8m0fnu: dtype = ...
float4_e2m1fn_x2: dtype = ...
half: dtype = ...
uint8: dtype = ...
uint16: dtype = ...
uint32: dtype = ...
uint64: dtype = ...
int8: dtype = ...
int16: dtype = ...
short: dtype = ...
int32: dtype = ...
int: dtype = ...
int64: dtype = ...
long: dtype = ...
complex32: dtype = ...
complex64: dtype = ...
chalf: dtype = ...
cfloat: dtype = ...
complex128: dtype = ...
cdouble: dtype = ...
quint8: dtype = ...
qint8: dtype = ...
qint32: dtype = ...
bool: dtype = ...
quint4x2: dtype = ...
quint2x4: dtype = ...
bits1x8: dtype = ...
bits2x4: dtype = ...
bits4x2: dtype = ...
bits8: dtype = ...
bits16: dtype = ...

class layout: ...

def DisableTorchFunction(): ...
def DisableTorchFunctionSubclass(): ...

strided: layout = ...
sparse_coo: layout = ...
sparse_csr: layout = ...
sparse_csc: layout = ...
sparse_bsr: layout = ...
sparse_bsc: layout = ...
_mkldnn: layout = ...
jagged: layout = ...

class memory_format: ...

contiguous_format: memory_format = ...
channels_last: memory_format = ...
channels_last_3d: memory_format = ...
preserve_format: memory_format = ...

class qscheme: ...

per_tensor_affine: qscheme = ...
per_channel_affine: qscheme = ...
per_tensor_symmetric: qscheme = ...
per_channel_symmetric: qscheme = ...
per_channel_affine_float_qparams: qscheme = ...

class _FunctionBase:
    saved_tensors: tuple[Tensor]
    _raw_saved_tensors: tuple[Any]
    next_functions: tuple[tuple[Any, _int], ...]
    needs_input_grad: tuple[_bool]
    metadata: dict
    _materialize_non_diff_grads: _bool

class _LegacyVariableBase(Tensor):
    def __init__(
        self,
        data: Tensor | None = ...,
        requires_grad: _bool | None = ...,
        volatile: _bool | None = ...,
        _grad_fn: _FunctionBase | None = ...,
    ) -> None: ...

class IODescriptor: ...
class JITException(Exception): ...

class Future[T]:
    def __init__(self, devices: list[device]) -> None: ...
    def done(self) -> _bool: ...
    def value(self) -> T: ...
    def wait(self) -> T: ...
    def add_done_callback(self, callback: Callable) -> None: ...
    def then(self, callback: Callable) -> Future[T]: ...
    def set_result(self, result: T) -> None: ...

class _Await:
    def __init__(self) -> None: ...
    def fn(self) -> Callable: ...
    def args(self) -> tuple[Any, ...]: ...
    def is_nowait(self) -> _bool: ...

class _MobileOptimizerType: ...

CONV_BN_FUSION: _MobileOptimizerType
INSERT_FOLD_PREPACK_OPS: _MobileOptimizerType
REMOVE_DROPOUT: _MobileOptimizerType
FUSE_ADD_RELU: _MobileOptimizerType
HOIST_CONV_PACKED_PARAMS: _MobileOptimizerType
VULKAN_AUTOMATIC_GPU_TRANSFER: _MobileOptimizerType

def fork(*args: Any, **kwargs: Any) -> Future: ...
def wait(fut: Future) -> Any: ...
def unify_type_list(types: list[JitType]) -> JitType: ...

type ResolutionCallback = Callable[[str], Callable[..., Any]]

def parse_type_comment(comment: str) -> Decl: ...
def merge_type_from_type_comment(decl: Decl, type_annotation_decl: Decl, is_method: _bool) -> Decl: ...
def parse_ir(input: str, parse_tensor_constants: _bool = ...) -> Graph: ...
def parse_schema(schema: str) -> FunctionSchema: ...
def get_device(input: Tensor) -> _int: ...
def import_ir_module(
    cu: CompilationUnit, filename: str | Path, map_location: DeviceLikeType | None, extra_files: dict[str, Any]
) -> ScriptModule: ...
def import_ir_module_from_buffer(
    cu: CompilationUnit, buffer: IO[bytes], map_location: DeviceLikeType | None, extra_files: dict[str, Any]
) -> ScriptModule: ...

class GraphExecutorState: ...
class AliasDb: ...

class _InsertPoint:
    def __enter__(self) -> None: ...
    def __exit__(self, *exc_info: object) -> None: ...

class Use:
    @property
    def user(self) -> Node: ...
    @property
    def offset(self) -> _int: ...
    def isAfter(self, other: Use) -> _bool: ...

class Value:
    def type(self) -> JitType: ...
    def setType(self, t: JitType) -> Value: ...
    def setTypeAs(self, other: Value) -> Value: ...
    def inferTypeFrom(self, t: Tensor) -> None: ...
    def debugName(self) -> str: ...
    def setDebugName(self, name: str) -> None: ...
    def unique(self) -> _int: ...
    def offset(self) -> _int: ...
    def node(self) -> Node: ...
    def uses(self) -> list[Use]: ...
    def replaceAllUsesWith(self, val: Value) -> None: ...
    def replaceAllUsesAfterNodeWith(self, node: Node, val: Value) -> None: ...
    def requires_grad(self) -> _bool: ...
    def requiresGrad(self) -> _bool: ...
    def copyMetadata(self, other: Value) -> Value: ...
    def isCompleteTensor(self) -> _bool: ...
    def toIValue(self) -> IValue: ...

class Block:
    def inputs(self) -> Iterator[Value]: ...
    def outputs(self) -> Iterator[Value]: ...
    def nodes(self) -> Iterator[Node]: ...
    def paramNode(self) -> Node: ...
    def returnNode(self) -> Node: ...
    def owningNode(self) -> Node: ...
    def registerOutput(self, n: Value) -> _int: ...
    def addNode(self, name: str, inputs: Sequence[Value]) -> Node: ...

class Node:
    def __getitem__(self, key: str) -> Any: ...
    def schema(self) -> str: ...
    def input(self) -> Value: ...
    def inputs(self) -> Iterator[Value]: ...
    def inputsAt(self, idx: _int) -> Value: ...
    def inputsSize(self) -> _int: ...
    def output(self) -> Value: ...
    def outputs(self) -> Iterator[Value]: ...
    def outputsAt(self, idx: _int) -> Value: ...
    def outputsSize(self) -> _int: ...
    def hasMultipleOutputs(self) -> _bool: ...
    def blocks(self) -> list[Block]: ...
    def addBlock(self) -> Block: ...
    def mustBeNone(self) -> _bool: ...
    def matches(self, pattern: str) -> _bool: ...
    def kind(self) -> str: ...
    def kindOf(self, name: str) -> str: ...
    def addInput(self, name: str) -> Value: ...
    def replaceInput(self, i: _int, newValue: Value) -> Value: ...
    def replaceInputWith(self, from_: Value, to: Value) -> None: ...
    def replaceAllUsesWith(self, n: Node) -> None: ...
    def insertBefore(self, n: Node) -> Node: ...
    def insertAfter(self, n: Node) -> Node: ...
    def isBefore(self, n: Node) -> _bool: ...
    def isAfter(self, n: Node) -> _bool: ...
    def moveBefore(self, n: Node) -> None: ...
    def moveAfter(self, n: Node) -> None: ...
    def removeInput(self, i: _int) -> None: ...
    def removeAllInputs(self, i: _int) -> None: ...
    def hasUses(self) -> _bool: ...
    def eraseOutput(self, i: _int) -> None: ...
    def addOutput(self) -> Value: ...
    def scopeName(self) -> str: ...
    def isNondeterministic(self) -> _bool: ...
    def copyAttributes(self, rhs: Node) -> Node: ...
    def copyMetadata(self, rhs: Node) -> Node: ...
    def hasAttributes(self) -> _bool: ...
    def hasAttribute(self, name: str) -> _bool: ...
    def removeAttribute(self, attr: str) -> Node: ...
    def namedInput(self, name: str) -> Value: ...
    def sourceRange(self) -> SourceRange: ...
    def owningBlock(self) -> Block: ...
    def findNode(self, kind: str, recurse: _bool = ...) -> Node: ...
    def findAllNodes(self, kind: str, recurse: _bool = ...) -> list[Node]: ...
    def getModuleHierarchy(self) -> str: ...
    def prev(self) -> Node: ...
    def destroy(self) -> None: ...
    def attributeNames(self) -> list[str]: ...
    def f(self, name: str) -> _float: ...
    def f_(self, name: str, val: _float) -> Node: ...
    def fs(self, name: str) -> list[_float]: ...
    def fs_(self, name: str, val: list[_float]) -> Node: ...
    def c(self, name: str) -> complex: ...
    def c_(self, name: str, val: complex) -> Node: ...
    def s(self, name: str) -> str: ...
    def s_(self, name: str, val: str) -> Node: ...
    def ss(self, name: str) -> list[str]: ...
    def ss_(self, name: str, val: list[str]) -> Node: ...
    def i(self, name: str) -> _int: ...
    def i_(self, name: str, val: _int) -> Node: ...
    def g(self, name: str) -> Graph: ...
    def g_(self, name: str, val: Graph) -> Node: ...
    def gs(self, name: str) -> list[Graph]: ...
    def gs_(self, name: str, val: list[Graph]) -> Node: ...
    def ival(self, name: str) -> IValue: ...
    def ival_(self, name: str, val: IValue) -> Node: ...
    def t(self, name: str) -> Tensor: ...
    def t_(self, name: str, val: Tensor) -> Node: ...
    def ts(self, name: str) -> list[Tensor]: ...
    def ts_(self, name: str, val: list[Tensor]) -> Node: ...
    def ty(self, name: str) -> JitType: ...
    def ty_(self, name: str, val: JitType) -> Node: ...
    def tys(self, name: str) -> list[JitType]: ...
    def tys_(self, name: str, val: list[JitType]) -> Node: ...

class Graph:
    def inputs(self) -> Iterator[Value]: ...
    def outputs(self) -> Iterator[Value]: ...
    def nodes(self) -> Iterator[Node]: ...
    def param_node(self) -> Node: ...
    def return_node(self) -> Node: ...
    def addInput(self, name: str = ...) -> Value: ...
    def eraseInput(self, i: _int) -> None: ...
    def registerOutput(self, n: Value) -> _int: ...
    def eraseOutput(self, i: _int) -> None: ...
    def create(self, name: str, args, num_outputs: _int) -> Node: ...
    def appendNode(self, n: Node) -> Node: ...
    def prependNode(self, n: Node) -> Node: ...
    def insertNode(self, n: Node) -> Node: ...
    def block(self) -> Block: ...
    def lint(self) -> None: ...
    def alias_db(self) -> AliasDb: ...
    def setInsertPoint(self, n: Block | Node) -> None: ...
    def insert_point_guard(self, n: Block | Node) -> _InsertPoint: ...
    def insertPoint(self) -> Node: ...
    def insertGraph(self, callee: Graph, inputs: list[Value]) -> list[Value]: ...
    def makeMultiOutputIntoTuple(self) -> None: ...
    def copy(self) -> Graph: ...

class AliasInfo:
    is_write: _bool
    before_set: set[str]
    after_set: set[str]
    def __init__(self, is_write: _bool, before_set: set[str], after_set: set[str]) -> None: ...

class Argument:
    name: str
    type: JitType
    default_value: Any | None
    def has_default_value(self) -> _bool: ...

    kwarg_only: _bool
    is_out: _bool
    alias_info: AliasInfo | None
    is_write: _bool
    real_type: JitType
    def __init__(
        self,
        name: str,
        type: JitType,
        N: _int | None,
        defualt_value: Any | None,
        kwarg_only: _bool,
        alias_info: AliasInfo | None,
    ) -> None: ...

class FunctionSchema:
    arguments: list[Argument]
    returns: list[Argument]
    name: str
    overload_name: str
    is_mutable: _bool
    def __init__(
        self,
        name: str,
        overload_name: str,
        arguments: list[Argument],
        returns: list[Argument],
        is_vararg: _bool,
        is_varret: _bool,
    ) -> None: ...

class _UpgraderEntry:
    bumped_at_version: _int
    upgrader_name: str
    old_schema: str
    def __init__(self, bumped_at_version: _int, upgrader_name: str, old_schema: str) -> None: ...

class _UpgraderRange:
    min_version: _int
    max_version: _int

class ScriptModuleSerializer:
    def __init__(self, export_writer: PyTorchFileWriter) -> None: ...
    def serialize(self, model: ScriptModule, script_module_id: _int) -> None: ...
    def write_files(self) -> None: ...
    def storage_context(self) -> SerializationStorageContext: ...

class SerializationStorageContext:
    def __init__(self) -> None: ...
    def has_storage(self, storage: Storage) -> _bool: ...
    def get_or_add_storage(self, storage: Storage) -> _int: ...

class DeserializationStorageContext:
    def __init__(self) -> None: ...
    def get_storage(self, name: str, dtype: _dtype) -> Tensor: ...
    def has_storage(self, name: str) -> _bool: ...
    def add_storage(self, name: str, tensor: Tensor) -> _int: ...

class ConcreteModuleTypeBuilder:
    def __init__(self, obj: Any) -> None: ...
    def set_module_dict(self): ...
    def set_module_list(self): ...
    def set_parameter_list(self): ...
    def set_parameter_dict(self): ...
    def add_attribute(self, name: str, ty: JitType, is_param: _bool, is_buffer: _bool): ...
    def add_module(self, name: str, meta: ConcreteModuleType): ...
    def add_constant(self, name: str, value: Any): ...
    def add_overload(self, method_name: str, overloaded_method_names: list[str]): ...
    def add_builtin_function(self, name: str, symbol_name: str): ...
    def add_failed_attribute(self, name: str, failure_reason: str): ...
    def add_function_attribute(self, name: str, ty: JitType, func: Callable[..., Any]): ...
    def add_ignored_attribute(self, name: str): ...
    def add_ignored_attributes(self, names: list[str]): ...
    def add_forward_hook(self, hook: Callable[..., Any]): ...
    def add_forward_pre_hook(self, pre_hook: Callable[..., Any]): ...

class ConcreteModuleType:
    def get_constants(self) -> dict[str, Any]: ...
    def equals(self, other: ConcreteModuleType) -> _bool: ...
    @staticmethod
    def from_jit_type(ty: JitType) -> ConcreteModuleType: ...

class CallStack:
    def __init__(self, name: str, range: SourceRange) -> None: ...

class ErrorReport:
    def __init__(self, range: SourceRange) -> None: ...
    def what(self) -> str: ...
    @staticmethod
    def call_stack() -> str: ...

class CompilationUnit:
    def __init__(self, lang: str = ..., _frames_up: _int = ...) -> None: ...
    def find_function(self, name: str) -> ScriptFunction: ...
    def __getattr__(self, name: str) -> ScriptFunction: ...
    def define(self, script: str, rcb: ResolutionCallback = ..., _frames_up: _int = ...): ...
    def get_interface(self, name: str) -> InterfaceType: ...
    def get_functions(self) -> list[ScriptFunction]: ...
    def create_function(self, name: str, graph: Graph, shouldMangle: _bool = ...) -> ScriptFunction: ...
    def get_class(self, name: str) -> ClassType: ...

class ScriptObject:
    def setattr(self, name: str, value: Any): ...

class ScriptModule(ScriptObject): ...

class LiteScriptModule:
    def __call__(self, *input): ...
    def find_method(self, method_name: str): ...
    def forward(self, *input) -> list[str]: ...
    def run_method(self, method_name: str, *input): ...

class ScriptFunction[**P, R]:
    def __call__(self, *args: P.args, **kwargs: P.kwargs) -> R: ...
    def save(self, filename: str, _extra_files: dict[str, bytes]) -> None: ...
    def save_to_buffer(self, _extra_files: dict[str, bytes]) -> bytes: ...
    @property
    def graph(self) -> Graph: ...
    def inlined_graph(self) -> Graph: ...
    def schema(self) -> FunctionSchema: ...
    def code(self) -> str: ...
    def name(self) -> str: ...
    @property
    def qualified_name(self) -> str: ...

class ScriptMethod[**P, R]:
    graph: Graph
    def __call__(self, *args: P.args, **kwargs: P.kwargs) -> R: ...
    @property
    def owner(self) -> ScriptModule: ...
    @property
    def name(self) -> str: ...
    @property
    def schema(self) -> FunctionSchema: ...

class ScriptDict[K, T]:
    def __init__(self, dict: dict[K, T]) -> None: ...
    def __len__(self) -> _int: ...
    def __contains__(self, key: K) -> _bool: ...
    def __getitem__(self, key: K) -> T: ...
    def __setitem__(self, key: K, value: T) -> None: ...
    def __delitem__(self, key: K) -> None: ...
    def __iter__(self) -> Iterator[K]: ...
    def items(self) -> Iterator[tuple[K, T]]: ...
    def keys(self) -> Iterator[K]: ...

class ScriptList[T]:
    def __init__(self, list: list[T]) -> None: ...
    def __len__(self) -> _int: ...
    def __contains__(self, item: T) -> _bool: ...
    @overload
    def __getitem__(self, idx: _int) -> T: ...
    @overload
    def __getitem__(self, idx: slice) -> ScriptList[T]: ...
    @overload
    def __setitem__(self, idx: _int, value: T) -> None: ...
    @overload
    def __setitem__(self, idx: slice, value: list[T]) -> None: ...
    def __delitem__(self, idx: _int) -> None: ...
    def __iter__(self) -> Iterator[T]: ...
    def count(self, value: T) -> _int: ...
    def remove(self, value: T) -> None: ...
    def append(self, value: T) -> None: ...
    def clear(self) -> None: ...
    @overload
    def extend(self, values: list[T]) -> None: ...
    @overload
    def extend(self, values: Iterable[T]) -> None: ...
    @overload
    def pop(self) -> T: ...
    @overload
    def pop(self, idx: _int) -> T: ...

class ModuleDict:
    def __init__(self, mod: ScriptModule) -> None: ...
    def items(self) -> list[tuple[str, Any]]: ...

class ParameterDict:
    def __init__(self, mod: ScriptModule) -> None: ...

class BufferDict:
    def __init__(self, mod: ScriptModule) -> None: ...

class Module: ...

def get_num_thread() -> _int: ...
def set_num_threads(nthreads: _int) -> None: ...
def get_num_interop_threads() -> _int: ...
def set_num_interop_threads(nthreads: _int) -> None: ...
def set_flush_denormal(arg: _bool) -> _bool: ...
def get_default_dtype() -> _dtype: ...

class _LinalgBackend:
    Default: _LinalgBackend
    Cusolver: _LinalgBackend
    Magma: _LinalgBackend

class BatchNormBackend(Enum): ...

class _BlasBackend:
    Default: _BlasBackend
    Cublas: _BlasBackend
    Cublaslt: _BlasBackend
    Ck: _BlasBackend

class _ROCmFABackend:
    Default: _ROCmFABackend
    AOTriton: _ROCmFABackend
    Ck: _ROCmFABackend

class ConvBackend(Enum): ...

class Tag(Enum):
    core = ...
    cudagraph_unsafe = ...
    data_dependent_output = ...
    dynamic_output_shape = ...
    flexible_layout = ...
    generated = ...
    inplace_view = ...
    maybe_aliasing_or_mutating = ...
    needs_contiguous_strides = ...
    needs_exact_strides = ...
    needs_fixed_stride_order = ...
    nondeterministic_bitwise = ...
    nondeterministic_seeded = ...
    pointwise = ...
    pt2_compliant_tag = ...
    view_copy = ...

has_openmp: _bool
has_mkl: _bool
_has_kleidiai: _bool
_has_mps: _bool
has_lapack: _bool
_has_cuda: _bool
_has_magma: _bool
_has_xpu: _bool
_has_mkldnn: _bool
_has_cudnn: _bool
_has_cusparselt: _bool
has_spectral: _bool
_GLIBCXX_USE_CXX11_ABI: _bool
default_generator: Generator

def is_grad_enabled() -> _bool: ...
def is_inference_mode_enabled() -> _bool: ...
@overload
def set_autocast_enabled(device_type: str, enabled: _bool) -> None: ...
@overload
def set_autocast_enabled(enabled: _bool) -> None: ...
@overload
def is_autocast_enabled(device_type: str) -> _bool: ...
@overload
def is_autocast_enabled() -> _bool: ...
def set_autocast_dtype(device_type: str, dtype: _dtype) -> None: ...
def get_autocast_dtype(device_type: str) -> _dtype: ...
def clear_autocast_cache() -> None: ...
def set_autocast_cpu_enabled(enabled: _bool) -> None: ...
def is_autocast_cpu_enabled() -> _bool: ...
def set_autocast_cpu_dtype(dtype: _dtype) -> None: ...
def set_autocast_gpu_dtype(dtype: _dtype) -> None: ...
def get_autocast_cpu_dtype() -> _dtype: ...
def get_autocast_gpu_dtype() -> _dtype: ...
def autocast_increment_nesting() -> _int: ...
def autocast_decrement_nesting() -> _int: ...
def is_autocast_cache_enabled() -> _bool: ...
def set_autocast_cache_enabled(enabled: _bool) -> None: ...
def set_anomaly_enabled(enabled: _bool, check_nan: _bool = ...) -> None: ...
def is_anomaly_enabled() -> _bool: ...
def is_anomaly_check_nan_enabled() -> _bool: ...

class _DisableTorchDispatch:
    def __init__(self) -> None: ...
    def __enter__(self): ...
    def __exit__(self, *exc_info: object) -> None: ...

class _EnableTorchFunction:
    def __init__(self) -> None: ...
    def __enter__(self): ...
    def __exit__(self, *exc_info: object) -> None: ...

class _EnablePythonDispatcher:
    def __init__(self) -> None: ...
    def __enter__(self): ...
    def __exit__(self, *exc_info: object) -> None: ...

class _DisablePythonDispatcher:
    def __init__(self) -> None: ...
    def __enter__(self): ...
    def __exit__(self, *exc_info: object) -> None: ...

class _EnablePreDispatch:
    def __init__(self) -> None: ...
    def __enter__(self): ...
    def __exit__(self, *exc_info: object) -> None: ...

class _DisableFuncTorch:
    def __init__(self) -> None: ...
    def __enter__(self): ...
    def __exit__(self, *exc_info: object) -> None: ...

class _DisableAutocast:
    def __init__(self) -> None: ...
    def __enter__(self): ...
    def __exit__(self, *exc_info: object) -> None: ...

class _InferenceMode:
    def __init__(self, enabled: _bool) -> None: ...
    def __enter__(self): ...
    def __exit__(self, *exc_info: object) -> None: ...

class LoggerBase: ...
class NoopLogger(LoggerBase): ...
class LockingLogger(LoggerBase): ...

class AggregationType(Enum):
    SUM = ...
    AVG = ...

class FileCheck:
    def run(self, test_string: str) -> None: ...
    def check(self, test_string: str) -> FileCheck: ...
    def check_not(self, test_string: str) -> FileCheck: ...
    def check_same(self, test_string: str) -> FileCheck: ...
    def check_next(self, test_string: str) -> FileCheck: ...
    def check_count(self, test_string: str, count: _int, exactly: _bool = ...) -> FileCheck: ...
    def check_dag(self, test_string: str) -> FileCheck: ...
    def check_source_highlighted(self, test_string: str) -> FileCheck: ...
    def check_regex(self, test_string: str) -> FileCheck: ...

class PyTorchFileReader:
    @overload
    def __init__(self, name: str) -> None: ...
    @overload
    def __init__(self, buffer: IO[bytes]) -> None: ...
    def get_record(self, name: str) -> bytes: ...
    def get_all_records(self) -> list[str]: ...
    def serialization_id(self) -> str: ...

class PyTorchFileWriter:
    @overload
    def __init__(self, name: str, compute_crc32: _bool = ..., storage_alignment: _int = ...) -> None: ...
    @overload
    def __init__(self, buffer: IO[bytes], compute_crc32: _bool = ..., storage_alignment: _int = ...) -> None: ...
    def write_record(self, name: str, data: Storage | bytes | _int, size: _int) -> None: ...
    def write_end_of_file(self) -> None: ...
    def set_min_version(self, version: _int) -> None: ...
    def get_all_written_records(self) -> list[str]: ...
    def archive_name(self) -> str: ...
    def serialization_id(self) -> str: ...

class Generator:
    device: _device
    def __init__(self, device: DeviceLikeType | None = ...) -> None: ...
    def __reduce__(self) -> tuple[type[Generator], tuple[_device], tuple[_int, _int | None, Tensor]]: ...
    def __setstate__(self, state: tuple[_int, _int | None, Tensor]) -> None: ...
    def get_state(self) -> Tensor: ...
    def set_state(self, _new_state: Tensor) -> Generator: ...
    def clone_state(self) -> Generator: ...
    def graphsafe_get_state(self) -> Generator: ...
    def graphsafe_set_state(self, _new_state: Generator) -> Generator: ...
    def set_offset(self, offset: _int) -> Generator: ...
    def get_offset(self) -> _int: ...
    def manual_seed(self, seed: _int) -> Generator: ...
    def seed(self) -> _int: ...
    def initial_seed(self) -> _int: ...

class _DispatchOperatorHandle:
    def schema(self) -> FunctionSchema: ...
    def debug(self) -> str: ...
    def redispatch_boxed(self, keyset: DispatchKeySet, *args, **kwargs) -> Any: ...

class _DispatchModule:
    def reset(self) -> None: ...
    def def_(self, schema: str, alias: str = ...) -> _DispatchModule: ...
    def def_legacy(self, schema: str) -> _DispatchModule: ...
    def def_name_t_t(self, name: str, dispatch: str, debug: str = ...) -> _DispatchModule: ...
    def def_schema_t_t(self, schema: str, dispatch: str, alias: str, debug: str = ...) -> _DispatchModule: ...
    def impl_t_t(self, name: str, dispatch: str, debug: str = ...) -> _DispatchModule: ...
    def impl_with_aoti_compile(self, ns: str, op_name_with_overload: str, dispatch: _dispatchkey) -> None: ...
    def impl(self, name: str, dispatch: _dispatchkey, func: Callable) -> None: ...
    def define(self, schema: str, alias: str = ...) -> str: ...
    def fallback_fallthrough(self, dispatch: str = ...) -> _DispatchModule: ...
    def fallback(self, dispatch: _dispatchkey, func: Callable, with_keyset: _bool = ...) -> None: ...

_after_ADInplaceOrView_keyset: DispatchKeySet
_after_autograd_keyset: DispatchKeySet

class _SafeKernelFunction:
    def call_boxed(self, keyset: DispatchKeySet, *args, **kwargs) -> Any: ...
    @property
    def op_handle(self) -> _DispatchOperatorHandle: ...

class DispatchKey(Enum):
    Undefined = ...
    FPGA = ...
    MAIA = ...
    Vulkan = ...
    Metal = ...
    MKLDNN = ...
    OpenGL = ...
    OpenCL = ...
    IDEEP = ...
    CustomRNGKeyId = ...
    MkldnnCPU = ...
    Sparse = ...
    SparseCsr = ...
    NestedTensor = ...
    Dense = ...
    PythonTLSSnapshot = ...
    PreDispatch = ...
    PythonDispatcher = ...
    Python = ...
    FuncTorchDynamicLayerBackMode = ...
    ZeroTensor = ...
    Conjugate = ...
    Negative = ...
    BackendSelect = ...
    Named = ...
    AutogradOther = ...
    AutogradFunctionality = ...
    AutogradNestedTensor = ...
    Tracer = ...
    Autocast = ...
    AutocastCPU = ...
    AutocastCUDA = ...
    Batched = ...
    VmapMode = ...
    FuncTorchGradWrapper = ...
    FuncTorchBatched = ...
    BatchedNestedTensor = ...
    FuncTorchVmapMode = ...
    FuncTorchDynamicLayerFrontMode = ...
    Functionalize = ...
    TESTING_ONLY_GenericWrapper = ...
    TESTING_ONLY_GenericMode = ...
    ADInplaceOrView = ...
    Autograd = ...
    CompositeImplicitAutograd = ...
    CompositeImplicitAutogradNestedTensor = ...
    CompositeExplicitAutograd = ...
    CompositeExplicitAutogradNonFunctional = ...
    FuncTorchBatchedDecomposition = ...
    CPU = ...
    CUDA = ...
    HIP = ...
    XLA = ...
    MTIA = ...
    MPS = ...
    IPU = ...
    XPU = ...
    HPU = ...
    VE = ...
    Lazy = ...
    Meta = ...
    PrivateUse1 = ...
    PrivateUse2 = ...
    PrivateUse3 = ...
    QuantizedCPU = ...
    QuantizedCUDA = ...
    QuantizedHIP = ...
    QuantizedXLA = ...
    QuantizedMTIA = ...
    QuantizedMPS = ...
    QuantizedIPU = ...
    QuantizedXPU = ...
    QuantizedHPU = ...
    QuantizedVE = ...
    QuantizedLazy = ...
    QuantizedMeta = ...
    QuantizedPrivateUse1 = ...
    QuantizedPrivateUse2 = ...
    QuantizedPrivateUse3 = ...
    SparseCPU = ...
    SparseCUDA = ...
    SparseHIP = ...
    SparseXLA = ...
    SparseMTIA = ...
    SparseMPS = ...
    SparseIPU = ...
    SparseXPU = ...
    SparseHPU = ...
    SparseVE = ...
    SparseLazy = ...
    SparseMeta = ...
    SparsePrivateUse1 = ...
    SparsePrivateUse2 = ...
    SparsePrivateUse3 = ...
    SparseCsrCPU = ...
    SparseCsrCUDA = ...
    SparseCsrHIP = ...
    SparseCsrXLA = ...
    SparseCsrMTIA = ...
    SparseCsrMPS = ...
    SparseCsrIPU = ...
    SparseCsrXPU = ...
    SparseCsrHPU = ...
    SparseCsrVE = ...
    SparseCsrLazy = ...
    SparseCsrMeta = ...
    SparseCsrPrivateUse1 = ...
    SparseCsrPrivateUse2 = ...
    SparseCsrPrivateUse3 = ...
    NestedTensorCPU = ...
    NestedTensorCUDA = ...
    NestedTensorHIP = ...
    NestedTensorXLA = ...
    NestedTensorMTIA = ...
    NestedTensorMPS = ...
    NestedTensorIPU = ...
    NestedTensorXPU = ...
    NestedTensorHPU = ...
    NestedTensorVE = ...
    NestedTensorLazy = ...
    NestedTensorMeta = ...
    NestedTensorPrivateUse1 = ...
    NestedTensorPrivateUse2 = ...
    NestedTensorPrivateUse3 = ...
    AutogradCPU = ...
    AutogradCUDA = ...
    AutogradHIP = ...
    AutogradXLA = ...
    AutogradMTIA = ...
    AutogradMPS = ...
    AutogradIPU = ...
    AutogradXPU = ...
    AutogradHPU = ...
    AutogradVE = ...
    AutogradLazy = ...
    AutogradMeta = ...
    AutogradPrivateUse1 = ...
    AutogradPrivateUse2 = ...
    AutogradPrivateUse3 = ...

class DispatchKeySet:
    def __init__(self, key: DispatchKey) -> None: ...
    def __or__(self, other: DispatchKeySet) -> DispatchKeySet: ...
    def __sub__(self, other: DispatchKeySet) -> DispatchKeySet: ...
    def __and__(self, other: DispatchKeySet) -> DispatchKeySet: ...
    def raw_repr(self) -> _int: ...
    @staticmethod
    def from_raw_repr(raw: _int) -> DispatchKeySet: ...
    def highestPriorityTypeId(self) -> DispatchKey: ...
    def has(self, k: _dispatchkey) -> _bool: ...
    def add(self, k: _dispatchkey) -> DispatchKeySet: ...
    def remove(self, k: _dispatchkey) -> DispatchKeySet: ...

_dispatch_autogradother_backends: DispatchKeySet
_additional_keys_to_prop_for_wrapper_tensors: DispatchKeySet

class _ExcludeDispatchKeyGuard:
    def __init__(self, keyset: DispatchKeySet) -> None: ...
    def __enter__(self): ...
    def __exit__(self, *exc_info: object) -> None: ...

class _IncludeDispatchKeyGuard:
    def __init__(self, k: DispatchKey) -> None: ...
    def __enter__(self): ...
    def __exit__(self, *exc_info: object) -> None: ...

class _ForceDispatchKeyGuard:
    def __init__(self, include: DispatchKeySet, exclude: DispatchKeySet) -> None: ...
    def __enter__(self): ...
    def __exit__(self, *exc_info: object) -> None: ...

class _PreserveDispatchKeyGuard:
    def __init__(self) -> None: ...
    def __enter__(self): ...
    def __exit__(self, *exc_info: object) -> None: ...

class _AutoDispatchBelowAutograd:
    def __init__(self) -> None: ...
    def __enter__(self): ...
    def __exit__(self, *exc_info: object) -> None: ...

class _AutoDispatchBelowADInplaceOrView:
    def __init__(self) -> None: ...
    def __enter__(self): ...
    def __exit__(self, *exc_info: object) -> None: ...

class _TorchDispatchModeKey(Enum):
    FAKE = ...
    PROXY = ...
    FUNCTIONAL = ...

class _SetExcludeDispatchKeyGuard:
    def __init__(self, k: DispatchKey, enabled: _bool) -> None: ...
    def __enter__(self): ...
    def __exit__(self, *exc_info: object) -> None: ...

class _SchemaInfo:
    def __init__(self, schema: FunctionSchema) -> None: ...
    @overload
    def is_mutable(self) -> _bool: ...
    @overload
    def is_mutable(self, name: str) -> _bool: ...
    def has_argument(self, name: str) -> _bool: ...

class BenchmarkConfig:
    num_calling_threads: _int
    num_worker_threads: _int
    num_warmup_iters: _int
    num_iters: _int
    profiler_output_path: str

class BenchmarkExecutionStats:
    latency_avg_ms: _float
    num_iters: _int

class ThroughputBenchmark:
    def __init__(self, module: Any) -> None: ...
    def add_input(self, *args: Any, **kwargs: Any) -> None: ...
    def run_once(self, *args: Any, **kwargs: Any) -> Any: ...
    def benchmark(self, config: BenchmarkConfig) -> BenchmarkExecutionStats: ...

class StorageBase: ...
class DoubleTensor(Tensor): ...
class FloatTensor(Tensor): ...
class BFloat16Tensor(Tensor): ...
class LongTensor(Tensor): ...
class IntTensor(Tensor): ...
class ShortTensor(Tensor): ...
class HalfTensor(Tensor): ...
class CharTensor(Tensor): ...
class ByteTensor(Tensor): ...
class BoolTensor(Tensor): ...

class _ImperativeEngine:
    def queue_callback(self, callback: Callable[[], None]) -> None: ...
    def run_backward(self, *args: Any, **kwargs: Any) -> tuple[Tensor, ...]: ...
    def is_checkpoint_valid(self) -> _bool: ...

class _TensorMeta(type): ...

type _Index = (
    SupportsIndex
    | _bool
    | _int
    | slice
    | EllipsisType
    | Tensor
    | None
    | _NestedSequence[_bool | _int | slice | EllipsisType | Tensor | None]
)

class TensorBase(metaclass=_TensorMeta):
    requires_grad: _bool
    retains_grad: _bool
    shape: Size
    data: Tensor
    names: list[str]
    device: _device
    dtype: _dtype
    layout: _layout
    real: Tensor
    imag: Tensor
    T: Tensor
    H: Tensor
    mT: Tensor
    mH: Tensor
    ndim: _int
    output_nr: _int
    _version: _int
    _base: Tensor | None
    _cdata: _int
    grad_fn: _Node | None
    _grad_fn: Any
    _grad: Tensor | None
    grad: Tensor | None
    _backward_hooks: dict[_int, Callable[[Tensor], Tensor | None]] | None
    nbytes: _int
    itemsize: _int
    _has_symbolic_sizes_strides: _bool
    def __abs__(self) -> Tensor: ...
    def __add__(self, other: Tensor | Number | _complex) -> Tensor: ...
    @overload
    def __and__(self, other: Tensor) -> Tensor: ...
    @overload
    def __and__(self, other: Number | _complex) -> Tensor: ...
    @overload
    def __and__(self, other: Tensor | _int) -> Tensor: ...
    def __bool__(self) -> _bool: ...
    def __complex__(self) -> _complex: ...
    def __contains__(self, item: Any, /) -> _bool: ...
    def __div__(self, other: Tensor | Number | _complex) -> Tensor: ...
    @overload
    def __eq__(self, other: Tensor | Number | _complex) -> Tensor: ...
    @overload
    def __eq__(self, other: object) -> _bool: ...
    def __float__(self) -> _float: ...
    def __floordiv__(self, other: Tensor | Number | _complex) -> Tensor: ...
    def __ge__(self, other: Tensor | Number | _complex) -> Tensor: ...
    def __getitem__(self, indices: _Index | tuple[_Index, ...], /) -> Tensor: ...
    def __gt__(self, other: Tensor | Number | _complex) -> Tensor: ...
    def __iadd__(self, other: Tensor | Number | _complex) -> Self: ...
    @overload
    def __iand__(self, other: Tensor) -> Tensor: ...
    @overload
    def __iand__(self, other: Number | _complex) -> Tensor: ...
    @overload
    def __iand__(self, other: Tensor | _int) -> Tensor: ...
    def __idiv__(self, other: Tensor | Number | _complex) -> Tensor: ...
    def __ifloordiv__(self, other: Tensor | Number | _complex) -> Self: ...
    @overload
    def __ilshift__(self, other: Tensor) -> Tensor: ...
    @overload
    def __ilshift__(self, other: Number | _complex) -> Tensor: ...
    @overload
    def __ilshift__(self, other: Tensor | _int) -> Tensor: ...
    def __imod__(self, other: Tensor | Number | _complex) -> Self: ...
    def __imul__(self, other: Tensor | Number | _complex) -> Self: ...
    def __index__(self) -> _int: ...
    @overload
    def __init__(self, *args: Any, device: DeviceLikeType | None = ...) -> None: ...
    @overload
    def __init__(self, storage: Storage) -> None: ...
    @overload
    def __init__(self, other: Tensor) -> None: ...
    @overload
    def __init__(self, size: _size, *, device: DeviceLikeType | None = ...) -> None: ...
    def __int__(self) -> _int: ...
    def __invert__(self) -> Tensor: ...
    @overload
    def __ior__(self, other: Tensor) -> Tensor: ...
    @overload
    def __ior__(self, other: Number | _complex) -> Tensor: ...
    @overload
    def __ior__(self, other: Tensor | _int) -> Tensor: ...
    @overload
    def __irshift__(self, other: Tensor) -> Tensor: ...
    @overload
    def __irshift__(self, other: Number | _complex) -> Tensor: ...
    @overload
    def __irshift__(self, other: Tensor | _int) -> Tensor: ...
    def __isub__(self, other: Tensor | Number | _complex) -> Self: ...
    @overload
    def __ixor__(self, other: Tensor) -> Tensor: ...
    @overload
    def __ixor__(self, other: Number | _complex) -> Tensor: ...
    @overload
    def __ixor__(self, other: Tensor | _int) -> Tensor: ...
    def __le__(self, other: Tensor | Number | _complex) -> Tensor: ...
    def __long__(self) -> _int: ...
    @overload
    def __lshift__(self, other: Tensor) -> Tensor: ...
    @overload
    def __lshift__(self, other: Number | _complex) -> Tensor: ...
    @overload
    def __lshift__(self, other: Tensor | _int) -> Tensor: ...
    def __lt__(self, other: Tensor | Number | _complex) -> Tensor: ...
    def __matmul__(self, other: Tensor | Number | _complex) -> Tensor: ...
    def __mod__(self, other: Tensor | Number | _complex) -> Tensor: ...
    def __mul__(self, other: Tensor | Number | _complex) -> Tensor: ...
    @overload
    def __ne__(self, other: Tensor | Number | _complex) -> Tensor: ...
    @overload
    def __ne__(self, other: object) -> _bool: ...
    def __neg__(self) -> Tensor: ...
    def __new__(cls, *args, **kwargs) -> Self: ...
    def __nonzero__(self) -> _bool: ...
    @overload
    def __or__(self, other: Tensor) -> Tensor: ...
    @overload
    def __or__(self, other: Number | _complex) -> Tensor: ...
    @overload
    def __or__(self, other: Tensor | _int) -> Tensor: ...
    def __pow__(self, other: Tensor | Number | _complex) -> Tensor: ...
    def __radd__(self, other: Tensor | Number | _complex) -> Tensor: ...
    def __rand__(self, other: Tensor | _int) -> Tensor: ...
    def __rfloordiv__(self, other: Tensor | Number | _complex) -> Tensor: ...
    def __rmul__(self, other: Tensor | Number | _complex) -> Tensor: ...
    def __ror__(self, other: Tensor | _int) -> Tensor: ...
    def __rpow__(self, other: Tensor | Number | _complex) -> Tensor: ...
    @overload
    def __rshift__(self, other: Tensor) -> Tensor: ...
    @overload
    def __rshift__(self, other: Number | _complex) -> Tensor: ...
    @overload
    def __rshift__(self, other: Tensor | _int) -> Tensor: ...
    def __rsub__(self, other: Tensor | Number | _complex) -> Tensor: ...
    def __rtruediv__(self, other: Tensor | Number | _complex) -> Tensor: ...
    def __rxor__(self, other: Tensor | _int) -> Tensor: ...
    def __setitem__(self, indices: _Index | tuple[_Index, ...], value: Tensor | Number, /) -> None: ...
    def __sub__(self, other: Tensor | Number | _complex) -> Tensor: ...
    def __truediv__(self, other: Tensor | Number | _complex) -> Tensor: ...
    @overload
    def __xor__(self, other: Tensor) -> Tensor: ...
    @overload
    def __xor__(self, other: Number | _complex) -> Tensor: ...
    @overload
    def __xor__(self, other: Tensor | _int) -> Tensor: ...
    def abs(self) -> Tensor: ...
    def abs_(self) -> Tensor: ...
    def absolute(self) -> Tensor: ...
    def absolute_(self) -> Tensor: ...
    def acos(self) -> Tensor: ...
    def acos_(self) -> Tensor: ...
    def acosh(self) -> Tensor: ...
    def acosh_(self) -> Tensor: ...
    def add(
        self,
        other: Tensor | Number | _complex | torch.SymInt | torch.SymFloat,
        *,
        alpha: Number | _complex | None = ...,
        out: Tensor | None = ...,
    ) -> Tensor: ...
    def add_(
        self,
        other: Tensor | Number | _complex | torch.SymInt | torch.SymFloat,
        *,
        alpha: Number | _complex | None = ...,
    ) -> Tensor: ...
    def addbmm(
        self, batch1: Tensor, batch2: Tensor, *, beta: Number | _complex = ..., alpha: Number | _complex = ...
    ) -> Tensor: ...
    def addbmm_(
        self, batch1: Tensor, batch2: Tensor, *, beta: Number | _complex = ..., alpha: Number | _complex = ...
    ) -> Tensor: ...
    def addcdiv(self, tensor1: Tensor, tensor2: Tensor, *, value: Number | _complex = ...) -> Tensor: ...
    def addcdiv_(self, tensor1: Tensor, tensor2: Tensor, *, value: Number | _complex = ...) -> Tensor: ...
    def addcmul(self, tensor1: Tensor, tensor2: Tensor, *, value: Number | _complex = ...) -> Tensor: ...
    def addcmul_(self, tensor1: Tensor, tensor2: Tensor, *, value: Number | _complex = ...) -> Tensor: ...
    def addmm(
        self, mat1: Tensor, mat2: Tensor, *, beta: Number | _complex = ..., alpha: Number | _complex = ...
    ) -> Tensor: ...
    def addmm_(
        self, mat1: Tensor, mat2: Tensor, *, beta: Number | _complex = ..., alpha: Number | _complex = ...
    ) -> Tensor: ...
    def addmv(
        self, mat: Tensor, vec: Tensor, *, beta: Number | _complex = ..., alpha: Number | _complex = ...
    ) -> Tensor: ...
    def addmv_(
        self, mat: Tensor, vec: Tensor, *, beta: Number | _complex = ..., alpha: Number | _complex = ...
    ) -> Tensor: ...
    def addr(
        self, vec1: Tensor, vec2: Tensor, *, beta: Number | _complex = ..., alpha: Number | _complex = ...
    ) -> Tensor: ...
    def addr_(
        self, vec1: Tensor, vec2: Tensor, *, beta: Number | _complex = ..., alpha: Number | _complex = ...
    ) -> Tensor: ...
    def adjoint(self) -> Tensor: ...
    def align_as(self, other: Tensor) -> Tensor: ...
    @overload
    def align_to(self, order: Sequence[str | EllipsisType | None], ellipsis_idx: _int) -> Tensor: ...
    @overload
    def align_to(self, names: Sequence[str | EllipsisType | None]) -> Tensor: ...
    @overload
    def all(self) -> Tensor: ...
    @overload
    def all(self, dim: _size | None = ..., keepdim: _bool = ...) -> Tensor: ...
    @overload
    def all(self, dim: _int, keepdim: _bool = ...) -> Tensor: ...
    @overload
    def all(self, dim: str | EllipsisType | None, keepdim: _bool = ...) -> Tensor: ...
    def allclose(self, other: Tensor, rtol: _float = ..., atol: _float = ..., equal_nan: _bool = ...) -> _bool: ...
    def amax(self, dim: _int | _size = ..., keepdim: _bool = ...) -> Tensor: ...
    def amin(self, dim: _int | _size = ..., keepdim: _bool = ...) -> Tensor: ...
    def aminmax(self, *, dim: _int | None = ..., keepdim: _bool = ...) -> torch.return_types.aminmax: ...
    def angle(self) -> Tensor: ...
    @overload
    def any(self) -> Tensor: ...
    @overload
    def any(self, dim: _size | None = ..., keepdim: _bool = ...) -> Tensor: ...
    @overload
    def any(self, dim: _int, keepdim: _bool = ...) -> Tensor: ...
    @overload
    def any(self, dim: str | EllipsisType | None, keepdim: _bool = ...) -> Tensor: ...
    def apply_(self, callable: Callable) -> Tensor: ...
    def arccos(self) -> Tensor: ...
    def arccos_(self) -> Tensor: ...
    def arccosh(self) -> Tensor: ...
    def arccosh_(self) -> Tensor: ...
    def arcsin(self) -> Tensor: ...
    def arcsin_(self) -> Tensor: ...
    def arcsinh(self) -> Tensor: ...
    def arcsinh_(self) -> Tensor: ...
    def arctan(self) -> Tensor: ...
    def arctan2(self, other: Tensor) -> Tensor: ...
    def arctan2_(self, other: Tensor) -> Tensor: ...
    def arctan_(self) -> Tensor: ...
    def arctanh(self) -> Tensor: ...
    def arctanh_(self) -> Tensor: ...
    def argmax(self, dim: _int | None = ..., keepdim: _bool = ...) -> Tensor: ...
    def argmin(self, dim: _int | None = ..., keepdim: _bool = ...) -> Tensor: ...
    @overload
    def argsort(self, *, stable: _bool, dim: _int = ..., descending: _bool = ...) -> Tensor: ...
    @overload
    def argsort(self, dim: _int = ..., descending: _bool = ...) -> Tensor: ...
    @overload
    def argsort(self, dim: str | EllipsisType | None, descending: _bool = ...) -> Tensor: ...
    def argwhere(self) -> Tensor: ...
    def as_strided(
        self, size: Sequence[_int | SymInt], stride: Sequence[_int | SymInt], storage_offset: _int | SymInt | None = ...
    ) -> Tensor: ...
    def as_strided_(
        self, size: Sequence[_int | SymInt], stride: Sequence[_int | SymInt], storage_offset: _int | SymInt | None = ...
    ) -> Tensor: ...
    def as_strided_scatter(
        self,
        src: Tensor,
        size: Sequence[_int | SymInt],
        stride: Sequence[_int | SymInt],
        storage_offset: _int | SymInt | None = ...,
    ) -> Tensor: ...
    def as_subclass(self, cls: type[S]) -> S: ...
    def asin(self) -> Tensor: ...
    def asin_(self) -> Tensor: ...
    def asinh(self) -> Tensor: ...
    def asinh_(self) -> Tensor: ...
    def atan(self) -> Tensor: ...
    def atan2(self, other: Tensor) -> Tensor: ...
    def atan2_(self, other: Tensor) -> Tensor: ...
    def atan_(self) -> Tensor: ...
    def atanh(self) -> Tensor: ...
    def atanh_(self) -> Tensor: ...
    def baddbmm(
        self, batch1: Tensor, batch2: Tensor, *, beta: Number | _complex = ..., alpha: Number | _complex = ...
    ) -> Tensor: ...
    def baddbmm_(
        self, batch1: Tensor, batch2: Tensor, *, beta: Number | _complex = ..., alpha: Number | _complex = ...
    ) -> Tensor: ...
    @overload
    def bernoulli(self, *, generator: Generator | None = ...) -> Tensor: ...
    @overload
    def bernoulli(self, p: _float, *, generator: Generator | None = ...) -> Tensor: ...
    @overload
    def bernoulli_(self, p: Tensor, *, generator: Generator | None = ...) -> Tensor: ...
    @overload
    def bernoulli_(self, p: _float = ..., *, generator: Generator | None = ...) -> Tensor: ...
    def bfloat16(self) -> Tensor: ...
    def bincount(self, weights: Tensor | None = ..., minlength: _int | SymInt = ...) -> Tensor: ...
    @overload
    def bitwise_and(self, other: Tensor) -> Tensor: ...
    @overload
    def bitwise_and(self, other: Number | _complex) -> Tensor: ...
    @overload
    def bitwise_and_(self, other: Tensor) -> Tensor: ...
    @overload
    def bitwise_and_(self, other: Number | _complex) -> Tensor: ...
    @overload
    def bitwise_left_shift(self, other: Tensor) -> Tensor: ...
    @overload
    def bitwise_left_shift(self, other: Number | _complex) -> Tensor: ...
    @overload
    def bitwise_left_shift_(self, other: Tensor) -> Tensor: ...
    @overload
    def bitwise_left_shift_(self, other: Number | _complex) -> Tensor: ...
    def bitwise_not(self) -> Tensor: ...
    def bitwise_not_(self) -> Tensor: ...
    @overload
    def bitwise_or(self, other: Tensor) -> Tensor: ...
    @overload
    def bitwise_or(self, other: Number | _complex) -> Tensor: ...
    @overload
    def bitwise_or_(self, other: Tensor) -> Tensor: ...
    @overload
    def bitwise_or_(self, other: Number | _complex) -> Tensor: ...
    @overload
    def bitwise_right_shift(self, other: Tensor) -> Tensor: ...
    @overload
    def bitwise_right_shift(self, other: Number | _complex) -> Tensor: ...
    @overload
    def bitwise_right_shift_(self, other: Tensor) -> Tensor: ...
    @overload
    def bitwise_right_shift_(self, other: Number | _complex) -> Tensor: ...
    @overload
    def bitwise_xor(self, other: Tensor) -> Tensor: ...
    @overload
    def bitwise_xor(self, other: Number | _complex) -> Tensor: ...
    @overload
    def bitwise_xor_(self, other: Tensor) -> Tensor: ...
    @overload
    def bitwise_xor_(self, other: Number | _complex) -> Tensor: ...
    def bmm(self, mat2: Tensor) -> Tensor: ...
    def bool(self) -> Tensor: ...
    @overload
    def broadcast_to(self, size: Sequence[_int | SymInt]) -> Tensor: ...
    @overload
    def broadcast_to(self, *size: _int | SymInt) -> Tensor: ...
    def byte(self) -> Tensor: ...
    def cauchy_(self, median: _float = ..., sigma: _float = ..., *, generator: Generator | None = ...) -> Tensor: ...
    def ccol_indices(self) -> Tensor: ...
    def ceil(self) -> Tensor: ...
    def ceil_(self) -> Tensor: ...
    def chalf(self, *, memory_format: memory_format | None = ...) -> Tensor: ...
    def char(self) -> Tensor: ...
    def cholesky(self, upper: _bool = ...) -> Tensor: ...
    def cholesky_inverse(self, upper: _bool = ...) -> Tensor: ...
    def cholesky_solve(self, input2: Tensor, upper: _bool = ...) -> Tensor: ...
    def chunk(self, chunks: _int, dim: _int = ...) -> tuple[Tensor, ...]: ...
    @overload
    def clamp(self, min: Tensor | None = ..., max: Tensor | None = ...) -> Tensor: ...
    @overload
    def clamp(self, min: Number | _complex | None = ..., max: Number | _complex | None = ...) -> Tensor: ...
    @overload
    def clamp_(self, min: Tensor | None = ..., max: Tensor | None = ...) -> Tensor: ...
    @overload
    def clamp_(self, min: Number | _complex | None = ..., max: Number | _complex | None = ...) -> Tensor: ...
    @overload
    def clamp_max(self, max: Tensor) -> Tensor: ...
    @overload
    def clamp_max(self, max: Number | _complex) -> Tensor: ...
    @overload
    def clamp_max_(self, max: Tensor) -> Tensor: ...
    @overload
    def clamp_max_(self, max: Number | _complex) -> Tensor: ...
    @overload
    def clamp_min(self, min: Tensor) -> Tensor: ...
    @overload
    def clamp_min(self, min: Number | _complex) -> Tensor: ...
    @overload
    def clamp_min_(self, min: Tensor) -> Tensor: ...
    @overload
    def clamp_min_(self, min: Number | _complex) -> Tensor: ...
    @overload
    def clip(self, min: Tensor | None = ..., max: Tensor | None = ...) -> Tensor: ...
    @overload
    def clip(self, min: Number | _complex | None = ..., max: Number | _complex | None = ...) -> Tensor: ...
    @overload
    def clip_(self, min: Tensor | None = ..., max: Tensor | None = ...) -> Tensor: ...
    @overload
    def clip_(self, min: Number | _complex | None = ..., max: Number | _complex | None = ...) -> Tensor: ...
    def clone(self, *, memory_format: memory_format | None = ...) -> Tensor: ...
    def coalesce(self) -> Tensor: ...
    def col_indices(self) -> Tensor: ...
    def conj(self) -> Tensor: ...
    def conj_physical(self) -> Tensor: ...
    def conj_physical_(self) -> Tensor: ...
    def contiguous(self, memory_format: torch.memory_format = ...) -> Tensor: ...
    def copy_(self, other: Tensor, non_blocking: _bool = ...) -> Tensor: ...
    @overload
    def copysign(self, other: Tensor) -> Tensor: ...
    @overload
    def copysign(self, other: Number | _complex) -> Tensor: ...
    @overload
    def copysign_(self, other: Tensor) -> Tensor: ...
    @overload
    def copysign_(self, other: Number | _complex) -> Tensor: ...
    def corrcoef(self) -> Tensor: ...
    def cos(self) -> Tensor: ...
    def cos_(self) -> Tensor: ...
    def cosh(self) -> Tensor: ...
    def cosh_(self) -> Tensor: ...
    @overload
    def count_nonzero(self, dim: _int | None = ...) -> Tensor: ...
    @overload
    def count_nonzero(self, dim: _size) -> Tensor: ...
    @overload
    def count_nonzero(self, *dim: _int) -> Tensor: ...
    def cov(
        self, *, correction: _int = ..., fweights: Tensor | None = ..., aweights: Tensor | None = ...
    ) -> Tensor: ...
    def cpu(self, memory_format: torch.memory_format = ...) -> Tensor: ...
    def cross(self, other: Tensor, dim: _int | None = ...) -> Tensor: ...
    def crow_indices(self) -> Tensor: ...
    def cuda(
        self,
        device: _device | _int | str | None = ...,
        non_blocking: _bool = ...,
        memory_format: torch.memory_format = ...,
    ) -> Tensor: ...
    @overload
    def cummax(self, dim: _int) -> torch.return_types.cummax: ...
    @overload
    def cummax(self, dim: str | EllipsisType | None) -> torch.return_types.cummax: ...
    @overload
    def cummin(self, dim: _int) -> torch.return_types.cummin: ...
    @overload
    def cummin(self, dim: str | EllipsisType | None) -> torch.return_types.cummin: ...
    @overload
    def cumprod(self, dim: _int, *, dtype: _dtype | None = ...) -> Tensor: ...
    @overload
    def cumprod(self, dim: str | EllipsisType | None, *, dtype: _dtype | None = ...) -> Tensor: ...
    @overload
    def cumprod_(self, dim: _int, *, dtype: _dtype | None = ...) -> Tensor: ...
    @overload
    def cumprod_(self, dim: str | EllipsisType | None, *, dtype: _dtype | None = ...) -> Tensor: ...
    @overload
    def cumsum(self, dim: _int, *, dtype: _dtype | None = ...) -> Tensor: ...
    @overload
    def cumsum(self, dim: str | EllipsisType | None, *, dtype: _dtype | None = ...) -> Tensor: ...
    @overload
    def cumsum_(self, dim: _int, *, dtype: _dtype | None = ...) -> Tensor: ...
    @overload
    def cumsum_(self, dim: str | EllipsisType | None, *, dtype: _dtype | None = ...) -> Tensor: ...
    def data_ptr(self) -> _int: ...
    def deg2rad(self) -> Tensor: ...
    def deg2rad_(self) -> Tensor: ...
    def dense_dim(self) -> _int: ...
    def dequantize(self) -> Tensor: ...
    def det(self) -> Tensor: ...
    def detach(self) -> Tensor: ...
    def detach_(self) -> Tensor: ...
    def diag(self, diagonal: _int = ...) -> Tensor: ...
    def diag_embed(self, offset: _int = ..., dim1: _int = ..., dim2: _int = ...) -> Tensor: ...
    def diagflat(self, offset: _int = ...) -> Tensor: ...
    @overload
    def diagonal(
        self,
        *,
        outdim: str | EllipsisType | None,
        dim1: str | EllipsisType | None,
        dim2: str | EllipsisType | None,
        offset: _int = ...,
    ) -> Tensor: ...
    @overload
    def diagonal(self, offset: _int = ..., dim1: _int = ..., dim2: _int = ...) -> Tensor: ...
    def diagonal_scatter(self, src: Tensor, offset: _int = ..., dim1: _int = ..., dim2: _int = ...) -> Tensor: ...
    def diff(
        self, n: _int = ..., dim: _int = ..., prepend: Tensor | None = ..., append: Tensor | None = ...
    ) -> Tensor: ...
    def digamma(self) -> Tensor: ...
    def digamma_(self) -> Tensor: ...
    def dim(self) -> _int: ...
    def dist(self, other: Tensor, p: Number | _complex = ...) -> Tensor: ...
    def div(self, other: Tensor | Number, *, rounding_mode: str | None = ...) -> Tensor: ...
    def div_(self, other: Tensor | Number, *, rounding_mode: str | None = ...) -> Tensor: ...
    @overload
    def divide(self, other: Tensor) -> Tensor: ...
    @overload
    def divide(self, other: Tensor, *, rounding_mode: str | None) -> Tensor: ...
    @overload
    def divide(self, other: Number | _complex, *, rounding_mode: str | None) -> Tensor: ...
    @overload
    def divide(self, other: Number | _complex) -> Tensor: ...
    @overload
    def divide_(self, other: Tensor) -> Tensor: ...
    @overload
    def divide_(self, other: Tensor, *, rounding_mode: str | None) -> Tensor: ...
    @overload
    def divide_(self, other: Number | _complex, *, rounding_mode: str | None) -> Tensor: ...
    @overload
    def divide_(self, other: Number | _complex) -> Tensor: ...
    def dot(self, tensor: Tensor) -> Tensor: ...
    def double(self) -> Tensor: ...
    @overload
    def dsplit(self, sections: _int) -> tuple[Tensor, ...]: ...
    @overload
    def dsplit(self, indices: _size) -> tuple[Tensor, ...]: ...
    @overload
    def dsplit(self, *indices: _int) -> tuple[Tensor, ...]: ...
    def element_size(self) -> _int: ...
    @overload
    def eq(self, other: Tensor) -> Tensor: ...
    @overload
    def eq(self, other: Number | _complex) -> Tensor: ...
    @overload
    def eq_(self, other: Tensor) -> Tensor: ...
    @overload
    def eq_(self, other: Number | _complex) -> Tensor: ...
    def equal(self, other: Tensor) -> _bool: ...
    def erf(self) -> Tensor: ...
    def erf_(self) -> Tensor: ...
    def erfc(self) -> Tensor: ...
    def erfc_(self) -> Tensor: ...
    def erfinv(self) -> Tensor: ...
    def erfinv_(self) -> Tensor: ...
    def exp(self) -> Tensor: ...
    def exp2(self) -> Tensor: ...
    def exp2_(self) -> Tensor: ...
    def exp_(self) -> Tensor: ...
    @overload
    def expand(self, size: Sequence[_int | SymInt], *, implicit: _bool = ...) -> Tensor: ...
    @overload
    def expand(self, *size: _int | SymInt, implicit: _bool = ...) -> Tensor: ...
    def expand_as(self, other: Tensor) -> Tensor: ...
    def expm1(self) -> Tensor: ...
    def expm1_(self) -> Tensor: ...
    def exponential_(self, lambd: _float = ..., *, generator: Generator | None = ...) -> Tensor: ...
    @overload
    def fill_(self, value: Tensor) -> Tensor: ...
    @overload
    def fill_(self, value: Number | _complex) -> Tensor: ...
    def fill_diagonal_(self, fill_value: Number | _complex, wrap: _bool = ...) -> Tensor: ...
    def fix(self) -> Tensor: ...
    def fix_(self) -> Tensor: ...
    @overload
    def flatten(self, start_dim: _int, end_dim: _int, out_dim: str | EllipsisType | None) -> Tensor: ...
    @overload
    def flatten(self, start_dim: _int = ..., end_dim: _int = ...) -> Tensor: ...
    @overload
    def flatten(
        self,
        start_dim: str | EllipsisType | None,
        end_dim: str | EllipsisType | None,
        out_dim: str | EllipsisType | None,
    ) -> Tensor: ...
    @overload
    def flatten(self, dims: Sequence[str | EllipsisType | None], out_dim: str | EllipsisType | None) -> Tensor: ...
    @overload
    def flip(self, dims: _size) -> Tensor: ...
    @overload
    def flip(self, *dims: _int) -> Tensor: ...
    def fliplr(self) -> Tensor: ...
    def flipud(self) -> Tensor: ...
    def float(self) -> Tensor: ...
    @overload
    def float_power(self, exponent: Tensor) -> Tensor: ...
    @overload
    def float_power(self, exponent: Number | _complex) -> Tensor: ...
    @overload
    def float_power_(self, exponent: Tensor) -> Tensor: ...
    @overload
    def float_power_(self, exponent: Number | _complex) -> Tensor: ...
    def floor(self) -> Tensor: ...
    def floor_(self) -> Tensor: ...
    def floor_divide(
        self, other: Tensor | Number | torch.SymInt | torch.SymFloat, *, out: Tensor | None = ...
    ) -> Tensor: ...
    def floor_divide_(self, other: Tensor | Number | torch.SymInt | torch.SymFloat) -> Tensor: ...
    def fmax(self, other: Tensor) -> Tensor: ...
    def fmin(self, other: Tensor) -> Tensor: ...
    @overload
    def fmod(self, other: Tensor) -> Tensor: ...
    @overload
    def fmod(self, other: Number | _complex) -> Tensor: ...
    @overload
    def fmod_(self, other: Tensor) -> Tensor: ...
    @overload
    def fmod_(self, other: Number | _complex) -> Tensor: ...
    def frac(self) -> Tensor: ...
    def frac_(self) -> Tensor: ...
    def frexp(self) -> torch.return_types.frexp: ...
    @overload
    def gather(self, dim: _int, index: Tensor, *, sparse_grad: _bool = ...) -> Tensor: ...
    @overload
    def gather(self, dim: str | EllipsisType | None, index: Tensor, *, sparse_grad: _bool = ...) -> Tensor: ...
    def gcd(self, other: Tensor) -> Tensor: ...
    def gcd_(self, other: Tensor) -> Tensor: ...
    @overload
    def ge(self, other: Tensor) -> Tensor: ...
    @overload
    def ge(self, other: Number | _complex) -> Tensor: ...
    @overload
    def ge_(self, other: Tensor) -> Tensor: ...
    @overload
    def ge_(self, other: Number | _complex) -> Tensor: ...
    def geometric_(self, p: _float, *, generator: Generator | None = ...) -> Tensor: ...
    def geqrf(self) -> torch.return_types.geqrf: ...
    def ger(self, vec2: Tensor) -> Tensor: ...
    def get_device(self) -> _int: ...
    @overload
    def greater(self, other: Tensor) -> Tensor: ...
    @overload
    def greater(self, other: Number | _complex) -> Tensor: ...
    @overload
    def greater_(self, other: Tensor) -> Tensor: ...
    @overload
    def greater_(self, other: Number | _complex) -> Tensor: ...
    @overload
    def greater_equal(self, other: Tensor) -> Tensor: ...
    @overload
    def greater_equal(self, other: Number | _complex) -> Tensor: ...
    @overload
    def greater_equal_(self, other: Tensor) -> Tensor: ...
    @overload
    def greater_equal_(self, other: Number | _complex) -> Tensor: ...
    @overload
    def gt(self, other: Tensor) -> Tensor: ...
    @overload
    def gt(self, other: Number | _complex) -> Tensor: ...
    @overload
    def gt_(self, other: Tensor) -> Tensor: ...
    @overload
    def gt_(self, other: Number | _complex) -> Tensor: ...
    def half(self) -> Tensor: ...
    def hardshrink(self, lambd: Number | _complex = ...) -> Tensor: ...
    def has_names(self) -> _bool: ...
    @overload
    def hash_tensor(self, dim: _int | _size = ..., *, keepdim: _bool = ..., mode: _int = ...) -> Tensor: ...
    @overload
    def hash_tensor(self, *dim: _int, keepdim: _bool = ..., mode: _int = ...) -> Tensor: ...
    def heaviside(self, values: Tensor) -> Tensor: ...
    def heaviside_(self, values: Tensor) -> Tensor: ...
    def histc(self, bins: _int = ..., min: Number | _complex = ..., max: Number | _complex = ...) -> Tensor: ...
    @overload
    def histogram(
        self, bins: Tensor, *, weight: Tensor | None = ..., density: _bool = ...
    ) -> torch.return_types.histogram: ...
    @overload
    def histogram(
        self,
        bins: _int = ...,
        *,
        range: Sequence[_float] | None = ...,
        weight: Tensor | None = ...,
        density: _bool = ...,
    ) -> torch.return_types.histogram: ...
    @overload
    def hsplit(self, sections: _int) -> tuple[Tensor, ...]: ...
    @overload
    def hsplit(self, indices: _size) -> tuple[Tensor, ...]: ...
    @overload
    def hsplit(self, *indices: _int) -> tuple[Tensor, ...]: ...
    def hypot(self, other: Tensor) -> Tensor: ...
    def hypot_(self, other: Tensor) -> Tensor: ...
    def i0(self) -> Tensor: ...
    def i0_(self) -> Tensor: ...
    def igamma(self, other: Tensor) -> Tensor: ...
    def igamma_(self, other: Tensor) -> Tensor: ...
    def igammac(self, other: Tensor) -> Tensor: ...
    def igammac_(self, other: Tensor) -> Tensor: ...
    @overload
    def index_add(self, dim: _int, index: Tensor, source: Tensor, *, alpha: Number | _complex = ...) -> Tensor: ...
    @overload
    def index_add(
        self, dim: str | EllipsisType | None, index: Tensor, source: Tensor, *, alpha: Number | _complex = ...
    ) -> Tensor: ...
    def index_add_(self, dim: _int, index: Tensor, source: Tensor, *, alpha: Number | _complex = ...) -> Tensor: ...
    @overload
    def index_copy(self, dim: _int, index: Tensor, source: Tensor) -> Tensor: ...
    @overload
    def index_copy(self, dim: str | EllipsisType | None, index: Tensor, source: Tensor) -> Tensor: ...
    @overload
    def index_copy_(self, dim: _int, index: Tensor, source: Tensor) -> Tensor: ...
    @overload
    def index_copy_(self, dim: str | EllipsisType | None, index: Tensor, source: Tensor) -> Tensor: ...
    @overload
    def index_fill(self, dim: _int, index: Tensor, value: Tensor) -> Tensor: ...
    @overload
    def index_fill(self, dim: str | EllipsisType | None, index: Tensor, value: Tensor) -> Tensor: ...
    @overload
    def index_fill(self, dim: _int, index: Tensor, value: Number | _complex) -> Tensor: ...
    @overload
    def index_fill(self, dim: str | EllipsisType | None, index: Tensor, value: Number | _complex) -> Tensor: ...
    @overload
    def index_fill_(self, dim: _int, index: Tensor, value: Tensor) -> Tensor: ...
    @overload
    def index_fill_(self, dim: str | EllipsisType | None, index: Tensor, value: Tensor) -> Tensor: ...
    @overload
    def index_fill_(self, dim: _int, index: Tensor, value: Number | _complex) -> Tensor: ...
    @overload
    def index_fill_(self, dim: str | EllipsisType | None, index: Tensor, value: Number | _complex) -> Tensor: ...
    def index_put(
        self, indices: tuple[Tensor, ...] | list[Tensor] | None, values: Tensor, accumulate: _bool = ...
    ) -> Tensor: ...
    def index_put_(
        self, indices: tuple[Tensor, ...] | list[Tensor] | None, values: Tensor, accumulate: _bool = ...
    ) -> Tensor: ...
    def index_reduce(
        self, dim: _int, index: Tensor, source: Tensor, reduce: str, *, include_self: _bool = ...
    ) -> Tensor: ...
    def index_reduce_(
        self, dim: _int, index: Tensor, source: Tensor, reduce: str, *, include_self: _bool = ...
    ) -> Tensor: ...
    @overload
    def index_select(self, dim: _int, index: Tensor) -> Tensor: ...
    @overload
    def index_select(self, dim: str | EllipsisType | None, index: Tensor) -> Tensor: ...
    def indices(self) -> Tensor: ...
    def inner(self, other: Tensor) -> Tensor: ...
    def int(self) -> Tensor: ...
    def int_repr(self) -> Tensor: ...
    def inverse(self) -> Tensor: ...
    def is_coalesced(self) -> _bool: ...
    def is_complex(self) -> _bool: ...
    def is_conj(self) -> _bool: ...
    def is_contiguous(self, memory_format: torch.memory_format = ...) -> _bool: ...

    is_cpu: _bool
    is_cuda: _bool
    def is_distributed(self) -> _bool: ...
    def is_floating_point(self) -> _bool: ...
    def is_inference(self) -> _bool: ...

    is_ipu: _bool
    is_leaf: _bool
    is_maia: _bool
    is_meta: _bool
    is_mkldnn: _bool
    is_mps: _bool
    is_mtia: _bool
    def is_neg(self) -> _bool: ...

    is_nested: _bool
    def is_nonzero(self) -> _bool: ...
    def is_pinned(self, device: DeviceLikeType | None = ...) -> _bool: ...

    is_quantized: _bool
    def is_same_size(self, other: Tensor) -> _bool: ...
    def is_set_to(self, tensor: Tensor) -> _bool: ...
    def is_signed(self) -> _bool: ...

    is_sparse: _bool
    is_sparse_csr: _bool
    is_vulkan: _bool
    is_xpu: _bool
    def isclose(self, other: Tensor, rtol: _float = ..., atol: _float = ..., equal_nan: _bool = ...) -> Tensor: ...
    def isfinite(self) -> Tensor: ...
    def isinf(self) -> Tensor: ...
    def isnan(self) -> Tensor: ...
    def isneginf(self) -> Tensor: ...
    def isposinf(self) -> Tensor: ...
    def isreal(self) -> Tensor: ...
    def istft(
        self,
        n_fft: _int,
        hop_length: _int | None = ...,
        win_length: _int | None = ...,
        window: Tensor | None = ...,
        center: _bool = ...,
        normalized: _bool = ...,
        onesided: _bool | None = ...,
        length: _int | None = ...,
        return_complex: _bool = ...,
    ) -> Tensor: ...
    def item(self) -> Number: ...
    def kron(self, other: Tensor) -> Tensor: ...
    @overload
    def kthvalue(self, k: _int | SymInt, dim: _int = ..., keepdim: _bool = ...) -> torch.return_types.kthvalue: ...
    @overload
    def kthvalue(
        self, k: _int | SymInt, dim: str | EllipsisType | None, keepdim: _bool = ...
    ) -> torch.return_types.kthvalue: ...
    def lcm(self, other: Tensor) -> Tensor: ...
    def lcm_(self, other: Tensor) -> Tensor: ...
    def ldexp(self, other: Tensor) -> Tensor: ...
    def ldexp_(self, other: Tensor) -> Tensor: ...
    @overload
    def le(self, other: Tensor) -> Tensor: ...
    @overload
    def le(self, other: Number | _complex) -> Tensor: ...
    @overload
    def le_(self, other: Tensor) -> Tensor: ...
    @overload
    def le_(self, other: Number | _complex) -> Tensor: ...
    @overload
    def lerp(self, end: Tensor, weight: Tensor) -> Tensor: ...
    @overload
    def lerp(self, end: Tensor, weight: Number | _complex) -> Tensor: ...
    @overload
    def lerp_(self, end: Tensor, weight: Tensor) -> Tensor: ...
    @overload
    def lerp_(self, end: Tensor, weight: Number | _complex) -> Tensor: ...
    @overload
    def less(self, other: Tensor) -> Tensor: ...
    @overload
    def less(self, other: Number | _complex) -> Tensor: ...
    @overload
    def less_(self, other: Tensor) -> Tensor: ...
    @overload
    def less_(self, other: Number | _complex) -> Tensor: ...
    @overload
    def less_equal(self, other: Tensor) -> Tensor: ...
    @overload
    def less_equal(self, other: Number | _complex) -> Tensor: ...
    @overload
    def less_equal_(self, other: Tensor) -> Tensor: ...
    @overload
    def less_equal_(self, other: Number | _complex) -> Tensor: ...
    def lgamma(self) -> Tensor: ...
    def lgamma_(self) -> Tensor: ...
    def log(self) -> Tensor: ...
    def log10(self) -> Tensor: ...
    def log10_(self) -> Tensor: ...
    def log1p(self) -> Tensor: ...
    def log1p_(self) -> Tensor: ...
    def log2(self) -> Tensor: ...
    def log2_(self) -> Tensor: ...
    def log_(self) -> Tensor: ...
    def log_normal_(self, mean: _float = ..., std: _float = ..., *, generator: Generator | None = ...) -> Tensor: ...
    @overload
    def log_softmax(self, dim: _int, dtype: _dtype | None = ...) -> Tensor: ...
    @overload
    def log_softmax(self, dim: str | EllipsisType | None, *, dtype: _dtype | None = ...) -> Tensor: ...
    def logaddexp(self, other: Tensor) -> Tensor: ...
    def logaddexp2(self, other: Tensor) -> Tensor: ...
    @overload
    def logcumsumexp(self, dim: _int) -> Tensor: ...
    @overload
    def logcumsumexp(self, dim: str | EllipsisType | None) -> Tensor: ...
    def logdet(self) -> Tensor: ...
    def logical_and(self, other: Tensor) -> Tensor: ...
    def logical_and_(self, other: Tensor) -> Tensor: ...
    def logical_not(self) -> Tensor: ...
    def logical_not_(self) -> Tensor: ...
    def logical_or(self, other: Tensor) -> Tensor: ...
    def logical_or_(self, other: Tensor) -> Tensor: ...
    def logical_xor(self, other: Tensor) -> Tensor: ...
    def logical_xor_(self, other: Tensor) -> Tensor: ...
    def logit(self, eps: _float | None = ...) -> Tensor: ...
    def logit_(self, eps: _float | None = ...) -> Tensor: ...
    @overload
    def logsumexp(self, dim: _int | _size, keepdim: _bool = ...) -> Tensor: ...
    @overload
    def logsumexp(self, dim: Sequence[str | EllipsisType | None], keepdim: _bool = ...) -> Tensor: ...
    def long(self) -> Tensor: ...
    @overload
    def lt(self, other: Tensor) -> Tensor: ...
    @overload
    def lt(self, other: Number | _complex) -> Tensor: ...
    @overload
    def lt_(self, other: Tensor) -> Tensor: ...
    @overload
    def lt_(self, other: Number | _complex) -> Tensor: ...
    def lu_solve(self, LU_data: Tensor, LU_pivots: Tensor) -> Tensor: ...
    def map2_(self, x: Tensor, y: Tensor, callable: Callable) -> Tensor: ...
    def map_(self, other: Tensor, callable: Callable) -> Tensor: ...
    @overload
    def masked_fill(self, mask: Tensor, value: Tensor) -> Tensor: ...
    @overload
    def masked_fill(self, mask: Tensor, value: Number | _complex) -> Tensor: ...
    @overload
    def masked_fill_(self, mask: Tensor, value: Tensor) -> Tensor: ...
    @overload
    def masked_fill_(self, mask: Tensor, value: Number | _complex) -> Tensor: ...
    def masked_scatter(self, mask: Tensor, source: Tensor) -> Tensor: ...
    def masked_scatter_(self, mask: Tensor, source: Tensor) -> Tensor: ...
    def masked_select(self, mask: Tensor) -> Tensor: ...
    def matmul(self, other: Tensor) -> Tensor: ...
    def matrix_exp(self) -> Tensor: ...
    def matrix_power(self, n: _int) -> Tensor: ...
    @overload
    def max(self) -> Tensor: ...
    @overload
    def max(self, other: Tensor) -> Tensor: ...
    @overload
    def max(self, dim: _int, keepdim: _bool = ...) -> torch.return_types.max: ...
    @overload
    def max(self, dim: str | EllipsisType | None, keepdim: _bool = ...) -> torch.return_types.max: ...
    def maximum(self, other: Tensor) -> Tensor: ...
    @overload
    def mean(self, *, dtype: _dtype | None = ...) -> Tensor: ...
    @overload
    def mean(self, dim: _int | _size | None, keepdim: _bool = ..., *, dtype: _dtype | None = ...) -> Tensor: ...
    @overload
    def mean(
        self, dim: Sequence[str | EllipsisType | None], keepdim: _bool = ..., *, dtype: _dtype | None = ...
    ) -> Tensor: ...
    @overload
    def median(self) -> Tensor: ...
    @overload
    def median(self, dim: _int, keepdim: _bool = ...) -> torch.return_types.median: ...
    @overload
    def median(self, dim: str | EllipsisType | None, keepdim: _bool = ...) -> torch.return_types.median: ...
    @overload
    def min(self) -> Tensor: ...
    @overload
    def min(self, other: Tensor) -> Tensor: ...
    @overload
    def min(self, dim: _int, keepdim: _bool = ...) -> torch.return_types.min: ...
    @overload
    def min(self, dim: str | EllipsisType | None, keepdim: _bool = ...) -> torch.return_types.min: ...
    def minimum(self, other: Tensor) -> Tensor: ...
    def mm(self, mat2: Tensor) -> Tensor: ...
    @overload
    def mode(self, dim: _int = ..., keepdim: _bool = ...) -> torch.return_types.mode: ...
    @overload
    def mode(self, dim: str | EllipsisType | None, keepdim: _bool = ...) -> torch.return_types.mode: ...
    @overload
    def moveaxis(self, source: _int, destination: _int) -> Tensor: ...
    @overload
    def moveaxis(self, source: _size, destination: _size) -> Tensor: ...
    @overload
    def movedim(self, source: _int, destination: _int) -> Tensor: ...
    @overload
    def movedim(self, source: _size, destination: _size) -> Tensor: ...
    def msort(self) -> Tensor: ...
    def mul(
        self, other: Tensor | Number | _complex | torch.SymInt | torch.SymFloat, *, out: Tensor | None = ...
    ) -> Tensor: ...
    def mul_(self, other: Tensor | Number | _complex | torch.SymInt | torch.SymFloat) -> Tensor: ...
    def multinomial(
        self, num_samples: _int | SymInt, replacement: _bool = ..., *, generator: Generator | None = ...
    ) -> Tensor: ...
    @overload
    def multiply(self, other: Tensor) -> Tensor: ...
    @overload
    def multiply(self, other: Number | _complex) -> Tensor: ...
    @overload
    def multiply_(self, other: Tensor) -> Tensor: ...
    @overload
    def multiply_(self, other: Number | _complex) -> Tensor: ...
    def mv(self, vec: Tensor) -> Tensor: ...
    def mvlgamma(self, p: _int) -> Tensor: ...
    def mvlgamma_(self, p: _int) -> Tensor: ...
    def nan_to_num(
        self, nan: _float | None = ..., posinf: _float | None = ..., neginf: _float | None = ...
    ) -> Tensor: ...
    def nan_to_num_(
        self, nan: _float | None = ..., posinf: _float | None = ..., neginf: _float | None = ...
    ) -> Tensor: ...
    def nanmean(
        self, dim: _int | _size | None = ..., keepdim: _bool = ..., *, dtype: _dtype | None = ...
    ) -> Tensor: ...
    @overload
    def nanmedian(self) -> Tensor: ...
    @overload
    def nanmedian(self, dim: _int, keepdim: _bool = ...) -> torch.return_types.nanmedian: ...
    @overload
    def nanmedian(self, dim: str | EllipsisType | None, keepdim: _bool = ...) -> torch.return_types.nanmedian: ...
    @overload
    def nanquantile(
        self, q: Tensor, dim: _int | None = ..., keepdim: _bool = ..., *, interpolation: str = ...
    ) -> Tensor: ...
    @overload
    def nanquantile(
        self, q: _float, dim: _int | None = ..., keepdim: _bool = ..., *, interpolation: str = ...
    ) -> Tensor: ...
    def nansum(self, dim: _int | _size | None = ..., keepdim: _bool = ..., *, dtype: _dtype | None = ...) -> Tensor: ...
    @overload
    def narrow(self, dim: _int, start: Tensor, length: _int | SymInt) -> Tensor: ...
    @overload
    def narrow(self, dim: _int, start: _int | SymInt, length: _int | SymInt) -> Tensor: ...
    def narrow_copy(self, dim: _int, start: _int | SymInt, length: _int | SymInt) -> Tensor: ...
    def ndimension(self) -> _int: ...
    @overload
    def ne(self, other: Tensor) -> Tensor: ...
    @overload
    def ne(self, other: Number | _complex) -> Tensor: ...
    @overload
    def ne_(self, other: Tensor) -> Tensor: ...
    @overload
    def ne_(self, other: Number | _complex) -> Tensor: ...
    def neg(self) -> Tensor: ...
    def neg_(self) -> Tensor: ...
    def negative(self) -> Tensor: ...
    def negative_(self) -> Tensor: ...
    def nelement(self) -> _int: ...
    @overload
    def new(self, *args: Any, device: DeviceLikeType | None = ...) -> Self: ...
    @overload
    def new(self, storage: Storage) -> Self: ...
    @overload
    def new(self, other: Tensor) -> Self: ...
    @overload
    def new(self, size: _size, *, device: DeviceLikeType | None = ...) -> Self: ...
    @overload
    def new_empty(
        self,
        size: Sequence[_int | SymInt],
        *,
        dtype: _dtype | None = ...,
        layout: _layout | None = ...,
        device: DeviceLikeType | None = ...,
        pin_memory: _bool | None = ...,
        requires_grad: _bool | None = ...,
    ) -> Tensor: ...
    @overload
    def new_empty(
        self,
        *size: _int | SymInt,
        dtype: _dtype | None = ...,
        layout: _layout | None = ...,
        device: DeviceLikeType | None = ...,
        pin_memory: _bool | None = ...,
        requires_grad: _bool | None = ...,
    ) -> Tensor: ...
    def new_empty_strided(
        self,
        size: Sequence[_int | SymInt],
        stride: Sequence[_int | SymInt],
        *,
        dtype: _dtype | None = ...,
        layout: _layout | None = ...,
        device: DeviceLikeType | None = ...,
        pin_memory: _bool | None = ...,
        requires_grad: _bool | None = ...,
    ) -> Tensor: ...
    def new_full(
        self,
        size: Sequence[_int | SymInt],
        fill_value: Number | _complex,
        *,
        dtype: _dtype | None = ...,
        layout: _layout | None = ...,
        device: DeviceLikeType | None = ...,
        pin_memory: _bool | None = ...,
        requires_grad: _bool | None = ...,
    ) -> Tensor: ...
    @overload
    def new_ones(
        self,
        size: _size,
        dtype: _dtype | None = ...,
        device: DeviceLikeType | None = ...,
        requires_grad: _bool = ...,
        pin_memory: _bool = ...,
    ) -> Tensor: ...
    @overload
    def new_ones(
        self,
        size: Sequence[_int | SymInt],
        *,
        dtype: _dtype | None = ...,
        layout: _layout | None = ...,
        device: DeviceLikeType | None = ...,
        pin_memory: _bool | None = ...,
        requires_grad: _bool | None = ...,
    ) -> Tensor: ...
    @overload
    def new_ones(
        self,
        *size: _int | SymInt,
        dtype: _dtype | None = ...,
        layout: _layout | None = ...,
        device: DeviceLikeType | None = ...,
        pin_memory: _bool | None = ...,
        requires_grad: _bool | None = ...,
    ) -> Tensor: ...
    def new_tensor(
        self,
        data: Any,
        dtype: _dtype | None = ...,
        device: DeviceLikeType | None = ...,
        requires_grad: _bool = ...,
        pin_memory: _bool = ...,
    ) -> Tensor: ...
    @overload
    def new_zeros(
        self,
        size: Sequence[_int | SymInt],
        *,
        dtype: _dtype | None = ...,
        layout: _layout | None = ...,
        device: DeviceLikeType | None = ...,
        pin_memory: _bool | None = ...,
        requires_grad: _bool | None = ...,
    ) -> Tensor: ...
    @overload
    def new_zeros(
        self,
        *size: _int | SymInt,
        dtype: _dtype | None = ...,
        layout: _layout | None = ...,
        device: DeviceLikeType | None = ...,
        pin_memory: _bool | None = ...,
        requires_grad: _bool | None = ...,
    ) -> Tensor: ...
    def nextafter(self, other: Tensor) -> Tensor: ...
    def nextafter_(self, other: Tensor) -> Tensor: ...
    @overload
    def nonzero(self, *, as_tuple: Literal[False] = ...) -> Tensor: ...
    @overload
    def nonzero(self, *, as_tuple: Literal[True]) -> tuple[Tensor, ...]: ...
    def nonzero_static(self, *, size: _int | SymInt, fill_value: _int = ...) -> Tensor: ...
    def normal_(self, mean: _float = ..., std: _float = ..., *, generator: Generator | None = ...) -> Tensor: ...
    @overload
    def not_equal(self, other: Tensor) -> Tensor: ...
    @overload
    def not_equal(self, other: Number | _complex) -> Tensor: ...
    @overload
    def not_equal_(self, other: Tensor) -> Tensor: ...
    @overload
    def not_equal_(self, other: Number | _complex) -> Tensor: ...
    def numel(self) -> _int: ...
    def numpy(self, *, force: _bool = ...) -> np.ndarray: ...
    def orgqr(self, input2: Tensor) -> Tensor: ...
    def ormqr(self, input2: Tensor, input3: Tensor, left: _bool = ..., transpose: _bool = ...) -> Tensor: ...
    def outer(self, vec2: Tensor) -> Tensor: ...
    @overload
    def permute(self, dims: _size) -> Tensor: ...
    @overload
    def permute(self, *dims: _int) -> Tensor: ...
    def pin_memory(self, device: DeviceLikeType | None = ...) -> Tensor: ...
    def pinverse(self, rcond: _float = ...) -> Tensor: ...
    def polygamma(self, n: _int) -> Tensor: ...
    def polygamma_(self, n: _int) -> Tensor: ...
    def positive(self) -> Tensor: ...
    @overload
    def pow(self, exponent: Tensor) -> Tensor: ...
    @overload
    def pow(self, exponent: Number | _complex) -> Tensor: ...
    @overload
    def pow_(self, exponent: Tensor) -> Tensor: ...
    @overload
    def pow_(self, exponent: Number | _complex) -> Tensor: ...
    def prelu(self, weight: Tensor) -> Tensor: ...
    @overload
    def prod(self, *, dtype: _dtype | None = ...) -> Tensor: ...
    @overload
    def prod(self, dim: _int, keepdim: _bool = ..., *, dtype: _dtype | None = ...) -> Tensor: ...
    @overload
    def prod(self, dim: str | EllipsisType | None, keepdim: _bool = ..., *, dtype: _dtype | None = ...) -> Tensor: ...
    def put(self, index: Tensor, source: Tensor, accumulate: _bool = ...) -> Tensor: ...
    def put_(self, index: Tensor, source: Tensor, accumulate: _bool = ...) -> Tensor: ...
    def q_per_channel_axis(self) -> _int: ...
    def q_per_channel_scales(self) -> Tensor: ...
    def q_per_channel_zero_points(self) -> Tensor: ...
    def q_scale(self) -> _float: ...
    def q_zero_point(self) -> _int: ...
    def qr(self, some: _bool = ...) -> torch.return_types.qr: ...
    def qscheme(self) -> _qscheme: ...
    @overload
    def quantile(
        self, q: Tensor, dim: _int | None = ..., keepdim: _bool = ..., *, interpolation: str = ...
    ) -> Tensor: ...
    @overload
    def quantile(
        self, q: _float, dim: _int | None = ..., keepdim: _bool = ..., *, interpolation: str = ...
    ) -> Tensor: ...
    def rad2deg(self) -> Tensor: ...
    def rad2deg_(self) -> Tensor: ...
    @overload
    def random_(self, *, generator: Generator | None = ...) -> Tensor: ...
    @overload
    def random_(self, from_: _int, to: _int | None, *, generator: Generator | None = ...) -> Tensor: ...
    @overload
    def random_(self, to: _int, *, generator: Generator | None = ...) -> Tensor: ...
    def ravel(self) -> Tensor: ...
    def reciprocal(self) -> Tensor: ...
    def reciprocal_(self) -> Tensor: ...
    def record_stream(self, s: Stream) -> None: ...
    def refine_names(self, names: Sequence[str | EllipsisType | None]) -> Tensor: ...
    def relu(self) -> Tensor: ...
    def relu_(self) -> Tensor: ...
    @overload
    def remainder(self, other: Tensor) -> Tensor: ...
    @overload
    def remainder(self, other: Number | _complex) -> Tensor: ...
    @overload
    def remainder_(self, other: Tensor) -> Tensor: ...
    @overload
    def remainder_(self, other: Number | _complex) -> Tensor: ...
    def rename(self, names: Sequence[str | EllipsisType | None] | None) -> Tensor: ...
    def rename_(self, names: Sequence[str | EllipsisType | None] | None) -> Tensor: ...
    def renorm(self, p: Number | _complex, dim: _int, maxnorm: Number | _complex) -> Tensor: ...
    def renorm_(self, p: Number | _complex, dim: _int, maxnorm: Number | _complex) -> Tensor: ...
    @overload
    def repeat(self, repeats: Sequence[_int | SymInt]) -> Tensor: ...
    @overload
    def repeat(self, *repeats: _int | SymInt) -> Tensor: ...
    @overload
    def repeat_interleave(
        self, repeats: Tensor, dim: _int | None = ..., *, output_size: _int | SymInt | None = ...
    ) -> Tensor: ...
    @overload
    def repeat_interleave(
        self, repeats: _int | SymInt, dim: _int | None = ..., *, output_size: _int | SymInt | None = ...
    ) -> Tensor: ...
    def requires_grad_(self, mode: _bool = ...) -> Tensor: ...
    @overload
    def reshape(self, shape: Sequence[_int | SymInt]) -> Tensor: ...
    @overload
    def reshape(self, *shape: _int | SymInt) -> Tensor: ...
    def reshape_as(self, other: Tensor) -> Tensor: ...
    @overload
    def resize_(self, size: Sequence[_int | SymInt], *, memory_format: memory_format | None = ...) -> Tensor: ...
    @overload
    def resize_(self, *size: _int | SymInt, memory_format: memory_format | None = ...) -> Tensor: ...
    def resize_as_(self, the_template: Tensor, *, memory_format: memory_format | None = ...) -> Tensor: ...
    def resize_as_sparse_(self, the_template: Tensor) -> Tensor: ...
    def resolve_conj(self) -> Tensor: ...
    def resolve_neg(self) -> Tensor: ...
    def retain_grad(self) -> None: ...
    def roll(self, shifts: _int | SymInt | Sequence[_int | SymInt], dims: _int | _size = ...) -> Tensor: ...
    def rot90(self, k: _int = ..., dims: _size = ...) -> Tensor: ...
    @overload
    def round(self) -> Tensor: ...
    @overload
    def round(self, *, decimals: _int) -> Tensor: ...
    @overload
    def round_(self) -> Tensor: ...
    @overload
    def round_(self, *, decimals: _int) -> Tensor: ...
    def row_indices(self) -> Tensor: ...
    def rsqrt(self) -> Tensor: ...
    def rsqrt_(self) -> Tensor: ...
    @overload
    def scatter(self, dim: _int, index: Tensor, src: Tensor) -> Tensor: ...
    @overload
    def scatter(self, dim: _int, index: Tensor, src: Tensor, *, reduce: str) -> Tensor: ...
    @overload
    def scatter(self, dim: _int, index: Tensor, value: Number | _complex, *, reduce: str) -> Tensor: ...
    @overload
    def scatter(self, dim: str | EllipsisType | None, index: Tensor, src: Tensor) -> Tensor: ...
    @overload
    def scatter(self, dim: _int, index: Tensor, value: Number | _complex) -> Tensor: ...
    @overload
    def scatter(self, dim: str | EllipsisType | None, index: Tensor, value: Number | _complex) -> Tensor: ...
    @overload
    def scatter_(self, dim: _int, index: Tensor, src: Tensor) -> Tensor: ...
    @overload
    def scatter_(self, dim: _int, index: Tensor, src: Tensor, *, reduce: str) -> Tensor: ...
    @overload
    def scatter_(self, dim: _int, index: Tensor, value: Number | _complex, *, reduce: str) -> Tensor: ...
    @overload
    def scatter_(self, dim: _int, index: Tensor, value: Number | _complex) -> Tensor: ...
    @overload
    def scatter_add(self, dim: _int, index: Tensor, src: Tensor) -> Tensor: ...
    @overload
    def scatter_add(self, dim: str | EllipsisType | None, index: Tensor, src: Tensor) -> Tensor: ...
    def scatter_add_(self, dim: _int, index: Tensor, src: Tensor) -> Tensor: ...
    def scatter_reduce(
        self, dim: _int, index: Tensor, src: Tensor, reduce: str, *, include_self: _bool = ...
    ) -> Tensor: ...
    def scatter_reduce_(
        self, dim: _int, index: Tensor, src: Tensor, reduce: str, *, include_self: _bool = ...
    ) -> Tensor: ...
    @overload
    def select(self, dim: _int, index: _int | SymInt) -> Tensor: ...
    @overload
    def select(self, dim: str | EllipsisType | None, index: _int) -> Tensor: ...
    def select_scatter(self, src: Tensor, dim: _int, index: _int | SymInt) -> Tensor: ...
    @overload
    def set_(
        self,
        source: Storage | TypedStorage | UntypedStorage,
        storage_offset: IntLikeType,
        size: _symsize,
        stride: _symsize,
    ) -> Tensor: ...
    @overload
    def set_(self, source: Storage | TypedStorage | UntypedStorage) -> Tensor: ...
    def sgn(self) -> Tensor: ...
    def sgn_(self) -> Tensor: ...
    def short(self) -> Tensor: ...
    def sigmoid(self) -> Tensor: ...
    def sigmoid_(self) -> Tensor: ...
    def sign(self) -> Tensor: ...
    def sign_(self) -> Tensor: ...
    def signbit(self) -> Tensor: ...
    def sin(self) -> Tensor: ...
    def sin_(self) -> Tensor: ...
    def sinc(self) -> Tensor: ...
    def sinc_(self) -> Tensor: ...
    def sinh(self) -> Tensor: ...
    def sinh_(self) -> Tensor: ...
    @overload
    def size(self, dim: None = ...) -> Size: ...
    @overload
    def size(self, dim: _int) -> _int: ...
    def slice_inverse(
        self,
        src: Tensor,
        dim: _int = ...,
        start: _int | SymInt | None = ...,
        end: _int | SymInt | None = ...,
        step: _int | SymInt = ...,
    ) -> Tensor: ...
    def slice_scatter(
        self,
        src: Tensor,
        dim: _int = ...,
        start: _int | SymInt | None = ...,
        end: _int | SymInt | None = ...,
        step: _int | SymInt = ...,
    ) -> Tensor: ...
    def slogdet(self) -> torch.return_types.slogdet: ...
    def smm(self, mat2: Tensor) -> Tensor: ...
    @overload
    def softmax(self, dim: _int, dtype: _dtype | None = ...) -> Tensor: ...
    @overload
    def softmax(self, dim: str | EllipsisType | None, *, dtype: _dtype | None = ...) -> Tensor: ...
    @overload
    def sort(self, *, stable: _bool | None, dim: _int = ..., descending: _bool = ...) -> torch.return_types.sort: ...
    @overload
    def sort(self, dim: _int = ..., descending: _bool = ...) -> torch.return_types.sort: ...
    @overload
    def sort(
        self, *, stable: _bool | None, dim: str | EllipsisType | None, descending: _bool = ...
    ) -> torch.return_types.sort: ...
    @overload
    def sort(self, dim: str | EllipsisType | None, descending: _bool = ...) -> torch.return_types.sort: ...
    def sparse_dim(self) -> _int: ...
    def sparse_mask(self, mask: Tensor) -> Tensor: ...
    def sparse_resize_(self, size: _size, sparse_dim: _int, dense_dim: _int) -> Tensor: ...
    def sparse_resize_and_clear_(self, size: _size, sparse_dim: _int, dense_dim: _int) -> Tensor: ...
    @overload
    def split(self, split_size: _int, dim: _int = ...) -> Sequence[Tensor]: ...
    @overload
    def split(self, split_size: tuple[_int, ...], dim: _int = ...) -> Sequence[Tensor]: ...
    def split_with_sizes(self, split_sizes: Sequence[_int | SymInt], dim: _int = ...) -> tuple[Tensor, ...]: ...
    def sqrt(self) -> Tensor: ...
    def sqrt_(self) -> Tensor: ...
    def square(self) -> Tensor: ...
    def square_(self) -> Tensor: ...
    @overload
    def squeeze(self) -> Tensor: ...
    @overload
    def squeeze(self, dim: _int) -> Tensor: ...
    @overload
    def squeeze(self, dim: _size) -> Tensor: ...
    @overload
    def squeeze(self, *dim: _int) -> Tensor: ...
    @overload
    def squeeze(self, dim: str | EllipsisType | None) -> Tensor: ...
    @overload
    def squeeze_(self) -> Tensor: ...
    @overload
    def squeeze_(self, dim: _int) -> Tensor: ...
    @overload
    def squeeze_(self, dim: _size) -> Tensor: ...
    @overload
    def squeeze_(self, *dim: _int) -> Tensor: ...
    @overload
    def squeeze_(self, dim: str | EllipsisType | None) -> Tensor: ...
    def sspaddmm(
        self, mat1: Tensor, mat2: Tensor, *, beta: Number | _complex = ..., alpha: Number | _complex = ...
    ) -> Tensor: ...
    @overload
    def std(self, dim: _int | _size | None, unbiased: _bool = ..., keepdim: _bool = ...) -> Tensor: ...
    @overload
    def std(
        self, dim: _int | _size | None = ..., *, correction: Number | _complex | None = ..., keepdim: _bool = ...
    ) -> Tensor: ...
    @overload
    def std(self, unbiased: _bool = ...) -> Tensor: ...
    @overload
    def std(self, dim: Sequence[str | EllipsisType | None], unbiased: _bool = ..., keepdim: _bool = ...) -> Tensor: ...
    @overload
    def std(
        self,
        dim: Sequence[str | EllipsisType | None],
        *,
        correction: Number | _complex | None = ...,
        keepdim: _bool = ...,
    ) -> Tensor: ...
    def untyped_storage(self) -> UntypedStorage: ...
    def storage_offset(self) -> _int | SymInt: ...
    def storage_type(self) -> Storage: ...
    @overload
    def stride(self, dim: None = ...) -> tuple[_int, ...]: ...
    @overload
    def stride(self, dim: _int) -> _int: ...
    def sub(
        self,
        other: Tensor | Number | _complex | torch.SymInt | torch.SymFloat,
        *,
        alpha: Number | _complex | None = ...,
        out: Tensor | None = ...,
    ) -> Tensor: ...
    def sub_(
        self,
        other: Tensor | Number | _complex | torch.SymInt | torch.SymFloat,
        *,
        alpha: Number | _complex | None = ...,
    ) -> Tensor: ...
    @overload
    def subtract(self, other: Tensor, *, alpha: Number | _complex = ...) -> Tensor: ...
    @overload
    def subtract(self, other: Number | _complex, alpha: Number | _complex = ...) -> Tensor: ...
    @overload
    def subtract_(self, other: Tensor, *, alpha: Number | _complex = ...) -> Tensor: ...
    @overload
    def subtract_(self, other: Number | _complex, alpha: Number | _complex = ...) -> Tensor: ...
    @overload
    def sum(self, *, dtype: _dtype | None = ...) -> Tensor: ...
    @overload
    def sum(self, dim: _int | _size | None, keepdim: _bool = ..., *, dtype: _dtype | None = ...) -> Tensor: ...
    @overload
    def sum(
        self, dim: Sequence[str | EllipsisType | None], keepdim: _bool = ..., *, dtype: _dtype | None = ...
    ) -> Tensor: ...
    @overload
    def sum_to_size(self, size: Sequence[_int | SymInt]) -> Tensor: ...
    @overload
    def sum_to_size(self, *size: _int | SymInt) -> Tensor: ...
    def svd(self, some: _bool = ..., compute_uv: _bool = ...) -> torch.return_types.svd: ...
    def swapaxes(self, axis0: _int, axis1: _int) -> Tensor: ...
    def swapaxes_(self, axis0: _int, axis1: _int) -> Tensor: ...
    def swapdims(self, dim0: _int, dim1: _int) -> Tensor: ...
    def swapdims_(self, dim0: _int, dim1: _int) -> Tensor: ...
    def t(self) -> Tensor: ...
    def t_(self) -> Tensor: ...
    def take(self, index: Tensor) -> Tensor: ...
    def take_along_dim(self, indices: Tensor, dim: _int | None = ...) -> Tensor: ...
    def tan(self) -> Tensor: ...
    def tan_(self) -> Tensor: ...
    def tanh(self) -> Tensor: ...
    def tanh_(self) -> Tensor: ...
    @overload
    def tensor_split(self, indices: Sequence[_int | SymInt], dim: _int = ...) -> tuple[Tensor, ...]: ...
    @overload
    def tensor_split(self, tensor_indices_or_sections: Tensor, dim: _int = ...) -> tuple[Tensor, ...]: ...
    @overload
    def tensor_split(self, sections: _int | SymInt, dim: _int = ...) -> tuple[Tensor, ...]: ...
    @overload
    def tile(self, dims: Sequence[_int | SymInt]) -> Tensor: ...
    @overload
    def tile(self, *dims: _int | SymInt) -> Tensor: ...
    @overload
    def to(
        self,
        dtype: _dtype,
        non_blocking: _bool = ...,
        copy: _bool = ...,
        *,
        memory_format: torch.memory_format | None = ...,
    ) -> Tensor: ...
    @overload
    def to(
        self,
        device: DeviceLikeType | None = ...,
        dtype: _dtype | None = ...,
        non_blocking: _bool = ...,
        copy: _bool = ...,
        *,
        memory_format: torch.memory_format | None = ...,
    ) -> Tensor: ...
    @overload
    def to(
        self,
        other: Tensor,
        non_blocking: _bool = ...,
        copy: _bool = ...,
        *,
        memory_format: torch.memory_format | None = ...,
    ) -> Tensor: ...
    def to_dense(self, dtype: _dtype | None = ..., *, masked_grad: _bool | None = ...) -> Tensor: ...
    def to_mkldnn(self, dtype: _dtype | None = ...) -> Tensor: ...
    def to_padded_tensor(self, padding: _float, output_size: Sequence[_int | SymInt] | None = ...) -> Tensor: ...
    @overload
    def to_sparse(
        self, *, layout: _layout | None = ..., blocksize: _int | _size | None = ..., dense_dim: _int | None = ...
    ) -> Tensor: ...
    @overload
    def to_sparse(self, sparse_dim: _int) -> Tensor: ...
    def to_sparse_bsc(self, blocksize: _int | _size, dense_dim: _int | None = ...) -> Tensor: ...
    def to_sparse_bsr(self, blocksize: _int | _size, dense_dim: _int | None = ...) -> Tensor: ...
    def to_sparse_csc(self, dense_dim: _int | None = ...) -> Tensor: ...
    def to_sparse_csr(self, dense_dim: _int | None = ...) -> Tensor: ...
    def tolist(self) -> list[Number]: ...
    def topk(
        self, k: _int | SymInt, dim: _int = ..., largest: _bool = ..., sorted: _bool = ...
    ) -> torch.return_types.topk: ...
    def trace(self) -> Tensor: ...
    @overload
    def transpose(self, dim0: _int, dim1: _int) -> Tensor: ...
    @overload
    def transpose(self, dim0: str | EllipsisType | None, dim1: str | EllipsisType | None) -> Tensor: ...
    def transpose_(self, dim0: _int, dim1: _int) -> Tensor: ...
    def triangular_solve(
        self, A: Tensor, upper: _bool = ..., transpose: _bool = ..., unitriangular: _bool = ...
    ) -> torch.return_types.triangular_solve: ...
    def tril(self, diagonal: _int = ...) -> Tensor: ...
    def tril_(self, diagonal: _int = ...) -> Tensor: ...
    def triu(self, diagonal: _int = ...) -> Tensor: ...
    def triu_(self, diagonal: _int = ...) -> Tensor: ...
    def true_divide(
        self, other: Tensor | Number | torch.SymInt | torch.SymFloat, *, out: Tensor | None = ...
    ) -> Tensor: ...
    def true_divide_(self, other: Tensor | Number | torch.SymInt | torch.SymFloat) -> Tensor: ...
    def trunc(self) -> Tensor: ...
    def trunc_(self) -> Tensor: ...
    @overload
    def type(self, dtype: None = ..., non_blocking: _bool = ...) -> str: ...
    @overload
    def type(self, dtype: str | _dtype, non_blocking: _bool = ...) -> Tensor: ...
    def type_as(self, other: Tensor) -> Tensor: ...
    @overload
    def unbind(self, dim: _int = ...) -> tuple[Tensor, ...]: ...
    @overload
    def unbind(self, dim: str | EllipsisType | None) -> tuple[Tensor, ...]: ...
    @overload
    def unflatten(
        self, dim: str | EllipsisType | None, sizes: Sequence[_int | SymInt], names: Sequence[str | EllipsisType | None]
    ) -> Tensor: ...
    @overload
    def unflatten(self, dim: _int, sizes: Sequence[_int | SymInt]) -> Tensor: ...
    def unfold(self, dimension: _int, size: _int, step: _int) -> Tensor: ...
    def uniform_(self, from_: _float = ..., to: _float = ..., *, generator: Generator | None = ...) -> Tensor: ...
    def unsafe_chunk(self, chunks: _int, dim: _int = ...) -> tuple[Tensor, ...]: ...
    def unsafe_split(self, split_size: _int | SymInt, dim: _int = ...) -> tuple[Tensor, ...]: ...
    def unsafe_split_with_sizes(self, split_sizes: Sequence[_int | SymInt], dim: _int = ...) -> tuple[Tensor, ...]: ...
    def unsqueeze(self, dim: _int) -> Tensor: ...
    def unsqueeze_(self, dim: _int) -> Tensor: ...
    def values(self) -> Tensor: ...
    @overload
    def var(self, dim: _int | _size | None, unbiased: _bool = ..., keepdim: _bool = ...) -> Tensor: ...
    @overload
    def var(
        self, dim: _int | _size | None = ..., *, correction: Number | _complex | None = ..., keepdim: _bool = ...
    ) -> Tensor: ...
    @overload
    def var(self, unbiased: _bool = ...) -> Tensor: ...
    @overload
    def var(self, dim: Sequence[str | EllipsisType | None], unbiased: _bool = ..., keepdim: _bool = ...) -> Tensor: ...
    @overload
    def var(
        self,
        dim: Sequence[str | EllipsisType | None],
        *,
        correction: Number | _complex | None = ...,
        keepdim: _bool = ...,
    ) -> Tensor: ...
    def vdot(self, other: Tensor) -> Tensor: ...
    @overload
    def view(self, dtype: _dtype) -> Tensor: ...
    @overload
    def view(self, size: Sequence[_int | SymInt]) -> Tensor: ...
    @overload
    def view(self, *size: _int | SymInt) -> Tensor: ...
    def view_as(self, other: Tensor) -> Tensor: ...
    @overload
    def vsplit(self, sections: _int) -> tuple[Tensor, ...]: ...
    @overload
    def vsplit(self, indices: _size) -> tuple[Tensor, ...]: ...
    @overload
    def vsplit(self, *indices: _int) -> tuple[Tensor, ...]: ...
    @overload
    def where(self, condition: Tensor, other: Tensor) -> Tensor: ...
    @overload
    def where(self, condition: Tensor, other: Number | _complex) -> Tensor: ...
    @overload
    def xlogy(self, other: Tensor) -> Tensor: ...
    @overload
    def xlogy(self, other: Number | _complex) -> Tensor: ...
    @overload
    def xlogy_(self, other: Tensor) -> Tensor: ...
    @overload
    def xlogy_(self, other: Number | _complex) -> Tensor: ...
    def xpu(
        self,
        device: _device | _int | str | None = ...,
        non_blocking: _bool = ...,
        memory_format: torch.memory_format = ...,
    ) -> Tensor: ...
    def zero_(self) -> Tensor: ...

_TensorBase = TensorBase

class _cuda_CUDAAllocator_AllocatorState: ...
class _cuda_CUDAAllocator: ...

class _CudaDeviceProperties:
    name: str
    major: _int
    minor: _int
    multi_processor_count: _int
    total_memory: _int
    is_integrated: _int
    is_multi_gpu_board: _int
    max_threads_per_multi_processor: _int
    gcnArchName: str
    warp_size: _int
    uuid: str
    L2_cache_size: _int

class _SDPAParams:
    query: Tensor
    key: Tensor
    value: Tensor
    attn_mask: Tensor | None
    dropout: _float
    is_causal: _bool
    enable_gqa: _bool
    def __init__(
        self,
        query: Tensor,
        key: Tensor,
        value: Tensor,
        attn_mask: Tensor | None,
        dropout: _float,
        is_causal: _bool,
        enable_gqa: _bool,
    ) -> None: ...

class _SDPBackend(Enum):
    ERROR = ...
    MATH = ...
    FLASH_ATTENTION = ...
    EFFICIENT_ATTENTION = ...
    CUDNN_ATTENTION = ...
    OVERRIDEABLE = ...

class _CudaStreamBase(Stream):
    stream_id: _int
    device_index: _int
    device_type: _int
    device: _device
    cuda_stream: _int
    priority: _int
    def __new__(
        cls, priority: _int = ..., stream_id: _int = ..., device_index: _int = ..., stream_ptr: _int = ...
    ) -> Self: ...
    def query(self) -> _bool: ...
    def synchronize(self) -> None: ...
    def priority_range(self) -> tuple[_int, _int]: ...

class _CudaEventBase:
    device: _device
    cuda_event: _int
    def __new__(
        cls, enable_timing: _bool = ..., blocking: _bool = ..., interprocess: _bool = ..., external: _bool = ...
    ) -> Self: ...
    @classmethod
    def from_ipc_handle(cls, device: _device, ipc_handle: bytes) -> _CudaEventBase: ...
    def record(self, stream: _CudaStreamBase) -> None: ...
    def wait(self, stream: _CudaStreamBase) -> None: ...
    def query(self) -> _bool: ...
    def elapsed_time(self, other: _CudaEventBase) -> _float: ...
    def synchronize(self) -> None: ...
    def ipc_handle(self) -> bytes: ...

class _CUDAGraph:
    def __new__(cls, keep_graph: _bool = ...) -> Self: ...
    def capture_begin(self, pool: _POOL_HANDLE | None = ..., capture_error_mode: str = ...) -> None: ...
    def capture_end(self) -> None: ...
    def instantiate(self) -> None: ...
    def register_generator_state(self, Generator) -> None: ...
    def replay(self) -> None: ...
    def reset(self) -> None: ...
    def pool(self) -> _POOL_HANDLE: ...
    def enable_debug_mode(self) -> None: ...
    def debug_dump(self, debug_path: str) -> None: ...
    def raw_cuda_graph(self) -> _int: ...
    def raw_cuda_graph_exec(self) -> _int: ...

class _MemPool:
    def __init__(
        self, allocator: _cuda_CUDAAllocator | None = ..., is_user_created: _bool = ..., use_on_oom: _bool = ...
    ) -> None: ...
    @property
    def id(self) -> tuple[_int, _int]: ...
    @property
    def allocator(self) -> _cuda_CUDAAllocator | None: ...
    def use_count(self) -> _int: ...

class _XpuDeviceProperties:
    name: str
    platform_name: str
    vendor: str
    device_id: _int
    driver_version: str
    version: str
    max_compute_units: _int
    gpu_eu_count: _int
    max_work_group_size: _int
    max_num_sub_groups: _int
    sub_group_sizes: list[_int]
    has_fp16: _bool
    has_fp64: _bool
    has_atomic64: _bool
    has_bfloat16_conversions: _bool
    has_subgroup_matrix_multiply_accumulate: _bool
    has_subgroup_matrix_multiply_accumulate_tensor_float32: _bool
    has_subgroup_2d_block_io: _bool
    total_memory: _int
    gpu_subslice_count: _int
    architecture: _int
    type: str
    uuid: Any

class _XpuStreamBase(Stream):
    stream_id: _int
    device_index: _int
    device_type: _int
    device: _device
    sycl_queue: _int
    priority: _int
    def __new__(
        cls, priority: _int = ..., stream_id: _int = ..., device_index: _int = ..., device_type: _int = ...
    ) -> Self: ...
    def query(self) -> _bool: ...
    def synchronize(self) -> None: ...
    @staticmethod
    def priority_range() -> tuple: ...

class _XpuEventBase:
    device: _device
    sycl_event: _int
    def __new__(cls, enable_timing: _bool = ...) -> Self: ...
    def record(self, stream: _XpuEventBase) -> None: ...
    def wait(self, stream: _XpuStreamBase) -> None: ...
    def query(self) -> _bool: ...
    def elapsed_time(self, other: _XpuEventBase) -> _float: ...
    def synchronize(self) -> None: ...

class TracingState:
    def push_scope(self, scope_name: str) -> None: ...
    def pop_scope(self) -> None: ...
    def current_scope(self) -> str: ...
    def set_graph(self, graph: Graph) -> None: ...
    def graph(self) -> Graph: ...

class IValue: ...

type Stack = list[IValue]

class JitType:
    annotation_str: str
    def isSubtypeOf(self, other: JitType) -> _bool: ...
    def with_dtype(self, dtype: _dtype) -> JitType: ...
    def with_sizes(self, sizes: list[_int | None]) -> JitType: ...
    def kind(self) -> str: ...
    def scalarType(self) -> str | None: ...
    def getElementType(self) -> JitType: ...
    def dtype(self) -> _dtype | None: ...

class InferredType:
    def __init__(self, arg: JitType | str) -> None: ...
    def type(self) -> JitType: ...
    def success(self) -> _bool: ...
    def reason(self) -> str: ...

class Type(JitType):
    def str(self) -> _str: ...
    def containedTypes(self) -> list[JitType]: ...
    def dim(self) -> _int | None: ...
    def undefined(self) -> _bool | None: ...
    def sizes(self) -> list[_int] | None: ...
    def symbol_sizes(self) -> list[_int] | None: ...
    def varyingSizes(self) -> list[_int | None] | None: ...
    def strides(self) -> list[_int] | None: ...
    def contiguous(self) -> Self: ...
    def device(self) -> _device | None: ...
    def is_interface_type(self) -> _bool: ...
    def requires_grad(self) -> _bool: ...
    @property
    def annotation_string(self) -> _str: ...

class AnyType(JitType):
    @staticmethod
    def get() -> AnyType: ...

class NoneType(JitType):
    @staticmethod
    def get() -> NoneType: ...

class BoolType(JitType):
    @staticmethod
    def get() -> BoolType: ...

class FloatType(JitType):
    @staticmethod
    def get() -> FloatType: ...

class ComplexType(JitType):
    @staticmethod
    def get() -> ComplexType: ...

class IntType(JitType):
    @staticmethod
    def get() -> IntType: ...

class SymIntType(JitType):
    @staticmethod
    def get() -> SymIntType: ...

class SymBoolType(JitType):
    @staticmethod
    def get() -> SymBoolType: ...

class NumberType(JitType):
    @staticmethod
    def get() -> NumberType: ...

class StringType(JitType):
    @staticmethod
    def get() -> StringType: ...

class DeviceObjType(JitType):
    @staticmethod
    def get() -> DeviceObjType: ...

class _GeneratorType(JitType):
    @staticmethod
    def get() -> _GeneratorType: ...

class StreamObjType(JitType):
    @staticmethod
    def get() -> StreamObjType: ...

class ListType(JitType):
    def __init__(self, a: JitType) -> None: ...
    def getElementType(self) -> JitType: ...
    @staticmethod
    def ofInts() -> ListType: ...
    @staticmethod
    def ofTensors() -> ListType: ...
    @staticmethod
    def ofFloats() -> ListType: ...
    @staticmethod
    def ofComplexDoubles() -> ListType: ...
    @staticmethod
    def ofBools() -> ListType: ...
    @staticmethod
    def ofStrings() -> ListType: ...

class DictType(JitType):
    def __init__(self, key: JitType, value: JitType) -> None: ...
    def getKeyType(self) -> JitType: ...
    def getValueType(self) -> JitType: ...

class TupleType(JitType):
    def __init__(self, a: list[JitType | None]) -> None: ...
    def elements(self) -> list[JitType]: ...

class UnionType(JitType):
    def __init__(self, a: list[JitType]) -> None: ...

class ClassType(JitType):
    def __init__(self, qualified_name: str) -> None: ...
    def qualified_name(self) -> str: ...

class InterfaceType(JitType):
    def __init__(self, qualified_name: str) -> None: ...
    def getMethod(self, name: str) -> FunctionSchema | None: ...
    def getMethodNames(self) -> list[str]: ...

JitTypeT = TypeVar("JitTypeT", bound=JitType)

class OptionalType[JitTypeT: JitType](JitType):
    def __init__(self, a: JitTypeT) -> None: ...
    def getElementType(self) -> JitTypeT: ...
    @staticmethod
    def ofTensor() -> OptionalType: ...

class FutureType(JitType):
    def __init__(self, a: JitType) -> None: ...
    def getElementType(self) -> JitType: ...

class AwaitType(JitType):
    def __init__(self, a: JitType) -> None: ...
    def getElementType(self) -> JitType: ...

class RRefType(JitType):
    def __init__(self, a: JitType) -> None: ...

class EnumType(JitType):
    def __init__(self, qualified_name: str, value_type: JitType, enum_names_values: list[Any]) -> None: ...

class TensorType(JitType):
    @classmethod
    def get(cls) -> TensorType: ...
    @classmethod
    def getInferred(cls) -> TensorType: ...
    def with_sizes(self, other: list[_int | None] | None) -> TensorType: ...
    def sizes(self) -> list[_int] | None: ...
    def varyingSizes(self) -> list[_int | None] | None: ...
    def strides(self) -> list[_int] | None: ...
    def device(self) -> _device | None: ...
    def dim(self) -> _int: ...
    def dtype(self) -> _dtype | None: ...
    @staticmethod
    def create_from_tensor(t: Tensor) -> TensorType: ...

class SourceRange: ...
class TreeView: ...

class Ident(TreeView):
    @property
    def name(self) -> str: ...

class ClassDef(TreeView): ...

class Def(TreeView):
    def name(self) -> Ident: ...

class Decl(TreeView): ...
class AcceleratorError(RuntimeError): ...
class OutOfMemoryError(RuntimeError): ...
class _DistError(RuntimeError): ...
class _DistBackendError(RuntimeError): ...
class _DistStoreError(RuntimeError): ...
class _DistNetworkError(RuntimeError): ...
class _DistQueueEmptyError(_DistStoreError): ...
class CapturedTraceback: ...

def gather_traceback(python: _bool, script: _bool, cpp: _bool) -> CapturedTraceback: ...
def symbolize_tracebacks(tracebacks: list[CapturedTraceback]) -> list[dict[str, Any]]: ...

class _NodeBase:
    _erased: _bool
    _prev: FxNode
    _next: FxNode
    def __init__(self, graph: Any, name: str, op: str, target: Any, return_type: Any) -> None: ...

class _NodeIter(Iterator[FxNode]):
    def __init__(self, root: FxNode, reversed: _bool) -> None: ...
    def __iter__(self) -> Self: ...
    def __next__(self) -> FxNode: ...

class _StaticCudaLauncher: ...
