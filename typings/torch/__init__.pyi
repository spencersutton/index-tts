import builtins
import functools
import os
import platform
import sys
from collections.abc import Callable as Callable
from typing import TYPE_CHECKING
from typing import Any as Any
from typing import TypeIs as _TypeIs
from typing import overload as overload

import torch
from torch import _C as _C
from torch import _VF as _VF
from torch import __config__ as __config__
from torch import __future__ as __future__
from torch import _awaits as _awaits
from torch import _dynamo as _dynamo
from torch import _inductor as _inductor
from torch import _library as _library
from torch import _ops as _ops
from torch import _subclasses as _subclasses
from torch import accelerator as accelerator
from torch import amp as amp
from torch import ao as ao
from torch import autograd as autograd
from torch import backends as backends
from torch import compiler as compiler
from torch import cpu as cpu
from torch import cuda as cuda
from torch import distributed as distributed
from torch import distributions as distributions
from torch import export as export
from torch import fft as fft
from torch import func as func
from torch import functional as functional
from torch import futures as futures
from torch import fx as fx
from torch import hub as hub
from torch import jit as jit
from torch import library as library
from torch import linalg as linalg
from torch import masked as masked
from torch import mps as mps
from torch import mtia as mtia
from torch import multiprocessing as multiprocessing
from torch import nested as nested
from torch import nn as nn
from torch import onnx as onnx
from torch import optim as optim
from torch import overrides as overrides
from torch import profiler as profiler
from torch import quantization as quantization
from torch import quasirandom as quasirandom
from torch import random as random
from torch import return_types as return_types
from torch import serialization as serialization
from torch import sparse as sparse
from torch import special as special
from torch import storage as storage
from torch import testing as testing
from torch import types as types
from torch import utils as utils
from torch import version as version
from torch import xpu as xpu
from torch._C import (
    BoolTensor,
    ByteTensor,
    DoubleTensor,
    FloatTensor,
    IntTensor,
    LongTensor,
    ShortTensor,
    bool,
    device,
    dtype,
    finfo,
    float,
    float16,
    float32,
    float64,
    half,
    iinfo,
    int,
    int16,
    int32,
    int64,
    long,
    short,
    uint8,
    void,
)
from torch._C._VariableFunctions import (
    __and__,
    __lshift__,
    __or__,
    __rshift__,
    __xor__,
    abs,
    abs_,
    absolute,
    acos,
    acos_,
    acosh,
    acosh_,
    adaptive_avg_pool1d,
    adaptive_max_pool1d,
    add,
    addbmm,
    addcdiv,
    addcmul,
    addmm,
    addmv,
    addmv_,
    addr,
    adjoint,
    affine_grid_generator,
    alias_copy,
    all,
    allclose,
    alpha_dropout,
    alpha_dropout_,
    amax,
    amin,
    aminmax,
    angle,
    any,
    arange,
    arccos,
    arccos_,
    arccosh,
    arccosh_,
    arcsin,
    arcsin_,
    arcsinh,
    arcsinh_,
    arctan,
    arctan2,
    arctan_,
    arctanh,
    arctanh_,
    argmax,
    argmin,
    argsort,
    argwhere,
    as_strided,
    as_strided_,
    as_strided_copy,
    as_strided_scatter,
    as_tensor,
    asarray,
    asin,
    asin_,
    asinh,
    asinh_,
    atan,
    atan2,
    atan_,
    atanh,
    atanh_,
    avg_pool1d,
    baddbmm,
    bartlett_window,
    batch_norm,
    batch_norm_backward_elemt,
    batch_norm_backward_reduce,
    batch_norm_elemt,
    batch_norm_gather_stats,
    batch_norm_gather_stats_with_counts,
    batch_norm_stats,
    batch_norm_update_stats,
    bernoulli,
    bilinear,
    binary_cross_entropy_with_logits,
    bincount,
    binomial,
    bitwise_and,
    bitwise_left_shift,
    bitwise_not,
    bitwise_or,
    bitwise_right_shift,
    bitwise_xor,
    blackman_window,
    bmm,
    broadcast_to,
    bucketize,
    can_cast,
    cat,
    ccol_indices_copy,
    ceil,
    ceil_,
    celu,
    celu_,
    channel_shuffle,
    cholesky,
    cholesky_inverse,
    cholesky_solve,
    choose_qparams_optimized,
    chunk,
    clamp,
    clamp_,
    clamp_max,
    clamp_max_,
    clamp_min,
    clamp_min_,
    clip,
    clip_,
    clone,
    col_indices_copy,
    column_stack,
    combinations,
    complex,
    concat,
    concatenate,
    conj,
    conj_physical,
    conj_physical_,
    constant_pad_nd,
    conv1d,
    conv2d,
    conv3d,
    conv_tbc,
    conv_transpose1d,
    conv_transpose2d,
    conv_transpose3d,
    convolution,
    copysign,
    corrcoef,
    cos,
    cos_,
    cosh,
    cosh_,
    cosine_embedding_loss,
    cosine_similarity,
    count_nonzero,
    cov,
    cross,
    crow_indices_copy,
    ctc_loss,
    cudnn_affine_grid_generator,
    cudnn_batch_norm,
    cudnn_convolution,
    cudnn_convolution_add_relu,
    cudnn_convolution_relu,
    cudnn_convolution_transpose,
    cudnn_grid_sampler,
    cudnn_is_acceptable,
    cummax,
    cummin,
    cumprod,
    cumsum,
    cumulative_trapezoid,
    deg2rad,
    deg2rad_,
    dequantize,
    det,
    detach,
    detach_,
    detach_copy,
    diag,
    diag_embed,
    diagflat,
    diagonal,
    diagonal_copy,
    diagonal_scatter,
    diff,
    digamma,
    dist,
    div,
    divide,
    dot,
    dropout,
    dropout_,
    dsmm,
    dsplit,
    dstack,
    embedding,
    embedding_bag,
    embedding_renorm_,
    empty,
    empty_like,
    empty_permuted,
    empty_quantized,
    empty_strided,
    eq,
    equal,
    erf,
    erf_,
    erfc,
    erfc_,
    erfinv,
    exp,
    exp2,
    exp2_,
    exp_,
    expand_copy,
    expm1,
    expm1_,
    eye,
    fake_quantize_per_channel_affine,
    fake_quantize_per_tensor_affine,
    fbgemm_linear_fp16_weight,
    fbgemm_linear_fp16_weight_fp32_activation,
    fbgemm_linear_int8_weight,
    fbgemm_linear_int8_weight_fp32_activation,
    fbgemm_linear_quantize_weight,
    fbgemm_pack_gemm_matrix_fp16,
    fbgemm_pack_quantized_matrix,
    feature_alpha_dropout,
    feature_alpha_dropout_,
    feature_dropout,
    feature_dropout_,
    fill,
    fill_,
    fix,
    fix_,
    flatten,
    flip,
    fliplr,
    flipud,
    float_power,
    floor,
    floor_,
    floor_divide,
    fmax,
    fmin,
    fmod,
    frac,
    frac_,
    frexp,
    frobenius_norm,
    from_file,
    from_numpy,
    frombuffer,
    full,
    full_like,
    fused_moving_avg_obs_fake_quant,
    gather,
    gcd,
    gcd_,
    ge,
    geqrf,
    ger,
    get_default_dtype,
    get_num_interop_threads,
    get_num_threads,
    gradient,
    greater,
    greater_equal,
    grid_sampler,
    grid_sampler_2d,
    grid_sampler_3d,
    group_norm,
    gru,
    gru_cell,
    gt,
    hamming_window,
    hann_window,
    hardshrink,
    hash_tensor,
    heaviside,
    hinge_embedding_loss,
    histc,
    histogram,
    histogramdd,
    hsmm,
    hsplit,
    hspmm,
    hstack,
    hypot,
    i0,
    i0_,
    igamma,
    igammac,
    imag,
    index_add,
    index_copy,
    index_fill,
    index_put,
    index_put_,
    index_reduce,
    index_select,
    indices_copy,
    init_num_threads,
    inner,
    instance_norm,
    int_repr,
    inverse,
    is_complex,
    is_conj,
    is_distributed,
    is_floating_point,
    is_grad_enabled,
    is_inference,
    is_inference_mode_enabled,
    is_neg,
    is_nonzero,
    is_same_size,
    is_signed,
    is_vulkan_available,
    isclose,
    isfinite,
    isin,
    isinf,
    isnan,
    isneginf,
    isposinf,
    isreal,
    istft,
    kaiser_window,
    kl_div,
    kron,
    kthvalue,
    layer_norm,
    lcm,
    lcm_,
    ldexp,
    ldexp_,
    le,
    lerp,
    less,
    less_equal,
    lgamma,
    linspace,
    log,
    log1p,
    log1p_,
    log2,
    log2_,
    log10,
    log10_,
    log_,
    log_softmax,
    logaddexp,
    logaddexp2,
    logcumsumexp,
    logdet,
    logical_and,
    logical_not,
    logical_or,
    logical_xor,
    logit,
    logit_,
    logspace,
    logsumexp,
    lstm,
    lstm_cell,
    lt,
    lu_solve,
    lu_unpack,
    margin_ranking_loss,
    masked_fill,
    masked_scatter,
    masked_select,
    matmul,
    matrix_exp,
    matrix_power,
    max,
    max_pool1d,
    max_pool1d_with_indices,
    max_pool2d,
    max_pool3d,
    maximum,
    mean,
    median,
    min,
    minimum,
    miopen_batch_norm,
    miopen_convolution,
    miopen_convolution_add_relu,
    miopen_convolution_relu,
    miopen_convolution_transpose,
    miopen_depthwise_convolution,
    miopen_rnn,
    mkldnn_adaptive_avg_pool2d,
    mkldnn_convolution,
    mkldnn_linear_backward_weights,
    mkldnn_max_pool2d,
    mkldnn_max_pool3d,
    mkldnn_rnn_layer,
    mm,
    mode,
    moveaxis,
    movedim,
    msort,
    mul,
    multinomial,
    multiply,
    mv,
    mvlgamma,
    nan_to_num,
    nan_to_num_,
    nanmean,
    nanmedian,
    nanquantile,
    nansum,
    narrow,
    narrow_copy,
    native_batch_norm,
    native_channel_shuffle,
    native_dropout,
    native_group_norm,
    native_layer_norm,
    native_norm,
    ne,
    neg,
    neg_,
    negative,
    negative_,
    nextafter,
    nonzero,
    nonzero_static,
    norm_except_dim,
    normal,
    not_equal,
    nuclear_norm,
    numel,
    ones,
    ones_like,
    orgqr,
    ormqr,
    outer,
    pairwise_distance,
    pdist,
    permute,
    permute_copy,
    pinverse,
    pixel_shuffle,
    pixel_unshuffle,
    poisson,
    poisson_nll_loss,
    polar,
    polygamma,
    positive,
    pow,
    prelu,
    prod,
    promote_types,
    put,
    q_per_channel_axis,
    q_per_channel_scales,
    q_per_channel_zero_points,
    q_scale,
    q_zero_point,
    qr,
    quantile,
    quantize_per_channel,
    quantize_per_tensor,
    quantize_per_tensor_dynamic,
    quantized_batch_norm,
    quantized_gru_cell,
    quantized_lstm_cell,
    quantized_max_pool1d,
    quantized_max_pool2d,
    quantized_max_pool3d,
    quantized_rnn_relu_cell,
    quantized_rnn_tanh_cell,
    rad2deg,
    rad2deg_,
    rand,
    rand_like,
    randint,
    randint_like,
    randn,
    randn_like,
    randperm,
    range,
    ravel,
    real,
    reciprocal,
    reciprocal_,
    relu,
    relu_,
    remainder,
    renorm,
    repeat_interleave,
    reshape,
    resize_as_,
    resize_as_sparse_,
    resolve_conj,
    resolve_neg,
    result_type,
    rms_norm,
    rnn_relu,
    rnn_relu_cell,
    rnn_tanh,
    rnn_tanh_cell,
    roll,
    rot90,
    round,
    round_,
    row_indices_copy,
    row_stack,
    rrelu,
    rrelu_,
    rsqrt,
    rsqrt_,
    rsub,
    saddmm,
    scalar_tensor,
    scatter,
    scatter_add,
    scatter_reduce,
    searchsorted,
    segment_reduce,
    select,
    select_copy,
    select_scatter,
    selu,
    selu_,
    set_flush_denormal,
    set_num_interop_threads,
    set_num_threads,
    sgn,
    sigmoid,
    sigmoid_,
    sign,
    signbit,
    sin,
    sin_,
    sinc,
    sinc_,
    sinh,
    sinh_,
    slice_copy,
    slice_inverse,
    slice_scatter,
    slogdet,
    smm,
    softmax,
    sort,
    sparse_bsc_tensor,
    sparse_bsr_tensor,
    sparse_compressed_tensor,
    sparse_coo_tensor,
    sparse_csc_tensor,
    sparse_csr_tensor,
    split_copy,
    split_with_sizes,
    split_with_sizes_copy,
    spmm,
    sqrt,
    sqrt_,
    square,
    square_,
    squeeze,
    squeeze_copy,
    sspaddmm,
    stack,
    std,
    std_mean,
    sub,
    subtract,
    sum,
    svd,
    swapaxes,
    swapdims,
    sym_constrain_range,
    sym_constrain_range_for_size,
    t,
    t_copy,
    take,
    take_along_dim,
    tan,
    tan_,
    tanh,
    tanh_,
    tensor,
    tensor_split,
    threshold,
    threshold_,
    tile,
    topk,
    trace,
    transpose,
    transpose_copy,
    trapezoid,
    trapz,
    triangular_solve,
    tril,
    tril_indices,
    triplet_margin_loss,
    triu,
    triu_indices,
    true_divide,
    trunc,
    trunc_,
    unbind,
    unbind_copy,
    unflatten,
    unfold_copy,
    unique_dim,
    unsafe_chunk,
    unsafe_split,
    unsafe_split_with_sizes,
    unsqueeze,
    unsqueeze_copy,
    values_copy,
    vander,
    var,
    var_mean,
    vdot,
    view_as_complex,
    view_as_complex_copy,
    view_as_real,
    view_as_real_copy,
    view_copy,
    vsplit,
    vstack,
    where,
    xlogy,
    xlogy_,
    zero_,
    zeros,
    zeros_like,
)
from torch._classes import classes as classes
from torch._higher_order_ops import cond as cond
from torch._higher_order_ops import while_loop as while_loop
from torch._lobpcg import lobpcg as lobpcg
from torch._ops import ops as ops
from torch._tensor import Tensor
from torch._tensor_str import set_printoptions
from torch._utils import classproperty
from torch._utils_internal import USE_RTLD_GLOBAL_WITH_LIBTORCH
from torch.amp import GradScaler, autocast
from torch.autograd import enable_grad as enable_grad
from torch.autograd import inference_mode as inference_mode
from torch.autograd import no_grad as no_grad
from torch.autograd import set_grad_enabled as set_grad_enabled
from torch.func import vmap as vmap
from torch.functional import *
from torch.random import get_rng_state, initial_seed, manual_seed, seed, set_rng_state
from torch.serialization import load, save
from torch.signal import windows as windows
from torch.storage import TypedStorage, UntypedStorage, _LegacyStorage
from torch.torch_version import __version__ as __version__
from torch.types import Device, IntLikeType

__all__ = [
    "BoolStorage",
    "BoolTensor",
    "ByteStorage",
    "ByteTensor",
    "CharStorage",
    "CharTensor",
    "DoubleStorage",
    "DoubleTensor",
    "FloatStorage",
    "FloatTensor",
    "GradScaler",
    "IntStorage",
    "IntTensor",
    "LongStorage",
    "LongTensor",
    "ShortStorage",
    "ShortTensor",
    "SymBool",
    "SymFloat",
    "SymInt",
    "Tensor",
    "TypedStorage",
    "UntypedStorage",
    "__and__",
    "__lshift__",
    "__or__",
    "__rshift__",
    "__xor__",
    "abs",
    "abs_",
    "absolute",
    "acos",
    "acos_",
    "acosh",
    "acosh_",
    "adaptive_avg_pool1d",
    "adaptive_max_pool1d",
    "add",
    "addbmm",
    "addcdiv",
    "addcmul",
    "addmm",
    "addmv",
    "addmv_",
    "addr",
    "adjoint",
    "affine_grid_generator",
    "alias_copy",
    "all",
    "allclose",
    "alpha_dropout",
    "alpha_dropout_",
    "amax",
    "amin",
    "aminmax",
    "angle",
    "any",
    "arange",
    "arccos",
    "arccos_",
    "arccosh",
    "arccosh_",
    "arcsin",
    "arcsin_",
    "arcsinh",
    "arcsinh_",
    "arctan",
    "arctan2",
    "arctan_",
    "arctanh",
    "arctanh_",
    "are_deterministic_algorithms_enabled",
    "argmax",
    "argmin",
    "argsort",
    "argwhere",
    "as_strided",
    "as_strided_",
    "as_strided_copy",
    "as_strided_scatter",
    "as_tensor",
    "asarray",
    "asin",
    "asin_",
    "asinh",
    "asinh_",
    "atan",
    "atan2",
    "atan_",
    "atanh",
    "atanh_",
    "autocast",
    "avg_pool1d",
    "baddbmm",
    "bartlett_window",
    "batch_norm",
    "batch_norm_backward_elemt",
    "batch_norm_backward_reduce",
    "batch_norm_elemt",
    "batch_norm_gather_stats",
    "batch_norm_gather_stats_with_counts",
    "batch_norm_stats",
    "batch_norm_update_stats",
    "bernoulli",
    "bilinear",
    "binary_cross_entropy_with_logits",
    "bincount",
    "binomial",
    "bitwise_and",
    "bitwise_left_shift",
    "bitwise_not",
    "bitwise_or",
    "bitwise_right_shift",
    "bitwise_xor",
    "blackman_window",
    "bmm",
    "bool",
    "broadcast_to",
    "bucketize",
    "can_cast",
    "cat",
    "ccol_indices_copy",
    "ceil",
    "ceil_",
    "celu",
    "celu_",
    "channel_shuffle",
    "cholesky",
    "cholesky_inverse",
    "cholesky_solve",
    "choose_qparams_optimized",
    "chunk",
    "chunk",
    "clamp",
    "clamp_",
    "clamp_max",
    "clamp_max_",
    "clamp_min",
    "clamp_min_",
    "clip",
    "clip_",
    "clone",
    "col_indices_copy",
    "column_stack",
    "combinations",
    "compile",
    "complex",
    "concat",
    "concatenate",
    "cond",
    "conj",
    "conj_physical",
    "conj_physical_",
    "constant_pad_nd",
    "conv1d",
    "conv2d",
    "conv3d",
    "conv_tbc",
    "conv_transpose1d",
    "conv_transpose2d",
    "conv_transpose3d",
    "convolution",
    "copysign",
    "corrcoef",
    "cos",
    "cos_",
    "cosh",
    "cosh_",
    "cosine_embedding_loss",
    "cosine_similarity",
    "count_nonzero",
    "cov",
    "cross",
    "crow_indices_copy",
    "ctc_loss",
    "cudnn_affine_grid_generator",
    "cudnn_batch_norm",
    "cudnn_convolution",
    "cudnn_convolution_add_relu",
    "cudnn_convolution_relu",
    "cudnn_convolution_transpose",
    "cudnn_grid_sampler",
    "cudnn_is_acceptable",
    "cummax",
    "cummin",
    "cumprod",
    "cumsum",
    "cumulative_trapezoid",
    "deg2rad",
    "deg2rad_",
    "dequantize",
    "det",
    "detach",
    "detach_",
    "detach_copy",
    "device",
    "device",
    "diag",
    "diag_embed",
    "diagflat",
    "diagonal",
    "diagonal_copy",
    "diagonal_scatter",
    "diff",
    "digamma",
    "dist",
    "div",
    "divide",
    "dot",
    "dropout",
    "dropout_",
    "dsmm",
    "dsplit",
    "dstack",
    "dtype",
    "dtype",
    "embedding",
    "embedding_bag",
    "embedding_renorm_",
    "empty",
    "empty_like",
    "empty_permuted",
    "empty_quantized",
    "empty_strided",
    "enable_grad",
    "eq",
    "equal",
    "erf",
    "erf_",
    "erfc",
    "erfc_",
    "erfinv",
    "exp",
    "exp2",
    "exp2_",
    "exp_",
    "expand_copy",
    "expm1",
    "expm1_",
    "export",
    "eye",
    "fake_quantize_per_channel_affine",
    "fake_quantize_per_tensor_affine",
    "fbgemm_linear_fp16_weight",
    "fbgemm_linear_fp16_weight_fp32_activation",
    "fbgemm_linear_int8_weight",
    "fbgemm_linear_int8_weight_fp32_activation",
    "fbgemm_linear_quantize_weight",
    "fbgemm_pack_gemm_matrix_fp16",
    "fbgemm_pack_quantized_matrix",
    "feature_alpha_dropout",
    "feature_alpha_dropout_",
    "feature_dropout",
    "feature_dropout_",
    "fill",
    "fill_",
    "finfo",
    "fix",
    "fix_",
    "flatten",
    "flip",
    "fliplr",
    "flipud",
    "float",
    "float16",
    "float32",
    "float64",
    "float_power",
    "floor",
    "floor_",
    "floor_divide",
    "fmax",
    "fmin",
    "fmod",
    "frac",
    "frac_",
    "frexp",
    "frobenius_norm",
    "from_file",
    "from_numpy",
    "frombuffer",
    "full",
    "full_like",
    "fused_moving_avg_obs_fake_quant",
    "gather",
    "gcd",
    "gcd_",
    "ge",
    "geqrf",
    "ger",
    "get_default_device",
    "get_default_dtype",
    "get_deterministic_debug_mode",
    "get_device_module",
    "get_float32_matmul_precision",
    "get_num_interop_threads",
    "get_num_threads",
    "get_rng_state",
    "gradient",
    "greater",
    "greater_equal",
    "grid_sampler",
    "grid_sampler_2d",
    "grid_sampler_3d",
    "group_norm",
    "gru",
    "gru_cell",
    "gt",
    "half",
    "hamming_window",
    "hann_window",
    "hardshrink",
    "hash_tensor",
    "heaviside",
    "hinge_embedding_loss",
    "histc",
    "histogram",
    "histogramdd",
    "hsmm",
    "hsplit",
    "hspmm",
    "hstack",
    "hypot",
    "i0",
    "i0_",
    "igamma",
    "igammac",
    "iinfo",
    "imag",
    "index_add",
    "index_copy",
    "index_fill",
    "index_put",
    "index_put_",
    "index_reduce",
    "index_select",
    "indices_copy",
    "inference_mode",
    "init_num_threads",
    "initial_seed",
    "inner",
    "instance_norm",
    "int",
    "int16",
    "int32",
    "int64",
    "int_repr",
    "inverse",
    "is_complex",
    "is_conj",
    "is_deterministic_algorithms_warn_only_enabled",
    "is_distributed",
    "is_floating_point",
    "is_grad_enabled",
    "is_inference",
    "is_inference_mode_enabled",
    "is_neg",
    "is_nonzero",
    "is_same_size",
    "is_signed",
    "is_storage",
    "is_tensor",
    "is_vulkan_available",
    "is_warn_always_enabled",
    "isclose",
    "isfinite",
    "isin",
    "isinf",
    "isnan",
    "isneginf",
    "isposinf",
    "isreal",
    "istft",
    "kaiser_window",
    "kl_div",
    "kron",
    "kthvalue",
    "layer_norm",
    "lcm",
    "lcm_",
    "ldexp",
    "ldexp_",
    "le",
    "lerp",
    "less",
    "less_equal",
    "lgamma",
    "linspace",
    "load",
    "lobpcg",
    "log",
    "log1p",
    "log1p_",
    "log2",
    "log2_",
    "log10",
    "log10_",
    "log_",
    "log_softmax",
    "logaddexp",
    "logaddexp2",
    "logcumsumexp",
    "logdet",
    "logical_and",
    "logical_not",
    "logical_or",
    "logical_xor",
    "logit",
    "logit_",
    "logspace",
    "logsumexp",
    "long",
    "lstm",
    "lstm_cell",
    "lt",
    "lu_solve",
    "lu_unpack",
    "manual_seed",
    "margin_ranking_loss",
    "masked_fill",
    "masked_scatter",
    "masked_select",
    "matmul",
    "matmul",
    "matrix_exp",
    "matrix_power",
    "max",
    "max_pool1d",
    "max_pool1d_with_indices",
    "max_pool2d",
    "max_pool3d",
    "maximum",
    "mean",
    "median",
    "min",
    "minimum",
    "miopen_batch_norm",
    "miopen_convolution",
    "miopen_convolution_add_relu",
    "miopen_convolution_relu",
    "miopen_convolution_transpose",
    "miopen_depthwise_convolution",
    "miopen_rnn",
    "mkldnn_adaptive_avg_pool2d",
    "mkldnn_convolution",
    "mkldnn_linear_backward_weights",
    "mkldnn_max_pool2d",
    "mkldnn_max_pool3d",
    "mkldnn_rnn_layer",
    "mm",
    "mode",
    "moveaxis",
    "movedim",
    "msort",
    "mul",
    "multinomial",
    "multiply",
    "mv",
    "mvlgamma",
    "nan_to_num",
    "nan_to_num_",
    "nanmean",
    "nanmedian",
    "nanquantile",
    "nansum",
    "narrow",
    "narrow_copy",
    "native_batch_norm",
    "native_channel_shuffle",
    "native_dropout",
    "native_group_norm",
    "native_layer_norm",
    "native_norm",
    "ne",
    "neg",
    "neg_",
    "negative",
    "negative_",
    "nextafter",
    "no_grad",
    "nonzero",
    "nonzero_static",
    "norm_except_dim",
    "normal",
    "not_equal",
    "nuclear_norm",
    "numel",
    "ones",
    "ones_like",
    "orgqr",
    "ormqr",
    "outer",
    "pairwise_distance",
    "pdist",
    "permute",
    "permute_copy",
    "pinverse",
    "pixel_shuffle",
    "pixel_unshuffle",
    "poisson",
    "poisson_nll_loss",
    "polar",
    "polygamma",
    "positive",
    "pow",
    "prelu",
    "prod",
    "promote_types",
    "put",
    "q_per_channel_axis",
    "q_per_channel_scales",
    "q_per_channel_zero_points",
    "q_scale",
    "q_zero_point",
    "qr",
    "quantile",
    "quantize_per_channel",
    "quantize_per_tensor",
    "quantize_per_tensor_dynamic",
    "quantized_batch_norm",
    "quantized_gru_cell",
    "quantized_lstm_cell",
    "quantized_max_pool1d",
    "quantized_max_pool2d",
    "quantized_max_pool3d",
    "quantized_rnn_relu_cell",
    "quantized_rnn_tanh_cell",
    "rad2deg",
    "rad2deg_",
    "rand",
    "rand",
    "rand_like",
    "randint",
    "randint_like",
    "randn",
    "randn",
    "randn_like",
    "randperm",
    "range",
    "ravel",
    "real",
    "reciprocal",
    "reciprocal_",
    "relu",
    "relu_",
    "remainder",
    "renorm",
    "repeat_interleave",
    "reshape",
    "resize_as_",
    "resize_as_sparse_",
    "resolve_conj",
    "resolve_neg",
    "result_type",
    "rms_norm",
    "rnn_relu",
    "rnn_relu_cell",
    "rnn_tanh",
    "rnn_tanh_cell",
    "roll",
    "rot90",
    "round",
    "round_",
    "row_indices_copy",
    "row_stack",
    "rrelu",
    "rrelu_",
    "rsqrt",
    "rsqrt_",
    "rsub",
    "saddmm",
    "save",
    "scalar_tensor",
    "scatter",
    "scatter_add",
    "scatter_reduce",
    "searchsorted",
    "seed",
    "segment_reduce",
    "select",
    "select_copy",
    "select_scatter",
    "selu",
    "selu_",
    "set_default_device",
    "set_default_tensor_type",
    "set_deterministic_debug_mode",
    "set_float32_matmul_precision",
    "set_flush_denormal",
    "set_num_interop_threads",
    "set_num_threads",
    "set_printoptions",
    "set_rng_state",
    "set_warn_always",
    "sgn",
    "short",
    "sigmoid",
    "sigmoid_",
    "sign",
    "signbit",
    "sin",
    "sin_",
    "sinc",
    "sinc_",
    "sinh",
    "sinh_",
    "slice_copy",
    "slice_inverse",
    "slice_scatter",
    "slogdet",
    "smm",
    "softmax",
    "sort",
    "sparse_bsc_tensor",
    "sparse_bsr_tensor",
    "sparse_compressed_tensor",
    "sparse_coo_tensor",
    "sparse_csc_tensor",
    "sparse_csr_tensor",
    "split",
    "split_copy",
    "split_with_sizes",
    "split_with_sizes_copy",
    "spmm",
    "sqrt",
    "sqrt_",
    "square",
    "square_",
    "squeeze",
    "squeeze_copy",
    "sspaddmm",
    "stack",
    "stack",
    "std",
    "std_mean",
    "sub",
    "subtract",
    "sum",
    "svd",
    "swapaxes",
    "swapdims",
    "sym_constrain_range",
    "sym_constrain_range_for_size",
    "sym_float",
    "sym_fresh_size",
    "sym_int",
    "sym_ite",
    "sym_max",
    "sym_min",
    "sym_not",
    "sym_sum",
    "t",
    "t_copy",
    "take",
    "take_along_dim",
    "tan",
    "tan_",
    "tanh",
    "tanh_",
    "tensor",
    "tensor_split",
    "threshold",
    "threshold_",
    "tile",
    "topk",
    "trace",
    "transpose",
    "transpose_copy",
    "trapezoid",
    "trapz",
    "triangular_solve",
    "tril",
    "tril_indices",
    "triplet_margin_loss",
    "triu",
    "triu_indices",
    "true_divide",
    "trunc",
    "trunc_",
    "typename",
    "uint8",
    "unbind",
    "unbind_copy",
    "unflatten",
    "unfold_copy",
    "unique_dim",
    "unravel_index",
    "unsafe_chunk",
    "unsafe_split",
    "unsafe_split_with_sizes",
    "unsqueeze",
    "unsqueeze_copy",
    "use_deterministic_algorithms",
    "values_copy",
    "vander",
    "var",
    "var_mean",
    "vdot",
    "view_as_complex",
    "view_as_complex_copy",
    "view_as_real",
    "view_as_real_copy",
    "view_copy",
    "vmap",
    "void",
    "vsplit",
    "vstack",
    "where",
    "xlogy",
    "xlogy_",
    "zero_",
    "zeros",
    "zeros_like",
]
if sys.platform == "win32": ...
if (USE_RTLD_GLOBAL_WITH_LIBTORCH or os.getenv("TORCH_USE_RTLD_GLOBAL")) and (platform.system() != "Windows"):
    old_flags = ...

class SymInt:
    def __init__(self, node) -> None: ...
    def __bool__(self) -> bool: ...
    def __int__(self) -> int: ...
    def __index__(self) -> int: ...
    def __round__(self, ndigits=...) -> Self: ...
    def __truediv__(self, other) -> SymFloat | Any | _NotImplementedType: ...
    def __rtruediv__(self, other) -> SymFloat | Any | _NotImplementedType: ...
    def __floordiv__(self, other) -> Any | SymFloat | float | _NotImplementedType: ...
    def __rfloordiv__(self, other) -> Any | SymFloat | float | _NotImplementedType: ...
    def __pow__(self, other) -> _NotImplementedType | SymFloat | Any | SymInt: ...
    def __rpow__(self, other) -> _NotImplementedType | SymFloat | Any | SymInt: ...
    def __eq__(self, other: object) -> builtins.bool: ...
    def __lt__(self, other) -> builtins.bool: ...
    def __gt__(self, other) -> builtins.bool: ...
    def __le__(self, other) -> builtins.bool: ...
    def __ge__(self, other) -> builtins.bool: ...
    def __add__(self, other) -> SymInt: ...
    def __radd__(self, other) -> SymInt: ...
    def __rmul__(self, other) -> SymInt: ...
    def __mod__(self, other: IntLikeType) -> SymInt: ...
    def __mul__(self, other) -> SymInt: ...
    def __pow_by_natural__(self, other) -> SymInt: ...
    def __rpow_by_natural__(self, other) -> SymInt: ...
    def __int_truediv__(self, other) -> SymFloat: ...
    def __rint_truediv__(self, other) -> SymFloat: ...
    def __int_floordiv__(self, other) -> SymFloat: ...
    def __rint_floordiv__(self, other) -> SymFloat: ...
    def __sym_max__(self, other): ...
    def __sym_min__(self, other): ...
    def __sym_float__(self): ...
    def __neg__(self): ...
    def __sub__(self, other: IntLikeType) -> SymInt: ...
    def __rsub__(self, other: IntLikeType) -> SymInt: ...
    def __and__(self, other) -> SymInt: ...
    def __or__(self, other) -> SymInt: ...
    def __hash__(self) -> builtins.int: ...
    def as_integer_ratio(self) -> tuple[SymInt, builtins.int]: ...
    def bit_length(self) -> builtins.int: ...
    def conjugate(self) -> SymInt: ...

class SymFloat:
    def __init__(self, node) -> None: ...
    def __truediv__(self, other) -> _NotImplementedType | SymFloat: ...
    def __rtruediv__(self, other) -> _NotImplementedType | SymFloat: ...
    def __floordiv__(self, other) -> _NotImplementedType | Any | SymFloat | float: ...
    def __rfloordiv__(self, other) -> _NotImplementedType | Any | SymFloat | float: ...
    def __bool__(self) -> bool: ...
    def __float__(self) -> float: ...
    def __pow__(self, other) -> _NotImplementedType | SymFloat: ...
    def __rpow__(self, other) -> _NotImplementedType | SymFloat: ...
    def __eq__(self, other: object) -> builtins.bool: ...
    def __lt__(self, other) -> builtins.bool: ...
    def __gt__(self, other) -> builtins.bool: ...
    def __le__(self, other) -> builtins.bool: ...
    def __ge__(self, other) -> builtins.bool: ...
    def __float_pow__(self, other) -> SymFloat: ...
    def __rfloat_pow__(self, other) -> SymFloat: ...
    def __float_truediv__(self, other) -> SymFloat: ...
    def __rfloat_truediv__(self, other) -> SymFloat: ...
    def __trunc__(self): ...
    def __sym_max__(self, other): ...
    def __sym_min__(self, other): ...
    def __sym_int__(self): ...
    def is_integer(self): ...
    def as_integer_ratio(self) -> tuple[builtins.int, builtins.int]: ...
    def __hash__(self) -> int: ...
    def conjugate(self) -> SymFloat: ...
    def hex(self) -> str: ...

class SymBool:
    def __init__(self, node) -> None: ...
    def __bool__(self) -> bool: ...
    def __int__(self) -> int: ...
    def __and__(self, other) -> SymBool: ...
    def __or__(self, other) -> SymBool: ...
    def __sym_not__(self) -> SymBool: ...
    def __sym_ite__(self, then_val, else_val): ...
    def __eq__(self, other) -> builtins.bool: ...
    def __hash__(self) -> int: ...

def sym_not(a) -> Any | bool: ...
def sym_float(a) -> Any | SymFloat | float: ...
def sym_int(a) -> Any | SymInt | int: ...
def sym_max(a, b) -> Any | float: ...
def sym_min(a, b) -> Any | float: ...
def sym_sum(args) -> Any | int | float | bool | SymInt | SymFloat | SymBool: ...

sym_sqrt = ...

def sym_ite(b, t, f) -> Any: ...
def sym_fresh_size(expr) -> Number: ...

if not TYPE_CHECKING: ...

def typename(obj: Any, /) -> str: ...
def is_tensor(obj: Any, /) -> _TypeIs[torch.Tensor]: ...
def is_storage(obj: Any, /) -> _TypeIs[TypedStorage | UntypedStorage]: ...

_GLOBAL_DEVICE_CONTEXT = ...

def get_default_device() -> torch.device: ...
def set_default_device(device: Device) -> None: ...
def set_default_tensor_type(t: type[torch.Tensor] | str, /) -> None: ...
def set_default_dtype(d: torch.dtype, /) -> None: ...
def use_deterministic_algorithms(mode: builtins.bool, *, warn_only: builtins.bool = ...) -> None: ...
def are_deterministic_algorithms_enabled() -> builtins.bool: ...
def is_deterministic_algorithms_warn_only_enabled() -> builtins.bool: ...
def set_deterministic_debug_mode(debug_mode: builtins.int | str) -> None: ...
def get_deterministic_debug_mode() -> builtins.int: ...
def get_float32_matmul_precision() -> str: ...
def set_float32_matmul_precision(precision: str) -> None: ...
def set_warn_always(b: builtins.bool, /) -> None: ...
def is_warn_always_enabled() -> builtins.bool: ...

newaxis: None = ...

class ByteStorage(_LegacyStorage):
    @classproperty
    def dtype(self): ...

class DoubleStorage(_LegacyStorage):
    @classproperty
    def dtype(self): ...

class FloatStorage(_LegacyStorage):
    @classproperty
    def dtype(self): ...

class HalfStorage(_LegacyStorage):
    @classproperty
    def dtype(self): ...

class LongStorage(_LegacyStorage):
    @classproperty
    def dtype(self): ...

class IntStorage(_LegacyStorage):
    @classproperty
    def dtype(self): ...

class ShortStorage(_LegacyStorage):
    @classproperty
    def dtype(self): ...

class CharStorage(_LegacyStorage):
    @classproperty
    def dtype(self): ...

class BoolStorage(_LegacyStorage):
    @classproperty
    def dtype(self): ...

class BFloat16Storage(_LegacyStorage):
    @classproperty
    def dtype(self): ...

class ComplexDoubleStorage(_LegacyStorage):
    @classproperty
    def dtype(self): ...

class ComplexFloatStorage(_LegacyStorage):
    @classproperty
    def dtype(self): ...

class QUInt8Storage(_LegacyStorage):
    @classproperty
    def dtype(self): ...

class QInt8Storage(_LegacyStorage):
    @classproperty
    def dtype(self): ...

class QInt32Storage(_LegacyStorage):
    @classproperty
    def dtype(self): ...

class QUInt4x2Storage(_LegacyStorage):
    @classproperty
    def dtype(self): ...

class QUInt2x4Storage(_LegacyStorage):
    @classproperty
    def dtype(self): ...

_storage_classes: set[type[TypedStorage | UntypedStorage]] = ...
_tensor_classes: set[type[torch.Tensor]] = ...
if TYPE_CHECKING:
    _segment_reduce = ...
PRIVATE_OPS = ...

def compiled_with_cxx11_abi() -> builtins.bool: ...

legacy_contiguous_format = ...
quantized_lstm = ...
quantized_gru = ...

class _TorchCompileInductorWrapper:
    compiler_name = ...
    def __init__(self, mode, options, dynamic) -> None: ...
    def __eq__(self, other) -> bool: ...
    def apply_mode(self, mode: str | None) -> None: ...
    def apply_options(self, options: dict[str, Any] | None) -> None: ...
    def __call__(self, model_, inputs_) -> Callable[[list[object]], Sequence[Tensor]] | str | list[str] | Weights: ...
    def get_compiler_config(self) -> dict[str, Any]: ...
    def reset(self) -> None: ...

class _TorchCompileWrapper:
    def __init__(self, backend, mode, options, dynamic) -> None: ...
    def __eq__(self, other) -> bool: ...
    def __call__(self, model_, inputs_) -> CompiledFn: ...
    def reset(self) -> None: ...

@overload
def compile[**InputT, RetT](
    model: Callable[InputT, RetT],
    *,
    fullgraph: builtins.bool = ...,
    dynamic: builtins.bool | None = ...,
    backend: str | Callable[..., Any] = ...,
    mode: str | None = ...,
    options: dict[str, str | builtins.int | builtins.bool | Callable[..., Any]] | None = ...,
    disable: builtins.bool = ...,
) -> Callable[InputT, RetT]: ...
@overload
def compile[**InputT, RetT](
    model: None = ...,
    *,
    fullgraph: builtins.bool = ...,
    dynamic: builtins.bool | None = ...,
    backend: str | Callable[..., Any] = ...,
    mode: str | None = ...,
    options: dict[str, str | builtins.int | builtins.bool | Callable[..., Any]] | None = ...,
    disable: builtins.bool = ...,
) -> Callable[[Callable[InputT, RetT]], Callable[InputT, RetT]]: ...
def compile[**InputT, RetT](
    model: Callable[InputT, RetT] | None = ...,
    *,
    fullgraph: builtins.bool = ...,
    dynamic: builtins.bool | None = ...,
    backend: str | Callable[..., Any] = ...,
    mode: str | None = ...,
    options: dict[str, str | builtins.int | builtins.bool | Callable[..., Any]] | None = ...,
    disable: builtins.bool = ...,
) -> Callable[[Callable[InputT, RetT]], Callable[InputT, RetT]] | Callable[InputT, RetT]: ...

if not TYPE_CHECKING: ...
if "TORCH_CUDA_SANITIZER" in os.environ: ...

class _TritonLibrary:
    lib = ...
    ops_table: dict[tuple[str, str], Callable] = ...
    @classmethod
    def registerOp(cls, op_key, full_schema, op_impl, dispatch_key) -> Callable[..., Any]: ...

_deprecated_attrs = ...

@functools.cache
def get_device_module(device: torch.device | str | None = ...) -> Any: ...

if _is_device_backend_autoload_enabled(): ...
