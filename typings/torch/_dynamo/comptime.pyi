"""
This module provides the public comptime interface to TorchDynamo, enabling users to execute
arbitrary Python code during symbolic evaluation of their programs.

The comptime interface allows inspection and modification of TorchDynamo's compilation
process while it is running. This can be useful for:

- Debugging compilation issues
- Inspecting intermediate state
- Adding custom guards or graph breaks
- Analyzing symbolic shapes and values

Example usage:

    import torch
    from torch._dynamo.comptime import comptime

    def my_model(x):
        # Print the compile-time known information about x
        comptime.print(x)

        # Print the current FX graph being constructed
        comptime.print_graph()

        # Force a value to be treated as static
        if comptime(lambda ctx: ctx.get_local("x").is_dynamic()):
            comptime.force_static(x)

        # Add a manual graph break
        comptime.graph_break()

Note: While this API provides significant flexibility, it intentionally avoids
exposing internal implementation details of TorchDynamo to maintain compatibility
across versions.
"""

from collections.abc import Callable, Sequence
from typing import Any, TextIO

import torch
from torch._dynamo.symbolic_convert import InstructionTranslatorBase
from torch._dynamo.variables.base import VariableTracker
from torch._subclasses.fake_tensor import FakeTensor

class ComptimeVar:
    """
    A ComptimeVar represents a Python value, at some particular point
    in time, in the Python code we are symbolically evaluating with
    torchdynamo.  This must be distinguished from a runtime value, as
    at compile-time there are some properties of the variable we
    do not know (for example, if the ComptimeVar represents a Tensor,
    we only know metadata about the tensor; we do NOT know what the
    actual data in the Tensor is.)
    """
    def __init__(self, v: VariableTracker) -> None: ...
    def as_proxy(self) -> VariableTracker | Sequence[VariableTracker]:
        """
        Returns an fx.Proxy (or tuple/list of fx.Proxy) representing
        this variable in the FX graph we are assembling to pass
        to the user compiler.

        This method only works for variables we actually track in
        the FX graph, aka Tensors (and ints, if you are compiling
        with dynamic shapes).  In particular, if you have a list
        or tuple of tensors, you will get a list/tuple of proxies
        (not a single proxy representing the entire list/tuple).
        """
    def is_proxy(self) -> bool:
        """Returns True if as_proxy() would succeed."""
    def as_fake(self) -> FakeTensor | torch.SymInt:
        """
        Returns a "fake" value (either a FakeTensor or a SymInt)
        representing the variable in question.  This only works
        for variables that denote Tensor or int.  You can use
        this to query metadata; e.g., v.as_fake().size(0) will
        tell you the compile-time known size of the tensor.

        WARNING: Do NOT mutate the returned tensor.
        """
    def size(self, dim: int | None = ...) -> int | torch.SymInt:
        """
        Returns the size of the tensor (if dim is None) or the size
        at the dimension dim.  The returned size may be a SymInt.
        """
    def python_type(self) -> type:
        """
        Returns what type(v) would have returned for the variable
        at compile time.
        """
    def as_python_constant(self) -> Any:
        """
        Returns the Python value this variable would have, but only if it is
        completely known at compile-time (e.g., it is constant).

        WARNING: Do NOT mutate the returned constant.  The returned constant
        may or may not correspond to the actual value this variable may take
        on at runtime; for example, if the variable in question is a constant
        list, we may return a copy of that list.
        """
    def is_python_constant(self) -> bool:
        """Returns True if as_python_constant would succeed."""
    def is_dynamic(self) -> bool: ...
    def force_static(self) -> None:
        """Forces that a value is static, inducing a guard on its specific value"""

class ComptimeContext:
    """
    This context class provides access to a public API for Dynamo's internals.
    If there is something here you would find useful that is missing, please
    file a feature request at https://github.com/pytorch/pytorch/
    """
    def __init__(self, tx: InstructionTranslatorBase) -> None: ...
    def get_local(self, name: str, *, stacklevel: int = ...) -> ComptimeVar:
        """Retrieve the compile-time known information about a local."""
    def graph_break(self, msg: str = ...) -> None:
        """Manually trigger a graph break"""
    def graph(self) -> torch.fx.Graph:
        """
        Retrieve the partially constructed FX graph that would be
        passed to the user compiler after compilation.
        """
    def assert_static(self, val: ComptimeVar) -> None:
        """Asserts that the int is static (and not dynamic, per dynamic shapes)"""
    def print_graph(self, *, verbose: bool = ..., file: TextIO | None = ...) -> None:
        """
        Print the partially constructed FX graph that would be passed
        to the user compiler after compilation.
        """
    def parent(self) -> ComptimeContext: ...
    def print(self, val: Any, *, file: TextIO | None = ...) -> None: ...
    def print_disas(self, *, file: TextIO | None = ..., stacklevel: int = ...) -> None:
        """
        Print the current series of opcodes being executed (not including
        parent frames), including where you are in the particular opcode
        stream.
        """
    def print_value_stack(self, *, file: TextIO | None = ..., stacklevel: int = ...) -> None:
        """
        Print the current Python value stack.  Note that this is NOT the same
        as the traceback; use print_bt() to print that.  Note that at
        stacklevel=0, this will typically be empty, as comptime cannot
        currently be used in an expression context where there would be
        intermediates on the stack.  If you would find this useful, please
        file a bug at https://github.com/pytorch/pytorch/

        NB: Stack grows downwards in our print
        """
    def print_locals(self, *, file: TextIO | None = ..., stacklevel: int = ...) -> None:
        """
        Print all of the locals available in the current context.
        By default this view is very limited; you can get more information
        about any individual local using get_local().
        """
    def print_bt(self, *, file: TextIO | None = ..., stacklevel: int = ...) -> None:
        """
        Print the user code backtrace, starting at the beginning of the
        frame Dynamo started evaluating.  Note that this MAY NOT go all
        the way to the torch.compile invocation, as we may have done
        a graph break and are compiling an intermediate frame as the
        starting point.  If you think the other behavior would be better,
        file a bug at https://github.com/pytorch/pytorch/
        """
    def print_guards(self, *, file: TextIO | None = ...) -> None:
        """
        Print the currently installed guards for the Dynamo context.
        This does NOT include guards associated with variables that
        may or may not be installed in the future if those variables
        are used.
        """
    def sleep(self, sec: float) -> None: ...

class _Comptime:
    @staticmethod
    def __call__(fn: Callable[[ComptimeContext], Any], fallback_fn: Callable[[], Any] = ...) -> Any:
        """fn gets called at compile time in TorchDynamo, calls fallback_fn otherwise"""
    @staticmethod
    def graph_break() -> None: ...
    @staticmethod
    def print(e: Any) -> None: ...
    @staticmethod
    def print_graph() -> None: ...
    @staticmethod
    def print_disas(*, stacklevel: int = ...) -> None: ...
    @staticmethod
    def print_value_stack(*, stacklevel: int = ...) -> None: ...
    @staticmethod
    def print_value_stack_and_return(e: Any, *, stacklevel: int = ...) -> Any: ...
    @staticmethod
    def print_locals(*, stacklevel: int = ...) -> None: ...
    @staticmethod
    def print_bt(*, stacklevel: int = ...) -> None: ...
    @staticmethod
    def print_guards() -> None: ...
    @staticmethod
    def assert_static(val: Any) -> None: ...
    @staticmethod
    def force_static(val: Any) -> None: ...
    @staticmethod
    def breakpoint() -> None:
        """
        Like pdb breakpoint(), but drop into pdb whenever this line
        of code is compiled by dynamo.  Use it by putting
        this in your model code::

            from torch._dynamo.comptime import comptime

            comptime.breakpoint()

        And then, inside pdb, you can access 'ctx' to query things
        about the compilation context::

            (Pdb) !ctx.print_bt()
            (Pdb) !ctx.print_locals()
            (Pdb) p ctx.get_local("attention").as_fake()
        """
    @staticmethod
    def sleep(sec: float) -> None: ...

comptime = ...
