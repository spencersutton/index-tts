"""
Device abstraction layer for TorchDynamo and Inductor backends.

This module provides a unified interface for different hardware backends (CUDA, XPU,
CPU, MPS, MTIA) through a common device interface. Key components include:

- DeviceInterface: Base class defining the common API for all device types
- Device-specific implementations: CudaInterface, XpuInterface, CpuInterface, MpsInterface, MtiaInterface
- Device registration system for managing available backends
- Worker APIs for multi-processing scenarios
- Stream and event management across different devices
- Device property caching for worker processes

The abstraction layer enables device-agnostic code in TorchDynamo while allowing
specialized implementations for each hardware backend's unique features.
"""

from collections.abc import Callable, Iterable
from dataclasses import dataclass
from typing import Any, Literal

import torch
from torch._C import (
    _cuda_getCurrentRawStream as get_cuda_stream,
    _mtia_getCurrentRawStream as get_mtia_stream,
    _xpu_getCurrentRawStream as get_xpu_stream,
)

get_cuda_stream: Callable[[int], int] | None
if torch.cuda._is_compiled(): ...
else:
    get_cuda_stream = ...
caching_worker_device_properties: dict[str, Any] = ...
caching_worker_current_devices: dict[str, int] = ...

class DeviceInterface:
    """
    This is a simple device runtime interface for Inductor. It enables custom
    backends to be integrated with Inductor in a device-agnostic semantic.
    """
    class device:
        def __new__(cls, device: torch.types.Device) -> Any: ...

    class Event:
        def __new__(cls, *args: Any, **kwargs: Any) -> Any: ...

    class Stream:
        def __new__(cls, *args: Any, **kwargs: Any) -> Any: ...

    class Worker:
        """
        Worker API to query device properties that will work in multi processing
        workers that cannot use the GPU APIs (due to processing fork() and
        initialization time issues). Properties are recorded in the main process
        before we fork the workers.
        """
        @staticmethod
        def set_device(device: int) -> None: ...
        @staticmethod
        def current_device() -> int: ...
        @staticmethod
        def get_device_properties(device: torch.types.Device = ...) -> Any: ...

    @staticmethod
    def current_device() -> int: ...
    @staticmethod
    def set_device(device: torch.types.Device) -> None: ...
    @staticmethod
    def maybe_exchange_device(device: int) -> int: ...
    @staticmethod
    def exchange_device(device: int) -> int: ...
    @staticmethod
    def device_count() -> int: ...
    @staticmethod
    def is_available() -> bool: ...
    @staticmethod
    def stream(stream: torch.Stream) -> Any: ...
    @staticmethod
    def current_stream() -> torch.Stream: ...
    @staticmethod
    def set_stream(stream: torch.Stream) -> None: ...
    @staticmethod
    def get_raw_stream(device_idx: int) -> int: ...
    @staticmethod
    def synchronize(device: torch.types.Device = ...) -> None: ...
    @classmethod
    def get_device_properties(cls, device: torch.types.Device = ...) -> Any: ...
    @staticmethod
    def get_compute_capability(device: torch.types.Device = ...) -> Any: ...
    @staticmethod
    def is_bf16_supported(including_emulation: bool = ...) -> bool: ...
    @classmethod
    def is_dtype_supported(cls, dtype: torch.dtype, including_emulation: bool = ...) -> bool: ...
    @staticmethod
    def memory_allocated(device: torch.types.Device = ...) -> int: ...
    @staticmethod
    def is_triton_capable(device: torch.types.Device = ...) -> bool:
        """
        Returns True if the device has Triton support, False otherwise, even if
        the appropriate Triton backend is not available.
        """
    @classmethod
    def raise_if_triton_unavailable(cls, device: torch.types.Device = ...) -> None:
        """
        Raises a `RuntimeError` with the appropriate human-readable instructions
        to resolve the issue if Triton is not available for the given device, or
        the default device if `device` is `None`.

        The caller should ensure the presence of the 'triton' package before
        calling this method.
        """

class DeviceGuard:
    """
    This class provides a context manager for device switching. This is a stripped
    down version of torch.{device_name}.device.

    The context manager changes the current device to the given device index
    on entering the context and restores the original device on exiting.
    The device is switched using the provided device interface.
    """
    def __init__(self, device_interface: type[DeviceInterface], index: int | None) -> None: ...
    def __enter__(self) -> None: ...
    def __exit__(self, type: Any, value: Any, traceback: Any) -> Literal[False]: ...

class CudaInterface(DeviceInterface):
    device = ...
    Event = torch.cuda.Event
    Stream = torch.cuda.Stream
    class Worker:
        @staticmethod
        def set_device(device: int) -> None: ...
        @staticmethod
        def current_device() -> int: ...
        @staticmethod
        def get_device_properties(device: torch.types.Device = ...) -> Any: ...

    current_device = ...
    set_device = ...
    device_count = ...
    stream = ...
    current_stream = ...
    set_stream = ...
    _set_stream_by_id = ...
    synchronize = ...
    get_device_properties = ...
    get_raw_stream = ...
    exchange_device = ...
    maybe_exchange_device = ...
    memory_allocated = ...
    is_bf16_supported = ...
    @staticmethod
    def is_available() -> bool: ...
    @staticmethod
    def get_compute_capability(device: torch.types.Device = ...) -> int | str: ...
    @staticmethod
    def is_triton_capable(device: torch.types.Device = ...) -> bool: ...
    @staticmethod
    def raise_if_triton_unavailable(device: torch.types.Device = ...) -> None: ...

get_mtia_stream: Callable[[int], int] | None
if torch.mtia._is_compiled(): ...
else:
    get_mtia_stream = ...

class MtiaInterface(DeviceInterface):
    device = ...
    Event = ...
    Stream = ...
    class Worker:
        @staticmethod
        def set_device(device: int) -> None: ...
        @staticmethod
        def current_device() -> int: ...
        @staticmethod
        def get_device_properties(device: torch.types.Device = ...) -> Any: ...

    current_device = ...
    set_device = ...
    device_count = ...
    stream = ...
    current_stream = ...
    set_stream = ...
    _set_stream_by_id = ...
    synchronize = ...
    get_device_properties = ...
    get_raw_stream = ...
    exchange_device = ...
    maybe_exchange_device = ...
    memory_allocated = ...
    is_bf16_supported = ...
    @staticmethod
    def is_available() -> bool: ...
    @staticmethod
    def get_compute_capability(device: torch.types.Device = ...) -> Any: ...
    @staticmethod
    def is_triton_capable(device: torch.types.Device = ...) -> bool: ...
    @staticmethod
    def raise_if_triton_unavailable(evice: torch.types.Device = ...) -> None: ...

get_xpu_stream: Callable[[int], int] | None
if torch.xpu._is_compiled(): ...
else:
    get_xpu_stream = ...

class XpuInterface(DeviceInterface):
    device = ...
    Event = torch.xpu.Event
    Stream = torch.xpu.Stream
    class Worker:
        @staticmethod
        def set_device(device: int) -> None: ...
        @staticmethod
        def current_device() -> int: ...
        @staticmethod
        def get_device_properties(device: torch.types.Device = ...) -> Any: ...

    current_device = ...
    set_device = ...
    device_count = ...
    stream = ...
    current_stream = ...
    set_stream = ...
    _set_stream_by_id = ...
    synchronize = ...
    get_device_properties = ...
    get_raw_stream = ...
    exchange_device = ...
    maybe_exchange_device = ...
    memory_allocated = ...
    @staticmethod
    def is_available() -> bool: ...
    @staticmethod
    def get_compute_capability(device: torch.types.Device = ...) -> Any: ...
    @staticmethod
    def is_bf16_supported(including_emulation: bool = ...) -> bool: ...
    @staticmethod
    def is_triton_capable(device: torch.types.Device = ...) -> bool: ...
    @staticmethod
    def raise_if_triton_unavailable(device: torch.types.Device = ...) -> None: ...

@dataclass
class CpuDeviceProperties:
    """CpuDeviceProperties(multi_processor_count: int)"""

    multi_processor_count: int

class CpuInterface(DeviceInterface):
    class Event(torch.Event):
        def __init__(self, enable_timing: bool = ...) -> None: ...
        def elapsed_time(self, end_event: Any) -> float: ...
        def record(self, stream: Any = ...) -> None: ...

    class Worker:
        @staticmethod
        def get_device_properties(device: torch.types.Device = ...) -> CpuDeviceProperties: ...

    @staticmethod
    def is_available() -> bool: ...
    @staticmethod
    def is_bf16_supported(including_emulation: bool = ...) -> bool: ...
    @staticmethod
    def get_compute_capability(device: torch.types.Device = ...) -> str: ...
    @staticmethod
    def get_raw_stream(device_idx: Any) -> int: ...
    @staticmethod
    def current_device() -> int: ...
    @staticmethod
    def synchronize(device: torch.types.Device = ...) -> None: ...
    @staticmethod
    def is_triton_capable(device: torch.types.Device = ...) -> bool: ...
    @staticmethod
    def raise_if_triton_unavailable(device: torch.types.Device = ...) -> None: ...

class MpsInterface(DeviceInterface):
    @staticmethod
    def is_bf16_supported(including_emulation: bool = ...) -> bool: ...
    @classmethod
    def is_dtype_supported(cls, dtype: torch.dtype, including_emulation: bool = ...) -> bool: ...
    @staticmethod
    def is_available() -> bool: ...
    @staticmethod
    def current_device() -> int: ...
    @staticmethod
    def get_compute_capability(device: torch.types.Device = ...) -> str: ...
    @staticmethod
    def synchronize(device: torch.types.Device = ...) -> None: ...

    class Worker:
        @staticmethod
        def get_device_properties(device: torch.types.Device = ...) -> Any: ...
        @staticmethod
        def current_device() -> int: ...

device_interfaces: dict[str, type[DeviceInterface]] = ...
_device_initialized = ...

def register_interface_for_device(device: str | torch.device, device_interface: type[DeviceInterface]) -> None: ...
def get_interface_for_device(device: str | torch.device) -> type[DeviceInterface]: ...
def get_registered_device_interfaces() -> Iterable[tuple[str, type[DeviceInterface]]]: ...
def init_device_reg() -> None: ...
