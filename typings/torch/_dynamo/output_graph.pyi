import contextlib
import traceback
import sympy
import torch._guards
import torch.nn
from collections.abc import Generator, Sequence
from dataclasses import dataclass
from types import CodeType
from typing import Any, Callable, Optional, TYPE_CHECKING, Union, TypeAlias
from typing_extensions import ParamSpec, TypeVar
from torch import Tensor, fx
from torch._C._dynamo import guards
from torch._guards import Source
from torch.export.dynamic_shapes import _ConstraintTarget
from torch.fx.experimental.symbolic_shapes import ShapeEnv
from torch.fx.node import Target
from .backends.registry import CompiledFn, CompilerFn
from .bytecode_transformation import Instruction
from .codegen import PyCodegen
from .side_effects import SideEffects
from .variables.base import VariableTracker
from .variables.builder import GraphArg
from torch._dynamo.package import CompilePackage
from torch._dynamo.symbolic_convert import InstructionTranslatorBase

"""
Core graph building functionality for PyTorch's Dynamo system. This module contains
the essential components for constructing and managing FX graphs during compilation:

- OutputGraph: Manages the overall graph construction and compilation process. It owns
  a SubgraphTracer and handles graph compilation, execution, and state management.
  OutputGraph also manages features like graph deduplication, symbolic shape handling,
  and tracking of side effects.

- SubgraphTracer: Handles the actual FX graph construction by tracing Python code.
  It supports advanced features like higher-order operators through nested tracers,
  lifting of free variables, and handling of symbolic shapes.

The module supports key Dynamo features including:
- Higher-order operators through nested SubgraphTracers
- Graph deduplication for optimization
- Symbolic shape handling and propagation
- Side effect tracking and management
- Guard insertion and management
"""
if TYPE_CHECKING: ...
log = ...
graph_tabular_log = ...
graph_code_log = ...
graph_sizes_log = ...
trace_call_log = ...
RootGuardManager = guards.RootGuardManager

@dataclass(frozen=True)
class VariableTrackerCacheKey:
    vt_id: int
    source: Source

@dataclass(frozen=True)
class AliasingInfo:
    has_aliasing: bool
    msg: str

@dataclass(frozen=True)
class MutationInfo:
    has_mutation: bool
    msg: str

class VariableTrackerCache:
    def __init__(self) -> None: ...
    def lookup(self, value: Any, source: Source) -> Optional[VariableTracker]: ...
    def add(self, value: Any, source: Source, vt: VariableTracker) -> None: ...
    def clone(self) -> VariableTrackerCache: ...
    def clear(self) -> None: ...

@dataclass
class GraphCompileReason:
    reason: str
    user_stack: list[traceback.FrameSummary]
    graph_break: bool = ...
    def __post_init__(self) -> None: ...

class FakeRootModule(torch.nn.Module):
    def __init__(self, nn_modules: dict[str, torch.nn.Module]) -> None: ...
    def add_nn_modules(self, nn_modules: dict[str, torch.nn.Module]) -> None: ...

class WrapperBackend:
    def __init__(self, backend: CompilerFn) -> None: ...
    def __call__(self, gm: torch.fx.GraphModule, example_inputs: list[torch.Tensor]) -> CompiledFn: ...

Scope: TypeAlias = dict[str, object]

@dataclass
class OutputGraphGuardsState:
    local_scope: Scope
    global_scope: Scope
    torch_function_mode_stack: list[torch.overrides.TorchFunctionMode]
    guard_on_key_order: set[Source]
    input_source_to_sizes_strides: dict[Source, dict[str, Any]]
    dual_level: int
    functorch_layers: list[torch._functorch.pyfunctorch.FuncTorchInterpreter]
    current_device: Optional[torch.device]
    global_state_guard: torch._C._dynamo.guards.GlobalStateGuard
    _guards: torch._guards.GuardsSet
    _aotautograd_guards: list[torch._guards.GuardEnvExpr]
    export: bool = ...
    skip_guards_check: bool = ...
    export_constraints: bool = ...
    name_of_builtins_dict_key_in_fglobals: Optional[str] = ...
    @property
    def shape_env(self) -> ShapeEnv: ...
    @property
    def guards(self) -> torch._guards.GuardsSet: ...
    @property
    def aotautograd_guards(self) -> list[torch._guards.GuardEnvExpr]: ...

@dataclass
class StackLocalsMetadata:
    num_stack: int = ...
    locals_names: dict[str, int] = ...
    stack_null_idxes: list[int] = ...
    locals_null_keys: list[str] = ...
    stack_ctx_args: list[tuple[int, tuple[Any, ...]]] = ...
    stack_ctx_idxes_orig: list[int] = ...
    locals_ctx_args: list[tuple[str, tuple[Any, ...]]] = ...

@dataclass
class ExportMetaData:
    graph_input_idx_to_local_source: dict[int, Source] = ...
    output_return_type: dict[int, tuple[str, Any]] = ...
    out_spec: Union[torch.utils._pytree.TreeSpec, torch.utils._pytree.LeafSpec] = ...

def get_builtins_dict(global_scope: Scope) -> dict[str, Any]: ...

class OutputGraph(OutputGraphGuardsState):
    side_effects: SideEffects
    def __init__(
        self,
        code_options: dict[str, Any],
        compiler_fn: Optional[CompilerFn],
        root_tx: InstructionTranslatorBase,
        export: bool,
        export_constraints: Sequence[_ConstraintTarget],
        frame_state: Any,
        local_scope: Scope,
        global_scope: Scope,
        f_code: CodeType,
        torch_function_mode_stack: list[torch.overrides.TorchFunctionMode],
        package: Optional[CompilePackage],
    ) -> None: ...
    def mark_bytecode_tracing_start(self) -> None: ...
    def mark_bytecode_tracing_stop(self) -> None: ...
    def install_builtins_dict_in_fglobals(self) -> str: ...
    def add_backward_state_hook(self, hook: VariableTracker, prefix: str = ...) -> tuple[str, torch.fx.Proxy]: ...
    def get_backward_state_proxy(self) -> torch.fx.Proxy: ...
    def init_ambient_guards(self) -> None: ...
    def maybe_install_saved_tensors_hooks_subgraphs(self) -> Optional[list[str]]: ...
    def dump_guards_state(self) -> OutputGraphGuardsState: ...
    def synthetic_graph_input(self, fn: Callable[..., Any], args: tuple[Any, ...]) -> VariableTracker: ...
    def add_cleanup_hook(self, fn: Callable[[], Any]) -> None: ...
    def call_cleanup_hooks(self) -> None: ...
    @property
    def root_tracer(self) -> SubgraphTracer: ...
    @property
    def current_tracer(self) -> SubgraphTracer: ...
    def is_root_tracer(self) -> bool: ...
    @property
    def graph(self) -> torch.fx.Graph: ...
    @graph.setter
    def graph(self, value: torch.fx.Graph) -> None: ...
    @property
    def input_name_to_proxy(self) -> dict[str, fx.Proxy]: ...
    @property
    def real_value_cache(self) -> dict[fx.Node, torch.Tensor]: ...
    @property
    def bound_symbols(self) -> dict[sympy.Symbol, Union[torch.fx.Proxy, LazyProxy]]: ...
    def create_proxy(self, *args: Any, **kwargs: Any) -> torch.fx.Proxy: ...
    def create_node(self, *args: Any, **kwargs: Any) -> torch.fx.Node: ...
    def remove_node(self, *args: Any, **kwargs: Any) -> None: ...
    @contextlib.contextmanager
    def subtracer(
        self, source_target: Optional[Target], prior_tracer: SubgraphTracer
    ) -> Generator[fx.Tracer, None, None]: ...
    @property
    def output(self) -> OutputGraph: ...
    @property
    def fake_mode(self) -> torch._subclasses.FakeTensorMode: ...
    @property
    def shape_env(self) -> ShapeEnv: ...
    @property
    def guards(self) -> torch._guards.GuardsSet: ...
    @property
    def nn_modules(self) -> dict[str, Any]: ...
    @property
    def aotautograd_guards(self) -> list[torch._guards.GuardEnvExpr]: ...
    def save_global_state(self, out: Optional[dict[str, tuple[Callable[..., Any], bool]]] = ...) -> None: ...
    def push_tx(self, tx: InstructionTranslatorBase) -> None: ...
    def pop_tx(self) -> InstructionTranslatorBase: ...
    @property
    def current_tx(self) -> InstructionTranslatorBase: ...
    def count_calls(self) -> int: ...
    def is_empty_graph(self) -> bool: ...
    def has_outputs(self) -> bool: ...
    def get_submodule(self, keys: str) -> Union[torch.nn.Module, Any]: ...
    def new_var(self, name: str = ...) -> str: ...
    def update_co_names(self, name: str) -> None: ...
    @staticmethod
    def module_key_name(*names: Any) -> str: ...
    def register_static_attr_and_return_proxy(self, attr_prefix: str, attr_value: Any) -> fx.Proxy: ...
    def register_attr_or_module(
        self, target: Union[torch.nn.Module, torch.Tensor, Any], *names: Any, **options: Any
    ) -> VariableTracker: ...
    def handle_aliases_for_stolen_lists(
        self, tx: InstructionTranslatorBase
    ) -> tuple[list[Instruction], dict[Source, Source]]: ...
    def compile_subgraph(
        self,
        tx: InstructionTranslatorBase,
        reason: GraphCompileReason,
        partial_convert: bool = ...,
        stack_pops: int = ...,
    ) -> list[StackLocalsMetadata]: ...
    def codegen_suffix(
        self, tx: InstructionTranslatorBase, stack_values: list[VariableTracker], cg: PyCodegen
    ) -> None: ...
    def cleanup_graph(self) -> None: ...
    def bypass_package(self, reason: str = ..., **kwargs: Any) -> None: ...
    def get_graph_sizes_structured(self) -> dict[str, list[Union[int, str]]]: ...
    def get_graph_sizes(self, name: str) -> str: ...
    @contextlib.contextmanager
    def restore_global_state(self) -> Any: ...
    def run_compiler_collective(self) -> None: ...
    def compile_and_call_fx_graph(
        self, tx: InstructionTranslatorBase, rv: list[VariableTracker], root: FakeRootModule
    ) -> list[Instruction]: ...
    @property
    def placeholders(self) -> list[fx.Node]: ...
    @property
    def graphargs(self) -> list[GraphArg]: ...
    def call_user_compiler(self, gm: fx.GraphModule, example_inputs: list[Tensor]) -> CompiledFn: ...
    def dedup_pass(self) -> dict[str, torch.fx.GraphModule]: ...
    def install_subgraph(self, name: str, sub_gm: torch.fx.GraphModule) -> str: ...
    def example_inputs(self) -> list[torch.Tensor]: ...
    def remove_unused_get_attr_nodes(self) -> None: ...
    def remove_unused_graphargs(self) -> None: ...
    def remove_tensorify_specialized_graphargs(self) -> None: ...
    def add_output_instructions(self, prefix: list[Instruction]) -> None: ...
    def install_global_unsafe(self, name: str, value: Any) -> None: ...
    def install_global_by_id(self, prefix: str, value: Any) -> str: ...
    def install_global(self, prefix: str, value: Any) -> str: ...
    def cleanup(self) -> None: ...
    def add_graph_finalizer(self, register_finalizer: Callable[[fx.GraphModule], None]) -> None: ...
    def example_value_from_input_node(self, node: torch.fx.Node) -> Any: ...

class DynamoTracerOutput:
    error_on_graph_break: bool
    is_tracing_resume_prologue: bool
    output_graph: Optional[OutputGraph]
    def __init__(self, tracer: InstructionTranslatorBase, error: Optional[Any] = ...) -> None: ...

err_epilogue = ...

def check_pt2_compliant_op(output_graph: OutputGraph, kind: str, target: Any, args: Any, kwargs: Any) -> None: ...

_compile_id_counter = ...
P = ParamSpec("P")
R = TypeVar("R")

class LazyProxy:
    def __init__(self, tracer: SubgraphTracer, fn: Callable[P, R], *args: P.args, **kwargs: P.kwargs) -> None: ...
    def __call__(self) -> Any: ...

class SubgraphTracer(fx.Tracer):
    def __init__(
        self,
        output_graph: OutputGraph,
        parent: Optional[SubgraphTracer] = ...,
        is_export: bool = ...,
        source_target: Optional[Target] = ...,
    ) -> None: ...
    def create_proxy(
        self,
        kind: str,
        target: Any,
        args: Any,
        kwargs: Any,
        name: Optional[str] = ...,
        type_expr: Optional[Any] = ...,
        proxy_factory_fn: Optional[Callable[[fx.Node], fx.Proxy]] = ...,
    ) -> fx.Proxy: ...
    def create_node(
        self,
        op: str,
        target: Target,
        args: Any = ...,
        kwargs: Any = ...,
        name: Optional[str] = ...,
        type_expr: Optional[Any] = ...,
    ) -> fx.Node: ...
    def remove_node(self, node: fx.Node) -> None: ...
    def create_graph_input(
        self, name: str, type_expr: Any, example_value: Any, before: bool = ..., source: Optional[Source] = ...
    ) -> fx.Proxy: ...
    def lift_tracked_freevar_to_input(self, proxy: fx.Proxy) -> Union[LazyProxy, fx.Proxy]: ...
    def maybe_lift_tracked_freevar_to_input(self, arg: Any) -> Any: ...
    def track_produced_symints(self, example_value: Any, e_proxy: Union[LazyProxy, torch.fx.Proxy]) -> None: ...
    def lookup_unbound_symbols(self, s: torch.SymInt) -> list[sympy.Symbol]: ...
    def has_input_mutation(self) -> MutationInfo: ...
    def has_aliasing(self) -> AliasingInfo: ...
