import dataclasses
import enum
import types
from collections import defaultdict
from typing import Optional, TYPE_CHECKING, TypeVar, Union
from typing import Self, override
from torch.compiler._cache import CacheArtifact, CacheArtifactFactory
from torch._dynamo.symbolic_convert import InstructionTranslator
from torch._inductor.remote_cache import JsonDataTy, RemoteCache

"""
Profile Guided Optimization (PGO) implementation for Dynamo.

This module provides functionality for caching and managing code state profiles
that guide optimization decisions in Dynamo. It implements both local and remote
caching mechanisms for storing profile information across runs, handles profile
merging across distributed ranks, and manages the lifecycle of profile data
during compilation. The profiles track dynamic vs static properties of tensors
and help Dynamo make better specialization decisions.
"""
if TYPE_CHECKING: ...

class ReservedWorkflowIdUserError(ValueError): ...

log = ...
LOCK_TIMEOUT = ...

@dataclasses.dataclass(frozen=True)
class CodeId:
    filename: str
    firstlineno: int
    name: str
    file_hash: str
    def __eq__(self, other: object) -> bool: ...
    def __hash__(self) -> int: ...
    @staticmethod
    def make(code: types.CodeType) -> CodeId: ...

@dataclasses.dataclass
class CodeState:
    automatic_dynamic: defaultdict[str, FrameStateSizeEntry] = ...

_INIT_CODE_STATE: defaultdict[CodeId, CodeState] | None = ...
_CODE_STATE: defaultdict[CodeId, CodeState] | None = ...
_LOGGED_DYNAMIC_ALLOWLIST: bool = ...

@dataclasses.dataclass(frozen=True)
class InferStride:
    dim: int

class AutoUnset(enum.Enum):
    token = ...

auto_unset = ...

class AutoDynamic(enum.Enum):
    token = ...

auto_dynamic = ...

@dataclasses.dataclass
class FrameStateSizeEntry:
    scalar: int | AutoDynamic | AutoUnset = ...
    size: AutoDynamic | AutoUnset | tuple[int | AutoDynamic, ...] = ...
    stride: AutoDynamic | AutoUnset | tuple[int | AutoDynamic | InferStride, ...] = ...
    def render(self) -> str: ...
    def __post_init__(self) -> None: ...
    def is_size_dynamic(self, dim: int) -> bool: ...
    def is_stride_dynamic(self, dim: int) -> bool: ...
    @classmethod
    def make_scalar(cls, x: int) -> FrameStateSizeEntry: ...
    @classmethod
    def make_tensor(cls, size: tuple[int, ...], stride: tuple[int, ...]) -> FrameStateSizeEntry: ...
    @classmethod
    def make_size(cls, size: tuple[int, ...]) -> FrameStateSizeEntry: ...
    def __ior__(self, other: Self) -> Self: ...

def update_automatic_dynamic(
    tx: InstructionTranslator, name: str, entry: FrameStateSizeEntry, *, is_unspecialized_nn_module: bool = ...
) -> FrameStateSizeEntry: ...
def process_automatic_dynamic(
    tx: InstructionTranslator, name: str, entry: FrameStateSizeEntry, *, is_unspecialized_nn_module: bool = ...
) -> FrameStateSizeEntry: ...
def format_cache_key(key: str) -> str: ...
def get_cache_key() -> str | None: ...
def get_extra_cache_key(sticky_key: str) -> str | None: ...
def code_state_path(cache_key: str) -> str | None: ...
def should_use_remote_dynamo_pgo_cache() -> bool: ...
def get_remote_cache() -> RemoteCache[JsonDataTy] | None: ...
def log_frame_dynamic_whitelist(f_code: types.CodeType) -> None: ...
def render_code_state(cs: defaultdict[CodeId, CodeState]) -> str: ...
def merge_pgo_entry(src: FrameStateSizeEntry, dst: FrameStateSizeEntry) -> None: ...

@CacheArtifactFactory.register
class PGOCacheArtifact(CacheArtifact):
    @override
    def populate_cache(self) -> None: ...
    @override
    @staticmethod
    def type() -> str: ...

def hit(key: str, ty: str) -> defaultdict[CodeId, CodeState]: ...
def get_local_code_state(cache_key: str) -> defaultdict[CodeId, CodeState] | None: ...
def lookup_remote_cache_entry(
    remote_cache: RemoteCache[JsonDataTy], cache_key: str, event_name: str | None = ...
) -> defaultdict[CodeId, CodeState] | None: ...
def get_remote_code_state(cache_key: str) -> defaultdict[CodeId, CodeState] | None: ...
def add_extra_remote_code_state(cache_key: str) -> None: ...
def get_code_state() -> defaultdict[CodeId, CodeState]: ...
def put_code_state() -> None: ...
def write_local_impl(cache_key: str, pickled_code: bytes) -> tuple[str, int] | None: ...
def put_local_code_state(cache_key: str) -> None: ...
def put_remote_code_state(cache_key: str) -> None: ...
def reset_code_state() -> None: ...
