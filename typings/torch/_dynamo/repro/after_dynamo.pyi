"""
Utilities for reproducing and debugging issues in Dynamo after graph capture.

This file provides tools and infrastructure for debugging problems that occur
after Dynamo has captured the graph but before/during backend compilation.
Key components include:

- Minification tools to reduce large graphs to minimal failing examples
- Accuracy testing to validate compiled graph outputs match eager mode
- Repro generation to create standalone reproduction scripts
- Debug backends for capturing and analyzing failures
- Utilities for saving/loading graph states and inputs

The tools here focus specifically on the post-graph-capture stage, making them
useful for debugging backend compilation issues, AOTAutograd problems, and
accuracy discrepancies between compiled and eager execution.
"""

from collections.abc import Sequence
from typing import Any

import torch
from torch import fx

from ..backends.registry import CompilerFn, register_debug_backend

log = ...
inductor_config = ...
use_buck = ...

class WrapBackendDebug:
    def __init__(self, unconfigured_compiler_fn: CompilerFn, compiler_name: str | None) -> None: ...
    def __call__(self, gm: torch.fx.GraphModule, example_inputs: list[Any], **kwargs: Any) -> torch.fx.GraphModule: ...

def wrap_backend_debug(unconfigured_compiler_fn: CompilerFn, compiler_name: str | None) -> WrapBackendDebug:
    """
    A minifier decorator that wraps the TorchDynamo produced Fx graph modules.
    As opposed to wrap_compiler_debug, this wrapper intercepts at the
    TorchDynamo produced Fx Graph Module. This makes it backend-agnostic to some
    level, e.g., it is useful for minifying issues related to Aot Autograd
    tracing.  If an error is found, we minify and save the minified repro in
    repro.tar.gz.
    """

def generate_dynamo_fx_repro_string(
    gm: torch.fx.GraphModule,
    args: Sequence[Any],
    compiler_name: str | None,
    check_accuracy: bool = ...,
    *,
    stable_output: bool = ...,
    save_dir: str | None = ...,
    command: str = ...,
) -> str:
    """Generate a repro string for backend-agnostic minified version."""

def dump_backend_repro_as_file(
    gm: torch.fx.GraphModule, args: Sequence[Any], compiler_name: str | None, check_accuracy: bool = ...
) -> None:
    """Saves the repro to a repro.py file"""

def dump_backend_state(
    gm: torch.fx.GraphModule, args: Sequence[Any], compiler_name: str | None, check_accuracy: bool = ...
) -> None:
    """
    Dumps the dynamo graph to repro the issue.
    1) It tries to convert Fx GraphModule to a string. If we can, it writes to a
    repro.py file.
    2) If we can't convert Fx GraphModule to a string, we use to_folder to save
    the module and save a tar file.
    """

def dump_to_minify_after_dynamo(gm: torch.fx.GraphModule, args: Sequence[Any], compiler_name: str | None) -> None: ...
@register_debug_backend
def dynamo_minifier_backend(
    gm: fx.GraphModule, example_inputs: Sequence[Any], compiler_name: str | None
) -> fx.GraphModule: ...
@register_debug_backend
def dynamo_accuracy_minifier_backend(
    gm: fx.GraphModule, example_inputs: Sequence[Any], compiler_name: str | None
) -> fx.GraphModule: ...
def backend_fails(
    gm: fx.GraphModule, example_inputs: Sequence[Any], compiler_fn: CompilerFn, orig_failure: Sequence[Any]
) -> bool:
    """
    Minifier uses this function to identify if the minified graph module fails
    with the same error.

    One caveat is that minifier can potentially go into a wrong direction when
    the resulting graph module fails for a different reason. To avoid this, we
    save the string for the original exception and check similarity between new
    and old exception. They can be somewhat different in some cases, when the
    exception string depends on the failing node information. So, we have a
    loose similarity metric to guide the minifier path.
    """

def run_load_args(options: Any, mod: torch.nn.Module, load_args: Any) -> list[Any]: ...
def repro_minify(options: Any, mod: torch.nn.Module, load_args: Any) -> None: ...
def repro_run(options: Any, mod: torch.nn.Module, load_args: Any) -> None: ...
def run_repro(
    mod: torch.nn.Module,
    load_args: Any,
    *,
    command: str = ...,
    accuracy: bool | str = ...,
    save_dir: str | None = ...,
    autocast: bool = ...,
    backend: str = ...,
    **kwargs: Any,
) -> None: ...
