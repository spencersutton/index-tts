"""
Utility functions and classes used throughout the TorchDynamo system.

This module contains a collection of helper utilities used by various parts of Dynamo for:
- Performance metrics collection and reporting
- Compilation timing and debugging
- Graph manipulation and tensor operations
- Runtime guards and checks
- Common data structure operations
- Testing and development tools

This is an internal module that provides shared functionality used across the Dynamo codebase.
"""

import atexit
import collections
import contextlib
import dataclasses
import dis
import enum
import functools
import types
import typing
import weakref
from collections import Counter
from collections.abc import (
    Callable,
    Container,
    Generator,
    ItemsView,
    Iterable,
    Iterator,
    KeysView,
    Mapping,
    Sequence,
    ValuesView,
)
from contextlib import AbstractContextManager, contextmanager
from functools import lru_cache
from types import CodeType, MethodWrapperType
from typing import Any, ClassVar, Literal, ParamSpec, TypeGuard, TypeIs, TypeVar, overload

import numpy as np
import torch
from torch import fx
from torch._dynamo.metrics_context import MetricsContext, RuntimeMetricsContext
from torch._dynamo.replay_record import ExecutionRecord
from torch._dynamo.symbolic_convert import InstructionTranslator, InstructionTranslatorBase
from torch._dynamo.variables.base import VariableTracker
from torch._guards import CompileId, Source
from torch._prims_common import DeviceLikeType
from torch.utils._triton import has_triton_package
from torch.utils.hooks import RemovableHandle

if typing.TYPE_CHECKING: ...
if np:
    NP_SUPPORTED_MODULES: tuple[types.ModuleType, ...] = ...
    NP_TO_TNP_MODULE = ...
else:
    NP_SUPPORTED_MODULES = ...
    NP_TO_TNP_MODULE = ...
T = TypeVar("T")
R = TypeVar("R")
_P = ParamSpec("_P")
unpatched_nn_module_getattr = ...
unpatched_nn_module_call = ...
unpatched_nn_module_call_impl = ...
counters: collections.defaultdict[str, Counter[str]] = ...
optimus_scuba_log: dict[str, Any] = ...
troubleshooting_url = ...
nnmodule_doc_url = ...
nnmodule_doc_url_msg = ...
log = ...
compilation_time_metrics: dict[str, list[float]] = ...
cumulative_time_spent_ns: dict[str, float] = ...
timer_counter = ...

class ReInplaceTrigger(enum.Enum):
    AUTO_FUNC_V1 = ...
    AUTO_FUNC_V2 = ...
    TRITON_OPS = ...

class ReinplaceCounters:
    _values: collections.defaultdict[str, int] = ...
    @classmethod
    def add_missed_bytes(cls, trigger: ReInplaceTrigger, bytes: int) -> None: ...
    @classmethod
    def add_missed_opportunities(cls, trigger: ReInplaceTrigger, count: int) -> None: ...
    @classmethod
    def clear(cls) -> None: ...
    @classmethod
    def get_total_missed(cls) -> int: ...
    @classmethod
    def get_total_missed_bytes(cls) -> int: ...
    @classmethod
    def log(cls) -> None: ...

def tabulate(rows: list[tuple[str, Any]] | list[list[Any]], headers: tuple[str, ...] | list[str]) -> str: ...

curr_frame = ...

def increment_frame() -> None: ...
def reset_frame_count() -> None: ...

_recompile_user_contexts: list[Callable[[], str]] | None = ...

def register_hook_for_recompile_user_context(hook: Callable[[], str]) -> None:
    """
    Register a hook to be called when a recompile is triggered. The hook
    should return a string describing user contexts that are not available
    to the compiler, such as the current training epoch. This is useful for
    debugging and data analysis for recompile. For data retention purposes,
    the user context string is capped at 256 characters.
    """

def get_hook_for_recompile_user_context() -> list[Callable[[], str]] | None: ...

op_count = ...

def increment_op_count(cnt: int) -> None: ...
def calculate_time_spent() -> dict[str, float]: ...
def print_time_report() -> None: ...

_METRICS_CONTEXT: MetricsContext
_RUNTIME_METRICS_CONTEXT: RuntimeMetricsContext

def get_metrics_context() -> MetricsContext: ...
def get_runtime_metrics_context() -> RuntimeMetricsContext: ...

class CompileEventLogLevel(enum.Enum):
    """
    Enum that loosely corresponds with a "log level" of a given event.

    CHROMIUM_EVENT: Logs only to tlparse.
    COMPILE_EVENT: Logs to tlparse + PT2 Compile Events
    COMPILATION_METRIC: Logs to tlparse, PT2 Compile Events, and dynamo_compile
    """

    CHROMIUM = ...
    PT2_COMPILE = ...
    COMPILATION_METRIC = ...

class CompileEventLogger:
    """
    Helper class for representing adding metadata(i.e. columns) to various compile events.
    Use CompileEventLogger to add event data to:
    - Chromium events
    - PT2 Compile Events
    - CompilationMetrics

    This should be used in conjunction with dynamo_timed() and metrics contexts, which create
    timed spans and events. CompileEventLogger uses three log levels (described in CompileEventLogLevel),
    where each log level logs to all sources below it in the hierarchy.

    Example usages:
    - I want to log to an existing chromium event within dynamo timed:
    with dynamo_timed("my_event"):
        CompileEventLogger.chromium("my_event", foo=bar)

    - I want to log my event to both chromium + pt2_compile_events:
    with dynamo_timed("my_event", log_pt2_compile_event=True):
        CompileEventLogger.pt2_compile("my_event", foo=bar)

    - I want to add information to dynamo events and dynamo_compile
        CompileEventLogger.compilation_metric(foo=bar)
    """
    @staticmethod
    def log_instant_event(
        event_name: str, metadata: dict[str, Any], time_ns: int | None = ..., log_level: CompileEventLogLevel = ...
    ) -> None: ...
    @staticmethod
    def add_data(event_name: str, log_level: CompileEventLogLevel, overwrite: bool = ..., **metadata: object) -> None:
        """
        Centralized API for adding data to various events
        Log an event to a toplevel "dynamo" event or metrics context
        depending on log level.
        """
    @staticmethod
    def add_toplevel(log_level: CompileEventLogLevel, overwrite: bool = ..., **metadata: object) -> None:
        """Syntactic sugar for logging to the toplevel event"""
    @staticmethod
    def increment(event_name: str, log_level: CompileEventLogLevel, key: str, value: int) -> None:
        """Increments an existing field, or adds it"""
    @staticmethod
    def increment_toplevel(key: str, value: int = ..., log_level: CompileEventLogLevel = ...) -> None:
        """Increments a value on the toplevel metric. By default, logs to metric."""
    @staticmethod
    def add_to_set(event_name: str, log_level: CompileEventLogLevel, key: str, value: Any) -> None:
        """Add metadata <value> to a set of values with key <key>. Creates a set if it doesn't exist."""
    @staticmethod
    def add_to_set_toplevel(key: str, value: Any, log_level: CompileEventLogLevel = ...) -> None:
        """
        Same as add to set, just does it automatically to the toplevel event instead of having to explicitly name it.
        Defaults to COMPILATION_METRIC log level.
        """
    @staticmethod
    def chromium(event_name: str, **metadata: object) -> None:
        """
        Add <metadata> to <event_name> in chromium. Each key/value of metadata will appear in the chromium trace.
        <event_name> should be the name of a timed event span passed to `dynamo_timed`.
        """
    @staticmethod
    def pt2_compile(event_name: str, **metadata: object) -> None:
        """
        Add <metadata> to <event_name> in chromium and PT2 Compile Events.
        Each key/value of metadata will appear in the chromium trace. Each kwarg name becomes
        a column in PT2 Compile Events, with the corresponding kwarg value.
        <event_name> should be the name of a timed event span passed to `dynamo_timed`,
        with log_to_pt2_compile_events=True.
        """
    @staticmethod
    def compilation_metric(overwrite: bool = ..., **metadata: object) -> None:
        """
        Add <metadata> to the CompilationMetrics context. Also logs to PT2 Compile Events
        and chromium.
        Each key/value of metadata will appear in the chromium trace. Each kwarg name becomes
        a column in PT2 Compile Events and Dynamo Compile, with the corresponding kwarg value.
        """
    @staticmethod
    def instant(event_name: str, metadata: dict[str, Any], time_ns: int | None = ...) -> None:
        """
        Log an instant event to chromium logs with name <event_name> at time <time_ns>. The `args` field in
        Perfetto will point to metadata. <time_ns> should be a value obtained from time.time_ns().
        """
    @staticmethod
    def try_add_pt2_compile(event_name: str, **metadata: object) -> None:
        """
        Adds to an existing pt2_compile event, but silently returns if the event doesn't exist
        or ChromiumEventLogger is not initialized.
        This function is syntactic sugar for chromium_event_logger().try_add_event_data.
        """
    @staticmethod
    def try_(method_fn: Callable[_P, Any], *args: _P.args, **kwargs: _P.kwargs) -> None:
        """Special function that quietly runs a given method, returning if CHROMIUM_EVENT_LOG is None or metrics context is not set"""

_dynamo_timed_tls = ...

@contextmanager
def dynamo_timed(
    key: str,
    phase_name: str | None = ...,
    log_pt2_compile_event: bool = ...,
    metadata: dict[str, object] | None = ...,
    dynamo_compile_column_us: str | None = ...,
    compile_id: CompileId | None = ...,
    is_backward: bool | None = ...,
    log_waitcounter: bool = ...,
    waitcounter_name_override: str | None = ...,
) -> Generator[Any]:
    """
    dynamo_timed is a context manager
    By wrapping a function in dynamo_timed, we can get a few things:

    1) Optionally log timings to pt2_compile_events.
    2) Optionally log timings to CompilationMetrics (dynamo_compile).
    3) Optionally log chromium events.
    4) Optionally increment a WaitCounter.
    5) Store a record in compilation_time_metrics
       For example:

        def _foo(...):
            with dynamo_timed("_foo"):
                ...

        Would show up as an entry in our timing dict:
        OrderedDict([('_foo', [0.083690, 0.23949, 3.1425e-05])])
        This is extremely useful for granular debugging.

    Although it is tempting to use dynamo_timed as a decorator, please do not.
    In its decorator form it makes cProfile traces less useful as dynamo_timed
    suddenly becomes a bottleneck for lots of function calls (as only one parent
    pointer is recorded).

    Params:
    - key: key into compile_time_metrics. If phase_name is not provided, this is
      also the event name used for pt2_compile_events logs and chromium events.
    - phase_name: Optional override for the event name.
    - log_pt2_compile_event: Whether to log a pt2 compile event internally.
    - metadata: Extra metadata to put in pt2_compile_events.
    - dynamo_compile_column_us: If provided, updates the specified CompilationMetrics
      field to be logged to dyname_compile column. We expect all columns to be _us;
      therefore, the field name must end with "_us".
    - compile_id: In the typical case, this parameter should not be needed. Use to
      supply the compile_id for those cases where we want to log a compile_id where
      it's not naturally available, e.g., for runtime autotuning.
    - is_backward: Specify forward/backward directly when not available in a
      CompileContext, e.g., during runtime autotuning.
      that support it.
    - log_waitcounter: If set, we'll log a waitcounter of the form "pytorch.dynamo_timed.{key}"
    """

@overload
def compile_times(repr: Literal["str"], aggregate: bool = ...) -> str:
    """
    Get metrics about torchdynamo frontend/backend compilation times.

    Accumulates information from functions tagged with `dynamo_timed`.

    repr='str' returns a printable string for user interaction, and 'csv'
    returns headers, rows which can be logged for output

    aggregate causes values from multiple compilations (e.g. split graphs)
    to be accumulated into one value.  If false, expect more than one value
    per metric.
    """

@overload
def compile_times(repr: Literal["csv"], aggregate: bool = ...) -> tuple[list[str], list[object]]:
    """
    Get metrics about torchdynamo frontend/backend compilation times.

    Accumulates information from functions tagged with `dynamo_timed`.

    repr='str' returns a printable string for user interaction, and 'csv'
    returns headers, rows which can be logged for output

    aggregate causes values from multiple compilations (e.g. split graphs)
    to be accumulated into one value.  If false, expect more than one value
    per metric.
    """

def compile_times(repr: str = ..., aggregate: bool = ...) -> str | None | tuple[list[str], list[str]]:
    """
    Get metrics about torchdynamo frontend/backend compilation times.

    Accumulates information from functions tagged with `dynamo_timed`.

    repr='str' returns a printable string for user interaction, and 'csv'
    returns headers, rows which can be logged for output

    aggregate causes values from multiple compilations (e.g. split graphs)
    to be accumulated into one value.  If false, expect more than one value
    per metric.
    """

@atexit.register
def dump_compile_times() -> None: ...

tensortype_to_dtype = ...

class DuplicateWarningChecker:
    def __init__(self, maxsize: int = ...) -> None: ...
    def reset(self) -> None: ...
    def add(self, key: str | tuple[object, object]) -> bool: ...

graph_break_dup_warning_checker = ...

def setup_compile_debug() -> contextlib.ExitStack: ...
def reset_graph_break_dup_checker() -> None: ...
def add_file_handler() -> contextlib.ExitStack: ...
def setup_log_file() -> contextlib.ExitStack: ...
def gen_record_file_name(exc: Exception, code: CodeType) -> str: ...
def write_record_to_file(filename: str, exec_record: ExecutionRecord) -> None: ...
def count_calls(g: fx.Graph) -> int: ...
def identity[T](x: T) -> T: ...
def hashable(x: Any) -> bool: ...
def nothing(*args: Any, **kwargs: Any) -> None: ...

class ExactWeakKeyDictionary:
    """Similar to weakref.WeakKeyDictionary, but use `is`/`id` rather than `==` to compare equality"""
    def __init__(self) -> None: ...
    def __getitem__(self, key: Any) -> Any: ...
    def get(self, key: Any, default: Any = ...) -> Any: ...
    def __contains__(self, key: Any) -> bool: ...
    def __setitem__(self, key: Any, value: Any) -> None: ...
    def clear(self) -> None: ...

@overload
def istype[T](obj: object, allowed_types: type[T]) -> TypeIs[T]:
    """isinstance() without subclasses"""

@overload
def istype(obj: object, allowed_types: tuple[type[list[T]], type[tuple[T, ...]]]) -> TypeIs[T]:
    """isinstance() without subclasses"""

@overload
def istype(obj: object, allowed_types: Iterable[type]) -> bool:
    """isinstance() without subclasses"""

def istype(obj: object, allowed_types: Any) -> bool:
    """isinstance() without subclasses"""

_builtin_final_typing_classes = ...

def is_typing(value: Any) -> bool: ...
def is_numpy_int_type(value: Any) -> bool: ...
def is_numpy_float_type(value: Any) -> bool: ...
@overload
def is_lru_cache_wrapped_function[T](value: Callable[..., T]) -> TypeGuard[functools._lru_cache_wrapper[T]]: ...
@overload
def is_lru_cache_wrapped_function(value: Any) -> TypeGuard[functools._lru_cache_wrapper[Any]]: ...
def is_lru_cache_wrapped_function(value: Any) -> bool: ...

type _FuncTypes = (
    types.FunctionType | types.BuiltinFunctionType | types.MethodDescriptorType | types.WrapperDescriptorType
)

def is_function_or_wrapper(value: Any) -> TypeIs[_FuncTypes | torch._ops.OpOverloadPacket | torch._ops.OpOverload]: ...
def is_function(value: Any) -> TypeIs[_FuncTypes]: ...

cmp_name_to_op_mapping = ...
cmp_name_to_op_str_mapping = ...

def is_wrapper_or_member_descriptor(
    value: Any,
) -> TypeIs[
    types.GetSetDescriptorType
    | types.MethodDescriptorType
    | types.WrapperDescriptorType
    | types.MemberDescriptorType
    | types.MethodWrapperType
]: ...
def unwrap_if_wrapper(fn: Any) -> Any: ...
def unwrap_with_attr_name_if_wrapper(fn: Any) -> tuple[Any, str | None]: ...
def is_numpy_ndarray(value: Any) -> TypeGuard[np.ndarray]: ...
def istensor(obj: Any) -> bool:
    """Check of obj is a tensor"""

def is_lazy_module(mod: Any) -> bool: ...
@functools.lru_cache(4096)
def print_once(*args: Any) -> None: ...
def make_cell(val: Any = ...) -> types.CellType:
    """Some black magic to create a cell object that usually only exists in a closure"""

def proxy_args_kwargs(args: Any, kwargs: Any) -> tuple[tuple[Any, ...], dict[str, Any]]: ...
def to_int_ms(v: float | None) -> int | None: ...
def to_int_us(v: float | None) -> int | None: ...

LOG_FORMAT_VERSION = ...

@dataclasses.dataclass
class CompilationMetrics:
    """CompilationMetrics(compile_id: 'Optional[str]' = None, frame_key: 'Optional[str]' = None, co_name: 'Optional[str]' = None, co_filename: 'Optional[str]' = None, co_firstlineno: 'Optional[int]' = None, cache_size: 'Optional[int]' = None, accumulated_cache_size: 'Optional[int]' = None, guard_count: 'Optional[int]' = None, shape_env_guard_count: 'Optional[int]' = None, graph_op_count: 'Optional[int]' = None, graph_node_count: 'Optional[int]' = None, graph_input_count: 'Optional[int]' = None, start_time: 'Optional[float]' = None, entire_frame_compile_time_s: 'Optional[float]' = None, backend_compile_time_s: 'Optional[float]' = None, inductor_compile_time_s: 'Optional[float]' = None, code_gen_time_s: 'Optional[float]' = None, fail_type: 'Optional[str]' = None, fail_reason: 'Optional[str]' = None, fail_user_frame_filename: 'Optional[str]' = None, fail_user_frame_lineno: 'Optional[int]' = None, non_compliant_ops: 'Optional[set[str]]' = None, compliant_custom_ops: 'Optional[set[str]]' = None, restart_reasons: 'Optional[set[str]]' = None, dynamo_time_before_restart_s: 'Optional[float]' = None, stack_trace: 'Optional[list[str]]' = None, exception_stack_trace: 'Optional[list[str]]' = None, graph_node_shapes: 'Optional[str]' = None, has_guarded_code: 'Optional[bool]' = None, remote_cache_time_saved_s: 'Optional[float]' = None, structured_logging_overhead_s: 'Optional[float]' = None, config_suppress_errors: 'Optional[bool]' = None, config_inline_inbuilt_nn_modules: 'Optional[bool]' = None, specialize_float: 'Optional[bool]' = None, dynamo_config: 'Optional[str]' = None, is_forward: 'Optional[bool]' = None, num_triton_bundles: 'Optional[int]' = None, remote_fx_graph_cache_get_time_ms: 'Optional[int]' = None, remote_fx_graph_cache_put_time_ms: 'Optional[int]' = None, start_time_us: 'Optional[int]' = None, duration_us: 'Optional[int]' = None, dynamo_cumulative_compile_time_us: 'Optional[int]' = None, aot_autograd_cumulative_compile_time_us: 'Optional[int]' = None, inductor_cumulative_compile_time_us: 'Optional[int]' = None, inductor_code_gen_cumulative_compile_time_us: 'Optional[int]' = None, triton_compile_time_us: 'Optional[int]' = None, runtime_cudagraphify_time_us: 'Optional[int]' = None, runtime_triton_autotune_time_us: 'Optional[int]' = None, dynamo_compile_time_before_restart_us: 'Optional[int]' = None, distributed_ephemeral_timeout_us: 'Optional[int]' = None, structured_logging_overhead_us: 'Optional[int]' = None, remote_fx_graph_cache_get_time_us: 'Optional[int]' = None, remote_fx_graph_cache_put_time_us: 'Optional[int]' = None, backward_cumulative_compile_time_us: 'Optional[int]' = None, end_time_us: 'Optional[int]' = None, pre_grad_pass_time_us: 'Optional[int]' = None, post_grad_pass_time_us: 'Optional[int]' = None, joint_graph_pass_time_us: 'Optional[int]' = None, log_format_version: 'int' = 3, inductor_config: 'Optional[str]' = None, remote_cache_version: 'Optional[int]' = None, inductor_fx_remote_cache_hit_count: 'Optional[int]' = None, inductor_fx_remote_cache_miss_count: 'Optional[int]' = None, inductor_fx_remote_cache_backend_type: 'Optional[str]' = None, inductor_fx_remote_cache_hit_keys: 'Optional[str]' = None, inductor_fx_remote_cache_miss_keys: 'Optional[str]' = None, cuda_version: 'Optional[str]' = None, triton_version: 'Optional[str]' = None, feature_usage: 'Optional[dict[str, bool]]' = None, compile_time_autotune_time_us: 'Optional[int]' = None, is_runtime: 'Optional[bool]' = False, gc_time_us: 'Optional[int]' = None, tensorify_float_attempt: 'Optional[bool]' = None, tensorify_float_success: 'Optional[bool]' = None, tensorify_float_failure: 'Optional[set[str]]' = None, guard_latency_us: 'Optional[float]' = None, recompile_reason: 'Optional[str]' = None, num_graph_breaks: 'Optional[int]' = None, triton_kernel_compile_times_us: 'Optional[str]' = None, ir_count: 'Optional[int]' = None, cudagraph_skip_reason: 'Optional[str]' = None, python_version: 'Optional[str]' = None, pgo_put_remote_code_state_time_us: 'Optional[int]' = None, pgo_get_remote_code_state_time_us: 'Optional[int]' = None, param_numel: 'Optional[int]' = None, param_bytes: 'Optional[int]' = None, param_count: 'Optional[int]' = None, recompile_user_contexts: 'Optional[set[str]]' = None, inline_inbuilt_nn_modules_candidate: 'Optional[bool]' = False)"""

    compile_id: str | None = ...
    frame_key: str | None = ...
    co_name: str | None = ...
    co_filename: str | None = ...
    co_firstlineno: int | None = ...
    cache_size: int | None = ...
    accumulated_cache_size: int | None = ...
    guard_count: int | None = ...
    shape_env_guard_count: int | None = ...
    graph_op_count: int | None = ...
    graph_node_count: int | None = ...
    graph_input_count: int | None = ...
    start_time: float | None = ...
    entire_frame_compile_time_s: float | None = ...
    backend_compile_time_s: float | None = ...
    inductor_compile_time_s: float | None = ...
    code_gen_time_s: float | None = ...
    fail_type: str | None = ...
    fail_reason: str | None = ...
    fail_user_frame_filename: str | None = ...
    fail_user_frame_lineno: int | None = ...
    non_compliant_ops: set[str] | None = ...
    compliant_custom_ops: set[str] | None = ...
    restart_reasons: set[str] | None = ...
    dynamo_time_before_restart_s: float | None = ...
    stack_trace: list[str] | None = ...
    exception_stack_trace: list[str] | None = ...
    graph_node_shapes: str | None = ...
    has_guarded_code: bool | None = ...
    remote_cache_time_saved_s: float | None = ...
    structured_logging_overhead_s: float | None = ...
    config_suppress_errors: bool | None = ...
    config_inline_inbuilt_nn_modules: bool | None = ...
    specialize_float: bool | None = ...
    dynamo_config: str | None = ...
    is_forward: bool | None = ...
    num_triton_bundles: int | None = ...
    remote_fx_graph_cache_get_time_ms: int | None = ...
    remote_fx_graph_cache_put_time_ms: int | None = ...
    start_time_us: int | None = ...
    duration_us: int | None = ...
    dynamo_cumulative_compile_time_us: int | None = ...
    aot_autograd_cumulative_compile_time_us: int | None = ...
    inductor_cumulative_compile_time_us: int | None = ...
    inductor_code_gen_cumulative_compile_time_us: int | None = ...
    triton_compile_time_us: int | None = ...
    runtime_cudagraphify_time_us: int | None = ...
    runtime_triton_autotune_time_us: int | None = ...
    dynamo_compile_time_before_restart_us: int | None = ...
    distributed_ephemeral_timeout_us: int | None = ...
    structured_logging_overhead_us: int | None = ...
    remote_fx_graph_cache_get_time_us: int | None = ...
    remote_fx_graph_cache_put_time_us: int | None = ...
    backward_cumulative_compile_time_us: int | None = ...
    end_time_us: int | None = ...
    pre_grad_pass_time_us: int | None = ...
    post_grad_pass_time_us: int | None = ...
    joint_graph_pass_time_us: int | None = ...
    log_format_version: int = ...
    inductor_config: str | None = ...
    remote_cache_version: int | None = ...
    inductor_fx_remote_cache_hit_count: int | None = ...
    inductor_fx_remote_cache_miss_count: int | None = ...
    inductor_fx_remote_cache_backend_type: str | None = ...
    inductor_fx_remote_cache_hit_keys: str | None = ...
    inductor_fx_remote_cache_miss_keys: str | None = ...
    cuda_version: str | None = ...
    triton_version: str | None = ...
    feature_usage: dict[str, bool] | None = ...
    compile_time_autotune_time_us: int | None = ...
    is_runtime: bool | None = ...
    gc_time_us: int | None = ...
    tensorify_float_attempt: bool | None = ...
    tensorify_float_success: bool | None = ...
    tensorify_float_failure: set[str] | None = ...
    guard_latency_us: float | None = ...
    recompile_reason: str | None = ...
    num_graph_breaks: int | None = ...
    triton_kernel_compile_times_us: str | None = ...
    ir_count: int | None = ...
    cudagraph_skip_reason: str | None = ...
    python_version: str | None = ...
    pgo_put_remote_code_state_time_us: int | None = ...
    pgo_get_remote_code_state_time_us: int | None = ...
    param_numel: int | None = ...
    param_bytes: int | None = ...
    param_count: int | None = ...
    recompile_user_contexts: set[str] | None = ...
    inline_inbuilt_nn_modules_candidate: bool | None = ...
    @classmethod
    def create(cls, metrics: dict[str, Any]) -> CompilationMetrics:
        """
        Factory method to create a CompilationMetrics from a dict of fields.
        Includes the logic to add legacy fields and any pre-processing, e.g.,
        we transform some fields to comma-separated strings for scuba logging.
        """

DEFAULT_COMPILATION_METRICS_LIMIT = ...
_compilation_metrics: collections.deque[CompilationMetrics] = ...

def add_compilation_metrics_to_chromium(c: CompilationMetrics) -> None:
    """
    These are the common fields in CompilationMetrics that existed before
    metrics_context, and aren't set by MetricsContext.set(). We add the subset
    of them that make sense in `dynamo`/toplevel events in PT2 Compile Events
    directly.

    If you're tempted to add to this list, consider using CompileEventLogger.compilation_metric()
    instead, which will automatically also add it to tlparse and PT2 Compile Events.
    TODO: Get rid of this function and replace it with CompileEventLogger directly instead.
    """

def record_compilation_metrics(
    start_time_ns: int,
    end_time_ns: int,
    metrics: dict[str, Any],
    exc_type: type[BaseException] | None,
    exc_value: BaseException | None,
) -> None: ...

_METRICS_CONTEXT = ...
_RUNTIME_METRICS_CONTEXT = ...

def set_compilation_metrics_limit(new_size: int) -> None: ...
def clear_compilation_metrics() -> None: ...
def get_compilation_metrics() -> list[CompilationMetrics]: ...

class ChromiumEventLogger:
    """
    Logs chromium events to structured logs. tlparse will concatenate these into a perfetto UI link.

    See https://docs.google.com/document/d/1CvAClvFfyA5R-PhYUmn5OOQtYMH4h6I0nSsKchNAySU/preview#heading=h.yr4qxyxotyw for
    a specification of the Chromium Event JSON format.
    """
    def get_stack(self) -> list[str]:
        """
        The main event stack, with every chromium event.
        Logged to tlparse.
        """
    def get_outermost_event(self) -> str | None:
        """
        Get the outermost event name (i.e. the longest running event)
        or None if the stack is empty.
        """
    def get_pt2_compile_substack(self) -> list[str]:
        """
        A smaller subset of the main stack that gets used to log
        PT2 Compile Events internally.
        """
    def get_event_data(self) -> dict[str, Any]: ...
    def __init__(self) -> None: ...
    def try_add_event_data(self, event_name: str, **kwargs: Any) -> None:
        """Same as add_event_data, but will silently not log if the event isn't in the stack."""
    def add_event_data(self, event_name: str, **kwargs: Any) -> None:
        """
        Adds additional metadata info to an in-progress event
        This metadata is recorded in the END event
        """
    def increment(self, event_name: str, key: str, value: int) -> None:
        """Increment an integer event data field by the given amount"""
    def add_to_set(self, event_name: str, key: str, value: Any) -> None:
        """Add a value to a set within a event_name's metadata if it exists"""
    def log_event_start(
        self,
        event_name: str,
        time_ns: int,
        metadata: dict[str, Any],
        log_pt2_compile_event: bool = ...,
        compile_id: CompileId | None = ...,
    ) -> None:
        """
        Logs the start of a single event.
        :param str event_name Name of event to appear in trace
        :param time_ns Timestamp in nanoseconds
        :param metadata: Any extra metadata associated with this event
        :param log_pt2_compile_event: If True, log to pt2_compile_events
        :param compile_id: Explicit compile_id (rather than using the current context)
        """
    def reset(self) -> None: ...
    def log_event_end(
        self,
        event_name: str,
        time_ns: int,
        metadata: dict[str, Any],
        start_time_ns: int,
        log_pt2_compile_event: bool,
        compile_id: CompileId | None = ...,
    ) -> None:
        """
        Logs the end of a single event. This function should only be
        called after log_event_start with the same event_name.
        :param event_name: Name of event to appear in trace
        :param time_ns: Timestamp in nanoseconds
        :param metadata: Any extra metadata associated with this event
        :param start_time_ns: The start time timestamp in nanoseconds
        :param log_pt_compile_event: If True, log to pt2_compile_events
        :param compile_id: Explicit compile_id (rather than using the current context)
        """
    def log_instant_event(
        self, event_name: str, time_ns: int, metadata: dict[str, Any] | None = ..., log_pt2_compile_event: bool = ...
    ) -> None:
        """
        Log an instant event with no associated duration.
        :param str event_name: Name of event to appear in trace
        :param int time_ns Timestamp in nanoseconds
        :param Optional[Dict[str, Any]] metadata: Any extra metadata associated with this event
        :param str cname optional color for the arrow in the trace
        """

CHROMIUM_EVENT_LOG: ChromiumEventLogger | None = ...

def get_chromium_event_logger() -> ChromiumEventLogger: ...
def chromium_event_log_active() -> bool: ...
@contextmanager
def chromium_event_timed(
    event_name: str, reset_event_log_on_exit: bool = ..., log_pt2_compile_event: bool = ...
) -> Generator[Any]:
    """
    Context manager that creates a chromium start and end event. Chromium event
    logging is integrated with dynamo_timed, so you probably want to use that
    instead. Use this context manager only if you want to avoid dynamo_timed.
    """

@dataclasses.dataclass
class CleanupHook:
    """Remove a global variable when hook is called"""

    scope: dict[str, Any]
    name: str
    def __call__(self, *args: Any) -> None: ...
    @staticmethod
    def create(scope: dict[str, Any], name: str, val: Any) -> CleanupHook: ...

class CleanupManager(ExactWeakKeyDictionary):
    count = ...
    instance: ClassVar[CleanupManager]

def clone_tensor(x: torch.Tensor) -> torch.Tensor:
    """Clone the tensor and its gradient"""

def clone_input(x: torch.Tensor, *, dtype: torch.dtype | None = ...) -> torch.Tensor:
    """copy while preserving strides"""

@overload
def clone_inputs(example_inputs: dict[str, T | tuple[T, ...]]) -> dict[str, list[T]]: ...
@overload
def clone_inputs[T](example_inputs: Sequence[T]) -> list[T]: ...
def clone_inputs(example_inputs: Any) -> Any: ...
def skip_frame_if_in_functorch_mode(val: torch.Tensor) -> None: ...
@contextmanager
def preserve_rng_state() -> Generator[None]: ...
def is_jit_model(
    model0: Any,
) -> TypeIs[
    torch.jit._trace.TopLevelTracedModule
    | torch.jit._script.RecursiveScriptModule
    | torch.jit.ScriptFunction[Any, Any]
    | torch.jit.ScriptModule
]: ...
def torchscript(model: Any, example_inputs: Any, verbose: bool = ...) -> Any: ...
def getfile(obj: Any) -> str | None: ...
def is_namedtuple(obj: Any) -> bool:
    """Test if an object is a namedtuple or a torch.return_types.* quasi-namedtuple"""

def is_namedtuple_cls(cls: Any) -> bool:
    """Test if an object is a namedtuple or a (torch.return_types|torch.autograd.forward_ad).* quasi-namedtuple"""

@functools.lru_cache(1)
def namedtuple_fields(cls: type) -> tuple[str, ...]:
    """Get the fields of a namedtuple or a torch.return_types.* quasi-namedtuple"""

def checkpoint_params(gm: torch.fx.GraphModule) -> Callable[[], None]: ...
def timed(model: Any, example_inputs: Iterable[Any], times: int = ...) -> tuple[Any, float]: ...
def check_is_cuda(gm: torch.fx.GraphModule, example_inputs: Iterable[Any]) -> bool: ...
@lru_cache(32)
def rot_n_helper(n: int) -> Callable[..., Any]: ...

common_constant_types: set[type] = ...
if has_triton_package(): ...

def is_safe_constant(v: Any) -> bool: ...
@functools.cache
def common_constants() -> set[int]: ...
def is_torch_sym(value: Any) -> TypeGuard[torch.SymBool | torch.SymInt]: ...
def is_int_specialization_case(value: Any, source: Any) -> bool: ...
def specialize_symnode(arg: Any) -> Any: ...
def guard_if_dyn(arg: Any) -> Any: ...
def check_constant_args(args: Iterable[Any], kwargs: Mapping[Any, Any]) -> bool: ...
def check_unspec_python_args(args: Iterable[Any], kwargs: Mapping[Any, Any]) -> bool: ...
def check_unspec_or_constant_args(args: Iterable[Any], kwargs: Mapping[Any, Any]) -> bool: ...
def check_numpy_ndarray_args(args: Iterable[Any], kwargs: Mapping[Any, Any]) -> bool: ...

dict_keys: type[KeysView[Any]] = ...
dict_values: type[ValuesView[Any]] = ...
dict_items: type[ItemsView[Any, Any]] = ...
odict_values: type[ValuesView[Any]] = ...
tuple_iterator: type[Iterator[Any]] = ...
range_iterator: type[Iterator[Any]] = ...
tuple_iterator_len = ...
object_new = ...
dict_new = ...
dict_methods = ...
set_methods = ...
frozenset_methods = ...
tuple_new = ...
tuple_methods = ...
list_methods = ...
list_getitem = ...
str_methods = ...
K = TypeVar("K")
V = TypeVar("V")

def builtin_dict_keys[K, V](d: dict[K, V]) -> KeysView[K]: ...
def get_items_from_dict[K, V](obj: dict[K, V]) -> Iterable[tuple[K, V | Any]]: ...
def nn_module_new(cls: Any) -> Any: ...
def product[T](it: Iterable[T]) -> int: ...
def tuple_iterator_getitem(it: Any, index: int) -> Any: ...
def dataclass_fields(cls: Any) -> Any: ...

iter_next = ...

def normalize_range_iter(range_iter: Any) -> tuple[int, int, int]: ...
def to_subclass(t: Any, cls: type) -> Any: ...

dict_getitem = ...

def dict_keys_getitem(d: dict[Any, Any], n: int) -> Any: ...
def set_getitem[T](s: set[T], n: int) -> T: ...
def enum_repr(value: Any, local: bool) -> str: ...
def set_example_value(node: torch.fx.Node, example_value: Any) -> None: ...
def slice_length(s: slice, seq_len: int) -> int: ...
def raise_args_mismatch(tx: InstructionTranslatorBase, name: str) -> None: ...
def iter_contains(
    items: Iterable[Any], search: Any, tx: InstructionTranslator, check_tensor_identity: bool = ...
) -> Any: ...
def key_is_id(k: Any) -> TypeIs[torch.Tensor | torch.nn.Module | MethodWrapperType]:
    """Returns whether it indexes dictionaries using its id"""

def key_to_id(value: Any) -> list[Any]: ...
def const_repr(x: Any, *, local: Any) -> str: ...
def dict_keys_repr(const_keys: Any, *, local: Any) -> str: ...

GLOBAL_KEY_PREFIX = ...

def get_safe_global_name(tx: InstructionTranslatorBase, root: str, obj: Any) -> str: ...
def is_in(item: T, *containers: Container[T]) -> bool: ...
def get_unique_name_wrt(prefix: str, *containers: Any, requires_suffix: bool = ...) -> str:
    """
    Return a name that starts with `prefix` and is not in any of the
    `containers` (e.g., map, set).
    """

def wrap_fake_exception(fn: Callable[[], Any]) -> Any: ...
def deepcopy_to_fake_tensor(obj: Any, fake_mode: torch._subclasses.fake_tensor.FakeTensorMode) -> Any: ...
def rmse(ref: torch.Tensor, res: torch.Tensor) -> torch.Tensor:
    """Calculate root mean squared error"""

def same(
    ref: Any,
    res: Any,
    fp64_ref: Any = ...,
    cos_similarity: bool = ...,
    tol: float = ...,
    equal_nan: bool = ...,
    exact_dtype: bool = ...,
    relax_numpy_equality: bool = ...,
    ignore_non_fp: bool = ...,
    log_error: Callable[..., None] = ...,
    use_larger_multiplier_for_smaller_tensor: bool = ...,
    force_max_multiplier: bool = ...,
) -> bool:
    """Check correctness to see if ref and res match"""

def format_func_info(code: CodeType) -> str: ...
@contextlib.contextmanager
def disable_cache_limit() -> Generator[None]: ...

orig_code_map = ...
guard_failures: collections.defaultdict[Any, list[Any]] = ...
graph_break_reasons: list[torch._dynamo.output_graph.GraphCompileReason] = ...
seen_code_map = ...

def get_debug_dir() -> str: ...
def extract_fake_example_value(node: torch.fx.Node, required: bool = ...) -> Any: ...
def ensure_graph_fake(e: Any, tx: InstructionTranslatorBase) -> Any: ...
def get_fake_values_from_nodes(tx: InstructionTranslatorBase, nodes: Any, allow_non_graph_fake: bool) -> Any: ...
def get_fake_value(node: torch.fx.Node, tx: InstructionTranslatorBase, allow_non_graph_fake: bool = ...) -> Any:
    """
    Run the computation represented by `node` using fake tensors and return the result.

    allow_non_graph_fake: whether to allow the return result to be:
        1. non-fake or 2. fake that is not created by this instance of Dynamo.
        If `True`, you must be prepared to deal with such return values, ideally
        by further wrapping them as this graph's fakes.
    """

_current_node = ...

def get_current_node() -> torch.fx.Node | None: ...
@contextmanager
def set_current_node(node: torch.fx.Node) -> Generator[None]: ...
def run_node(tracer: Any, node: torch.fx.Node, args: Any, kwargs: Any, nnmodule: Any) -> Any:
    """
    Runs a given node, with the given args and kwargs.

    Behavior is dictated by a node's op.

    run_node is useful for extracting real values out of nodes.
    See get_real_value for more info on common usage.

    Note: The tracer arg is only used for 'get_attr' ops
    Note: The nnmodule arg is only used for 'call_module' ops

    Nodes that are not call_function, call_method, call_module, or get_attr will
    raise an AssertionError.
    """

def get_real_value(node: torch.fx.Node, tracer: Any) -> Any:
    """
    Run the actual computation represented by `node` and return the result.
    This will execute any dependent nodes in the graph as well.
    """

def assert_no_fake_params_or_buffers(gm: torch.fx.GraphModule) -> None: ...
def fqn(obj: Any) -> str:
    """Returns the fully qualified name of the object."""

def ifdynstaticdefault(count1: Any, count2: Any) -> Any: ...
def import_submodule(mod: types.ModuleType) -> None:
    """Ensure all the files in a given submodule are imported"""

def object_has_getattribute(value: Any) -> bool: ...
def object_setattr_ignore_descriptor(obj: Any, name: str, value: Any) -> None: ...
def class_has_getattribute(cls: type) -> bool: ...
def get_custom_getattr(value: Any, ignore_nn_module_getattr: bool = ...) -> Any | None: ...

class TensorStaticReason(enum.Enum):
    PARAMETER = ...
    NOT_TENSOR = ...
    NN_MODULE_PROPERTY = ...

def tensor_static_reason_to_message(reason: TensorStaticReason) -> str: ...
def tensor_always_has_static_shape(
    tensor: torch.Tensor | Any, is_tensor: bool, tensor_source: Source
) -> tuple[bool, TensorStaticReason | None]:
    """
    Given a tensor, source, and is_tensor flag, determine if a shape should be static.

    Args:
    tensor - the real tensor to evaluate, parameters force a static shape.
    is_tensor - internal dynamo check, essentially "is_tensor": target_cls is TensorVariable,
    tensors not in a TensorVariable for whatever reason are forced static.

    Returns a tuple, where the first element is the bool of whether or not this tensor should have a static shape.
    The second element is a TensorStaticReason, useful for passing to tensor_static_reason_to_message if needed.
    """

def lazy_format_graph_tabular(fn_name: str, gm: torch.fx.GraphModule) -> Any: ...
def format_bytecode(prefix: str, name: str, filename: str, line_no: int, code: Any) -> str: ...

forward_hook_names = ...
backward_hook_names = ...
state_dict_hook_names = ...
all_hook_names = ...

def nn_module_has_global_hooks() -> bool: ...
def nn_module_get_all_hooks(
    mod: torch.nn.Module,
    check_forward_hooks: bool = ...,
    check_backward_hooks: bool = ...,
    check_state_dict_hooks: bool = ...,
) -> list[Any]:
    """
    Sometimes its useful to differentiate between types of hooks such as forward/backward/pre
    hooks executed during module.__call__, and state_dict hooks which are executed separately.
    """

def nnmodule_has_hooks(
    mod: torch.nn.Module,
    check_forward_hooks: bool = ...,
    check_backward_hooks: bool = ...,
    check_state_dict_hooks: bool = ...,
) -> bool:
    """Helper function to check if a module has any hooks attached to it."""

def to_numpy_helper(value: Any) -> Any:
    """Convert tensor and tnp.ndarray to numpy.ndarray."""

def numpy_to_tensor(value: Any) -> Any:
    """Convert tnp.ndarray to tensor, leave other types intact. If a list/tuple, loop through it to convert."""

class numpy_to_tensor_wrapper[P, R]:
    def __init__(self, f: Callable[_P, R]) -> None: ...
    def __call__(self, *args: _P.args, **kwargs: _P.kwargs) -> Any: ...

def numpy_attr_wrapper(obj: Any, name: str) -> Any: ...

class numpy_method_wrapper:
    """Convert obj from torch.Tensor to tnp.ndarray and call method. Then convert result back to torch.Tensor."""
    def __init__(self, method: str) -> None: ...
    def __call__(self, *args: Any, **kwargs: Any) -> Any: ...

class numpy_operator_wrapper[P, R]:
    """Implements dunder methods for tnp.ndarray via functions from the operator library"""
    def __init__(self, op: Callable[..., Any]) -> None: ...
    def __call__(self, *args: _P.args, **kwargs: _P.kwargs) -> Any: ...

def defake(x: Any) -> Any: ...
def is_utils_checkpoint(obj: Any) -> bool: ...
def is_invoke_subgraph(obj: Any) -> bool: ...
def build_invoke_subgraph_variable(**options: Any) -> Any: ...
def build_checkpoint_variable(**options: Any) -> Any: ...
def is_compile_supported(device_type: DeviceLikeType) -> Any: ...

@dataclasses.dataclass
class _Anchors:
    """_Anchors(left_end_lineno: 'int', left_end_offset: 'int', right_start_lineno: 'int', right_start_offset: 'int')"""

    left_end_lineno: int
    left_end_offset: int
    right_start_lineno: int
    right_start_offset: int

def get_instruction_source_311(code: types.CodeType, inst: dis.Instruction) -> str:
    """
    Python 3.11+ only. Returns lines of source code (from code object `code`)
    corresponding to `inst`'s location data, and underlines relevant code to `inst`.

    Example: CALL on `g`:
    f(g(
      ^^
        h(x)))
        ^^^^^

    We need our own implementation in < 3.13 since `format_frame_summary` in
    Python's `traceback` module doesn't handle multi-line expressions
    (and their anchor extraction code is not completely correct).
    """

def get_static_address_type(t: Any) -> Any: ...
def is_rng_state_getter_or_setter(value: Any) -> bool: ...
def is_tensor_base_attr_getter(value: Any) -> bool: ...
def is_tensor_getset_descriptor(name: str) -> bool: ...
def is_torch_function_object(value: Any) -> bool: ...
def has_torch_function(vt: VariableTracker) -> bool: ...
def to_fake_tensor(t: torch.Tensor, fake_mode: torch._subclasses.fake_tensor.FakeTensorMode) -> Any: ...
def is_frozen_dataclass(value: Any) -> bool: ...
def get_first_attr(obj: Any, *attrs: str) -> Any:
    """Return the first available attribute or throw an exception if none is present."""

@contextlib.contextmanager
def maybe_enable_compiled_autograd(
    should_enable: bool, fullgraph: bool = ..., dynamic: bool = ...
) -> Generator[Any]: ...
def invalid_removeable_handle() -> RemovableHandle: ...
def nn_module_proxy(mod: Any) -> Any: ...

class GmWrapper(torch.nn.Module):
    def __init__(self, gm: torch.fx.GraphModule, unflatten_fn: Callable[[list[Any]], Any]) -> None: ...
    def forward(self, *args: Any) -> Any: ...

def flatten_graph_inputs(
    gm: torch.fx.GraphModule, inputs: Any, compile_gm: Callable[[Any, Any], Any]
) -> Callable[..., Any]:
    """
    Mutate inputs so that they are flat and wrap gm such that it
    accepts those inputs.  This is needed for graphs that take
    bumpy inputs.
    """

def get_locals_to_steal(maybe_gm: Any) -> list[Any]: ...
def set_locals_to_steal(gm: torch.fx.GraphModule, locals_to_steal: list[Any]) -> None: ...

class Lit:
    def __init__(self, s: str) -> None: ...

warn_once_cache: set[str] = ...

def warn_once(msg: str, stacklevel: int = ...) -> None: ...
def strip_color_from_string(text: str) -> str: ...
def is_parameter_freezing() -> bool: ...
def get_torch_function_mode_stack() -> list[Any]: ...
def get_torch_function_mode_stack_at(ind: int) -> Any: ...
def set_torch_function_mode_stack(stack: list[Any]) -> None: ...
def clear_torch_function_mode_stack() -> None: ...
def verify_guard_fn_signature(value: Any) -> None: ...
def does_not_override_dict_iter_methods(user_cls: Any) -> bool: ...
@torch._disable_dynamo
def call_size(x: Any, i: int) -> int: ...
@torch._disable_dynamo
def call_stride(x: Any, i: int) -> int: ...
@torch._disable_dynamo
def call_storage_offset(x: Any) -> int: ...

user_obj_id_to_weakref: dict[int, weakref.ReferenceType[object]] = ...

def get_user_object_from_id(obj_id: int) -> Any: ...
def store_user_object_weakref(obj: object) -> None: ...

class CompileTimeInstructionCounter:
    _counter: int = ...
    _id: int = ...
    _depth = ...
    @classmethod
    def start(cls) -> None: ...
    @classmethod
    def end(cls) -> None: ...
    @classmethod
    def clear(cls) -> None: ...
    @classmethod
    def value(cls) -> int: ...
    @classmethod
    @contextmanager
    def record(cls) -> Generator[None]: ...

class CompileCounterInt(int):
    def __add__(self, other: Any) -> CompileCounterInt: ...

def set_feature_use(feature: str, usage: bool) -> None:
    """
    Records whether we are using a feature
    Generally a feature is a JK.
    """

_ddp_optimization_mode: tuple[str, ...] = ...

def get_optimize_ddp_mode() -> str: ...
@contextmanager
def maybe_disable_inference_mode() -> Generator[None]:
    """
    Disables torch.inference_mode for the compilation (still on at runtime).
    This simplifies the compile stack where we can assume that inference_mode
    will always be off.

    Since inference_mode is equivalent to no_grad + some optimizations (version
    counts etc), we turn on no_grad here. The other optimizations are not
    relevant to torch.compile.
    """

@contextmanager
def maybe_disable_inference_mode_for_fake_prop() -> Generator[None]:
    """
    Turns off tracking of inference_mode for fake tensor propagation. With this
    context manager, when a real tensor is converted to fake tensor, the fake
    tensor looses its inference-ness.
    """

def is_node_meta_valid(node: torch.fx.Node | None) -> bool: ...

_error_on_graph_break = ...

@torch._disable_dynamo
def record_pregraph_bytecode_enter() -> AbstractContextManager[None]: ...
@torch._disable_dynamo
def record_pregraph_bytecode_exit(cm: AbstractContextManager[None]) -> None: ...
def get_traced_code() -> list[CodeType] | None: ...
