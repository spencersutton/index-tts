import atexit
import collections
import contextlib
import dataclasses
import dis
import enum
import functools
import sys
import types
import typing
import weakref
import torch
import numpy as np
from collections import Counter
from contextlib import AbstractContextManager, contextmanager
from functools import lru_cache
from types import CodeType, MethodWrapperType
from typing import Any, ClassVar, Generic, Optional, TypeVar, Union, overload
from collections.abc import Callable
from typing import Literal, ParamSpec, TypeAlias, TypeGuard, TypeIs
from torch import fx
from torch._dynamo.metrics_context import MetricsContext, RuntimeMetricsContext
from torch._guards import CompileId, Source
from torch.utils._triton import has_triton_package
from torch.utils.hooks import RemovableHandle
from collections.abc import Container, Generator, ItemsView, Iterable, Iterator, KeysView, Mapping, Sequence, ValuesView
from torch._dynamo.replay_record import ExecutionRecord
from torch._dynamo.symbolic_convert import InstructionTranslator, InstructionTranslatorBase
from torch._dynamo.variables.base import VariableTracker
from torch._prims_common import DeviceLikeType

"""
Utility functions and classes used throughout the TorchDynamo system.

This module contains a collection of helper utilities used by various parts of Dynamo for:
- Performance metrics collection and reporting
- Compilation timing and debugging
- Graph manipulation and tensor operations
- Runtime guards and checks
- Common data structure operations
- Testing and development tools

This is an internal module that provides shared functionality used across the Dynamo codebase.
"""
if typing.TYPE_CHECKING: ...
if np:
    NP_SUPPORTED_MODULES: tuple[types.ModuleType, ...] = ...
    NP_TO_TNP_MODULE = ...
else:
    NP_SUPPORTED_MODULES = ...
    NP_TO_TNP_MODULE = ...
T = TypeVar("T")
R = TypeVar("R")
_P = ParamSpec("_P")
unpatched_nn_module_getattr = ...
unpatched_nn_module_call = ...
unpatched_nn_module_call_impl = ...
counters: collections.defaultdict[str, Counter[str]] = ...
optimus_scuba_log: dict[str, Any] = ...
troubleshooting_url = ...
nnmodule_doc_url = ...
nnmodule_doc_url_msg = ...
log = ...
compilation_time_metrics: dict[str, list[float]] = ...
cumulative_time_spent_ns: dict[str, float] = ...
timer_counter = ...

class ReInplaceTrigger(enum.Enum):
    AUTO_FUNC_V1 = ...
    AUTO_FUNC_V2 = ...
    TRITON_OPS = ...

class ReinplaceCounters:
    _values: collections.defaultdict[str, int] = ...
    @classmethod
    def add_missed_bytes(cls, trigger: ReInplaceTrigger, bytes: int) -> None: ...
    @classmethod
    def add_missed_opportunities(cls, trigger: ReInplaceTrigger, count: int) -> None: ...
    @classmethod
    def clear(cls) -> None: ...
    @classmethod
    def get_total_missed(cls) -> int: ...
    @classmethod
    def get_total_missed_bytes(cls) -> int: ...
    @classmethod
    def log(cls) -> None: ...

def tabulate(rows: list[tuple[str, Any]] | list[list[Any]], headers: tuple[str, ...] | list[str]) -> str: ...

curr_frame = ...

def increment_frame() -> None: ...
def reset_frame_count() -> None: ...

_recompile_user_contexts: list[Callable[[], str]] | None = ...

def register_hook_for_recompile_user_context(hook: Callable[[], str]) -> None: ...
def get_hook_for_recompile_user_context() -> list[Callable[[], str]] | None: ...

op_count = ...

def increment_op_count(cnt: int) -> None: ...
def calculate_time_spent() -> dict[str, float]: ...
def print_time_report() -> None: ...

_METRICS_CONTEXT: MetricsContext
_RUNTIME_METRICS_CONTEXT: RuntimeMetricsContext

def get_metrics_context() -> MetricsContext: ...
def get_runtime_metrics_context() -> RuntimeMetricsContext: ...

class CompileEventLogLevel(enum.Enum):
    CHROMIUM = ...
    PT2_COMPILE = ...
    COMPILATION_METRIC = ...

class CompileEventLogger:
    @staticmethod
    def log_instant_event(
        event_name: str, metadata: dict[str, Any], time_ns: int | None = ..., log_level: CompileEventLogLevel = ...
    ) -> None: ...
    @staticmethod
    def add_data(
        event_name: str, log_level: CompileEventLogLevel, overwrite: bool = ..., **metadata: object
    ) -> None: ...
    @staticmethod
    def add_toplevel(log_level: CompileEventLogLevel, overwrite: bool = ..., **metadata: object) -> None: ...
    @staticmethod
    def increment(event_name: str, log_level: CompileEventLogLevel, key: str, value: int) -> None: ...
    @staticmethod
    def increment_toplevel(key: str, value: int = ..., log_level: CompileEventLogLevel = ...) -> None: ...
    @staticmethod
    def add_to_set(event_name: str, log_level: CompileEventLogLevel, key: str, value: Any) -> None: ...
    @staticmethod
    def add_to_set_toplevel(key: str, value: Any, log_level: CompileEventLogLevel = ...) -> None: ...
    @staticmethod
    def chromium(event_name: str, **metadata: object) -> None: ...
    @staticmethod
    def pt2_compile(event_name: str, **metadata: object) -> None: ...
    @staticmethod
    def compilation_metric(overwrite: bool = ..., **metadata: object) -> None: ...
    @staticmethod
    def instant(event_name: str, metadata: dict[str, Any], time_ns: int | None = ...) -> None: ...
    @staticmethod
    def try_add_pt2_compile(event_name: str, **metadata: object) -> None: ...
    @staticmethod
    def try_(method_fn: Callable[_P, Any], *args: _P.args, **kwargs: _P.kwargs) -> None: ...

_dynamo_timed_tls = ...

@contextmanager
def dynamo_timed(
    key: str,
    phase_name: str | None = ...,
    log_pt2_compile_event: bool = ...,
    metadata: dict[str, object] | None = ...,
    dynamo_compile_column_us: str | None = ...,
    compile_id: CompileId | None = ...,
    is_backward: bool | None = ...,
    log_waitcounter: bool = ...,
    waitcounter_name_override: str | None = ...,
) -> Generator[Any]: ...
@overload
def compile_times(repr: Literal["str"], aggregate: bool = ...) -> str: ...
@overload
def compile_times(repr: Literal["csv"], aggregate: bool = ...) -> tuple[list[str], list[object]]: ...
def compile_times(repr: str = ..., aggregate: bool = ...) -> str | None | tuple[list[str], list[str]]: ...
@atexit.register
def dump_compile_times() -> None: ...

tensortype_to_dtype = ...

class DuplicateWarningChecker:
    def __init__(self, maxsize: int = ...) -> None: ...
    def reset(self) -> None: ...
    def add(self, key: str | tuple[object, object]) -> bool: ...

graph_break_dup_warning_checker = ...

def setup_compile_debug() -> contextlib.ExitStack: ...
def reset_graph_break_dup_checker() -> None: ...
def add_file_handler() -> contextlib.ExitStack: ...
def setup_log_file() -> contextlib.ExitStack: ...
def gen_record_file_name(exc: Exception, code: CodeType) -> str: ...
def write_record_to_file(filename: str, exec_record: ExecutionRecord) -> None: ...
def count_calls(g: fx.Graph) -> int: ...
def identity[T](x: T) -> T: ...
def hashable(x: Any) -> bool: ...
def nothing(*args: Any, **kwargs: Any) -> None: ...

class ExactWeakKeyDictionary:
    def __init__(self) -> None: ...
    def __getitem__(self, key: Any) -> Any: ...
    def get(self, key: Any, default: Any = ...) -> Any: ...
    def __contains__(self, key: Any) -> bool: ...
    def __setitem__(self, key: Any, value: Any) -> None: ...
    def clear(self) -> None: ...

@overload
def istype[T](obj: object, allowed_types: type[T]) -> TypeIs[T]: ...
@overload
def istype(obj: object, allowed_types: tuple[type[list[T]], type[tuple[T, ...]]]) -> TypeIs[T]: ...
@overload
def istype(obj: object, allowed_types: Iterable[type]) -> bool: ...
def istype(obj: object, allowed_types: Any) -> bool: ...

_builtin_final_typing_classes = ...

def is_typing(value: Any) -> bool: ...
def is_numpy_int_type(value: Any) -> bool: ...
def is_numpy_float_type(value: Any) -> bool: ...
@overload
def is_lru_cache_wrapped_function[T](value: Callable[..., T]) -> TypeGuard[functools._lru_cache_wrapper[T]]: ...
@overload
def is_lru_cache_wrapped_function(value: Any) -> TypeGuard[functools._lru_cache_wrapper[Any]]: ...
def is_lru_cache_wrapped_function(value: Any) -> bool: ...

type _FuncTypes = (
    types.FunctionType | types.BuiltinFunctionType | types.MethodDescriptorType | types.WrapperDescriptorType
)

def is_function_or_wrapper(
    value: Any,
) -> TypeIs[_FuncTypes | torch._ops.OpOverloadPacket | torch._ops.OpOverload]: ...
def is_function(value: Any) -> TypeIs[_FuncTypes]: ...

cmp_name_to_op_mapping = ...
cmp_name_to_op_str_mapping = ...

def is_wrapper_or_member_descriptor(
    value: Any,
) -> TypeIs[
    types.GetSetDescriptorType
    | types.MethodDescriptorType
    | types.WrapperDescriptorType
    | types.MemberDescriptorType
    | types.MethodWrapperType
]: ...
def unwrap_if_wrapper(fn: Any) -> Any: ...
def unwrap_with_attr_name_if_wrapper(fn: Any) -> tuple[Any, str | None]: ...
def is_numpy_ndarray(value: Any) -> TypeGuard[np.ndarray]: ...
def istensor(obj: Any) -> bool: ...
def is_lazy_module(mod: Any) -> bool: ...
@functools.lru_cache(4096)
def print_once(*args: Any) -> None: ...
def make_cell(val: Any = ...) -> types.CellType: ...
def proxy_args_kwargs(args: Any, kwargs: Any) -> tuple[tuple[Any, ...], dict[str, Any]]: ...
def to_int_ms(v: float | None) -> int | None: ...
def to_int_us(v: float | None) -> int | None: ...

LOG_FORMAT_VERSION = ...

@dataclasses.dataclass
class CompilationMetrics:
    compile_id: str | None = ...
    frame_key: str | None = ...
    co_name: str | None = ...
    co_filename: str | None = ...
    co_firstlineno: int | None = ...
    cache_size: int | None = ...
    accumulated_cache_size: int | None = ...
    guard_count: int | None = ...
    shape_env_guard_count: int | None = ...
    graph_op_count: int | None = ...
    graph_node_count: int | None = ...
    graph_input_count: int | None = ...
    start_time: float | None = ...
    entire_frame_compile_time_s: float | None = ...
    backend_compile_time_s: float | None = ...
    inductor_compile_time_s: float | None = ...
    code_gen_time_s: float | None = ...
    fail_type: str | None = ...
    fail_reason: str | None = ...
    fail_user_frame_filename: str | None = ...
    fail_user_frame_lineno: int | None = ...
    non_compliant_ops: set[str] | None = ...
    compliant_custom_ops: set[str] | None = ...
    restart_reasons: set[str] | None = ...
    dynamo_time_before_restart_s: float | None = ...
    stack_trace: list[str] | None = ...
    exception_stack_trace: list[str] | None = ...
    graph_node_shapes: str | None = ...
    has_guarded_code: bool | None = ...
    remote_cache_time_saved_s: float | None = ...
    structured_logging_overhead_s: float | None = ...
    config_suppress_errors: bool | None = ...
    config_inline_inbuilt_nn_modules: bool | None = ...
    specialize_float: bool | None = ...
    dynamo_config: str | None = ...
    is_forward: bool | None = ...
    num_triton_bundles: int | None = ...
    remote_fx_graph_cache_get_time_ms: int | None = ...
    remote_fx_graph_cache_put_time_ms: int | None = ...
    start_time_us: int | None = ...
    duration_us: int | None = ...
    dynamo_cumulative_compile_time_us: int | None = ...
    aot_autograd_cumulative_compile_time_us: int | None = ...
    inductor_cumulative_compile_time_us: int | None = ...
    inductor_code_gen_cumulative_compile_time_us: int | None = ...
    triton_compile_time_us: int | None = ...
    runtime_cudagraphify_time_us: int | None = ...
    runtime_triton_autotune_time_us: int | None = ...
    dynamo_compile_time_before_restart_us: int | None = ...
    distributed_ephemeral_timeout_us: int | None = ...
    structured_logging_overhead_us: int | None = ...
    remote_fx_graph_cache_get_time_us: int | None = ...
    remote_fx_graph_cache_put_time_us: int | None = ...
    backward_cumulative_compile_time_us: int | None = ...
    end_time_us: int | None = ...
    pre_grad_pass_time_us: int | None = ...
    post_grad_pass_time_us: int | None = ...
    joint_graph_pass_time_us: int | None = ...
    log_format_version: int = ...
    inductor_config: str | None = ...
    remote_cache_version: int | None = ...
    inductor_fx_remote_cache_hit_count: int | None = ...
    inductor_fx_remote_cache_miss_count: int | None = ...
    inductor_fx_remote_cache_backend_type: str | None = ...
    inductor_fx_remote_cache_hit_keys: str | None = ...
    inductor_fx_remote_cache_miss_keys: str | None = ...
    cuda_version: str | None = ...
    triton_version: str | None = ...
    feature_usage: dict[str, bool] | None = ...
    compile_time_autotune_time_us: int | None = ...
    is_runtime: bool | None = ...
    gc_time_us: int | None = ...
    tensorify_float_attempt: bool | None = ...
    tensorify_float_success: bool | None = ...
    tensorify_float_failure: set[str] | None = ...
    guard_latency_us: float | None = ...
    recompile_reason: str | None = ...
    num_graph_breaks: int | None = ...
    triton_kernel_compile_times_us: str | None = ...
    ir_count: int | None = ...
    cudagraph_skip_reason: str | None = ...
    python_version: str | None = ...
    pgo_put_remote_code_state_time_us: int | None = ...
    pgo_get_remote_code_state_time_us: int | None = ...
    param_numel: int | None = ...
    param_bytes: int | None = ...
    param_count: int | None = ...
    recompile_user_contexts: set[str] | None = ...
    inline_inbuilt_nn_modules_candidate: bool | None = ...
    @classmethod
    def create(cls, metrics: dict[str, Any]) -> CompilationMetrics: ...

DEFAULT_COMPILATION_METRICS_LIMIT = ...
_compilation_metrics: collections.deque[CompilationMetrics] = ...

def add_compilation_metrics_to_chromium(c: CompilationMetrics) -> None: ...
def record_compilation_metrics(
    start_time_ns: int,
    end_time_ns: int,
    metrics: dict[str, Any],
    exc_type: type[BaseException] | None,
    exc_value: BaseException | None,
) -> None: ...

_METRICS_CONTEXT = ...
_RUNTIME_METRICS_CONTEXT = ...

def set_compilation_metrics_limit(new_size: int) -> None: ...
def clear_compilation_metrics() -> None: ...
def get_compilation_metrics() -> list[CompilationMetrics]: ...

class ChromiumEventLogger:
    def get_stack(self) -> list[str]: ...
    def get_outermost_event(self) -> str | None: ...
    def get_pt2_compile_substack(self) -> list[str]: ...
    def get_event_data(self) -> dict[str, Any]: ...
    def __init__(self) -> None: ...
    def try_add_event_data(self, event_name: str, **kwargs: Any) -> None: ...
    def add_event_data(self, event_name: str, **kwargs: Any) -> None: ...
    def increment(self, event_name: str, key: str, value: int) -> None: ...
    def add_to_set(self, event_name: str, key: str, value: Any) -> None: ...
    def log_event_start(
        self,
        event_name: str,
        time_ns: int,
        metadata: dict[str, Any],
        log_pt2_compile_event: bool = ...,
        compile_id: CompileId | None = ...,
    ) -> None: ...
    def reset(self) -> None: ...
    def log_event_end(
        self,
        event_name: str,
        time_ns: int,
        metadata: dict[str, Any],
        start_time_ns: int,
        log_pt2_compile_event: bool,
        compile_id: CompileId | None = ...,
    ) -> None: ...
    def log_instant_event(
        self, event_name: str, time_ns: int, metadata: dict[str, Any] | None = ..., log_pt2_compile_event: bool = ...
    ) -> None: ...

CHROMIUM_EVENT_LOG: ChromiumEventLogger | None = ...

def get_chromium_event_logger() -> ChromiumEventLogger: ...
def chromium_event_log_active() -> bool: ...
@contextmanager
def chromium_event_timed(
    event_name: str, reset_event_log_on_exit: bool = ..., log_pt2_compile_event: bool = ...
) -> Generator[Any]: ...

@dataclasses.dataclass
class CleanupHook:
    scope: dict[str, Any]
    name: str
    def __call__(self, *args: Any) -> None: ...
    @staticmethod
    def create(scope: dict[str, Any], name: str, val: Any) -> CleanupHook: ...

class CleanupManager(ExactWeakKeyDictionary):
    count = ...
    instance: ClassVar[CleanupManager]

def clone_tensor(x: torch.Tensor) -> torch.Tensor: ...
def clone_input(x: torch.Tensor, *, dtype: torch.dtype | None = ...) -> torch.Tensor: ...
@overload
def clone_inputs(example_inputs: dict[str, T | tuple[T, ...]]) -> dict[str, list[T]]: ...
@overload
def clone_inputs[T](example_inputs: Sequence[T]) -> list[T]: ...
def clone_inputs(example_inputs: Any) -> Any: ...
def skip_frame_if_in_functorch_mode(val: torch.Tensor) -> None: ...
@contextmanager
def preserve_rng_state() -> Generator[None]: ...
def is_jit_model(
    model0: Any,
) -> TypeIs[
    torch.jit._trace.TopLevelTracedModule
    | torch.jit._script.RecursiveScriptModule
    | torch.jit.ScriptFunction[Any, Any]
    | torch.jit.ScriptModule
]: ...
def torchscript(model: Any, example_inputs: Any, verbose: bool = ...) -> Any: ...
def getfile(obj: Any) -> str | None: ...
def is_namedtuple(obj: Any) -> bool: ...
def is_namedtuple_cls(cls: Any) -> bool: ...
@functools.lru_cache(1)
def namedtuple_fields(cls: type) -> tuple[str, ...]: ...
def checkpoint_params(gm: torch.fx.GraphModule) -> Callable[[], None]: ...
def timed(model: Any, example_inputs: Iterable[Any], times: int = ...) -> tuple[Any, float]: ...
def check_is_cuda(gm: torch.fx.GraphModule, example_inputs: Iterable[Any]) -> bool: ...
@lru_cache(32)
def rot_n_helper(n: int) -> Callable[..., Any]: ...

common_constant_types: set[type] = ...
if has_triton_package(): ...

def is_safe_constant(v: Any) -> bool: ...
@functools.cache
def common_constants() -> set[int]: ...
def is_torch_sym(value: Any) -> TypeGuard[torch.SymBool | torch.SymInt]: ...
def is_int_specialization_case(value: Any, source: Any) -> bool: ...
def specialize_symnode(arg: Any) -> Any: ...
def guard_if_dyn(arg: Any) -> Any: ...
def check_constant_args(args: Iterable[Any], kwargs: Mapping[Any, Any]) -> bool: ...
def check_unspec_python_args(args: Iterable[Any], kwargs: Mapping[Any, Any]) -> bool: ...
def check_unspec_or_constant_args(args: Iterable[Any], kwargs: Mapping[Any, Any]) -> bool: ...
def check_numpy_ndarray_args(args: Iterable[Any], kwargs: Mapping[Any, Any]) -> bool: ...

dict_keys: type[KeysView[Any]] = ...
dict_values: type[ValuesView[Any]] = ...
dict_items: type[ItemsView[Any, Any]] = ...
odict_values: type[ValuesView[Any]] = ...
tuple_iterator: type[Iterator[Any]] = ...
range_iterator: type[Iterator[Any]] = ...
tuple_iterator_len = ...
object_new = ...
dict_new = ...
dict_methods = ...
set_methods = ...
frozenset_methods = ...
tuple_new = ...
tuple_methods = ...
list_methods = ...
list_getitem = ...
str_methods = ...
K = TypeVar("K")
V = TypeVar("V")

def builtin_dict_keys[K, V](d: dict[K, V]) -> KeysView[K]: ...
def get_items_from_dict[K, V](obj: dict[K, V]) -> Iterable[tuple[K, V | Any]]: ...
def nn_module_new(cls: Any) -> Any: ...
def product[T](it: Iterable[T]) -> int: ...
def tuple_iterator_getitem(it: Any, index: int) -> Any: ...
def dataclass_fields(cls: Any) -> Any: ...

iter_next = ...

def normalize_range_iter(range_iter: Any) -> tuple[int, int, int]: ...
def to_subclass(t: Any, cls: type) -> Any: ...

dict_getitem = ...

def dict_keys_getitem(d: dict[Any, Any], n: int) -> Any: ...
def set_getitem[T](s: set[T], n: int) -> T: ...
def enum_repr(value: Any, local: bool) -> str: ...
def set_example_value(node: torch.fx.Node, example_value: Any) -> None: ...
def slice_length(s: slice, seq_len: int) -> int: ...
def raise_args_mismatch(tx: InstructionTranslatorBase, name: str) -> None: ...
def iter_contains(
    items: Iterable[Any], search: Any, tx: InstructionTranslator, check_tensor_identity: bool = ...
) -> Any: ...
def key_is_id(k: Any) -> TypeIs[torch.Tensor | torch.nn.Module | MethodWrapperType]: ...
def key_to_id(value: Any) -> list[Any]: ...
def const_repr(x: Any, *, local: Any) -> str: ...
def dict_keys_repr(const_keys: Any, *, local: Any) -> str: ...

GLOBAL_KEY_PREFIX = ...

def get_safe_global_name(tx: InstructionTranslatorBase, root: str, obj: Any) -> str: ...
def is_in(item: T, *containers: Container[T]) -> bool: ...
def get_unique_name_wrt(prefix: str, *containers: Any, requires_suffix: bool = ...) -> str: ...
def wrap_fake_exception(fn: Callable[[], Any]) -> Any: ...
def deepcopy_to_fake_tensor(obj: Any, fake_mode: torch._subclasses.fake_tensor.FakeTensorMode) -> Any: ...
def rmse(ref: torch.Tensor, res: torch.Tensor) -> torch.Tensor: ...
def same(
    ref: Any,
    res: Any,
    fp64_ref: Any = ...,
    cos_similarity: bool = ...,
    tol: float = ...,
    equal_nan: bool = ...,
    exact_dtype: bool = ...,
    relax_numpy_equality: bool = ...,
    ignore_non_fp: bool = ...,
    log_error: Callable[..., None] = ...,
    use_larger_multiplier_for_smaller_tensor: bool = ...,
    force_max_multiplier: bool = ...,
) -> bool: ...
def format_func_info(code: CodeType) -> str: ...
@contextlib.contextmanager
def disable_cache_limit() -> Generator[None]: ...

orig_code_map = ...
guard_failures: collections.defaultdict[Any, list[Any]] = ...
graph_break_reasons: list[torch._dynamo.output_graph.GraphCompileReason] = ...
seen_code_map = ...

def get_debug_dir() -> str: ...
def extract_fake_example_value(node: torch.fx.Node, required: bool = ...) -> Any: ...
def ensure_graph_fake(e: Any, tx: InstructionTranslatorBase) -> Any: ...
def get_fake_values_from_nodes(tx: InstructionTranslatorBase, nodes: Any, allow_non_graph_fake: bool) -> Any: ...
def get_fake_value(node: torch.fx.Node, tx: InstructionTranslatorBase, allow_non_graph_fake: bool = ...) -> Any: ...

_current_node = ...

def get_current_node() -> torch.fx.Node | None: ...
@contextmanager
def set_current_node(node: torch.fx.Node) -> Generator[None]: ...
def run_node(tracer: Any, node: torch.fx.Node, args: Any, kwargs: Any, nnmodule: Any) -> Any: ...
def get_real_value(node: torch.fx.Node, tracer: Any) -> Any: ...
def assert_no_fake_params_or_buffers(gm: torch.fx.GraphModule) -> None: ...
def fqn(obj: Any) -> str: ...
def ifdynstaticdefault(count1: Any, count2: Any) -> Any: ...
def import_submodule(mod: types.ModuleType) -> None: ...
def object_has_getattribute(value: Any) -> bool: ...
def object_setattr_ignore_descriptor(obj: Any, name: str, value: Any) -> None: ...
def class_has_getattribute(cls: type) -> bool: ...
def get_custom_getattr(value: Any, ignore_nn_module_getattr: bool = ...) -> Any | None: ...

class TensorStaticReason(enum.Enum):
    PARAMETER = ...
    NOT_TENSOR = ...
    NN_MODULE_PROPERTY = ...

def tensor_static_reason_to_message(reason: TensorStaticReason) -> str: ...
def tensor_always_has_static_shape(
    tensor: torch.Tensor | Any, is_tensor: bool, tensor_source: Source
) -> tuple[bool, TensorStaticReason | None]: ...
def lazy_format_graph_tabular(fn_name: str, gm: torch.fx.GraphModule) -> Any: ...
def format_bytecode(prefix: str, name: str, filename: str, line_no: int, code: Any) -> str: ...

forward_hook_names = ...
backward_hook_names = ...
state_dict_hook_names = ...
all_hook_names = ...

def nn_module_has_global_hooks() -> bool: ...
def nn_module_get_all_hooks(
    mod: torch.nn.Module,
    check_forward_hooks: bool = ...,
    check_backward_hooks: bool = ...,
    check_state_dict_hooks: bool = ...,
) -> list[Any]: ...
def nnmodule_has_hooks(
    mod: torch.nn.Module,
    check_forward_hooks: bool = ...,
    check_backward_hooks: bool = ...,
    check_state_dict_hooks: bool = ...,
) -> bool: ...
def to_numpy_helper(value: Any) -> Any: ...
def numpy_to_tensor(value: Any) -> Any: ...

class numpy_to_tensor_wrapper[**P, R]:
    def __init__(self, f: Callable[_P, R]) -> None: ...
    def __call__(self, *args: _P.args, **kwargs: _P.kwargs) -> Any: ...

def numpy_attr_wrapper(obj: Any, name: str) -> Any: ...

class numpy_method_wrapper:
    def __init__(self, method: str) -> None: ...
    def __call__(self, *args: Any, **kwargs: Any) -> Any: ...

class numpy_operator_wrapper[**P, R]:
    def __init__(self, op: Callable[..., Any]) -> None: ...
    def __call__(self, *args: _P.args, **kwargs: _P.kwargs) -> Any: ...

def defake(x: Any) -> Any: ...
def is_utils_checkpoint(obj: Any) -> bool: ...
def is_invoke_subgraph(obj: Any) -> bool: ...
def build_invoke_subgraph_variable(**options: Any) -> Any: ...
def build_checkpoint_variable(**options: Any) -> Any: ...
def is_compile_supported(device_type: DeviceLikeType) -> Any: ...

@dataclasses.dataclass
class _Anchors:
    left_end_lineno: int
    left_end_offset: int
    right_start_lineno: int
    right_start_offset: int

def get_instruction_source_311(code: types.CodeType, inst: dis.Instruction) -> str: ...
def get_static_address_type(t: Any) -> Any: ...
def is_rng_state_getter_or_setter(value: Any) -> bool: ...
def is_tensor_base_attr_getter(value: Any) -> bool: ...
def is_tensor_getset_descriptor(name: str) -> bool: ...
def is_torch_function_object(value: Any) -> bool: ...
def has_torch_function(vt: VariableTracker) -> bool: ...
def to_fake_tensor(t: torch.Tensor, fake_mode: torch._subclasses.fake_tensor.FakeTensorMode) -> Any: ...
def is_frozen_dataclass(value: Any) -> bool: ...
def get_first_attr(obj: Any, *attrs: str) -> Any: ...
@contextlib.contextmanager
def maybe_enable_compiled_autograd(
    should_enable: bool, fullgraph: bool = ..., dynamic: bool = ...
) -> Generator[Any]: ...
def invalid_removeable_handle() -> RemovableHandle: ...
def nn_module_proxy(mod: Any) -> Any: ...

class GmWrapper(torch.nn.Module):
    def __init__(self, gm: torch.fx.GraphModule, unflatten_fn: Callable[[list[Any]], Any]) -> None: ...
    def forward(self, *args: Any) -> Any: ...

def flatten_graph_inputs(
    gm: torch.fx.GraphModule, inputs: Any, compile_gm: Callable[[Any, Any], Any]
) -> Callable[..., Any]: ...
def get_locals_to_steal(maybe_gm: Any) -> list[Any]: ...
def set_locals_to_steal(gm: torch.fx.GraphModule, locals_to_steal: list[Any]) -> None: ...

class Lit:
    def __init__(self, s: str) -> None: ...

warn_once_cache: set[str] = ...

def warn_once(msg: str, stacklevel: int = ...) -> None: ...
def strip_color_from_string(text: str) -> str: ...
def is_parameter_freezing() -> bool: ...
def get_torch_function_mode_stack() -> list[Any]: ...
def get_torch_function_mode_stack_at(ind: int) -> Any: ...
def set_torch_function_mode_stack(stack: list[Any]) -> None: ...
def clear_torch_function_mode_stack() -> None: ...
def verify_guard_fn_signature(value: Any) -> None: ...
def does_not_override_dict_iter_methods(user_cls: Any) -> bool: ...
@torch._disable_dynamo
def call_size(x: Any, i: int) -> int: ...
@torch._disable_dynamo
def call_stride(x: Any, i: int) -> int: ...
@torch._disable_dynamo
def call_storage_offset(x: Any) -> int: ...

user_obj_id_to_weakref: dict[int, weakref.ReferenceType[object]] = ...

def get_user_object_from_id(obj_id: int) -> Any: ...
def store_user_object_weakref(obj: object) -> None: ...

class CompileTimeInstructionCounter:
    _counter: int = ...
    _id: int = ...
    _depth = ...
    @classmethod
    def start(cls) -> None: ...
    @classmethod
    def end(cls) -> None: ...
    @classmethod
    def clear(cls) -> None: ...
    @classmethod
    def value(cls) -> int: ...
    @classmethod
    @contextmanager
    def record(cls) -> Generator[None]: ...

class CompileCounterInt(int):
    def __add__(self, other: Any) -> CompileCounterInt: ...

def set_feature_use(feature: str, usage: bool) -> None: ...

_ddp_optimization_mode: tuple[str, ...] = ...

def get_optimize_ddp_mode() -> str: ...
@contextmanager
def maybe_disable_inference_mode() -> Generator[None]: ...
@contextmanager
def maybe_disable_inference_mode_for_fake_prop() -> Generator[None]: ...
def is_node_meta_valid(node: torch.fx.Node | None) -> bool: ...

_error_on_graph_break = ...

@torch._disable_dynamo
def record_pregraph_bytecode_enter() -> AbstractContextManager[None]: ...
@torch._disable_dynamo
def record_pregraph_bytecode_exit(cm: AbstractContextManager[None]) -> None: ...
def get_traced_code() -> list[CodeType] | None: ...
