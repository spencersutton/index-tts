"""
This file contains a collection of context manager classes used by Dynamo for tracking
and managing various PyTorch runtime states during graph compilation. These context
managers handle different aspects of PyTorch's execution environment, including:

- Autograd states (grad mode, inference mode)
- CUDA streams and events
- Profiling contexts
- Deterministic algorithms
- Forward/backward AD modes
- SDPA (Scaled Dot Product Attention) kernels
- FSDP (Fully Sharded Data Parallel) states
- AMP (Automatic Mixed Precision) autocast states

The context managers ensure proper state transitions during graph compilation by
tracking enter/exit points and managing cleanup operations. They help maintain
consistency between eager execution and compiled graph behavior by capturing and
restoring state changes.
"""

from torch._dynamo.codegen import PyCodegen
from torch._dynamo.symbolic_convert import InstructionTranslator

from .base import VariableTracker
from .user_defined import UserDefinedObjectVariable

class ContextWrappingVariable(VariableTracker):
    _nonvar_fields = ...
    def __init__(self, target_values, initial_values=..., **kwargs) -> None: ...
    def enter(self, tx): ...
    def set_cleanup_hook(self, tx: InstructionTranslator, fn=...): ...
    def exit(self, tx: InstructionTranslator, *args): ...
    def reconstruct_type(self, codegen: PyCodegen): ...
    def reconstruct(self, codegen: PyCodegen): ...
    def module_name(self): ...
    def fn_name(self): ...
    def call_function(
        self, tx: InstructionTranslator, args: list[VariableTracker], kwargs: dict[str, VariableTracker]
    ) -> VariableTracker: ...
    def supports_graph_breaks(self): ...
    def exit_on_graph_break(self): ...
    def cleanup(self): ...
    def cleanup_assert(self): ...

class GenericContextWrappingVariable(UserDefinedObjectVariable):
    def __init__(self, cm_obj, **kwargs) -> None: ...
    def module_name(self): ...
    def fn_name(self): ...
    def enter(self, tx): ...
    def exit(self, tx: InstructionTranslator, *args): ...
    def supports_graph_breaks(self): ...
    def exit_on_graph_break(self): ...

class RepararametrizeModuleContextVariable(GenericContextWrappingVariable):
    def __init__(self, ctx_manager_vt, mod) -> None: ...
    def enter(self, tx: InstructionTranslator): ...
    def exit(self, tx: InstructionTranslator, *args): ...
    def __getattr__(self, name): ...

class GradInplaceRequiresGradCtxManagerVariable(ContextWrappingVariable):
    """represents torch grad requires grad"""
    @staticmethod
    def create(tx: InstructionTranslator, target_values, **kwargs): ...
    def enter(self, tx): ...
    def exit(self, tx: InstructionTranslator, *args): ...

class TemporarilyPopInterpreterStackCtxManagerVariable(ContextWrappingVariable):
    """represents torch._functorch.pyfunction.temporarily_pop_interpreter_stack()"""
    @staticmethod
    def create(tx: InstructionTranslator, target_values, **kwargs): ...
    def enter(self, tx): ...
    def exit(self, tx: InstructionTranslator, *args): ...

class JvpIncrementNestingCtxManagerVariable(ContextWrappingVariable):
    """represents torch.func.jvp increment/decrement nesting"""

    _guards_singleton = ...
    @staticmethod
    def create(tx: InstructionTranslator, **kwargs): ...
    def enter(self, tx): ...
    def exit(self, tx: InstructionTranslator, *args): ...

class SetFwdGradEnabledContextManager(ContextWrappingVariable):
    """represents torch.autograd.forward_ad._set_fwd_grad_enabled() to enable/disable fwd grad"""
    @staticmethod
    def create(tx: InstructionTranslator, target_values, **kwargs): ...
    def enter(self, tx): ...
    def exit(self, tx: InstructionTranslator, *args): ...

class DualLevelContextManager(ContextWrappingVariable):
    """Represents torch.autograd.forward_ad.dual_level ctx manager"""

    _guards_singleton = ...
    @staticmethod
    def create(tx: InstructionTranslator, **kwargs): ...
    def enter(self, tx): ...
    def exit(self, tx: InstructionTranslator, *args): ...

class GradIncrementNestingCtxManagerVariable(ContextWrappingVariable):
    """represents torch.func.grad increment/decrement nesting"""

    _guards_singleton = ...
    @staticmethod
    def create(tx: InstructionTranslator, **kwargs): ...
    def enter(self, tx): ...
    def exit(self, tx: InstructionTranslator, *args): ...

class CatchWarningsCtxManagerVariable(ContextWrappingVariable):
    """Delay a call to warnings.catch_warnings"""
    @staticmethod
    def create(tx: InstructionTranslator, catch_warnings_args): ...
    def __init__(self, catch_warnings_args, **kwargs) -> None: ...
    def enter(self, tx): ...
    def reconstruct(self, cg): ...

class VmapIncrementNestingCtxManagerVariable(ContextWrappingVariable):
    """represents torch VMap increment/decrement nesting"""

    _guards_singleton = ...
    @staticmethod
    def create(tx: InstructionTranslator, target_values, **kwargs): ...
    def enter(self, tx): ...
    def exit(self, tx: InstructionTranslator, *args): ...

class GradModeVariable(ContextWrappingVariable):
    """represents torch.{no_grad,enable_grad,set_grad_mode}()"""

    _guards_singleton = ...
    @staticmethod
    def create(tx: InstructionTranslator, target_value, initialized=..., **kwargs): ...
    def __init__(self, target_values, initial_values=..., initialized=..., **kwargs) -> None: ...
    def enter(self, tx): ...
    def exit(self, tx: InstructionTranslator, *args): ...
    def call_function(
        self, tx: InstructionTranslator, args: list[VariableTracker], kwargs: dict[str, VariableTracker]
    ): ...
    def module_name(self): ...
    def fn_name(self): ...

class InferenceModeVariable(ContextWrappingVariable):
    @staticmethod
    def create(tx: InstructionTranslator, target_value, **kwargs): ...
    def __init__(self, target_values, initial_values=..., **kwargs) -> None: ...
    def exit(self, tx: InstructionTranslator, *args): ...
    def enter(self, tx): ...
    def module_name(self): ...
    def fn_name(self): ...

class CUDADeviceVariable(ContextWrappingVariable):
    """represents torch.cuda.device"""
    @staticmethod
    def create(tx: InstructionTranslator, device, **kwargs): ...
    def __init__(self, target_values, initial_values=..., **kwargs) -> None: ...
    def exit(self, tx: InstructionTranslator, *args): ...
    def enter(self, tx): ...
    def module_name(self): ...
    def fn_name(self): ...

class TorchFunctionDisableVariable(ContextWrappingVariable):
    """represents whether torch function overrides are enabled or not"""

    _guards_singleton = ...
    @staticmethod
    def create(tx: InstructionTranslator, **kwargs): ...
    def __init__(self, target_values, initial_values=..., only_subclass=..., **kwargs) -> None: ...
    def set_cleanup_hook(self, tx: InstructionTranslator, fn=...): ...
    def module_name(self): ...
    def fn_name(self): ...

class DeterministicAlgorithmsVariable(ContextWrappingVariable):
    """represents torch.{are_deterministic_algorithms_enabled,use_deterministic_algorithms}()"""

    _guards_singleton = ...
    @staticmethod
    def create(tx: InstructionTranslator, target_value, **kwargs): ...
    def __init__(self, target_values, initial_values=..., **kwargs) -> None: ...
    def enter(self, tx): ...
    def module_name(self): ...
    def fn_name(self): ...

class DisabledSavedTensorsHooksVariable(ContextWrappingVariable):
    """represents torch.autograd.graph.disable_saved_tensors_hook."""
    @staticmethod
    def create(tx: InstructionTranslator, target_value, **kwargs): ...
    def __init__(self, target_values, initial_values=..., **kwargs) -> None: ...
    def enter(self, tx): ...
    def module_name(self): ...
    def fn_name(self): ...

class AutocastModeVariable(ContextWrappingVariable):
    @staticmethod
    def create(func, args, kwargs): ...
    def __init__(self, target_values, initial_values=..., **kwargs) -> None: ...
    def exit(self, tx: InstructionTranslator, *args): ...
    def enter(self, tx): ...
    def module_name(self): ...
    def fn_name(self): ...

class NullContextVariable(ContextWrappingVariable):
    """This class represents Python contextlib.nullcontext."""
    def __init__(self, target_values=..., **kwargs) -> None: ...
    def enter(self, tx): ...
    def exit(self, tx: InstructionTranslator, *args): ...
    def module_name(self): ...
    def fn_name(self): ...

class ProfilerContextVariable(ContextWrappingVariable):
    """
    This class represents a set of torch profiler context objects, where Dynamo
    ignores all the side-effects in the __init__, __enter__ and __exit__ methods
    by treating the object mostly as a `contextlib.nullcontext`, except for edge
    cases like the `__enter__` method which returns the object itself rather
    than `None`, per implementation of the torch objects.
    """
    def __init__(self, **kwargs) -> None: ...
    def enter(self, tx): ...
    def exit(self, tx: InstructionTranslator, *args): ...
    def module_name(self): ...
    def fn_name(self): ...
    def reconstruct(self, cg): ...

class StreamContextVariable(ContextWrappingVariable):
    @staticmethod
    def create(tx: InstructionTranslator, target_value, **kwargs): ...
    def __init__(self, target_values, device, initial_values=..., **kwargs) -> None: ...
    def enter(self, tx): ...
    def exit(self, tx: InstructionTranslator, *args): ...

class PreserveVersionContextVariable(ContextWrappingVariable):
    """Wraps torch.autograd._unsafe_preserve_version_counter"""
    @staticmethod
    def constructor(tx): ...
    def __init__(self, tensors, prev_versions, **kwargs) -> None: ...
    def enter(self, tx): ...
    def exit(self, tx: InstructionTranslator, *args): ...
    def reconstruct(self, codegen: PyCodegen): ...

class FSDPParamGroupUseTrainingStateVariable(ContextWrappingVariable):
    _guards_singleton = ...
    @staticmethod
    def create(tx: InstructionTranslator, param_group_var, target_value, **kwargs): ...
    def __init__(self, param_group_var, target_values, initial_values=..., **kwargs) -> None: ...
    def enter(self, tx): ...
    def exit(self, tx: InstructionTranslator, *args): ...
    def call_function(
        self, tx: InstructionTranslator, args: list[VariableTracker], kwargs: dict[str, VariableTracker]
    ): ...
    def module_name(self): ...
    def fn_name(self): ...

class SDPAKernelVariable(ContextWrappingVariable):
    """represents torch.nn.attention.sdpa_kernel"""
    @staticmethod
    def create(tx: InstructionTranslator, backends, set_priority=..., **kwargs): ...
    def __init__(
        self, target_values: list[torch.nn.attention.SDPBackend], initial_values=..., set_priority: bool = ..., **kwargs
    ) -> None: ...
    def enter(self, tx): ...
    def exit(self, tx: InstructionTranslator, *args): ...
    def module_name(self): ...
    def fn_name(self): ...

class StreamVariable(VariableTracker):
    def __init__(self, proxy, value, device, **kwargs) -> None: ...
    def python_type(self): ...
    def call_method(
        self, tx, name, args: list[VariableTracker], kwargs: dict[str, VariableTracker]
    ) -> VariableTracker: ...
    def as_proxy(self): ...
    def reconstruct(self, codegen: PyCodegen): ...

class EventVariable(VariableTracker):
    def __init__(self, proxy, value, **kwargs) -> None: ...
    def call_method(
        self, tx, name, args: list[VariableTracker], kwargs: dict[str, VariableTracker]
    ) -> VariableTracker: ...
    def as_proxy(self): ...
    def reconstruct(self, codegen: PyCodegen): ...

class DynamoConfigPatchVariable(ContextWrappingVariable):
    """represents torch._dynamo.patch_dynamo_config"""
    def __init__(self, target_values, **kwargs) -> None: ...
    def module_name(self): ...
    def fn_name(self): ...

class ErrorOnGraphBreakVariable(ContextWrappingVariable):
    """represents torch._dynamo.error_on_graph_break"""
    def __init__(self, error_on_graph_break, **kwargs) -> None: ...
    def module_name(self): ...
    def fn_name(self): ...

class WithExitFunctionVariable(VariableTracker):
    _nonvar_fields = ...
    def __init__(self, ctx: ContextWrappingVariable | GenericContextWrappingVariable, target, **kwargs) -> None: ...
    def call_function(
        self, tx: InstructionTranslator, args: list[VariableTracker], kwargs: dict[str, VariableTracker]
    ) -> VariableTracker: ...
    def reconstruct(self, codegen: PyCodegen): ...
