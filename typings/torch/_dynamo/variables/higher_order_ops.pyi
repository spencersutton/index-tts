import contextlib
from collections.abc import Sequence
from dataclasses import dataclass
from typing import Any

from torch._dynamo.symbolic_convert import InstructionTranslator
from torch._dynamo.variables.functions import UserFunctionVariable
from torch._guards import Source
from torch._ops import HigherOrderOperator
from torch.utils import _pytree as pytree

from .base import VariableTracker

log = ...
hc_log = ...

@dataclass
class OutputSpec:
    treespec: pytree.TreeSpec
    masks_to_filter_const_values: list[bool] | None = ...
    const_values: list[Any] | None = ...
    def __post_init__(self): ...

def raise_hard_error_if_graph_break(reason): ...
@contextlib.contextmanager
def discard_graph_changes(tx): ...
def check_meta_consistency_vt(
    vars1: list[VariableTracker],
    vars2: list[VariableTracker],
    lhs_name: str,
    rhs_name: str,
    include_contiguity: bool = ...,
) -> None: ...
@contextlib.contextmanager
def dynamo_enable_grad(tx: InstructionTranslator, enable=...): ...
@contextlib.contextmanager
def dynamo_under_activation_checkpoint(tx: InstructionTranslator): ...
def find_mismatched_vars(var, types, allow_none=...): ...
def only_consist_of(var, types, allow_none=...): ...
def are_same_graph_modules(fn_name, a_mod, b_mod, fake_mode): ...
def validate_args_and_maybe_create_graph_inputs(
    sub_args, tracer, tx, set_subgraph_inputs, description, sub_args_names=...
): ...
def speculate_subgraph(
    tx,
    f,
    sub_args,
    sub_kwargs,
    description,
    *,
    source_target=...,
    always_restore=...,
    enable_grad=...,
    set_subgraph_inputs=...,
    restore_side_effects=...,
    should_flatten_outputs=...,
    remove_consts_from_outputs=...,
    under_activation_checkpoint=...,
    supports_input_mutation=...,
    supports_aliasing=...,
    tracer=...,
): ...
def make_attr(tx: InstructionTranslator, name): ...

class TorchHigherOrderOperatorVariable(VariableTracker):
    def __init__(self, value: HigherOrderOperator, source: Source | None = ..., **kwargs) -> None: ...
    @staticmethod
    def make(value, source=..., **kwargs): ...
    def call_function(
        self, tx: InstructionTranslator, args: Sequence[VariableTracker], kwargs: dict[str, VariableTracker]
    ) -> VariableTracker: ...
    def as_python_constant(self): ...

class CustomFunctionHigherOrderOperatorVariable(TorchHigherOrderOperatorVariable): ...

class CondHigherOrderVariable(TorchHigherOrderOperatorVariable):
    supports_input_mutation = ...
    supports_aliasing = ...

class CallTorchbindHigherOrderVariable(TorchHigherOrderOperatorVariable):
    def __init__(self, hop, source, script_obj_var, method_name) -> None: ...

def validate_subgraph_output_types(output: VariableTracker): ...

class WhileLoopHigherOrderVariable(TorchHigherOrderOperatorVariable):
    supports_input_mutation = ...
    supports_aliasing = ...

class WhileLoopStackOutputHigherOrderVariable(TorchHigherOrderOperatorVariable):
    supports_input_mutation = ...
    supports_aliasing = ...

class AssociativeScanHigherOrderVariable(TorchHigherOrderOperatorVariable):
    supports_input_mutation = ...
    supports_aliasing = ...

class ScanHigherOrderVariable(TorchHigherOrderOperatorVariable):
    supports_input_mutation = ...
    supports_aliasing = ...

def non_single_tensor_return_unsupported(api, ret): ...

class MapHigherOrderVariable(TorchHigherOrderOperatorVariable):
    supports_input_mutation = ...
    supports_aliasing = ...

class ExecutorchCallDelegateHigherOrderVariable(TorchHigherOrderOperatorVariable): ...

class FunctorchHigherOrderVariable(UserFunctionVariable):
    def call_function(
        self, tx: InstructionTranslator, args: list[VariableTracker], kwargs: dict[str, VariableTracker]
    ) -> VariableTracker: ...

class FunctionalCallVariable(FunctorchHigherOrderVariable):
    def call_function(self, tx, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker: ...

class ReparametrizeModuleCallVariable(FunctorchHigherOrderVariable):
    def __init__(self, *args, **kwargs) -> None: ...
    def call_function(self, tx, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker: ...

class WrapHigherOrderVariable(TorchHigherOrderOperatorVariable):
    supports_input_mutation = ...
    supports_aliasing = ...
    def install_subgraph_in_output_graph(self, tx, fn_vt, fn_args_vt, kwargs, body_gmod, attr_name=...): ...
    def create_wrapped_node(
        self,
        tx: InstructionTranslator,
        fn_vt,
        fn_args_vt,
        kwargs,
        description,
        under_activation_checkpoint=...,
        *,
        subgraph_name=...,
    ): ...

class WrapWithSetGradEnabledHigherOrderVariable(TorchHigherOrderOperatorVariable):
    def call_function(
        self, tx: InstructionTranslator, args: list[VariableTracker], kwargs: dict[str, VariableTracker]
    ) -> VariableTracker: ...

class WrapWithAutocastHigherOrderVariable(TorchHigherOrderOperatorVariable):
    def call_function(
        self, tx: InstructionTranslator, args: list[VariableTracker], kwargs: dict[str, VariableTracker]
    ) -> VariableTracker: ...

class HintsWrapperHigherOrderVariable(TorchHigherOrderOperatorVariable): ...
class OutDtypeHigherOrderVariable(TorchHigherOrderOperatorVariable): ...
class StrictModeHigherOrderVariable(TorchHigherOrderOperatorVariable): ...
class CheckpointHigherOrderVariable(WrapHigherOrderVariable): ...

class DynamoBypassingWrapperHigherOrderVariable(WrapHigherOrderVariable):
    def __init__(self, hop, source) -> None: ...

class ExportTracepointHigherOrderVariable(TorchHigherOrderOperatorVariable):
    def call_function(
        self, tx: InstructionTranslator, args: list[VariableTracker], kwargs: dict[str, VariableTracker]
    ) -> VariableTracker: ...

class RunWithRNGStateHigherOrderVariable(TorchHigherOrderOperatorVariable): ...
class AutoFunctionalizeHigherOrderVariable(TorchHigherOrderOperatorVariable): ...

class FlexAttentionBackwardHighOrderVariable(TorchHigherOrderOperatorVariable):
    def proxy_submod(self, tx, arg): ...
    def to_proxy(self, tx, arg): ...

class TraceWrappedHigherOrderOperatorVariable(TorchHigherOrderOperatorVariable): ...

class FlexAttentionHigherOrderVariable(TorchHigherOrderOperatorVariable):
    @staticmethod
    def normalize_to_args(args, kwargs): ...
    def create_wrapped_node(
        self, tx: InstructionTranslator, query: VariableTracker, fn: VariableTracker, fn_name: str
    ): ...

class AutogradFunctionApplyVariable(VariableTracker):
    def __init__(self, fwd_graph, bwd_graph, parent_source, **kwargs) -> None: ...
    def call_function(
        self, tx: InstructionTranslator, args: list[VariableTracker], kwargs: dict[str, VariableTracker]
    ) -> VariableTracker: ...

def maybe_positional_arg_names(func): ...

class BaseHOPVariable(WrapHigherOrderVariable):
    supports_input_mutation = ...
    supports_aliasing = ...
    def python_type(self): ...

class InvokeSubgraphHigherOrderVariable(WrapHigherOrderVariable):
    supports_input_mutation = ...
    supports_aliasing = ...
    def install_subgraph_in_output_graph(self, tx, fn_vt, fn_args_vt, kwargs, body_gmod, attr_name): ...

_hop_name_to_variable_class = ...
