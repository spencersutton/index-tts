from typing import TYPE_CHECKING

import torch.fx
from torch._dynamo.codegen import PyCodegen
from torch._dynamo.symbolic_convert import InstructionTranslator

from .base import VariableTracker
from .user_defined import UserDefinedClassVariable

"""
This module contains variable tracker classes for handling tensors and tensor-related operations in Dynamo.

The main class is TensorVariable which represents torch.Tensor inputs and intermediate values in the FX graph.
It handles tensor operations, method calls, and maintains metadata about tensor properties like dtype, device, etc.

Other key classes include:
- SymNodeVariable: Represents symbolic scalars (int/float/bool) used for size computation and unspecialized values
- NumpyNdarrayVariable: Handles numpy array interop through torch._numpy
- UnspecializedPythonVariable: Represents unspecialized Python numeric values as 1-element tensors
- TensorSubclassVariable: Handles tensor subclasses with __torch_function__ overrides
- UntypedStorageVariable: Represents tensor storage objects
- DataPtrVariable: Handles tensor data pointer operations

These classes work together to track tensor operations and properties during Dynamo's tracing process.
"""
if TYPE_CHECKING: ...
log = ...
supported_tensor_comparison_ops = ...
supported_const_comparison_ops = ...
supported_comparison_ops = ...
supported_tensor_comparison_op_values = ...
supported_const_comparison_op_values = ...

def is_bound_tensor_method(value):  # -> TypeIs[Callable[..., object]] | Any | Literal[False] | None:
    ...

all_tensor_attrs = ...

class TensorVariable(VariableTracker):
    _nonvar_fields = ...
    def get_real_value(self):  # -> Any:

        ...
    def __init__(
        self,
        proxy: torch.fx.Proxy,
        *,
        dtype,
        device,
        layout,
        ndim,
        requires_grad,
        is_nested,
        is_quantized,
        is_sparse,
        class_type,
        has_grad_fn,
        _size=...,
        stride=...,
        is_contiguous=...,
        _is_name_set=...,
        **kwargs,
    ) -> None: ...
    def debug_repr(self):  # -> str:
        ...
    def as_proxy(self):  # -> Proxy:
        ...
    def python_type(self): ...
    @staticmethod
    def specialize(value: torch.Tensor):  # -> dict[str, dtype | device | Any | int | type[Tensor]]:
        ...
    def dynamic_getattr(self, tx: InstructionTranslator, name):  # -> VariableTracker | Any | GetAttrVariable:
        ...
    def method_attr_ndim(self, tx):  # -> VariableTracker:
        ...
    def method_attr_dtype(self, tx):  # -> VariableTracker | None:
        ...
    def method_attr_device(self, tx):  # -> VariableTracker | None:
        ...
    def method_attr_layout(self, tx):  # -> VariableTracker | None:
        ...
    def method_attr_is_cuda(self, tx):  # -> VariableTracker | None:
        ...
    def method_attr_shape(self, tx):  # -> SizeVariable | VariableTracker:
        ...
    def method_attr_requires_grad(self, tx):  # -> VariableTracker | None:
        ...
    def method_attr_is_quantized(self, tx):  # -> VariableTracker | None:
        ...
    def method_attr_is_sparse(self, tx):  # -> VariableTracker | None:
        ...
    def method_attr_is_nested(self, tx):  # -> VariableTracker | None:
        ...
    def method_attr_retain_grad(self, tx): ...
    def method_attr_data(self, tx):  # -> VariableTracker:
        ...
    def method_attr_grad_fn(self, tx):  # -> ConstantVariable:
        ...
    def method_attr__version(self, tx):  # -> VariableTracker:
        ...
    def call_obj_hasattr(self, tx: InstructionTranslator, name):  # -> ConstantVariable:
        ...
    def var_getattr(
        self, tx: InstructionTranslator, name
    ):  # -> UserDefinedClassVariable | DelayGraphBreakVariable | VariableTracker | Any | GetAttrVariable:
        ...
    def call_id(self, tx):  # -> VariableTracker:
        ...
    def has_unpack_var_sequence(self, tx): ...
    def unpack_var_sequence(self, tx: InstructionTranslator, idxes=...):  # -> list[Any]:
        ...
    def valid_size(self):  # -> bool:
        ...
    @property
    def size(self): ...
    def call_method(
        self, tx, name, args: list[VariableTracker], kwargs: dict[str, VariableTracker]
    ) -> VariableTracker: ...
    def method_size(self, *args, **kwargs):  # -> VariableTracker | SizeVariable | None:
        ...
    def method_stride(self, *args, **kwargs):  # -> VariableTracker | SizeVariable | None:
        ...
    def method_numel(self):  # -> VariableTracker | None:
        ...

    method_nelement = ...
    def method_dim(self):  # -> VariableTracker | None:
        ...

    method_ndimension = ...
    def method_is_floating_point(self):  # -> VariableTracker | None:
        ...
    def method_is_inference(self):  # -> VariableTracker | None:
        ...
    def method_is_complex(self):  # -> VariableTracker | None:
        ...
    def method_is_contiguous(self, memory_format=...):  # -> VariableTracker | None:
        ...
    def method_type(self, dtype=..., non_blocking=..., **kwargs):  # -> VariableTracker | None:
        ...
    def method_as_subclass(self, cls):  # -> TensorWithTFOverrideVariable:
        ...
    def method_get_device(self):  # -> VariableTracker | None:
        ...
    def method_element_size(self):  # -> VariableTracker:
        ...
    def method_numpy(self, *, force=...): ...
    def method_tolist(self):  # -> Any:
        ...
    def method_backward(self, *args, **kwargs): ...
    def method_data_ptr(self, *args, **kwargs):  # -> DataPtrVariable:
        ...
    def method_item(self, *args, **kwargs):  # -> None:
        ...
    def method___getitem__(self, *args, **kwargs):  # -> VariableTracker:
        ...
    def method___len__(self):  # -> VariableTracker:
        ...
    def method_addcmul_(self, tensor1, tensor2, *, value=...):  # -> Any | None:
        ...
    def method___setitem__(self, key, value):  # -> VariableTracker:
        ...
    def method_resize_(self, *args, **kwargs): ...
    def method_resize_as_(self, *args, **kwargs): ...
    def method_sparse_resize_(self, *args, **kwargs): ...
    def method_sparse_resize_and_clear_(self, *args, **kwargs): ...
    def method_set_(self, *args, **kwargs):  # -> None:
        ...
    def method_add_(self, other, *, alpha=...):  # -> VariableTracker | None:
        ...
    def method_addcdiv_(self, tensor1, tensor2, *, value=...):  # -> VariableTracker | None:
        ...
    def method___contains__(self, arg):  # -> VariableTracker:
        ...
    def method_redistribute(self, *args, **kwargs):  # -> VariableTracker:
        ...
    def method_to_local(self, *args, **kwargs):  # -> VariableTracker:
        ...
    def method_register_hook(self, *args, **kwargs):  # -> VariableTracker | RemovableHandleVariable:
        ...
    def method_register_post_accumulate_grad_hook(
        self, *args, **kwargs
    ):  # -> VariableTracker | RemovableHandleVariable:
        ...
    def method_requires_grad_(self, requires_grad=...):  # -> Self:
        ...
    def method_new(self, *args, **kwargs):  # -> VariableTracker | None:
        ...
    def method_untyped_storage(self):  # -> UntypedStorageVariable:
        ...
    def set_name_hint(self, name: str):  # -> None:
        ...

class SymNodeVariable(VariableTracker):
    _nonvar_fields = ...
    def debug_repr(self):  # -> str:
        ...
    @classmethod
    def create(cls, tx, proxy, sym_num=..., **options):  # -> VariableTracker | SymNodeVariable:
        ...
    def __init__(self, proxy, sym_num, **kwargs) -> None: ...
    def python_type(self):  # -> Any:
        ...
    def as_proxy(self):  # -> Any:
        ...
    def as_tensor(self, tx, dtype):  # -> Any:
        ...
    def evaluate_expr(self, output_graph=...):  # -> bool | int | float:
        ...
    def call_method(
        self, tx, name, args: list[VariableTracker], kwargs: dict[str, VariableTracker]
    ) -> VariableTracker: ...

class NumpyNdarrayVariable(TensorVariable):
    @staticmethod
    def create(tx: InstructionTranslator, proxy, **options): ...
    def var_getattr(self, tx: InstructionTranslator, name):  # -> VariableTracker:
        ...
    @staticmethod
    def patch_args(name, args, kwargs):  # -> tuple[Any, dict[str | None, Any] | Any]:
        ...
    def call_method(
        self, tx, name, args: list[VariableTracker], kwargs: dict[str, VariableTracker]
    ) -> VariableTracker: ...
    def python_type(self):  # -> type[ndarray[_AnyShape, dtype[Any]]]:
        ...

class UnspecializedPythonVariable(TensorVariable):
    _nonvar_fields = ...
    def __init__(self, proxy: torch.fx.Proxy, *, raw_value=..., need_unwrap=..., **kwargs) -> None: ...
    @classmethod
    def from_tensor_variable(cls, tensor_variable, raw_value, need_unwrap=...):  # -> UnspecializedPythonVariable:
        ...

class FakeItemVariable(TensorVariable):
    _nonvar_fields = ...
    def __init__(self, proxy: torch.fx.Proxy, **kwargs) -> None: ...
    @classmethod
    def from_tensor_variable(cls, tensor_variable):  # -> FakeItemVariable:
        ...

class TensorSubclassVariable(UserDefinedClassVariable):
    def call_function(
        self, tx: InstructionTranslator, args: list[VariableTracker], kwargs: dict[str, VariableTracker]
    ) -> VariableTracker: ...
    def as_python_constant(self):  # -> type[object]:
        ...

class UntypedStorageVariable(VariableTracker):
    _nonvar_fields = ...
    def __init__(self, from_tensor: TensorVariable, example_value: torch.UntypedStorage, **kwargs) -> None: ...
    def call_method(
        self, tx, name, args: list[VariableTracker], kwargs: dict[str, VariableTracker]
    ) -> VariableTracker: ...
    def reconstruct(self, codegen: PyCodegen):  # -> None:
        ...

class DataPtrVariable(VariableTracker):
    def __init__(self, from_tensor: TensorVariable, **kwargs) -> None: ...
    def reconstruct(self, codegen: PyCodegen):  # -> None:
        ...
