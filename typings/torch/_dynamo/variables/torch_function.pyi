import contextlib
import functools
from typing import TYPE_CHECKING

from torch._dynamo.codegen import PyCodegen
from torch._dynamo.symbolic_convert import InstructionTranslator

from .base import VariableTracker
from .ctx_manager import GenericContextWrappingVariable
from .tensor import TensorVariable

"""TorchDynamo support for __torch_function__ tensor subclasses.

This module implements support for tensor subclasses with __torch_function__ overrides.
A tensor subclass instance is represented as a TensorWithTFOverrideVariable, which handles
dispatching __torch_function__ on attribute accesses, method calls, and torch API calls.

Unsupported features:
- Triggering __torch_function__ on tensor subclass non-tensor custom attributes
- Graph breaking on mutating guardable tensor properties within a __torch_function__ context
  (can cause excessive recompiles in certain cases)
- Matching exact eager behavior of ignoring __torch_function__ objects in non-tensor
  argument positions of Torch API calls

Supported features:
- Static method implementations of __torch_function__ on custom objects (triggers on torch
  API calls with the object as any argument)
- Triggering __torch_function__ on torch API calls with tensor subclass arguments
- __torch_function__ calls on base tensor attribute access and method calls for tensor
  subclass instances
- Matches dispatch ordering behavior of eager __torch_function__ with subclass/object
  arguments in any position

See https://docs.google.com/document/d/1WBxBSvW3NXhRp9ncmtokJloMLCtF4AYNhJaffvHe8Kw/edit#heading=h.vacn73lozd9w
for more information on the design.
"""
if TYPE_CHECKING: ...
bin_ops = ...
bin_int_ops = ...
un_int_ops = ...
tensor_and_int_ops = ...
un_ops = ...
banned_attrs = ...

@functools.cache
def get_prev_stack_var_name():  # -> str:
    ...

class TorchFunctionModeStackStateManager:
    def __init__(self) -> None: ...
    def __enter__(self):  # -> None:
        ...
    def __exit__(self, exc_type, exc_value, traceback):  # -> None:
        ...
    @contextlib.contextmanager
    def temp_restore_stack(self):  # -> Generator[None, Any, None]:
        ...

torch_function_mode_stack_state_mgr = ...

class SymbolicTorchFunctionState:
    def __init__(self, py_stack) -> None: ...
    def in_torch_function_mode(self):  # -> bool:
        ...
    def pop_torch_function_mode(self):  # -> TorchFunctionModeVariable:
        ...
    def push_torch_function_mode(self, mode_var):  # -> None:
        ...
    def call_torch_function_mode(self, tx, fn, types, args, kwargs): ...

class TorchFunctionModeStackVariable(VariableTracker):
    stack_value_singleton = ...
    offset = ...
    def __init__(self, source, symbolic_stack) -> None: ...
    @classmethod
    def reset(cls):  # -> None:
        ...
    @classmethod
    def register_mutation(cls, tx: InstructionTranslator):  # -> None:
        ...
    @classmethod
    def register_device_context_insertion(cls, tx: InstructionTranslator):  # -> None:
        ...
    @classmethod
    def clear_default_device(cls, tx: InstructionTranslator):  # -> None:
        ...
    @staticmethod
    def is_device_context(var):  # -> bool:
        ...
    @classmethod
    def get_mode_index(cls, ind): ...

class TorchFunctionModeVariable(GenericContextWrappingVariable):
    @staticmethod
    def is_supported_torch_function_mode(ty):  # -> Any | bool:
        ...
    def __init__(self, value, source=..., **kwargs) -> None: ...
    def reconstruct(self, codegen: PyCodegen):  # -> None:
        ...
    def module_name(self):  # -> str:
        ...
    def fn_name(self):  # -> str:
        ...
    def python_type(self):  # -> type[object]:
        ...
    def call_torch_function(self, tx: InstructionTranslator, fn, types, args, kwargs): ...
    def enter(self, tx):  # -> VariableTracker:
        ...
    def exit(self, tx: InstructionTranslator, *args):  # -> VariableTracker:
        ...
    def reconstruct_type(self, codegen: PyCodegen):  # -> None:
        ...
    def supports_graph_breaks(self):  # -> Literal[True]:
        ...
    def exit_on_graph_break(self):  # -> Literal[False]:
        ...

def call_torch_function(tx, torch_function_var, fn, types, args, kwargs): ...
def get_torch_function_fn(tx: InstructionTranslator, vt):  # -> VariableTracker:
    ...
def can_dispatch_torch_function(tx: InstructionTranslator, args, kwargs):  # -> bool:
    ...
def dispatch_torch_function(tx: InstructionTranslator, fn, args, kwargs):  # -> ConstantVariable:

    ...

class TensorWithTFOverrideVariable(TensorVariable):
    @classmethod
    def from_tensor_var(cls, tx, tensor_var, class_type, cls_source):  # -> Self:
        ...
    def install_global(self, tx):  # -> None:
        ...
    def python_type(self): ...
    def class_type_var(self, tx):  # -> TensorSubclassVariable:
        ...
    def global_mangled_class_name(self, tx):  # -> str:
        ...
    def var_getattr(
        self, tx: InstructionTranslator, name
    ):  # -> UserMethodVariable | VariableTracker | UserDefinedClassVariable | DelayGraphBreakVariable | Any | GetAttrVariable:
        ...
    def call_torch_function(self, tx: InstructionTranslator, fn, types, args, kwargs): ...
    def call_method(
        self, tx, name, args: list[VariableTracker], kwargs: dict[str, VariableTracker]
    ) -> VariableTracker: ...
