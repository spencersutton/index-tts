"""
TorchDynamo support for __torch_function__ tensor subclasses.

This module implements support for tensor subclasses with __torch_function__ overrides.
A tensor subclass instance is represented as a TensorWithTFOverrideVariable, which handles
dispatching __torch_function__ on attribute accesses, method calls, and torch API calls.

Unsupported features:
- Triggering __torch_function__ on tensor subclass non-tensor custom attributes
- Graph breaking on mutating guardable tensor properties within a __torch_function__ context
  (can cause excessive recompiles in certain cases)
- Matching exact eager behavior of ignoring __torch_function__ objects in non-tensor
  argument positions of Torch API calls

Supported features:
- Static method implementations of __torch_function__ on custom objects (triggers on torch
  API calls with the object as any argument)
- Triggering __torch_function__ on torch API calls with tensor subclass arguments
- __torch_function__ calls on base tensor attribute access and method calls for tensor
  subclass instances
- Matches dispatch ordering behavior of eager __torch_function__ with subclass/object
  arguments in any position

See https://docs.google.com/document/d/1WBxBSvW3NXhRp9ncmtokJloMLCtF4AYNhJaffvHe8Kw/edit#heading=h.vacn73lozd9w
for more information on the design.
"""

import contextlib
import functools

from torch._dynamo.codegen import PyCodegen
from torch._dynamo.symbolic_convert import InstructionTranslator

from .base import VariableTracker
from .ctx_manager import GenericContextWrappingVariable
from .tensor import TensorVariable

bin_ops = ...
bin_int_ops = ...
un_int_ops = ...
tensor_and_int_ops = ...
un_ops = ...
banned_attrs = ...

@functools.cache
def get_prev_stack_var_name(): ...

class TorchFunctionModeStackStateManager:
    def __init__(self) -> None: ...
    def __enter__(self): ...
    def __exit__(self, exc_type, exc_value, traceback): ...
    @contextlib.contextmanager
    def temp_restore_stack(self): ...

torch_function_mode_stack_state_mgr = ...

class SymbolicTorchFunctionState:
    def __init__(self, py_stack) -> None: ...
    def in_torch_function_mode(self): ...
    def pop_torch_function_mode(self): ...
    def push_torch_function_mode(self, mode_var): ...
    def call_torch_function_mode(self, tx, fn, types, args, kwargs): ...

class TorchFunctionModeStackVariable(VariableTracker):
    """Fake VT to use as a dummy object, indicating the presence of torch function mode stack mutation"""

    stack_value_singleton = ...
    offset = ...
    def __init__(self, source, symbolic_stack) -> None: ...
    @classmethod
    def reset(cls): ...
    @classmethod
    def register_mutation(cls, tx: InstructionTranslator): ...
    @classmethod
    def register_device_context_insertion(cls, tx: InstructionTranslator): ...
    @classmethod
    def clear_default_device(cls, tx: InstructionTranslator): ...
    @staticmethod
    def is_device_context(var): ...
    @classmethod
    def get_mode_index(cls, ind): ...

class TorchFunctionModeVariable(GenericContextWrappingVariable):
    @staticmethod
    def is_supported_torch_function_mode(ty): ...
    def __init__(self, value, source=..., **kwargs) -> None: ...
    def reconstruct(self, codegen: PyCodegen): ...
    def module_name(self): ...
    def fn_name(self): ...
    def python_type(self): ...
    def call_torch_function(self, tx: InstructionTranslator, fn, types, args, kwargs): ...
    def enter(self, tx): ...
    def exit(self, tx: InstructionTranslator, *args): ...
    def reconstruct_type(self, codegen: PyCodegen): ...
    def supports_graph_breaks(self): ...
    def exit_on_graph_break(self): ...

def call_torch_function(tx, torch_function_var, fn, types, args, kwargs): ...
def get_torch_function_fn(tx: InstructionTranslator, vt): ...
def can_dispatch_torch_function(tx: InstructionTranslator, args, kwargs): ...
def dispatch_torch_function(tx: InstructionTranslator, fn, args, kwargs):
    """Gathers all args that are TensorWithTFOverrideVariable and dispatches based on the ordering in _get_overloaded_args"""

class TensorWithTFOverrideVariable(TensorVariable):
    """Represents a tensor subclass instance with a __torch_function__ override."""
    @classmethod
    def from_tensor_var(cls, tx, tensor_var, class_type, cls_source): ...
    def install_global(self, tx): ...
    def python_type(self): ...
    def class_type_var(self, tx): ...
    def global_mangled_class_name(self, tx): ...
    def var_getattr(self, tx: InstructionTranslator, name): ...
    def call_torch_function(self, tx: InstructionTranslator, fn, types, args, kwargs): ...
    def call_method(
        self, tx, name, args: list[VariableTracker], kwargs: dict[str, VariableTracker]
    ) -> VariableTracker: ...
