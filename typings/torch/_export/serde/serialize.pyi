import dataclasses
import json
from collections import UserDict
from collections.abc import Callable, Iterator, Sequence
from contextlib import contextmanager
from dataclasses import dataclass
from typing import Any, final

import sympy
import torch
import torch.export.exported_program as ep
from torch._subclasses.fake_tensor import FakeTensor
from torch.fx._symbolic_trace import _ConstantAttributeType
from torch.fx.experimental import symbolic_shapes
from torch.utils._sympy.value_ranges import ValueRanges

from .schema import (
    Argument,
    ConstantValue,
    CustomObjArgument,
    Device,
    ExportedProgram,
    Graph,
    GraphModule,
    GraphSignature,
    InputSpec,
    ModuleCallEntry,
    ModuleCallSignature,
    NamedArgument,
    Node,
    OutputSpec,
    RangeConstraint,
    ScalarType,
    SymBool,
    SymFloat,
    SymFloatArgument,
    SymInt,
    SymIntArgument,
    TensorArgument,
    TensorMeta,
)

__all__ = [
    "ExportedProgramDeserializer",
    "ExportedProgramSerializer",
    "GraphModuleDeserializer",
    "GraphModuleSerializer",
    "serialize",
]
log = ...

class SerializeError(RuntimeError): ...

type MetaType = FakeTensor | int | torch.SymInt | float | torch.SymFloat | bool | torch.SymBool | ep.CustomObjArgument
DEFAULT_PICKLE_PROTOCOL = ...
ST_DELIMITER = ...
_TORCH_TO_SERIALIZE_DTYPE = ...
_SERIALIZE_TO_TORCH_DTYPE = ...
_TORCH_TO_SERIALIZE_LAYOUT = ...
_SERIALIZE_TO_TORCH_LAYOUT = ...
_TORCH_TO_SERIALIZE_MEMORY_FORMAT = ...
_SERIALIZE_TO_TORCH_MEMORY_FORMAT = ...
_SYM_OPS = ...

@dataclass
class SerializedArtifact:
    """SerializedArtifact(exported_program: bytes, state_dict: bytes, constants: bytes, example_inputs: bytes)"""

    exported_program: bytes
    state_dict: bytes
    constants: bytes
    example_inputs: bytes

@dataclass
class _SerializedProgram:
    """_SerializedProgram(exported_program: torch._export.serde.schema.ExportedProgram, state_dict: bytes, constants: bytes, example_inputs: bytes)"""

    exported_program: ExportedProgram
    state_dict: bytes
    constants: bytes
    example_inputs: bytes

class LazyMap(UserDict):
    """
    Dictionary class for deferred instantiation of node metadata values.
    Purpose is to avoid creation of symbolic-shape tensors before relevant shape guards are parsed.
    """
    def __init__(self) -> None: ...
    def __setitem__(self, k, v) -> None: ...
    def __getitem__(self, k): ...

def deserialize_device(d: Device) -> torch.device: ...
def deserialize_size(sizes: Sequence[SymInt]) -> tuple[int, ...]: ...
def deserialize_stride(strides: Sequence[SymInt]) -> tuple[int, ...]: ...
def deserialize_scalar_type(st: ScalarType) -> torch.dtype: ...
def deserialize_storage_offset(offset: SymInt) -> int: ...
def serialize_sym_int(s: int | torch.SymInt) -> SymInt: ...
def serialize_sym_float(s: float | torch.SymFloat) -> SymFloat: ...
def serialize_sym_bool(s: bool | torch.SymBool) -> SymBool: ...
def serialize_tensor_meta(t: torch.Tensor) -> TensorMeta:
    """Extract a TensorMeta describing `t`."""

_CURRENT_DESERIALIZER: GraphModuleDeserializer | None = ...

def serialize_torch_artifact(artifact: Any | None, pickle_protocol: int = ...) -> bytes: ...
def deserialize_torch_artifact(serialized: dict[str, Any] | tuple[Any, ...] | bytes): ...
def serialize_range_constraints(range_constraints: dict[sympy.Symbol, ValueRanges]) -> dict[str, RangeConstraint]: ...

@dataclass
class GraphState:
    """GraphState(inputs: list[torch._export.serde.schema.Argument] = <factory>, outputs: list[torch._export.serde.schema.Argument] = <factory>, nodes: list[torch._export.serde.schema.Node] = <factory>, tensor_values: dict[str, torch._export.serde.schema.TensorMeta] = <factory>, sym_int_values: dict[str, torch._export.serde.schema.SymInt] = <factory>, sym_bool_values: dict[str, torch._export.serde.schema.SymBool] = <factory>, sym_float_values: dict[str, torch._export.serde.schema.SymFloat] = <factory>, is_single_tensor_return: bool = False, custom_obj_values: dict[str, torch._export.serde.schema.CustomObjArgument] = <factory>)"""

    inputs: list[Argument] = ...
    outputs: list[Argument] = ...
    nodes: list[Node] = ...
    tensor_values: dict[str, TensorMeta] = ...
    sym_int_values: dict[str, SymInt] = ...
    sym_bool_values: dict[str, SymBool] = ...
    sym_float_values: dict[str, SymFloat] = ...
    is_single_tensor_return: bool = ...
    custom_obj_values: dict[str, CustomObjArgument] = ...

class Final(type):
    def __new__(metacls, name, bases, classdict): ...

@final
class GraphModuleSerializer(metaclass=Final):
    def __init__(
        self, graph_signature: ep.ExportGraphSignature, module_call_graph: list[ep.ModuleCallEntry]
    ) -> None: ...
    @contextmanager
    def save_graph_state(self): ...
    def handle_placeholder(self, node: torch.fx.Node): ...
    def handle_output(self, node: torch.fx.Node): ...
    def serialize_operator(self, target) -> str: ...
    def handle_call_function(self, node: torch.fx.Node): ...
    def handle_get_attr(self, node): ...
    def serialize_metadata(self, node: torch.fx.Node) -> dict[str, str]: ...
    def serialize_script_obj_meta(self, script_obj_meta: ep.CustomObjArgument) -> CustomObjArgument: ...
    def serialize_sym_op_inputs(self, op, args) -> list[NamedArgument]: ...
    def serialize_inputs(self, target: Any, args, kwargs=...) -> list[NamedArgument]: ...
    def serialize_hoo_inputs(self, args, kwargs) -> list[NamedArgument]:
        """For serializing HOO inputs since HOOs do not have a schema."""
    def is_inductor_sym_int_arg(self, arg) -> bool: ...
    def is_sym_int_arg(self, arg) -> bool: ...
    def is_sym_float_arg(self, arg) -> bool: ...
    def is_sym_bool_arg(self, arg) -> bool: ...
    def serialize_input(self, arg, arg_type: Any | None = ...) -> Argument: ...
    def serialize_tensor_output(self, name, meta_val) -> TensorArgument: ...
    def serialize_sym_int_output(self, name, meta_val) -> SymIntArgument: ...
    def serialize_sym_float_output(self, name, meta_val) -> SymFloatArgument: ...
    def serialize_sym_bool_output(self, name, meta_val) -> SymIntArgument: ...
    def serialize_input_spec(self, spec: ep.InputSpec) -> InputSpec: ...
    def serialize_output_spec(self, spec: ep.OutputSpec) -> OutputSpec: ...
    def serialize_signature(self, sig: ep.ExportGraphSignature) -> GraphSignature: ...
    def serialize_argument_spec(self, x: ep.ArgumentSpec) -> Argument: ...
    def serialize_treespec(self, treespec): ...
    def serialize_module_call_signature(self, module_call_signature: ep.ModuleCallSignature) -> ModuleCallSignature: ...
    def serialize_module_call_graph(self, module_call_graph: list[ep.ModuleCallEntry]) -> list[ModuleCallEntry]: ...
    def serialize_outputs(self, node: torch.fx.Node) -> list[Argument]:
        """
        For a given node, return the dataclass representing its output values.

        [NOTE: Multiple outputs] We handle aggregates differently than FX. For
        FX, it looks like:

            x = call_function("multiple_return", ...)
            element0 = call_function(getitem, x, 0)
            foo = call_function("use_output", element0)

        We do not want the intermediate `getitem` call, so our serialized thing looks like:

            element0, element1, element2 = call_function("multiple_return", ...)
            foo = call_function("use_output", element0)

        We want names to be consistent across these two schemes, so that we can
        mostly reuse the names coming from FX. This function computes a mapping from
        the FX representation to our representation, preserving the names.
        """
    def serialize_hoo_outputs(self, node: torch.fx.Node) -> list[Argument]:
        """For serializing HOO outputs since HOOs do not have a schema."""
    def serialize_output(self, name: str, meta_val: Any) -> Argument: ...
    def serialize_graph(self, graph_module: torch.fx.GraphModule) -> Graph: ...
    def serialize_graph_module_metadata(self, meta: dict[str, Any]): ...
    def serialize(self, graph_module: torch.fx.GraphModule) -> GraphModule: ...

@final
class ExportedProgramSerializer(metaclass=Final):
    def __init__(self, opset_version: dict[str, int] | None = ..., pickle_protocol: int = ...) -> None: ...
    def serialize(self, exported_program: ep.ExportedProgram) -> _SerializedProgram:
        """
        Args:
            exported_program: Exported Program to serialize
        """

@final
class GraphModuleDeserializer(metaclass=Final):
    @dataclasses.dataclass
    class Result:
        """Result(graph_module: torch.fx.graph_module.GraphModule, signature: torch.export.graph_signature.ExportGraphSignature, module_call_graph: list[torch.export.exported_program.ModuleCallEntry], names_to_symbols: dict[str, sympy.core.symbol.Symbol], state_dict: dict[str, typing.Union[torch.Tensor, torch.nn.parameter.Parameter]], constants: dict[str, typing.Union[torch.Tensor, torch.ScriptObject, torch._library.fake_class_registry.FakeScriptObject, torch.utils._pytree.TreeSpec]], example_inputs: Optional[tuple[tuple[torch.Tensor, ...], dict[str, Any]]])"""

        graph_module: torch.fx.GraphModule
        signature: ep.ExportGraphSignature
        module_call_graph: list[ep.ModuleCallEntry]
        names_to_symbols: dict[str, sympy.Symbol]
        state_dict: dict[str, torch.Tensor | torch.nn.Parameter]
        constants: dict[str, _ConstantAttributeType]
        example_inputs: tuple[tuple[torch.Tensor, ...], dict[str, Any]] | None

    def __init__(self) -> None: ...
    @contextmanager
    def save_graph_module(self) -> Iterator[None]: ...
    def deserialize_extension_operator(self, serialized_target: str): ...
    def deserialize_operator(self, serialized_target: str): ...
    def deserialize_sym_int(self, s: SymInt) -> int | torch.SymInt: ...
    def deserialize_sym_float(self, s: SymFloat) -> float | torch.SymFloat: ...
    def deserialize_sym_bool(self, s: SymBool) -> bool | torch.SymBool: ...
    def deserialize_tensor_meta(self, tensor_meta: TensorMeta) -> FakeTensor: ...
    def deserialize_script_obj_meta(self, script_obj_meta: CustomObjArgument) -> ep.CustomObjArgument: ...
    def deserialize_graph_output(self, output) -> torch.fx.Node | int | None: ...
    def deserialize_graph(self, serialized_graph: Graph) -> torch.fx.Graph: ...
    def deserialize_node(self, serialized_node: Node, target: Callable) -> None: ...
    def deserialize_input_spec(self, i: InputSpec) -> ep.InputSpec: ...
    def deserialize_output_spec(self, o: OutputSpec) -> ep.OutputSpec: ...
    def deserialize_signature(self, sig: GraphSignature) -> ep.ExportGraphSignature: ...
    def deserialize(
        self,
        serialized_graph_module: GraphModule,
        serialized_state_dict: dict[str, torch.Tensor] | bytes,
        constants: dict[str, Any] | bytes,
        example_inputs: tuple[tuple[torch.Tensor, ...], dict[str, Any]] | bytes | None = ...,
        symbol_name_to_range: dict[str, symbolic_shapes.ValueRanges] | None = ...,
    ) -> Result: ...
    def sync_fx_node(self, name: str, fx_node: torch.fx.Node): ...
    def deserialize_sym_op_inputs(self, inputs): ...
    def deserialize_inputs(self, target, serialized_node: Node): ...
    def deserialize_hoo_inputs(self, inputs: list[NamedArgument]):
        """For deserializing HOO inputs since HOOs do not have a schema."""
    def deserialize_input(self, inp: Argument) -> Any: ...
    def deserialize_constant_input(self, inp: ConstantValue) -> Any: ...
    def deserialize_sym_argument(self, sym_arg): ...
    def deserialize_sym_op_outputs(self, serialized_node: Node, fx_node: torch.fx.Node): ...
    def deserialize_outputs(self, serialized_node: Node, fx_node: torch.fx.Node): ...
    def generate_getitem(
        self,
        meta_val,
        fx_node: torch.fx.Node,
        arg: TensorArgument | SymIntArgument | SymFloatArgument,
        idx: int,
        deserialized_metadata: dict[str, Any],
    ): ...
    def generate_getitems(self, meta_val, fx_node: torch.fx.Node, args, deserialized_metadata: dict[str, Any]): ...
    def deserialize_multiple_outputs(self, serialized_node: Node, fx_node: torch.fx.Node) -> None: ...
    def deserialize_metadata(self, metadata: dict[str, str]) -> dict[str, Any]: ...
    def deserialize_argument_spec(self, x: Argument) -> ep.ArgumentSpec: ...
    def deserialize_module_call_signature(
        self, module_call_signature: ModuleCallSignature
    ) -> ep.ModuleCallSignature: ...
    def deserialize_module_call_graph(self, module_call_graph: list[ModuleCallEntry]) -> list[ep.ModuleCallEntry]: ...

@final
class ExportedProgramDeserializer(metaclass=Final):
    def __init__(self, expected_opset_version: dict[str, int] | None = ...) -> None: ...
    def deserialize_range_constraints(
        self,
        symbol_name_to_range: dict[str, symbolic_shapes.ValueRanges],
        symbol_name_to_symbol: dict[str, sympy.Symbol],
    ) -> dict[sympy.Symbol, ValueRanges]: ...
    def deserialize(
        self,
        exported_program: ExportedProgram,
        state_dict: dict[str, torch.Tensor] | bytes,
        constants: dict[str, torch.Tensor] | bytes,
        example_inputs: tuple[tuple[torch.Tensor, ...], dict[str, Any]] | bytes | None = ...,
        *,
        _unsafe_skip_version_check=...,
    ) -> ep.ExportedProgram: ...

class EnumEncoder(json.JSONEncoder):
    def default(self, obj): ...

def serialize(
    exported_program: ep.ExportedProgram, opset_version: dict[str, int] | None = ..., pickle_protocol: int = ...
) -> SerializedArtifact: ...
def deserialize(
    artifact: SerializedArtifact, expected_opset_version: dict[str, int] | None = ..., *, _unsafe_skip_version_check=...
) -> ep.ExportedProgram: ...
def canonicalize(ep: ExportedProgram, constants: set[str] | None = ...) -> ExportedProgram:
    """
    Normalize a serialized ExportedProgram, so that different eager program which
    shares the same semantics can get a single representation on disk.

    This function canonicalizes an ExportedProgram by:

    1. Sorting nodes in topological order.
    2. Rename nodes to have unique names.
    3. Remove unstable fields.
    4. Aggregate the above program fields.
    5. Recurse in subgraphs.

    Args:
        ep (ExportedProgram): The ExportedProgram to canonicalize.
        constants (Optional[set[str]]): Set of constants names

    Returns:
        ExportedProgram: The canonicalized exported program.
    """

class ExtensionHandler:
    """Base class for handling extension operators."""
    @classmethod
    def namespace(cls) -> str: ...
    @classmethod
    def to_op_name(cls, op) -> str: ...
    @classmethod
    def from_op_name(cls, name: str): ...
    @classmethod
    def op_schema(cls, op) -> torch.FunctionSchema: ...

def register_extension(op_type: type[Any], extension_handler: type[ExtensionHandler]):
    """Register custom de/serialization method for a node with non-standard type."""

_serialization_registry: dict[type[Any], type[ExtensionHandler]] = ...
_deserialization_registry: dict[str, type[ExtensionHandler]] = ...
