from collections.abc import Callable
from typing import Any

import torch
from torch.export import ExportedProgram
from torch.export.graph_signature import ExportGraphSignature
from torch.utils._pytree import FlattenFunc, FromDumpableContextFn, KeyPath, ToDumpableContextFn, UnflattenFunc

placeholder_prefixes = ...
_DISABLE_ATEN_TO_ASSERTION_PASS = ...

def get_keystr(key_path: KeyPath) -> str:
    """
    For a given index into the flat_args, return a human readable string
    describing how to access it, e.g. "*args["foo"][0].bar"
    """

def register_dataclass_as_pytree_node(
    cls: type[Any],
    flatten_fn: FlattenFunc | None = ...,
    unflatten_fn: UnflattenFunc | None = ...,
    *,
    serialized_type_name: str | None = ...,
    to_dumpable_context: ToDumpableContextFn | None = ...,
    from_dumpable_context: FromDumpableContextFn | None = ...,
    return_none_fields: bool = ...,
) -> None: ...
def is_param(program: ExportedProgram, node: torch.fx.Node) -> bool:
    """Checks if the given node is a parameter within the exported program"""

def get_param(program: ExportedProgram, node: torch.fx.Node) -> torch.nn.Parameter | None:
    """
    Returns the parameter associated with the given node in the exported program.
    Returns None if the node is not a parameter within the exported program
    """

def is_buffer(program: ExportedProgram, node: torch.fx.Node) -> bool:
    """Checks if the given node is a buffer within the exported program"""

def get_buffer(program: ExportedProgram, node: torch.fx.Node) -> torch.Tensor | None:
    """
    Returns the buffer associated with the given node in the exported program.
    Returns None if the node is not a buffer within the exported program
    """

def is_lifted_tensor_constant(program: ExportedProgram, node: torch.fx.Node) -> bool:
    """Checks if the given node is a lifted tensor constant within the exported program"""

def get_lifted_tensor_constant(program: ExportedProgram, node: torch.fx.Node) -> torch.Tensor | None:
    """
    Returns the lifted tensor constant associated with the given node in the exported program.
    Returns None if the node is not a lifted tensor constant within the exported program
    """

def sequential_split(
    gm: torch.fx.GraphModule, node_call_back: Callable[[torch.fx.Node], torch.fx.Node | bool]
) -> torch.fx.GraphModule:
    """
    sequential_split creates a new graph module that splits the input graph module into multiple submodules
    based on the node_call_back. It doesn't mutate the input graph module. The node_call_back should return
    True if the node is a delimiter.  Delimiter will be the first node in the next submodule.
    """

def nodes_filter(nodes: list[torch.fx.Node], node_call_back) -> list[torch.fx.Node]:
    """Returns the nodes that match the node_call_back as a list."""

def apply_runtime_assertion_pass(gm: torch.fx.GraphModule, graph_signature): ...
def nodes_first(nodes: list[torch.fx.Node], node_call_back=...) -> torch.fx.Node | None:
    """
    Returns the first node that matches the node_call_back. If no node matches, returns None.
    When node_call_back is None, returns the first node in the node list.
    """

def nodes_count(nodes: list[torch.fx.Node], node_call_back) -> int:
    """Returns the number of nodes that match the node_call_back."""

def nodes_map(nodes: list[torch.fx.Node], node_call_back) -> list[torch.fx.Node]:
    """
    Sequentially visit the nodes list and invoke node_call_back on each element.
    Returns the nodes list after the node_call_back is invoked on each element.
    """

def node_replace_(old_node: torch.fx.Node, new_node: torch.fx.Node) -> None:
    """Replace all uses of old_node with new_node."""

def node_inline_(call_mod_node: torch.fx.Node) -> torch.fx.GraphModule | None:
    """
    Inline the submodule of the given node into the parent module.
    Note: we only support the case where submodule takes tensors inputs.
    """

def placeholder_naming_pass(
    gm: torch.fx.GraphModule,
    export_graph_signature: ExportGraphSignature,
    mod: torch.nn.Module,
    fake_args,
    fake_kwargs,
    fake_params_buffers,
    constants: dict[str, Any],
) -> None:
    """
    This pass is run at the end of _export_non_strict() to assign better placeholder node names:
        - User inputs:
            These follow the signature of mod.forward(), e.g. forward(x, y) produces nodes x, y.
            For nested inputs from dictionaries, lists, tuples, or dataclasses,
            the names are a concatenation of the path to the tensor.
                e.g. x = {
                    'a': torch.randn(),
                    'b': [torch.randn(), torch.randn()]
                }
            produces nodes x_a, x_b_0, x_b_1.
        - Parameters/buffers/constants/custom objects:
            These follow the FQN of the object, prefixed by "p", "b", "c", "obj" respectively.
                e.g. self.bar.l0.weight produces "p_bar_l0_weight".
        - Effect tokens:
            These are named token, token_1, ...
    """

def remove_proxy_from_state_dict(state_dict: dict, in_place: bool) -> dict:
    """
    If `in_place` is false, return a new copy of `state_dict` with "proxy" removed from `v.__dict__`.
    `v` is the values in the dictionary.
    If `in_place` is true, modify `state_dict` in place.
    """

def register_module_as_pytree_input_node(cls: type[torch.nn.Module]) -> None:
    """
    Registers a module as a valid input type for :func:`torch.export.export`.

    Args:
        mod: the module instance
        serialized_type_name: The serialized name for the module. This is
        required if you want to serialize the pytree TreeSpec containing this
        module.

    Example::

        import torch


        class Module(torch.nn.Module):
            def __init__(self):
                super().__init__()
                self.linear = torch.nn.Linear(3, 3)

            def forward(self, x):
                return self.linear(x)


        torch._export.utils.register_module_as_pytree_node(InputDataClass)


        class Mod(torch.nn.Module):
            def forward(self, x, m):
                return m(x) + x


        ep = torch.export.export(Mod(), (torch.randn(3), Module()))
        print(ep)
    """

def deregister_module_as_pytree_input_node(cls: type[torch.nn.Module]) -> None: ...
def sync_state(*wrapped_method_modules):
    """
    Sync state between exported modules corresponding to wrapped methods.
    This might be necessary after serializing/deserializing due to copying.
    """

class _WrappedMethod(torch.nn.Module):
    def __init__(self, method) -> None: ...

def wrap_method(method):
    """
    Wrap a method as a module so that it can be exported.
    The wrapped module's forward points to the method, and
    the method's original module state is shared.
    """
