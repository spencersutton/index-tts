from dataclasses import dataclass

import torch
from torch._subclasses.functional_tensor import FunctionalTensor
from torch.fx.experimental.symbolic_shapes import SymIntEqByExpr

aot_joint_log = ...

def to_fun(t): ...
def sync_functional_tensor(t): ...
def from_fun(t): ...
def is_fun(t): ...
def has_data_mutation(t): ...
def are_all_mutations_hidden_from_autograd(t): ...
def are_all_mutations_under_no_grad_or_inference_mode(t): ...
def was_inductor_storage_resized(t): ...
def has_metadata_mutation(f_arg, arg, *, check_only_storage_mutation: bool): ...
def gen_alias_from_base(
    aliased_base_tensor,
    target_meta_tensor,
    target_requires_grad,
    target_view_meta_sequence: ViewMetaSequence | None = ...,
    *,
    replay_views: bool,
): ...
def has_same_metadata(t1, t2): ...

@dataclass(frozen=True)
class MetadataKey:
    size: tuple[SymIntEqByExpr, ...]
    layout: torch.layout
    is_sparse: bool
    stride: tuple[SymIntEqByExpr, ...] | None
    storage_offset: SymIntEqByExpr | None
    is_conj: bool
    is_neg: bool
    @staticmethod
    def make(t): ...

class ViewMetaSequence:
    def __init__(self, tensor: FunctionalTensor) -> None: ...
    def __eq__(self, other: object) -> bool: ...

def was_tensor_updated(arg, new_arg): ...
def was_tensor_metadata_updated(arg, new_arg): ...
def assert_functional_graph(fx_g: torch.fx.Graph) -> int: ...
def propagate_input_mutation_stacktraces(fx_g: torch.fx.Graph) -> None: ...
