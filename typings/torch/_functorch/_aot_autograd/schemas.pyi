"""
The various dataclasses, Enums, namedtuples etc used in AOTAutograd. This includes
input/output types, metadata, config, function signatures etc.
"""

import contextlib
import functools
from collections.abc import Callable, Iterable, Sequence
from dataclasses import dataclass
from enum import Enum
from typing import Any, NewType, Protocol, TypeVar

import torch
import torch.utils._pytree as pytree
from torch import SymInt, Tensor
from torch._guards import Source
from torch._inductor.output_code import OutputCode
from torch._inductor.utils import InputType
from torch._ops import OpOverload
from torch.fx.experimental._backward_state import BackwardState

from .descriptors import AOTInput, AOTOutput
from .functional_utils import ViewMetaSequence
from .graph_capture_wrappers import JointFnHandle

zip = ...
OutputType = ...

@dataclass(frozen=True)
class OutputAliasInfo:
    """OutputAliasInfo(output_type: 'OutputType', raw_type: 'type', base_idx: 'Optional[int]', dynamic_dims: 'Optional[set[int]]', requires_grad: 'bool', view_meta_sequence: 'Optional[ViewMetaSequence]' = None)"""

    output_type: OutputType
    raw_type: type
    base_idx: int | None
    dynamic_dims: set[int] | None
    requires_grad: bool
    view_meta_sequence: ViewMetaSequence | None = ...

class MutationType(Enum):
    NOT_MUTATED = ...
    MUTATED_IN_GRAPH = ...
    MUTATED_OUT_GRAPH = ...

@dataclass(frozen=True)
class InputAliasInfo:
    """InputAliasInfo(is_leaf: 'bool', mutates_data: 'bool', mutates_metadata: 'bool', mutations_hidden_from_autograd: 'bool', mutations_under_no_grad_or_inference_mode: 'bool', mutation_inductor_storage_resize: 'bool', mutates_storage_metadata: 'bool', requires_grad: 'bool', keep_input_mutations: 'bool')"""

    is_leaf: bool
    mutates_data: bool
    mutates_metadata: bool
    mutations_hidden_from_autograd: bool
    mutations_under_no_grad_or_inference_mode: bool
    mutation_inductor_storage_resize: bool
    mutates_storage_metadata: bool
    requires_grad: bool
    keep_input_mutations: bool
    def __post_init__(self): ...
    @functools.cached_property
    def mutation_type(self) -> MutationType: ...

@dataclass
class MemoryFormatMeta:
    """MemoryFormatMeta(size: 'Optional[Sequence[int]]' = None, stride: 'Optional[Sequence[int]]' = None, memory_format: 'Optional[torch.memory_format]' = None)"""

    size: Sequence[int] | None = ...
    stride: Sequence[int] | None = ...
    memory_format: torch.memory_format | None = ...
    @staticmethod
    def from_tensor(t: torch.Tensor) -> MemoryFormatMeta | None: ...

@dataclass
class PlainTensorMeta:
    """PlainTensorMeta(unwrapped_idx: 'int', memory_format: 'Optional[MemoryFormatMeta]' = None)"""

    unwrapped_idx: int
    memory_format: MemoryFormatMeta | None = ...

@dataclass
class SubclassCreationMeta:
    """
    Used for AOTDispatch.
    This dataclass gives us the information we need to reconstruct a tensor subclass
    from our flat inputs.
    Why is this important? The graph that we'd like to trace out contains flat tensor inputs,
    But the user's original model may have subclass inputs and outputs.
    So we need to wrap/unwrap subclasses as necessary to translate between the user's
    view (subclass inps/outs), and the backend compiler's view (graph with no subclass args).

    Complications arise mostly from the fact that a subclass can hold more than one inner tensor;
    So for a given subclass input/output, we need to carefully track which indices map
    to the subclass tensor in the corresponding "dense-tensor-only" graph.
    """

    flat_tensor_start_idx: int
    arg_count: int
    included_subclass_symints: bool
    attrs: dict[str, SubclassCreationMeta | PlainTensorMeta]
    outer_size: Iterable[None | int | torch.SymInt]
    outer_stride: Iterable[None | int | torch.SymInt]
    meta: Any
    original_subclass: torch.Tensor | None
    original_subclass_type: type | None = ...
    memory_format: MemoryFormatMeta | None = ...
    def compute_outer_size_and_stride(self, all_args, *, curr_start_idx: int): ...
    def creation_fn(self, all_args, *, is_runtime: bool): ...
    def make_runtime_safe(self): ...
    def __post_init__(self): ...

@dataclass(eq=False)
class ViewAndMutationMeta:
    """ViewAndMutationMeta(input_info: 'list[InputAliasInfo]', output_info: 'list[OutputAliasInfo]', num_intermediate_bases: 'int', keep_input_mutations: 'bool', traced_tangents: 'list[Any]', traced_tangents_descs: 'list[AOTInput]', subclass_inp_meta: 'list[Union[PlainTensorMeta, SubclassCreationMeta]]', subclass_fw_graph_out_meta: 'list[Union[PlainTensorMeta, SubclassCreationMeta]]', subclass_tangent_meta: 'list[Union[PlainTensorMeta, SubclassCreationMeta]]', is_train: 'bool' = False, traced_tangent_metas: 'Optional[list[Any]]' = None, num_symints_saved_for_bw: 'Optional[int]' = None, grad_enabled_mutation: 'Optional[bool]' = None, deterministic: 'Optional[bool]' = None, static_input_indices: 'list[int]' = <factory>, tokens: 'dict[Any, torch.Tensor]' = <factory>, indices_of_inputs_that_requires_grad_with_mutations_in_bw: 'list[int]' = <factory>, bw_donated_idxs: 'Optional[list[int]]' = None, num_backward_tokens: 'int' = 0, num_graphsafe_rng_states: 'int' = 0, graphsafe_rng_state_index: 'Optional[int]' = None)"""

    input_info: list[InputAliasInfo]
    output_info: list[OutputAliasInfo]
    num_intermediate_bases: int
    keep_input_mutations: bool
    traced_tangents: list[Any]
    traced_tangents_descs: list[AOTInput]
    subclass_inp_meta: list[PlainTensorMeta | SubclassCreationMeta]
    subclass_fw_graph_out_meta: list[PlainTensorMeta | SubclassCreationMeta]
    subclass_tangent_meta: list[PlainTensorMeta | SubclassCreationMeta]
    is_train: bool = ...
    traced_tangent_metas: list[Any] | None = ...
    num_symints_saved_for_bw: int | None = ...
    grad_enabled_mutation: bool | None = ...
    deterministic: bool | None = ...
    static_input_indices: list[int] = ...
    tokens: dict[Any, torch.Tensor] = ...
    indices_of_inputs_that_requires_grad_with_mutations_in_bw: list[int] = ...
    bw_donated_idxs: list[int] | None = ...
    num_backward_tokens: int = ...
    num_graphsafe_rng_states: int = ...
    graphsafe_rng_state_index: int | None = ...
    def __post_init__(self): ...
    def make_runtime_safe(self):
        """
        There are various fields in ViewAndMutationMeta that aren't serializable. This function is called after all tracing
        is completed to simplify certain fields in the metadata so that they can be safely cached.

        Doing so may lose information (in the case of traced_tangents), but none of the information is needed at runtime.
        """
    @property
    def tensors_saved_for_backwards_slice(self): ...
    @property
    def symints_saved_for_backwards_slice(self): ...
    def __eq__(self, other) -> bool: ...

@dataclass(eq=False)
class SubclassMeta:
    """SubclassMeta() -> 'None'"""

    fw_metadata: ViewAndMutationMeta
    grad_input_metas: list[PlainTensorMeta | SubclassCreationMeta] | None = ...
    def __init__(self) -> None: ...

@dataclass(frozen=True)
class TensorAlias:
    """TensorAlias(alias: 'torch.Tensor')"""

    alias: torch.Tensor

@dataclass
class BackwardSignature:
    """
    Provides information about the backward section of an exported
    joint forward-backward graph.
    For a particular fx GraphModule, this class contains information on:
    (1) A mapping from each gradient (backwards output) to the parameter
        it corresponds to (forward input)
    (2) A mapping from each gradient (backwards output) to the user input
        it corresponds to (forward input)
    (3) Which of the forward outputs corresponds to the loss, that we backprop on.

    Each string name is the `node.name` of the corresponding node in the fx graph.
    """

    gradients_to_parameters: dict[str, str]
    gradients_to_user_inputs: dict[str, str]
    loss_output: str

GraphOutputName = NewType("GraphOutputName", str)
GraphInputName = NewType("GraphInputName", str)
FQN = NewType("FQN", str)

@dataclass
class GraphSignature:
    """
    Provides information about an exported module.
    For a particular fx GraphModule, this class contains information on:
    (1) Which graph inputs are parameters, buffers, or user inputs
    (2) (for params/buffers) a mapping from the name of each graph argument
        to its parameter/buffer FQN in the original nn.Module.
    (3) If there are input mutations, these are represented as extra outputs
        in the fx GraphModule. We provide a mapping from these
        extra output names to the names of the actual inputs.
    (4) The pytree metadata on how to flatten/unflatten inputs and outputs.
        The corresponding FX GraphModule only accepts and returns
        pytree-flattened inputs/outputs.
    (5) (Optionally) if the FX is a joint forward-backward graph, we provide
        a signature on the backward section of the joint graph.
    """

    parameters: list[FQN]
    buffers: list[FQN]
    user_inputs: list[GraphInputName]
    user_outputs: list[GraphOutputName]
    inputs_to_parameters: dict[GraphInputName, FQN]
    inputs_to_buffers: dict[GraphInputName, FQN]
    buffers_to_mutate: dict[GraphOutputName, FQN]
    parameters_to_mutate: dict[GraphOutputName, FQN]
    user_inputs_to_mutate: dict[GraphOutputName, GraphInputName]
    in_spec: pytree.TreeSpec
    out_spec: pytree.TreeSpec
    backward_signature: BackwardSignature | None
    input_tokens: list[GraphInputName]
    output_tokens: list[GraphOutputName]
    @classmethod
    def from_tracing_metadata(
        cls,
        *,
        in_spec: pytree.TreeSpec,
        out_spec: pytree.TreeSpec,
        graph_input_names: list[str],
        graph_output_names: list[str],
        view_mutation_metadata: ViewAndMutationMeta,
        named_parameters: list[str],
        named_buffers: list[str],
        num_user_inputs: int,
        num_user_outputs: int,
        trace_joint: bool,
        loss_index: int | None,
        backward_signature: BackwardSignature | None,
    ) -> GraphSignature: ...

@dataclass
class AOTAutogradCacheInfo:
    """AOTAutogradCacheInfo(cache_key: 'str', start_time_ns: 'int', forward_symints: 'list[torch.SymInt]')"""

    cache_key: str
    start_time_ns: int
    forward_symints: list[torch.SymInt]

@dataclass
class AOTConfig:
    """Configuration for AOTDispatcher"""

    fw_compiler: Callable
    bw_compiler: Callable
    partition_fn: Callable
    decompositions: dict[OpOverload, Callable]
    num_params_buffers: int
    aot_id: int
    keep_inference_input_mutations: bool
    is_export: bool = ...
    no_tangents: bool = ...
    dynamic_shapes: bool = ...
    aot_autograd_arg_pos_to_source: list[Source] | None = ...
    static_input_indices: list[int] | None = ...
    inference_compiler: Callable | None = ...
    enable_log: bool = ...
    pre_dispatch: bool = ...
    cache_info: AOTAutogradCacheInfo | None = ...
    ignore_shape_env: bool = ...
    precompile_backend_id: str | None = ...
    force_non_lazy_backward_lowering: bool = ...
    export_trace_joint: bool = ...
    def __post_init__(self): ...

SubclassTracingInfo = ...

@dataclass
class AOTState:
    """
    When we run AOTAutograd, this class encapsulates the state in the compiler which
    must be preserved across stages.  This is state in the traditional sense (not an
    environment) because some values in this structure change as we progress through
    pipelines in AOTAutograd.
    """

    needs_autograd: bool
    flat_args: list[FxValue]
    flat_args_descs: list[AOTInput]
    fw_metadata: ViewAndMutationMeta
    aot_config: AOTConfig
    stack: contextlib.ExitStack

type FxValue = Tensor | int | SymInt | BackwardState

class CompilerWrapper:
    """
    AOTAutograd needs to do many transformations to the calling convention of the user function
    it is tracing, e.g., deduplicating inputs, unpacking subclasses, etc.  CompilerWrapper lets
    us factor these into compositional stages so we can handle each transformation incrementally
    instead of having to do it all at once.

    Since there is a calling convention change, there are two parts to the wrpaper:

    1. The prologue, which is about compile-time behavior: given this original function, what
       is the new function with modified calling convention that we should trace with AOTAutograd
       to get the FX graph we will do joint passes, partitioning and ultimate Inductor compilation on?
       We get (flat_fn, flat_args), the original function under trace and inputs we were
       going to feed it, and produce a new function and new inputs to feed it.

    2. The epilogue, which is about run-time behavior: we have now compiled the modified calling
       convention function, we need to wrap it so that we have a new function that has the
       original calling convention of the original function, so that our users can call it
       at the old signature they expected.  We get (compiled_fn, real arguments), the newly
       compiled function we need to wrap.

    Note about caching: we do NOT directly serialize the runtime wrappers; instead, they
    are reapplied to compiled_fn after we have finished deserializing the compiled_fn.

    Extra metadata that is needed to compute pre or post compile can be passed in via attributes.
    """
    def pre_compile(
        self,
        flat_fn,
        flat_args: list[FxValue],
        flat_args_descs: list[AOTInput],
        aot_config: AOTConfig,
        *,
        fw_metadata: ViewAndMutationMeta,
    ) -> tuple[Callable, list[FxValue], list[AOTInput], ViewAndMutationMeta]:
        """
        Process the inputs to the compiler_fn. You can pass in extra metadata via kwargs.
        Args:
        flat_fn: The function to compile
        flat_args: Metadata from example inputs of the function to compile
        aot_config: AOTConfig passed in at compile time
        fw_metadata: ViewAndMutationMeta generated from flat_fn and flat_args
        """
    def post_compile(self, compiled_fn, aot_config, *, runtime_metadata) -> Callable:
        """
        Given an output of the compiler, wrap it with information received from prologue.
        Args:
        compiled_fn: Callable after calling compiler_fn
        aot_config: AOTConfig after calling prologue
        runtime_metadata: ViewAndMutationMeta after calling all wrappers's pre_compile steps.
        Example:

        def wrapped_compiled_fn(args):
            # do something with args, aot_config, fw_metadata
            return compiled_fn(args)

        return wrapped_compiled_fn
        """

class InductorWrapper:
    """
    This is sort of like CompilerWrapper, but it happens at a different part of the lifecycle:
    it talks about transformations we do to the traced and partitioned FX graph before we
    send it to the Inductor compiler.

    Once again, there are two parts:

    1. The prologue, which "modifies" the FX graph before we send it to
       Inductor.  I say "modifies" because... we don't really actually do
       anything nontrivial in either of our two implementations.
    2. The epilogue, which modifies the compiled function produced by Inductor

    Although hypothetically these wrappers could be used compositionally in a centralized
    wrappers list, in practice they seem to just be invoked manually when needed.

    NB: The flat_args input is sometimes mutated.  This is probably naughty but whatever.
    """
    def pre_compile(
        self,
        fw_module: torch.fx.GraphModule,
        flat_args: list[Tensor],
        aot_config: AOTConfig,
        *,
        fw_metadata: ViewAndMutationMeta,
    ) -> None:
        """
        Process the inputs to the compiler_fn. You can pass in extra metadata via kwargs.
        Args:
        flat_fn: The function to compile
        flat_args: Metadata from example inputs of the function to compile
        aot_config: AOTConfig passed in at compile time
        fw_metadata: ViewAndMutationMeta generated from flat_fn and flat_args
        """
    def post_compile(self, compiled_fn, aot_config, *, runtime_metadata) -> Callable:
        """
        Given an output of the compiler, wrap it with information received from prologue.
        Args:
        compiled_fn: Callable after calling compiler_fn
        aot_config: AOTConfig after calling prologue
        runtime_metadata: ViewAndMutationMeta after calling all wrappers's pre_compile steps.
        Example:

        def wrapped_compiled_fn(args):
            # do something with args, aot_config, fw_metadata
            return compiled_fn(args)

        return wrapped_compiled_fn
        """

@dataclass
class AOTGraphCapture:
    """AOTGraphCapture(wrappers: 'list[CompilerWrapper]', graph_module: 'torch.fx.GraphModule', updated_flat_args: 'Union[list[Any], tuple[list[Any], list[Any]]]', updated_flat_args_descs: 'Union[list[AOTInput], tuple[list[AOTInput], list[AOTInput]]]', maybe_subclass_meta: 'Any')"""

    wrappers: list[CompilerWrapper]
    graph_module: torch.fx.GraphModule
    updated_flat_args: list[Any] | tuple[list[Any], list[Any]]
    updated_flat_args_descs: list[AOTInput] | tuple[list[AOTInput], list[AOTInput]]
    maybe_subclass_meta: Any

FakifiedFlatArgs = NewType("FakifiedFlatArgs", list[Any])
TOutputCode = TypeVar("TOutputCode", bound=OutputCode)

class AOTDispatchCompiler(Protocol):
    """Represents a fw or bw_compiler passed to AOTAutograd."""
    def __call__(self, gm: torch.fx.GraphModule, example_inputs: Sequence[InputType]) -> Any: ...

class SerializableAOTDispatchCompiler(AOTDispatchCompiler):
    """
    Represents an AOTDispatchCompiler that returns an OutputCode, and is
    therefore cacheable. SerializableAOTDispatchCompiler always return an OutputCode.
    A _CompileFxCallable usually gets converted into an AOTDispatchCompiler after binding all of
    the kwargs in _CompileFxKwargs.
    """
    def __init__(
        self,
        output_code_ty: type[TOutputCode],
        compiler_fn: Callable[[torch.fx.GraphModule, Sequence[InputType]], TOutputCode],
    ) -> None: ...
    def __call__(self, gm: torch.fx.GraphModule, example_inputs: Sequence[InputType]) -> OutputCode: ...

class FlatFn(Protocol):
    def __call__(self, *args: FxValue) -> list[FxValue]: ...

class TraceFn(Protocol):
    def __call__(self, *args: FxValue) -> tuple[list[FxValue], list[AOTOutput]]: ...

class PreppedForAutogradTraceFn(Protocol):
    def __call__(self, *args: FxValue) -> tuple[tuple[list[FxValue], list[bool]], list[AOTOutput]]: ...

class JointTraceFn(Protocol):
    handle: JointFnHandle
    def __call__(
        self, primals: list[FxValue], tangents: list[FxValue]
    ) -> tuple[tuple[list[FxValue], list[Tensor | None]], tuple[list[AOTOutput], list[AOTOutput | None]]]: ...

@dataclass
class JointWithDescriptors:
    """JointWithDescriptors(_aot_state: 'AOTState', _aot_graph_capture: 'AOTGraphCapture', params_spec: 'list[str]', buffers_spec: 'list[str]', in_spec: 'pytree.TreeSpec', out_spec: 'pytree.TreeSpec')"""

    _aot_state: AOTState
    _aot_graph_capture: AOTGraphCapture
    params_spec: list[str]
    buffers_spec: list[str]
    in_spec: pytree.TreeSpec
    out_spec: pytree.TreeSpec
    @property
    def graph_module(self): ...
    @graph_module.setter
    def graph_module(self, value): ...
