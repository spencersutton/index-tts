from collections.abc import Callable
from typing import Any, ParamSpec, TypeVar

import torch
import torch.utils._pytree as pytree

KNOWN_TYPES = ...
original_zip = zip
aot_graphs_effects_log = ...

def strict_zip(*iterables, strict=..., **kwargs): ...
def partial_flatten_asdict(obj: Any) -> Any: ...
def normalize_as_list(x): ...
def make_boxed_func(f): ...
def make_boxed_compiler(compiler): ...
def call_func_at_runtime_with_args(f, args: tuple[Any] | list[Any], steal_args=..., disable_amp=...): ...

class PytreeThunk:
    spec: pytree.TreeSpec | None = ...
    is_simple: bool | None = ...
    is_really_simple: bool | None = ...
    def set(self, spec: pytree.TreeSpec) -> None: ...
    def unflatten(self, x: list[Any]) -> Any: ...

def create_tree_flattened_fn(fn, args, kwargs=...) -> tuple[Callable, PytreeThunk]: ...
def maybe_to_fresh_input(idx, t, meta): ...
def is_with_effects(node): ...
def is_with_effects_op(node, op): ...
def unlift_tokens(fw_module, fw_metadata, aot_config, bw_module=...): ...
def root_module_when_exporting_non_strict(flat_fn): ...
def copy_fwd_metadata_to_bw_nodes(fx_g): ...
def register_buffer_assignment_hook(mod, assigned_buffers): ...
def contain_metadata_mutation_ops(module: torch.fx.GraphModule) -> bool: ...
def get_cuda_generator_meta_val(device_idx: int): ...
def top_saved_tensors_hooks(): ...
def saved_tensors_hooks_are_inlineable(hooks) -> bool: ...

_P = ParamSpec("_P")
_T = TypeVar("_T")
_S = TypeVar("_S")

def without_output_descs[**P, T, S](f: Callable[_P, tuple[_T, _S]]) -> Callable[_P, _T]: ...

_P2 = ParamSpec("_P2")
_R = TypeVar("_R")
_R2 = TypeVar("_R2")

def simple_wraps[**P, R](f: Callable[_P, _R]) -> Callable[[Callable[_P2, _R2]], Callable[_P2, _R2]]: ...
def call_and_expect_output_descs(fn, args): ...
def fn_wrappers(fn): ...
