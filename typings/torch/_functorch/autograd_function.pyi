from typing import NamedTuple

from torch._C._functorch import TransformType
from torch._ops import HigherOrderOperator

class CustomFunctionHigherOrderOperator(HigherOrderOperator):
    def __init__(self) -> None: ...
    def __call__(self, autograd_function, *args, **kwargs): ...

custom_function_call = ...

@custom_function_call.py_impl(TransformType.Grad)
@custom_function_call.py_impl(TransformType.Jvp)
def custom_function_call_grad(interpreter, autograd_function, *operands): ...
def generate_single_level_function(interpreter, autograd_function): ...

NO_OUT_DIMS = ...

def wrap_outputs_maintaining_identity(outputs, unwrapped_inputs, orig_inputs, wrap_fn, out_dims=...): ...

class VmapInfo(NamedTuple):
    batch_size: int
    randomness: str

def has_overridden_vmap_rule(autograd_function): ...
def validate_vmap_returns_tuple_of_two_elements(result): ...
@custom_function_call.py_impl(TransformType.Vmap)
def custom_function_call_vmap(interpreter, autograd_function, *operands, **kwargs): ...
def custom_function_call_vmap_helper(interpreter, vmap_function, op, *operands, **kwargs): ...
def unpack_outputs(outputs): ...
def custom_function_call_vmap_generate_rule(interpreter, autograd_function, *operands): ...
@custom_function_call.py_impl(TransformType.Functionalize)
def custom_function_call_functionalize(interpreter, autograd_function, generate_vmap_rule, *operands): ...
def vmapify_autograd_function(autograd_function, in_dims, batch_size, randomness): ...
def get_tangents_in_dims(input_dims, tangents): ...

class WrappedCtx:
    _pt_reserved_attrs: tuple[str, ...] = ...
    def __init__(self, ctx) -> None: ...
    def __getattr__(self, name): ...
    def __setattr__(self, name, value) -> None: ...

class CtxWithSavedTensors(WrappedCtx):
    _pt_reserved_attrs = ...
    def __init__(self, ctx, new_saved_tensors) -> None: ...
    @property
    def saved_tensors(self): ...

class CtxCustomSave(WrappedCtx):
    _pt_reserved_attrs = ...
    def __init__(self, ctx, current_level) -> None: ...
    def save_for_backward(self, *tensors): ...
    def save_for_forward(self, *tensors): ...

def reductify(grad_input, grad_input_bdim, input_bdim, batch_size, target_shape_without_bdim_to_reduce_to=...): ...
def reductify_leaf(grad_input, grad_input_bdim, input_bdim, batch_size, target_shape_without_bdim_to_reduce_to=...): ...
def autograd_function_forward_rewritten(original_forward, original_setup_context): ...

class AutogradFunctionApply(HigherOrderOperator):
    def __init__(self) -> None: ...
    def __call__(self, fwd, bwd, *fwd_args, **fwd_kwargs): ...

autograd_function_apply = ...
