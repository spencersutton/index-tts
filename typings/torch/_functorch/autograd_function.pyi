from typing import NamedTuple

import torch
from torch._C._functorch import TransformType
from torch._ops import HigherOrderOperator

class CustomFunctionHigherOrderOperator(HigherOrderOperator):
    def __init__(self) -> None: ...
    def __call__(self, autograd_function, *args, **kwargs):  # -> Any | None:
        ...

custom_function_call = ...

@custom_function_call.py_impl(TransformType.Grad)
@custom_function_call.py_impl(TransformType.Jvp)
def custom_function_call_grad(interpreter, autograd_function, *operands): ...
def generate_single_level_function(interpreter, autograd_function):  # -> type[_]:
    ...

NO_OUT_DIMS = ...

def wrap_outputs_maintaining_identity(outputs, unwrapped_inputs, orig_inputs, wrap_fn, out_dims=...):  # -> PyTree:
    ...

class VmapInfo(NamedTuple):
    batch_size: int
    randomness: str

def has_overridden_vmap_rule(autograd_function):  # -> bool:
    ...
def validate_vmap_returns_tuple_of_two_elements(result):  # -> None:
    ...
@custom_function_call.py_impl(TransformType.Vmap)
def custom_function_call_vmap(interpreter, autograd_function, *operands, **kwargs):  # -> tuple[Any, ...] | Any | None:
    ...
def custom_function_call_vmap_helper(interpreter, vmap_function, op, *operands, **kwargs):  # -> Any | None:
    ...
def unpack_outputs(outputs):  # -> tuple[Any, tuple[Any, ...] | Any]:
    ...
def custom_function_call_vmap_generate_rule(interpreter, autograd_function, *operands):  # -> tuple[Any, ...]:
    ...
@custom_function_call.py_impl(TransformType.Functionalize)
def custom_function_call_functionalize(interpreter, autograd_function, generate_vmap_rule, *operands): ...
def vmapify_autograd_function(autograd_function, in_dims, batch_size, randomness):  # -> type[_]:
    ...
def get_tangents_in_dims(input_dims, tangents):  # -> PyTree:
    ...

class WrappedCtx:
    _pt_reserved_attrs: tuple[str, ...] = ...
    def __init__(self, ctx) -> None: ...
    def __getattr__(self, name):  # -> Any:
        ...
    def __setattr__(self, name, value):  # -> None:
        ...

class CtxWithSavedTensors(WrappedCtx):
    _pt_reserved_attrs = ...
    def __init__(self, ctx, new_saved_tensors) -> None: ...
    @property
    def saved_tensors(self):  # -> Any:
        ...

class CtxCustomSave(WrappedCtx):
    _pt_reserved_attrs = ...
    def __init__(self, ctx, current_level) -> None: ...
    def save_for_backward(self, *tensors):  # -> None:
        ...
    def save_for_forward(self, *tensors):  # -> None:
        ...

def reductify(
    grad_input, grad_input_bdim, input_bdim, batch_size, target_shape_without_bdim_to_reduce_to=...
):  # -> tuple[Any | None, ...]:
    ...
def reductify_leaf(
    grad_input, grad_input_bdim, input_bdim, batch_size, target_shape_without_bdim_to_reduce_to=...
):  # -> None:
    ...
def autograd_function_forward_rewritten(original_forward, original_setup_context):  # -> Callable[..., Any]:
    ...

class AutogradFunctionApply(HigherOrderOperator):
    def __init__(self) -> None: ...
    def __call__(self, fwd, bwd, *fwd_args, **fwd_kwargs):  # -> Any | None:
        ...

autograd_function_apply = ...
