from typing import Literal, Optional, TYPE_CHECKING
from torch.utils._config_typing import *

"""
Global flags for aot autograd
"""
functionalize_rng_ops = ...
fake_tensor_allow_meta = ...
debug_assert = ...
debug_partitioner = ...
decompose_custom_triton_ops = ...
static_weight_shapes = ...
treat_parameters_as_free_to_save = ...
cse = ...
enable_autograd_cache: bool = ...
autograd_cache_allow_custom_autograd_functions: bool = ...
bundled_autograd_cache: bool = ...
autograd_cache_normalize_inputs = ...

def remote_autograd_cache_default() -> Optional[bool]: ...

enable_remote_autograd_cache = ...
view_replay_for_aliased_outputs = ...
max_dist_from_bw = ...
ban_recompute_used_far_apart = ...
ban_recompute_long_fusible_chains = ...
ban_recompute_materialized_backward = ...
ban_recompute_not_in_allowlist = ...
ban_recompute_reductions = ...
recompute_views = ...
activation_memory_budget = ...
activation_memory_budget_runtime_estimator = ...
activation_memory_budget_solver = ...
visualize_memory_budget_pareto = ...
memory_budget_pareto_dir = ...
aggressive_recomputation = ...
fake_tensor_allow_unsafe_data_ptr_access = ...
unlift_effect_tokens = ...
custom_op_default_layout_constraint: Literal["needs_exact_strides", "needs_fixed_stride_order", "flexible_layout"] = ...
fake_tensor_crossref = ...
fake_tensor_propagate_real_tensors = ...
backward_pass_autocast = ...
donated_buffer = ...
torch_compile_graph_format = ...
generate_fake_kernels_from_real_mismatches = ...
fake_tensor_prefer_device_type: Optional[str] = ...
graphsafe_rng_functionalization = ...
strict_autograd_cache = ...
unsafe_allow_optimization_of_collectives = ...
disable_guess_zero_tangent_for_mutated_input_subclass = ...
guess_tangent_strides_as_outputs = ...
_sync_decision_cross_ranks = ...
saved_tensors_hooks_filtering_mode = ...
if TYPE_CHECKING: ...
