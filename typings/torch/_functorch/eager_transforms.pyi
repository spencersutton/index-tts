import contextlib
from collections.abc import Callable
from typing import Any, Optional, Union

import torch
from torch._functorch.utils import argnums_t, exposed_in

from .vmap import doesnt_support_saved_tensors_hooks

def lazy_dynamo_disallow(func):  # -> Any:
    ...
@contextlib.contextmanager
def enable_inplace_requires_grad(enabled):  # -> Generator[None, Any, None]:
    ...
@exposed_in("torch.func")
def vjp(
    func: Callable, *primals, has_aux: bool = ...
):  # -> tuple[PyTree, Callable[..., PyTree], PyTree | Any] | tuple[PyTree, Callable[..., PyTree]]:

    ...
@contextlib.contextmanager
def grad_increment_nesting():  # -> Generator[int, Any, None]:
    ...
def enter_jvp_nesting():  # -> int:
    ...
def exit_jvp_nesting():  # -> None:
    ...
@contextlib.contextmanager
def jvp_increment_nesting():  # -> Generator[int, Any, None]:
    ...
def error_if_complex(func_name, args, is_input):  # -> None:
    ...
@exposed_in("torch.func")
def jacrev(
    func: Callable,
    argnums: int | tuple[int] = ...,
    *,
    has_aux=...,
    chunk_size: int | None = ...,
    _preallocate_and_copy=...,
):  # -> _Wrapped[..., Any, ..., tuple[PyTree, PyTree | Any] | PyTree]:

    ...

JVP_NESTING = ...

def assert_flat_tuple_of_tensors(elts: Any, api: str, argname: str) -> None: ...
def assert_non_empty_tensor_output(output: list[Any], api: str) -> None: ...
def assert_output_is_tensor_or_tensors(output: Any, api: str) -> None: ...
def assert_non_empty_list_of_tensors(output: list[torch.Tensor], api: str, argname: str) -> None: ...

jvp_str = ...

def safe_unpack_dual(dual, strict):  # -> tuple[Tensor, Tensor]:
    ...
@exposed_in("torch.func")
def jvp(
    func: Callable, primals: Any, tangents: Any, *, strict: bool = ..., has_aux: bool = ...
):  # -> tuple[PyTree, PyTree, PyTree | Any] | tuple[PyTree, PyTree]:

    ...
def safe_unflatten(tensor, dim, shape): ...
@exposed_in("torch.func")
def jacfwd(
    func: Callable, argnums: argnums_t = ..., has_aux: bool = ..., *, randomness: str = ...
):  # -> _Wrapped[..., Any, ..., tuple[PyTree, PyTree | Any] | PyTree]:

    ...
@exposed_in("torch.func")
def hessian(func, argnums=...):  # -> _Wrapped[..., Any, ..., tuple[PyTree, PyTree | Any] | PyTree]:

    ...
@doesnt_support_saved_tensors_hooks
def grad_and_value_impl(func, argnums, has_aux, args, kwargs) -> Callable: ...
def grad_impl(func: Callable, argnums: argnums_t, has_aux: bool, args, kwargs):  # -> tuple[Any, Any]:
    ...
@exposed_in("torch.func")
def functionalize(func: Callable, *, remove: str = ...) -> Callable: ...
@exposed_in("torch.func")
def linearize(func: Callable, *primals) -> tuple[Any, Callable]: ...
@exposed_in("torch.func")
def debug_unwrap(tensor: torch.Tensor, *, recurse=...) -> torch.Tensor: ...
