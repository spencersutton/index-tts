import functools
import logging
from collections.abc import Callable
from dataclasses import dataclass

import torch
from torch import fx
from torch.utils._ordered_set import OrderedSet

AOT_PARTITIONER_DEBUG: bool = ...
log: logging.Logger = ...
aten = ...
prims = ...

@dataclass
class OpTypes:
    fusible_ops: OrderedSet[Callable]
    compute_intensive_ops: OrderedSet[Callable]
    random_ops: OrderedSet[Callable]
    view_ops: OrderedSet[Callable]
    recomputable_ops: OrderedSet[Callable]
    def is_fusible(self, node: fx.Node): ...
    def is_compute_intensive(self, node: fx.Node): ...
    def is_random(self, node: fx.Node): ...
    def is_view(self, node: fx.Node): ...
    def is_recomputable(self, node: fx.Node): ...

@dataclass
class NodeInfo:
    inputs: list[fx.Node]
    _required_fw_nodes: OrderedSet[fx.Node]
    required_bw_nodes: OrderedSet[fx.Node]
    unclaimed_nodes: OrderedSet[fx.Node]
    fw_order: dict[fx.Node, int]
    static_lifetime_input_nodes: OrderedSet[fx.Node]
    @functools.cached_property
    def required_fw_nodes(self) -> list[fx.Node]: ...
    def is_required_fw(self, n: fx.Node) -> bool: ...
    def is_required_bw(self, n: fx.Node) -> bool: ...
    def is_unclaimed(self, n: fx.Node) -> bool: ...
    def get_fw_order(self, n: fx.Node) -> int: ...

@dataclass
class MinCutOptions:
    ban_if_used_far_apart: bool
    ban_if_long_fusible_chains: bool
    ban_if_materialized_backward: bool
    ban_if_not_in_allowlist: bool
    ban_if_reduction: bool

def must_recompute(node: fx.Node) -> bool: ...
def has_recomputable_ops(fx_g: fx.GraphModule) -> bool: ...
def has_recomputable_rng_ops(fx_g: fx.GraphModule) -> bool: ...
def sym_node_size(node: fx.Node) -> int: ...

class InvalidNodeBase: ...

InvalidNode = ...

def find_first_sym_node(fwd_module_outputs: list[fx.Node] | tuple[fx.Node]) -> int: ...
def calculate_quantization_scaling(graph: torch.fx.Graph, node: torch.fx.Node, max: float = ..., min: float = ...): ...
def perform_quantization(
    graph: torch.fx.Graph,
    node: torch.fx.Node,
    scale_node: torch.fx.Node,
    quant_type: torch.dtype,
    clamp_min: float,
    clamp_max: float,
) -> torch.fx.Node: ...
def calculate_tensor_size(tensor: torch.Tensor) -> float: ...
def get_allowed_dtypes() -> list[torch.dtype]: ...
def should_quantize(node: torch.fx.Node) -> bool: ...
def get_quant_type() -> torch.dtype: ...
def calculate_range(dtype: torch.dtype) -> tuple: ...
def quantize_activation_fw(graph: torch.fx.Graph) -> None: ...
def quantize_activation_bw(graph: torch.fx.Graph) -> None: ...
def perform_fp8_activation_quantization(
    fwd_module: fx.GraphModule, bwd_module: fx.GraphModule, bwd_module_inputs: dict[str, fx.Node]
) -> None: ...
def enable_activation_quantization(
    saved_values: list[fx.Node],
    fwd_module: fx.GraphModule,
    bwd_module: fx.GraphModule,
    static_lifetime_input_nodes: OrderedSet[fx.Node] | None = ...,
) -> None: ...
def default_partition(
    joint_module: fx.GraphModule,
    _joint_inputs,
    *,
    num_fwd_outputs,
    static_lifetime_input_indices: list[int] | None = ...,
    static_lifetime_input_nodes: OrderedSet[fx.Node] | None = ...,
) -> tuple[fx.GraphModule, fx.GraphModule]: ...

INT_INF = ...

@functools.cache
def pointwise_ops(): ...
def sort_depths(args, depth_map: dict[fx.Node, int]) -> list[tuple[fx.Node, int]]: ...
def reordering_to_mimic_autograd_engine(gm: fx.GraphModule) -> fx.GraphModule: ...
def apply_graphsafe_rng_functionalization(
    fw_module: torch.fx.GraphModule,
    bw_module: torch.fx.GraphModule,
    fw_node: torch.fx.Node,
    bw_node: torch.fx.Node,
    device: torch.device,
    rng_count: int,
    last_fwd_input: torch.fx.Node,
    last_bwd_input: torch.fx.Node,
): ...
def functionalize_rng_ops(
    joint_module: fx.GraphModule, fw_module: fx.GraphModule, bw_module: fx.GraphModule, num_sym_nodes: int
) -> tuple[fx.GraphModule, fx.GraphModule]: ...
def force_save_collectives(joint_module: fx.GraphModule) -> None: ...
def force_save_bw_mutation_src(joint_module: fx.GraphModule) -> None: ...
def cleanup_recompute_tags(joint_module: fx.GraphModule) -> fx.GraphModule: ...
def solve_min_cut(
    joint_graph: fx.Graph,
    node_info: NodeInfo,
    min_cut_options: MinCutOptions,
    dont_ban: OrderedSet[fx.Node] | None = ...,
): ...
def visualize_min_cut_graph(nx_graph): ...
def get_default_op_list() -> OpTypes: ...
def get_name_to_node(graph: fx.Graph): ...
def estimate_runtime(node): ...
def choose_saved_values_set(joint_graph: fx.Graph, node_info: NodeInfo, memory_budget=...) -> list[fx.Node]: ...
def thread_graphsafe_rng_from_hops(module, is_backward): ...
def min_cut_rematerialization_partition(
    joint_module: fx.GraphModule,
    _joint_inputs,
    compiler=...,
    *,
    num_fwd_outputs,
    static_lifetime_input_indices: list[int] | None = ...,
) -> tuple[fx.GraphModule, fx.GraphModule]: ...
def draw_graph(
    traced: torch.fx.GraphModule,
    fname: str,
    figname: str = ...,
    clear_meta: bool = ...,
    prog: str | list[str] | None = ...,
    parse_stack_trace: bool = ...,
    dot_graph_shape: str | None = ...,
) -> None: ...
