import contextlib
import dataclasses
import enum
import traceback
import weakref
import torch
import sympy
from abc import abstractmethod
from contextlib import contextmanager
from dataclasses import dataclass
from typing import Any, Callable, Generic, NamedTuple, Optional, TYPE_CHECKING, TypeVar, Union
from torch.utils._backport_slots import dataclass_slots
from torch.utils._traceback import CapturedTraceback
from collections.abc import Generator, Iterator
from types import CodeType
from torch._dynamo.codegen import PyCodegen
from torch._subclasses.fake_tensor import FakeTensorMode

log = ...
if TYPE_CHECKING: ...
COMPILE_ID_PATTERN = ...
CA_COMPILE_ID_PATTERN = ...

@dataclass(frozen=True)
class CompileId:
    frame_id: Optional[int]
    frame_compile_id: Optional[int]
    compiled_autograd_id: Optional[int] = ...

    @classmethod
    def from_string(cls, compile_id: Optional[str]) -> Optional[CompileId]: ...

class TraceId(NamedTuple):
    compile_id: CompileId
    attempt: int

class GuardSource(enum.Enum):
    LOCAL = ...
    GLOBAL = ...
    LOCAL_SPECIALIZED_NN_MODULE = ...
    GLOBAL_SPECIALIZED_NN_MODULE = ...
    CONSTANT = ...
    RANDOM_VALUE = ...
    SHAPE_ENV = ...
    LOCAL_FSDP_MODULE = ...
    GLOBAL_FSDP_MODULE = ...
    BACKWARD_STATE = ...
    EPHEMERAL = ...
    SYNTHETIC_LOCAL = ...
    LOCAL_UNSPECIALIZED_NN_MODULE = ...
    GLOBAL_UNSPECIALIZED_NN_MODULE = ...
    LOCAL_UNSPECIALIZED_BUILTIN_NN_MODULE = ...
    GLOBAL_UNSPECIALIZED_BUILTIN_NN_MODULE = ...
    def is_fsdp_module(self) -> bool: ...
    def is_specialized_nn_module(self) -> bool: ...
    def is_unspecialized_nn_module(self) -> bool: ...
    def is_unspecialized_builtin_nn_module(self) -> bool: ...
    def is_local(self) -> bool: ...

class GuardBuilderBase: ...

@dataclasses.dataclass(frozen=True)
class SLoc:
    framework_loc: Optional[Union[traceback.FrameSummary, str]]
    maybe_user_loc: Optional[str]

class ShapeGuard(NamedTuple):
    expr: sympy.logic.boolalg.Boolean
    sloc: SLoc
    size_oblivious: bool

@dataclass_slots
@dataclasses.dataclass
class Guard:
    originating_source: Source
    create_fn: Callable[[GuardBuilderBase, Guard], None]
    guard_types: Optional[list[str]] = ...
    code_list: Optional[list[str]] = ...
    obj_weakref: Optional[object] = ...
    guarded_class_weakref: Optional[weakref.ReferenceType[Any]] = ...
    stack: Optional[CapturedTraceback] = ...
    user_stack: Optional[traceback.StackSummary] = ...
    _hash: Optional[int] = ...
    _unserializable: bool = ...
    def __hash__(self) -> int: ...
    def sort_key(self) -> tuple[bool, int, int, str, int]: ...
    def __lt__(self, other: Guard) -> bool: ...
    def inner_create_fn(self) -> Callable[[GuardBuilderBase, Guard], Any]: ...
    @property
    def name(self) -> str: ...
    @property
    def source(self) -> GuardSource: ...
    @staticmethod
    def weakref_to_str(obj_weakref: object) -> str: ...
    def create(self, builder: GuardBuilderBase) -> Any: ...
    def is_specialized_nn_module(self) -> bool: ...
    def is_fsdp_module(self) -> bool: ...
    def is_local(self) -> bool: ...
    def create_fn_name(self) -> str: ...
    def set_export_info(
        self,
        guard_type: str,
        guarded_class: Optional[weakref.ReferenceType[Any]],
        code_list: list[str],
        obj_weakref: object,
    ) -> None: ...

T = TypeVar("T")

@dataclasses.dataclass(frozen=True)
class GuardEnvExpr: ...

@dataclasses.dataclass(frozen=True)
class DuplicateInputs(GuardEnvExpr):
    input_source_a: Source
    input_source_b: Source
    def __post_init__(self) -> None: ...

@dataclasses.dataclass(frozen=True)
class StorageOverlap(GuardEnvExpr):
    overlapping_sources: list[Source]
    non_overlapping_sources: list[Source]

class Checkpointable(Generic[T]):
    @abstractmethod
    def copy_graphstate(self) -> T: ...
    @abstractmethod
    def restore_graphstate(self, state: T) -> None: ...

class GuardsCheckpointState:
    dynamo_guards: set[Guard] = ...
    def __init__(self, dynamo_guards: set[Guard]) -> None: ...
    def diff(self, other: GuardsCheckpointState) -> Optional[set[Guard]]: ...
    def __eq__(self, other: object) -> bool: ...

class ModuleContextCheckpointState:
    nn_modules: dict[str, torch.nn.Module] = ...
    def __init__(self, nn_modules: dict[str, torch.nn.Module]) -> None: ...
    def diff(self, other: ModuleContextCheckpointState) -> Optional[set[str]]: ...
    def __eq__(self, other: object) -> bool: ...

class ModuleContext(Checkpointable[ModuleContextCheckpointState]):
    def __init__(self) -> None: ...
    def copy_graphstate(self) -> ModuleContextCheckpointState: ...
    def restore_graphstate(self, state: ModuleContextCheckpointState) -> None: ...

class GlobalContextCheckpointState:
    global_state: dict[str, tuple[Callable, Any]] = ...
    def __init__(self, global_states: dict[str, tuple[Callable, Any]]) -> None: ...
    def diff(self, other: GlobalContextCheckpointState) -> Optional[set[str]]: ...
    def __eq__(self, other: object) -> bool: ...

class GlobalContext(Checkpointable[GlobalContextCheckpointState]):
    _supported_global_states = ...
    def __init__(self) -> None: ...
    def copy_graphstate(self) -> GlobalContextCheckpointState: ...
    def restore_graphstate(self, state: GlobalContextCheckpointState) -> None: ...

class GuardsSet:
    def __init__(self, inner: Optional[set[Guard]] = ...) -> None: ...
    def __iter__(self) -> Iterator[Guard]: ...
    def __len__(self) -> int: ...
    def __sub__(self, other: GuardsSet) -> GuardsSet: ...
    def __bool__(self) -> bool: ...
    def add(self, guard: Guard, *, collect_debug_stack: bool = ..., skip: int = ...) -> None: ...
    def update(self, *others: set[Guard]) -> None: ...
    def remove_guards_with_source(self, source: Source) -> None: ...

class GuardsContext(Checkpointable[GuardsCheckpointState]):
    def __init__(self) -> None: ...
    def copy_graphstate(self) -> GuardsCheckpointState: ...
    def restore_graphstate(self, state: GuardsCheckpointState) -> None: ...

class HopSubgraphCache:
    @abstractmethod
    def add_dynamo_installed_submodule(self, fn_id: int, identifier: str) -> None: ...
    @abstractmethod
    def get_dynamo_installed_submodules(self, fn_id: int) -> list[str]: ...
    @abstractmethod
    def add_autograd_key_entry(self, identifier: str, key: Callable) -> None: ...
    @abstractmethod
    def get_autograd_key_entry(self, identifier: str) -> Optional[Callable]: ...
    @abstractmethod
    def add_proxy_dispatch_entry(self, identifier: str, key: Callable) -> None: ...
    @abstractmethod
    def get_proxy_dispatch_entry(self, identifier: str) -> Optional[Callable]: ...
    @abstractmethod
    def add_lazy_bwd_entry(
        self, identifier: str, tangent_metadata: tuple[object], gmod: torch.fx.GraphModule
    ) -> int: ...
    @abstractmethod
    def get_lazy_bwd_entry(
        self, identifier: str, tangent_metadata: tuple[object]
    ) -> tuple[Optional[torch.fx.GraphModule], Optional[int]]: ...

class InvokeSubgraphCache(HopSubgraphCache):
    def __init__(self) -> None: ...
    def add_dynamo_installed_submodule(self, fn_id: int, identifier: str) -> None: ...
    def get_dynamo_installed_submodules(self, fn_id: int) -> list[str]: ...
    def add_autograd_key_entry(self, identifier: str, key: Callable) -> None: ...
    def get_autograd_key_entry(self, identifier: str) -> Optional[Callable]: ...
    def add_proxy_dispatch_entry(self, identifier: str, key: Callable) -> None: ...
    def get_proxy_dispatch_entry(self, identifier: str) -> Optional[Callable]: ...
    def add_lazy_bwd_entry(
        self, identifier: str, tangent_metadata: tuple[object], gmod: torch.fx.GraphModule
    ) -> int: ...
    def get_lazy_bwd_entry(
        self, identifier: str, tangent_metadata: tuple[object]
    ) -> tuple[Optional[torch.fx.GraphModule], Optional[int]]: ...

class HopDispatchSetCache:
    def __init__(self) -> None: ...
    def get_cache(self, op: torch._ops.HigherOrderOperator) -> Optional[HopSubgraphCache]: ...

_TLS = ...

class CompileContext:
    @staticmethod
    def get() -> CompileContext: ...
    @staticmethod
    def try_get() -> Optional[CompileContext]: ...
    def __init__(self, compile_id: Optional[CompileId]) -> None: ...
    @staticmethod
    def current_compile_id() -> Optional[CompileId]: ...
    @staticmethod
    def current_trace_id() -> Optional[TraceId]: ...

class TracingContext:
    @staticmethod
    def try_get() -> Optional[TracingContext]: ...
    @staticmethod
    def get() -> TracingContext: ...
    def __init__(self, fake_mode: Optional[FakeTensorMode]) -> None: ...
    def clear(self) -> None: ...
    @staticmethod
    @contextmanager
    def patch(**kwargs: Any) -> Generator[None, None, None]: ...
    @staticmethod
    def extract_stack() -> traceback.StackSummary: ...
    @staticmethod
    @contextlib.contextmanager
    def clear_frame() -> Generator[None, None, None]: ...
    @staticmethod
    @contextlib.contextmanager
    def current_frame(frame_summary: Optional[traceback.FrameSummary]) -> Generator[None, None, None]: ...
    @staticmethod
    @contextlib.contextmanager
    def report_output_strides() -> Generator[Optional[list[Optional[tuple[int, ...]]]], None, None]: ...
    @staticmethod
    def set_current_loc(filename: str, lineno: int, frame_name: str) -> None: ...
    @staticmethod
    def get_traced_code() -> Optional[list[CodeType]]: ...

@contextmanager
def compile_context(context: Optional[CompileContext]) -> Generator[Optional[CompileContext], None, None]: ...
@contextmanager
def tracing(context: Optional[TracingContext]) -> Generator[Optional[TracingContext], None, None]: ...

@dataclasses.dataclass(frozen=True)
class Source:
    def is_dict_key(self) -> bool: ...
    def is_ephemeral(self) -> bool: ...
    def reconstruct(self, codegen: PyCodegen) -> None: ...
    def guard_source(self) -> GuardSource: ...
    def name(self) -> str: ...
    def make_guard(self, fn: Callable[..., Any]) -> Guard: ...
    def is_specialized_nn_module(self) -> bool: ...
    def subguards_allowed(self) -> bool: ...

@dataclasses.dataclass(frozen=True)
class ChainedSource(Source):
    base: Source
    def is_dict_key(self) -> bool: ...
    def is_ephemeral(self) -> bool: ...
    def get_base(self) -> Source: ...

def detect_fake_mode(inputs: Any = ...) -> Optional[FakeTensorMode]: ...
def active_fake_mode() -> Optional[FakeTensorMode]: ...
