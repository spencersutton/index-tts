import contextlib
import dataclasses
import enum
import traceback
import weakref
from abc import abstractmethod
from collections.abc import Callable, Generator, Iterator
from contextlib import contextmanager
from dataclasses import dataclass
from types import CodeType
from typing import Any, NamedTuple, TypeVar

import sympy
import torch
from torch._dynamo.codegen import PyCodegen
from torch._subclasses.fake_tensor import FakeTensorMode
from torch.utils._backport_slots import dataclass_slots
from torch.utils._traceback import CapturedTraceback

log = ...

COMPILE_ID_PATTERN = ...
CA_COMPILE_ID_PATTERN = ...

@dataclass(frozen=True)
class CompileId:
    """CompileId(frame_id: 'Optional[int]', frame_compile_id: 'Optional[int]', compiled_autograd_id: 'Optional[int]' = None)"""

    frame_id: int | None
    frame_compile_id: int | None
    compiled_autograd_id: int | None = ...

    @classmethod
    def from_string(cls, compile_id: str | None) -> CompileId | None:
        """
        Factory method that creates a CompileId from its string representation.
        Keep this in sync with the __str__ method.
        """

class TraceId(NamedTuple):
    """TraceId(compile_id, attempt)"""

    compile_id: CompileId
    attempt: int

class GuardSource(enum.Enum):
    LOCAL = ...
    GLOBAL = ...
    LOCAL_SPECIALIZED_NN_MODULE = ...
    GLOBAL_SPECIALIZED_NN_MODULE = ...
    CONSTANT = ...
    RANDOM_VALUE = ...
    SHAPE_ENV = ...
    LOCAL_FSDP_MODULE = ...
    GLOBAL_FSDP_MODULE = ...
    BACKWARD_STATE = ...
    EPHEMERAL = ...
    SYNTHETIC_LOCAL = ...
    LOCAL_UNSPECIALIZED_NN_MODULE = ...
    GLOBAL_UNSPECIALIZED_NN_MODULE = ...
    LOCAL_UNSPECIALIZED_BUILTIN_NN_MODULE = ...
    GLOBAL_UNSPECIALIZED_BUILTIN_NN_MODULE = ...
    def is_fsdp_module(self) -> bool: ...
    def is_specialized_nn_module(self) -> bool: ...
    def is_unspecialized_nn_module(self) -> bool: ...
    def is_unspecialized_builtin_nn_module(self) -> bool: ...
    def is_local(self) -> bool: ...

class GuardBuilderBase: ...

@dataclasses.dataclass(frozen=True)
class SLoc:
    """SLoc(framework_loc: 'Optional[Union[traceback.FrameSummary, str]]', maybe_user_loc: 'Optional[str]')"""

    framework_loc: traceback.FrameSummary | str | None
    maybe_user_loc: str | None

class ShapeGuard(NamedTuple):
    """ShapeGuard(expr, sloc, size_oblivious)"""

    expr: sympy.logic.boolalg.Boolean
    sloc: SLoc
    size_oblivious: bool

@dataclass_slots
@dataclasses.dataclass
class Guard:
    """Guard(originating_source: 'Source', create_fn: 'Callable[[GuardBuilderBase, Guard], None]', guard_types: 'Optional[list[str]]' = None, code_list: 'Optional[list[str]]' = None, obj_weakref: 'Optional[object]' = None, guarded_class_weakref: 'Optional[weakref.ReferenceType[Any]]' = None, stack: 'Optional[CapturedTraceback]' = None, user_stack: 'Optional[traceback.StackSummary]' = None, _hash: 'Optional[int]' = None, _unserializable: 'bool' = False)"""

    originating_source: Source
    create_fn: Callable[[GuardBuilderBase, Guard], None]
    guard_types: list[str] | None = ...
    code_list: list[str] | None = ...
    obj_weakref: object | None = ...
    guarded_class_weakref: weakref.ReferenceType[Any] | None = ...
    stack: CapturedTraceback | None = ...
    user_stack: traceback.StackSummary | None = ...
    _hash: int | None = ...
    _unserializable: bool = ...
    def __hash__(self) -> int: ...
    def sort_key(self) -> tuple[bool, int, int, str, int]: ...
    def __lt__(self, other: Guard) -> bool: ...
    def inner_create_fn(self) -> Callable[[GuardBuilderBase, Guard], Any]: ...
    @property
    def name(self) -> str: ...
    @property
    def source(self) -> GuardSource: ...
    @staticmethod
    def weakref_to_str(obj_weakref: object) -> str:
        """
        This is a workaround of a Python weakref bug.

        `obj_weakref` is instance returned by `weakref.ref`,
        `str(obj_weakref)` is buggy if the original obj overrides __getattr__, e.g:

            class MyConfig(dict):
                def __getattr__(self, x):
                    return self[x]

            obj = MyConfig(offset=5)
            obj_weakref = weakref.ref(obj)
            str(obj_weakref)  # raise error: KeyError: '__name__'
        """
    def create(self, builder: GuardBuilderBase) -> Any: ...
    def is_specialized_nn_module(self) -> bool: ...
    def is_fsdp_module(self) -> bool: ...
    def is_local(self) -> bool: ...
    def create_fn_name(self) -> str: ...
    def set_export_info(
        self,
        guard_type: str,
        guarded_class: weakref.ReferenceType[Any] | None,
        code_list: list[str],
        obj_weakref: object,
    ) -> None: ...

T = TypeVar("T")

@dataclasses.dataclass(frozen=True)
class GuardEnvExpr:
    """GuardEnvExpr()"""

@dataclasses.dataclass(frozen=True)
class DuplicateInputs(GuardEnvExpr):
    """DuplicateInputs(input_source_a: 'Source', input_source_b: 'Source')"""

    input_source_a: Source
    input_source_b: Source
    def __post_init__(self) -> None: ...

@dataclasses.dataclass(frozen=True)
class StorageOverlap(GuardEnvExpr):
    """StorageOverlap(overlapping_sources: 'list[Source]', non_overlapping_sources: 'list[Source]')"""

    overlapping_sources: list[Source]
    non_overlapping_sources: list[Source]

class Checkpointable[T]:
    @abstractmethod
    def copy_graphstate(self) -> T: ...
    @abstractmethod
    def restore_graphstate(self, state: T) -> None: ...

class GuardsCheckpointState:
    """The GuardCheckpointState - it is the T of Checkpointable[T] for GuardsContext"""

    dynamo_guards: set[Guard] = ...
    def __init__(self, dynamo_guards: set[Guard]) -> None: ...
    def diff(self, other: GuardsCheckpointState) -> set[Guard] | None:
        """
        Produces a delta against another GuardsCheckpointState.

        Returns None if no delta is found, otherwise, return a set() of mismatched
        Guard type objects.
        """
    def __eq__(self, other: object) -> bool: ...

class ModuleContextCheckpointState:
    nn_modules: dict[str, torch.nn.Module] = ...
    def __init__(self, nn_modules: dict[str, torch.nn.Module]) -> None: ...
    def diff(self, other: ModuleContextCheckpointState) -> set[str] | None:
        """
        Produces a delta against another ModuleContextCheckpointState.

        Returns None if no delta is found, otherwise, return a set() of mismatched
        module key names.
        """
    def __eq__(self, other: object) -> bool: ...

class ModuleContext(Checkpointable[ModuleContextCheckpointState]):
    def __init__(self) -> None: ...
    def copy_graphstate(self) -> ModuleContextCheckpointState: ...
    def restore_graphstate(self, state: ModuleContextCheckpointState) -> None: ...

class GlobalContextCheckpointState:
    global_state: dict[str, tuple[Callable, Any]] = ...
    def __init__(self, global_states: dict[str, tuple[Callable, Any]]) -> None: ...
    def diff(self, other: GlobalContextCheckpointState) -> set[str] | None:
        """
        Produces a delta against another GlobalContextCheckpointState.

        Returns None if no delta is found, otherwise, return a set() of mismatched
        global key names.
        """
    def __eq__(self, other: object) -> bool: ...

class GlobalContext(Checkpointable[GlobalContextCheckpointState]):
    """
    This keeps track of the global torch state during tracing of a function.
    For example, torch.is_grad_enabled.
    """

    _supported_global_states = ...
    def __init__(self) -> None: ...
    def copy_graphstate(self) -> GlobalContextCheckpointState: ...
    def restore_graphstate(self, state: GlobalContextCheckpointState) -> None: ...

class GuardsSet:
    def __init__(self, inner: set[Guard] | None = ...) -> None: ...
    def __iter__(self) -> Iterator[Guard]: ...
    def __len__(self) -> int: ...
    def __sub__(self, other: GuardsSet) -> GuardsSet: ...
    def __bool__(self) -> bool: ...
    def add(self, guard: Guard, *, collect_debug_stack: bool = ..., skip: int = ...) -> None: ...
    def update(self, *others: set[Guard]) -> None: ...
    def remove_guards_with_source(self, source: Source) -> None:
        """Delete all guards that contains a given source"""

class GuardsContext(Checkpointable[GuardsCheckpointState]):
    def __init__(self) -> None: ...
    def copy_graphstate(self) -> GuardsCheckpointState: ...
    def restore_graphstate(self, state: GuardsCheckpointState) -> None: ...

class HopSubgraphCache:
    @abstractmethod
    def add_dynamo_installed_submodule(self, fn_id: int, identifier: str) -> None: ...
    @abstractmethod
    def get_dynamo_installed_submodules(self, fn_id: int) -> list[str]: ...
    @abstractmethod
    def add_autograd_key_entry(self, identifier: str, key: Callable) -> None: ...
    @abstractmethod
    def get_autograd_key_entry(self, identifier: str) -> Callable | None: ...
    @abstractmethod
    def add_proxy_dispatch_entry(self, identifier: str, key: Callable) -> None: ...
    @abstractmethod
    def get_proxy_dispatch_entry(self, identifier: str) -> Callable | None: ...
    @abstractmethod
    def add_lazy_bwd_entry(
        self, identifier: str, tangent_metadata: tuple[object], gmod: torch.fx.GraphModule
    ) -> int: ...
    @abstractmethod
    def get_lazy_bwd_entry(
        self, identifier: str, tangent_metadata: tuple[object]
    ) -> tuple[torch.fx.GraphModule | None, int | None]: ...

class InvokeSubgraphCache(HopSubgraphCache):
    def __init__(self) -> None: ...
    def add_dynamo_installed_submodule(self, fn_id: int, identifier: str) -> None: ...
    def get_dynamo_installed_submodules(self, fn_id: int) -> list[str]: ...
    def add_autograd_key_entry(self, identifier: str, key: Callable) -> None: ...
    def get_autograd_key_entry(self, identifier: str) -> Callable | None: ...
    def add_proxy_dispatch_entry(self, identifier: str, key: Callable) -> None: ...
    def get_proxy_dispatch_entry(self, identifier: str) -> Callable | None: ...
    def add_lazy_bwd_entry(
        self, identifier: str, tangent_metadata: tuple[object], gmod: torch.fx.GraphModule
    ) -> int: ...
    def get_lazy_bwd_entry(
        self, identifier: str, tangent_metadata: tuple[object]
    ) -> tuple[torch.fx.GraphModule | None, int | None]: ...

class HopDispatchSetCache:
    def __init__(self) -> None: ...
    def get_cache(self, op: torch._ops.HigherOrderOperator) -> HopSubgraphCache | None: ...

_TLS = ...

class CompileContext:
    @staticmethod
    def get() -> CompileContext: ...
    @staticmethod
    def try_get() -> CompileContext | None: ...
    def __init__(self, compile_id: CompileId | None) -> None: ...
    @staticmethod
    def current_compile_id() -> CompileId | None: ...
    @staticmethod
    def current_trace_id() -> TraceId | None: ...

class TracingContext:
    """
    Provides the currently installed TracingContext, or None.

    Note that it is a staticmethod, and invocations outside of `with tracing()` (see below), are valid but
    will return None.
    """
    @staticmethod
    def try_get() -> TracingContext | None: ...
    @staticmethod
    def get() -> TracingContext: ...
    def __init__(self, fake_mode: FakeTensorMode | None) -> None: ...
    def clear(self) -> None: ...
    @staticmethod
    @contextmanager
    def patch(**kwargs: Any) -> Generator[None]: ...
    @staticmethod
    def extract_stack() -> traceback.StackSummary: ...
    @staticmethod
    @contextlib.contextmanager
    def clear_frame() -> Generator[None]: ...
    @staticmethod
    @contextlib.contextmanager
    def current_frame(frame_summary: traceback.FrameSummary | None) -> Generator[None]: ...
    @staticmethod
    @contextlib.contextmanager
    def report_output_strides() -> Generator[list[tuple[int, ...] | None] | None]: ...
    @staticmethod
    def set_current_loc(filename: str, lineno: int, frame_name: str) -> None: ...
    @staticmethod
    def get_traced_code() -> list[CodeType] | None: ...

@contextmanager
def compile_context(context: CompileContext | None) -> Generator[CompileContext | None]: ...
@contextmanager
def tracing(context: TracingContext | None) -> Generator[TracingContext | None]:
    """
    This function installs the passed in tracing context as a dynamic scoped
    global variable.

    Calls to TracingContext.get() while not under a `with tracing()` context
    will return None.
    """

@dataclasses.dataclass(frozen=True)
class Source:
    """Source()"""
    def is_dict_key(self) -> bool: ...
    def is_ephemeral(self) -> bool: ...
    def reconstruct(self, codegen: PyCodegen) -> None: ...
    def guard_source(self) -> GuardSource: ...
    def name(self) -> str: ...
    def make_guard(self, fn: Callable[..., Any]) -> Guard: ...
    def is_specialized_nn_module(self) -> bool: ...
    def subguards_allowed(self) -> bool:
        """True if you can guard on attributes of this"""

@dataclasses.dataclass(frozen=True)
class ChainedSource(Source):
    """ChainedSource(base: 'Source')"""

    base: Source
    def is_dict_key(self) -> bool: ...
    def is_ephemeral(self) -> bool: ...
    def get_base(self) -> Source: ...

def detect_fake_mode(inputs: Any = ...) -> FakeTensorMode | None:
    """
    Attempts to "detect" what the current fake mode is.  If there is one ambiently
    available from TracingContext, we preferentially use that.  Otherwise, we
    heuristically detect the fake mode via the following sources, in order of
    priority:

        - Currently active fake mode on stack
        - Fake mode associated with passed in tensors (inputs does not
          have to be flattened)
    """

def active_fake_mode() -> FakeTensorMode | None:
    """
    Inspects the dispatch mode stack for an active fake mode and returns it.
    Returns None if no fake mode is active.
    """
