import abc

import torch
from torch._ops import HigherOrderOperator

class BaseHOP(HigherOrderOperator, abc.ABC):
    def __init__(self, hop_name) -> None: ...
    def __call__(self, subgraph, *operands, **kwargs):  # -> Any | None:
        ...
    def gen_schema(self, subgraph, *operands, **kwargs):  # -> FunctionSchema:
        ...

class BaseHOPFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, hop, subgraph, kwargs, *operands): ...
    @staticmethod
    def backward(ctx, *grad_outputs):  # -> tuple[None, None, None, *tuple[Any, ...]]:
        ...

class FunctionWithNoFreeVars:
    def __init__(self, fn) -> None: ...
    def __call__(self, *args, **kwargs): ...
