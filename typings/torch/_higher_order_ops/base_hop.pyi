import abc

import torch
from torch._ops import HigherOrderOperator

class BaseHOP(HigherOrderOperator, abc.ABC):
    def __init__(self, hop_name) -> None: ...
    def __call__(self, subgraph, *operands, **kwargs): ...
    def gen_schema(self, subgraph, *operands, **kwargs): ...

class BaseHOPFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, hop, subgraph, kwargs, *operands): ...
    @staticmethod
    def backward(ctx, *grad_outputs): ...

class FunctionWithNoFreeVars:
    def __init__(self, fn) -> None: ...
    def __call__(self, *args, **kwargs): ...
