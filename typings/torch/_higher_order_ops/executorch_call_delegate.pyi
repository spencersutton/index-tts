from typing import Any

import torch
from torch._ops import HigherOrderOperator
from torch._subclasses.fake_tensor import FakeTensorMode
from torch.fx.experimental.proxy_tensor import ProxyTorchDispatchMode

class ExecutorchCallDelegate(HigherOrderOperator):
    def __init__(self) -> None: ...
    def __call__(self, lowered_module, *args):  # -> Any | None:
        ...

executorch_call_delegate = ...
LOWERED_BACKEND_MODULE_TYPE = ...

def trace_call_delegate(proxy_mode, func_overload, lowered_module, *args): ...
@executorch_call_delegate.py_impl(torch._C.DispatchKey.CompositeExplicitAutograd)
def call_delegate_cpu(lowered_module, *args): ...
@executorch_call_delegate.py_autograd_impl
def call_delegate_autograd(lowered_module, *args):  # -> PyTree | None:
    ...
@executorch_call_delegate.py_impl(ProxyTorchDispatchMode)
def call_delegate_proxy_torch_dispatch_mode(mode, lowered_module, *args): ...
@executorch_call_delegate.py_impl(FakeTensorMode)
def call_delegate_fake_tensor_mode(mode, lowered_module, *args): ...
@executorch_call_delegate.py_functionalize_impl
def call_delegate_functionalize(ctx, lowered_module, *args): ...
def is_lowered_module(obj: Any) -> bool: ...
def get_lowered_module_name(root: torch.nn.Module, lowered_module: LOWERED_BACKEND_MODULE_TYPE) -> str: ...
