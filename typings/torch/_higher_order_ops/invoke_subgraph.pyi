from dataclasses import dataclass

import torch
from torch._C import DispatchKey
from torch._higher_order_ops.utils import FunctionalizeCtxWrapper, register_fake
from torch._ops import HigherOrderOperator
from torch.fx.experimental.proxy_tensor import ProxyTorchDispatchMode
from torch.fx.graph_module import GraphModule

invoke_subgraph_counter = ...

@dataclass
class OutputMetadata:
    num_fw_outs: int | None = ...
    indexes_with_symint: set[int] = ...
    indexes_with_no_grad: set[int] = ...

class InvokeSubgraphHOP(HigherOrderOperator):
    def __init__(self) -> None: ...
    def __call__(self, subgraph: GraphModule | FunctionalizeCtxWrapper, identifier: str | None, *operands): ...
    def gen_schema(self, subgraph, identifier, *operands): ...

invoke_subgraph = ...

def invoke_subgraph_placeholder(func, *args, **kwargs): ...
def mark_compile_region(fn=...): ...
def get_invoke_subgraph_cache(): ...
def trace_joint_graph(fn, fw_inputs, fw_outputs): ...
def create_fw_bw_graph(subgraph, operands, grad_outputs=...): ...
def get_output_metadata(subgraph, *operands): ...
def trace_joint_graph_as_bwd(subgraph, num_primals, joint_operands, include_key_set, exclude_key_set): ...

class InvokeSubgraphAutogradOp(torch.autograd.Function):
    @staticmethod
    def forward(ctx, subgraph, identifier, output_metadata, *operands): ...
    @staticmethod
    def backward(ctx, *grad_outs): ...

@invoke_subgraph.py_autograd_impl
def _(subgraph, identifier, *operands): ...
@invoke_subgraph.py_impl(DispatchKey.CompositeExplicitAutograd)
def _(subgraph, identifier, *operands): ...
@invoke_subgraph.py_functionalize_impl
def _(ctx, subgraph, identifier, *operands): ...
@register_fake(invoke_subgraph)
def _(subgraph, identifier, *operands): ...
@invoke_subgraph.py_impl(ProxyTorchDispatchMode)
def _(proxy_mode: ProxyTorchDispatchMode, subgraph, identifier, *operands): ...
