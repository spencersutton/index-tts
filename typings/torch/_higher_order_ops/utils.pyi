from collections.abc import Callable, Iterable, Sequence
from dataclasses import dataclass
from typing import Any, Optional, TypeVar, Union, overload

import torch
from torch._higher_order_ops.schema import HopSchema
from torch._ops import HigherOrderOperator, OperatorBase, OpOverload
from torch._subclasses.fake_tensor import FakeTensor
from torch.fx.passes.shape_prop import TensorMetadata

@dataclass
class UnsupportedAliasMutationException(RuntimeError):
    reason: str

def autograd_not_implemented_inner(operator: OperatorBase, delayed_error: bool, *args: Any, **kwargs: Any) -> Any: ...
def autograd_not_implemented(op: OperatorBase, deferred_error: bool) -> Callable: ...
def reenter_make_fx(fn):  # -> _Wrapped[..., Any, ..., GraphModule]:
    ...
def check_meta_consistency(
    lhs_list: list[torch.Tensor | torch.SymInt | int],
    rhs_list: list[torch.Tensor | torch.SymInt | int],
    lhs_name: str,
    rhs_name: str,
    include_contiguity: bool = ...,
) -> None: ...
def potential_input_alias_or_mutation(
    gm, inputs, pre_dispatch=...
):  # -> tuple[tuple[dict[int, int], dict[int, int], dict[int, int]], list[int]] | Literal[True]:
    ...
def analyze_potential_input_alias_or_mutation(name, aliases, input_mutations):  # -> None:
    ...
def has_potential_input_alias_or_mutation(gm, inputs, pre_dispatch=...):  # -> tuple[bool, bool]:
    ...
def unique_graph_id(proxy_mode, prefix):  # -> tuple[int, str]:

    ...
def unique_graph_name_with_root(root: torch.fx.GraphModule, prefix: str) -> tuple[int, str]: ...
def clone_outputs_aliasing_inputs(args):  # -> Callable[..., Tensor | Any]:
    ...
def prepare_fw_with_masks(fn):  # -> Callable[..., tuple[Any, list[bool]]]:
    ...
def prepare_fw_with_masks_all_requires_grad(fn):  # -> Callable[..., tuple[PyTree | Any, PyTree]]:
    ...
def unmask_none_gradients(grads, operands):  # -> list[Any]:
    ...
def redirect_to_mode(hop: OperatorBase, mode):  # -> Callable[..., Any]:

    ...
def create_fw_bw_graph(fn, use_output_and_grad_bw, fw_inputs, fw_outputs):  # -> tuple[GraphModule, GraphModule]:
    ...
def save_tensors_and_symints_for_backward(ctx, args):  # -> None:
    ...
def saved_tensors_and_symints(ctx):  # -> tuple[Any, ...]:
    ...
def split_into_chunks(iterable: Sequence[Any], chunk_sizes: list[int]) -> list[Any]: ...
def create_bw_fn(fn: Callable, args: tuple[Any]) -> Callable: ...
def get_dummy_aot_autograd_config():  # -> AOTConfig:
    ...
def first_slice_copy(t: torch.Tensor, dim: int = ...) -> torch.Tensor: ...
def get_tensor_mask(tensor_list: Iterable[Any]) -> list[bool]: ...
def mask_list(mask: list[bool], inp: list[Any], other: list[Any] | None = ...) -> list[Any]: ...
def first_slice_copy_with_grad(li: Iterable[Any]) -> list[Any]: ...
def diff_tensor_meta(meta1: TensorMetadata, meta2: TensorMetadata, check_grad=...) -> list[str]: ...
def validate_subgraph_args_types(lifted_args: tuple[Any, ...] | list[Any]):  # -> None:
    ...
def check_input_alias_and_mutation(
    gm: torch.fx.GraphModule, fake_args: list[FakeTensor]
) -> tuple[dict[int, int], dict[int, int], dict[int, int], list[int]]: ...
def check_input_alias_and_mutation_return_outputs(
    gm: torch.fx.GraphModule, fake_args: list[FakeTensor] | tuple[FakeTensor, ...]
) -> tuple[
    dict[int, int],
    dict[int, int],
    dict[int, int],
    list[int],
    tuple[Any, ...] | list[Any],
]: ...

registered_hop_fake_fns: dict[torch._ops.OpOverload, Callable] = ...
F = TypeVar("F", bound=Callable)

@overload
def register_fake(hop, fn: None = ...) -> Callable[[F], F]: ...
@overload
def register_fake[F: Callable](hop, fn: F) -> F: ...
def register_fake(hop, fn=...):  # -> Callable[..., F]:

    ...

class FunctionalizeCtxWrapper:
    @torch._disable_dynamo
    def __init__(self, ctx, subgraph) -> None: ...
    def __hash__(self) -> int: ...
    def __repr__(self):  # -> str:
        ...
    def __call__(self, *args, **kwargs): ...

class HopInstance:
    def __init__(self, op: HigherOrderOperator, schema: HopSchema) -> None: ...
    def __call__(self, *args, **kwargs):  # -> Any | None:
        ...
    @staticmethod
    def create(hop: HigherOrderOperator, *args, **kwargs):  # -> HopInstance:
        ...

def call_op(op: OpOverload | HopInstance, args, kwargs):  # -> Any | None:
    ...
def materialize_as_graph(
    fn: Callable,
    args: tuple[Any],
    include_key_set: torch._C.DispatchKeySet | None = ...,
    exclude_key_set: torch._C.DispatchKeySet | None = ...,
    force_enable_grad=...,
) -> torch.fx.GraphModule: ...
def materialize_callable_in_args(op: HopInstance, args, kwargs):  # -> PyTree:
    ...
def has_user_subclass(args, allowed_subclasses):  # -> bool:

    ...
def filter_with_masks(data: list[torch.Tensor | None], masks: list[bool]):  # -> list[Tensor | None]:
    ...
def fill_none_with_masks(data: list[torch.Tensor | None], masks: list[bool]):  # -> list[Tensor | None]:
    ...
