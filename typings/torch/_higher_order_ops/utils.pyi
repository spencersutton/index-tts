from collections.abc import Callable, Iterable, Sequence
from dataclasses import dataclass
from typing import Any, TypeVar, overload

import torch
from torch._higher_order_ops.schema import HopSchema
from torch._ops import HigherOrderOperator, OperatorBase, OpOverload
from torch._subclasses.fake_tensor import FakeTensor
from torch.fx.passes.shape_prop import TensorMetadata

@dataclass
class UnsupportedAliasMutationException(RuntimeError):
    reason: str

def autograd_not_implemented_inner(operator: OperatorBase, delayed_error: bool, *args: Any, **kwargs: Any) -> Any: ...
def autograd_not_implemented(op: OperatorBase, deferred_error: bool) -> Callable: ...
def reenter_make_fx(fn): ...
def check_meta_consistency(
    lhs_list: list[torch.Tensor | torch.SymInt | int],
    rhs_list: list[torch.Tensor | torch.SymInt | int],
    lhs_name: str,
    rhs_name: str,
    include_contiguity: bool = ...,
) -> None: ...
def potential_input_alias_or_mutation(gm, inputs, pre_dispatch=...): ...
def analyze_potential_input_alias_or_mutation(name, aliases, input_mutations): ...
def has_potential_input_alias_or_mutation(gm, inputs, pre_dispatch=...): ...
def unique_graph_id(proxy_mode, prefix): ...
def unique_graph_name_with_root(root: torch.fx.GraphModule, prefix: str) -> tuple[int, str]: ...
def clone_outputs_aliasing_inputs(args): ...
def prepare_fw_with_masks(fn): ...
def prepare_fw_with_masks_all_requires_grad(fn): ...
def unmask_none_gradients(grads, operands): ...
def redirect_to_mode(hop: OperatorBase, mode): ...
def create_fw_bw_graph(fn, use_output_and_grad_bw, fw_inputs, fw_outputs): ...
def save_tensors_and_symints_for_backward(ctx, args): ...
def saved_tensors_and_symints(ctx): ...
def split_into_chunks(iterable: Sequence[Any], chunk_sizes: list[int]) -> list[Any]: ...
def create_bw_fn(fn: Callable, args: tuple[Any]) -> Callable: ...
def get_dummy_aot_autograd_config(): ...
def first_slice_copy(t: torch.Tensor, dim: int = ...) -> torch.Tensor: ...
def get_tensor_mask(tensor_list: Iterable[Any]) -> list[bool]: ...
def mask_list(mask: list[bool], inp: list[Any], other: list[Any] | None = ...) -> list[Any]: ...
def first_slice_copy_with_grad(li: Iterable[Any]) -> list[Any]: ...
def diff_tensor_meta(meta1: TensorMetadata, meta2: TensorMetadata, check_grad=...) -> list[str]: ...
def validate_subgraph_args_types(lifted_args: tuple[Any, ...] | list[Any]): ...
def check_input_alias_and_mutation(
    gm: torch.fx.GraphModule, fake_args: list[FakeTensor]
) -> tuple[dict[int, int], dict[int, int], dict[int, int], list[int]]: ...
def check_input_alias_and_mutation_return_outputs(
    gm: torch.fx.GraphModule, fake_args: list[FakeTensor] | tuple[FakeTensor, ...]
) -> tuple[dict[int, int], dict[int, int], dict[int, int], list[int], tuple[Any, ...] | list[Any]]: ...

registered_hop_fake_fns: dict[torch._ops.OpOverload, Callable] = ...
F = TypeVar("F", bound=Callable)

@overload
def register_fake(hop, fn: None = ...) -> Callable[[F], F]: ...
@overload
def register_fake[F: Callable](hop, fn: F) -> F: ...
def register_fake(hop, fn=...): ...

class FunctionalizeCtxWrapper:
    @torch._disable_dynamo
    def __init__(self, ctx, subgraph) -> None: ...
    def __hash__(self) -> int: ...
    def __call__(self, *args, **kwargs): ...

class HopInstance:
    def __init__(self, op: HigherOrderOperator, schema: HopSchema) -> None: ...
    def __call__(self, *args, **kwargs): ...
    @staticmethod
    def create(hop: HigherOrderOperator, *args, **kwargs): ...

def call_op(op: OpOverload | HopInstance, args, kwargs): ...
def materialize_as_graph(
    fn: Callable,
    args: tuple[Any],
    include_key_set: torch._C.DispatchKeySet | None = ...,
    exclude_key_set: torch._C.DispatchKeySet | None = ...,
    force_enable_grad=...,
) -> torch.fx.GraphModule: ...
def materialize_callable_in_args(op: HopInstance, args, kwargs): ...
def has_user_subclass(args, allowed_subclasses): ...
def filter_with_masks(data: list[torch.Tensor | None], masks: list[bool]): ...
def fill_none_with_masks(data: list[torch.Tensor | None], masks: list[bool]): ...
