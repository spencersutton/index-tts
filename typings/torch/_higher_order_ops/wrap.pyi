import torch
from typing import Any, Optional
from torch._ops import HigherOrderOperator
from torch.fx import GraphModule
from torch.fx.experimental.proxy_tensor import ProxyTorchDispatchMode
from torch.types import _dtype

log = ...
uid = ...

class Wrap(HigherOrderOperator):
    def __init__(self) -> None: ...
    def __call__(self, func, *args, **kwargs):  # -> Any:
        ...

wrap = ...

class WrapWithSetGradEnabled(HigherOrderOperator):
    def __init__(self) -> None: ...
    def __call__(self, enable_grad, wrapped_func, *args, **kwargs):  # -> Any:
        ...

wrap_with_set_grad_enabled = ...

class WrapWithAutocast(HigherOrderOperator):
    def __init__(self) -> None: ...
    def __call__(
        self,
        device_type: str,
        dtype: Optional[_dtype],
        enabled: bool,
        cache_enabled: Optional[bool],
        wrapped_func,
        *args,
        **kwargs,
    ):  # -> Any:
        ...

wrap_with_autocast = ...

class DynamoBypassingWrapper(HigherOrderOperator):
    def __init__(self) -> None: ...
    def __call__(self, wrapper_fn_or_key, inner_fn, *args, **kwargs):  # -> Any:
        ...

dynamo_bypassing_wrapper = ...

class WrapActivationCheckpoint(HigherOrderOperator):
    def __init__(self) -> None: ...
    def __call__(self, function, *args, **kwargs):  # -> Any | None:
        ...

wrap_activation_checkpoint = ...

class TagActivationCheckpoint(HigherOrderOperator):
    def __init__(self) -> None: ...
    @staticmethod
    def divide_kwargs(kwargs):  # -> tuple[dict[Any, Any], dict[Any, Any]]:

        ...
    @staticmethod
    def tag_nodes(gmod, is_sac): ...
    def __call__(self, gmod, *args, **kwargs):  # -> Any | None:
        ...

tag_activation_checkpoint = ...

def tag_activation_checkpoint_impl(gmod, *args, **kwargs):  # -> Any | None:
    ...
@tag_activation_checkpoint.py_impl(ProxyTorchDispatchMode)
def proxy_mode_key(
    proxy_mode: ProxyTorchDispatchMode, gmod: GraphModule, *args: Any, **kwargs: Any
) -> tuple[torch.Tensor]: ...
