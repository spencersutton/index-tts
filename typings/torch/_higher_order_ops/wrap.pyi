from typing import Any

import torch
from torch._ops import HigherOrderOperator
from torch.fx import GraphModule
from torch.fx.experimental.proxy_tensor import ProxyTorchDispatchMode
from torch.types import _dtype

log = ...
uid = ...

class Wrap(HigherOrderOperator):
    def __init__(self) -> None: ...
    def __call__(self, func, *args, **kwargs): ...

wrap = ...

class WrapWithSetGradEnabled(HigherOrderOperator):
    def __init__(self) -> None: ...
    def __call__(self, enable_grad, wrapped_func, *args, **kwargs): ...

wrap_with_set_grad_enabled = ...

class WrapWithAutocast(HigherOrderOperator):
    def __init__(self) -> None: ...
    def __call__(
        self,
        device_type: str,
        dtype: _dtype | None,
        enabled: bool,
        cache_enabled: bool | None,
        wrapped_func,
        *args,
        **kwargs,
    ): ...

wrap_with_autocast = ...

class DynamoBypassingWrapper(HigherOrderOperator):
    def __init__(self) -> None: ...
    def __call__(self, wrapper_fn_or_key, inner_fn, *args, **kwargs): ...

dynamo_bypassing_wrapper = ...

class WrapActivationCheckpoint(HigherOrderOperator):
    def __init__(self) -> None: ...
    def __call__(self, function, *args, **kwargs): ...

wrap_activation_checkpoint = ...

class TagActivationCheckpoint(HigherOrderOperator):
    def __init__(self) -> None: ...
    @staticmethod
    def divide_kwargs(kwargs): ...
    @staticmethod
    def tag_nodes(gmod, is_sac): ...
    def __call__(self, gmod, *args, **kwargs): ...

tag_activation_checkpoint = ...

def tag_activation_checkpoint_impl(gmod, *args, **kwargs): ...
@tag_activation_checkpoint.py_impl(ProxyTorchDispatchMode)
def proxy_mode_key(
    proxy_mode: ProxyTorchDispatchMode, gmod: GraphModule, *args: Any, **kwargs: Any
) -> tuple[torch.Tensor]: ...
