import dataclasses
import functools
from collections.abc import Callable, Iterable, Sequence
from typing import IO, TYPE_CHECKING, Any, Optional, TypeAlias, Union

import torch
from torch._inductor import ir
from torch._inductor.select_algorithm import PartialRender, TritonTemplateCaller

if TYPE_CHECKING: ...
CUDA_VISIBLE_DEVICES = ...
autotuning_log = ...

class NonzeroWorkspaceNotSupportedError(Exception): ...

class TuningProcess:
    @staticmethod
    def process_main(read_pipe: IO[bytes], write_pipe: IO[bytes]) -> None: ...
    @staticmethod
    def send(obj: Any, write_pipe: IO[bytes]) -> None: ...
    @staticmethod
    def recv(read_pipe: IO[bytes]) -> Any: ...
    def __init__(self, device: int | None) -> None: ...
    def start(self):  # -> None:

        ...
    def alive(self) -> bool: ...
    def put(self, req: Any) -> None: ...
    def get(self, timeout: float = ...) -> Any: ...
    def shutdown(self, wait: bool = ...) -> None: ...
    def wait(self) -> None: ...
    def close(self) -> None: ...
    def kill(self) -> None: ...

class TuningProcessPool:
    def __init__(self) -> None: ...
    @staticmethod
    def get_device_list() -> Sequence[int | None]: ...
    def shutdown(self) -> None: ...
    def target(self, choice: TritonTemplateCaller) -> float: ...
    def benchmark(self, choices: list[TritonTemplateCaller]) -> dict[TritonTemplateCaller, float]: ...

type LayoutOrBuffer = ir.Layout | ir.Buffer

@dataclasses.dataclass
class TensorMeta:
    device: torch.device
    dtype: torch.dtype
    sizes: torch._prims_common.ShapeType
    strides: torch._prims_common.StrideType
    offset: int
    name: str | None = ...
    @classmethod
    def from_irnodes(cls, irnodes: LayoutOrBuffer | Sequence[LayoutOrBuffer]) -> TensorMeta | list[TensorMeta]: ...
    def to_tensor(self) -> torch.Tensor: ...

@dataclasses.dataclass
class BenchmarkRequest:
    def __init__(
        self,
        kernel_name: str,
        input_tensor_meta: TensorMeta | list[TensorMeta],
        output_tensor_meta: TensorMeta | list[TensorMeta],
        extra_args: Iterable[Any],
    ) -> None: ...
    def make_run_fn(self, *input_tensors: torch.Tensor, out: torch.Tensor) -> Callable[[], None]: ...
    def cleanup_run_fn(self) -> None: ...
    def do_bench(self, fn, *input_tensors: torch.Tensor, out: torch.Tensor | None = ...) -> float: ...
    def benchmark(self, *input_tensors: torch.Tensor, out: torch.Tensor | None = ...) -> float: ...

class _TestBenchmarkRequest(BenchmarkRequest):
    def __init__(
        self,
        result: float = ...,
        device: int | None = ...,
        sleep: float | None = ...,
        exc: Exception | None = ...,
        crash: bool = ...,
    ) -> None: ...
    def benchmark(self, *input_tensors: torch.Tensor, out: torch.Tensor | None = ...) -> float: ...

class GPUDeviceBenchmarkMixin:
    def do_bench(self, fn, *input_tensors: torch.Tensor, out: torch.Tensor | None = ...) -> float: ...

class CPUDeviceBenchmarkMixin:
    def do_bench(self, fn, *input_tensors: torch.Tensor, out: torch.Tensor | None = ...) -> float: ...

class TritonBenchmarkRequest(BenchmarkRequest):
    def __init__(
        self,
        kernel_name: str,
        input_tensor_meta: TensorMeta | list[TensorMeta],
        output_tensor_meta: TensorMeta | list[TensorMeta],
        extra_args: Iterable[Any],
        module_path: str,
        module_cache_key: str,
        num_stages: int,
        num_warps: int,
        num_consumer_groups: int = ...,
        num_buffers_warp_spec: int = ...,
        matrix_instr_nonkdim: int = ...,
        waves_per_eu: int = ...,
        kpack: int = ...,
    ) -> None: ...
    def make_run_fn(self, *input_tensors: torch.Tensor, out: torch.Tensor) -> Callable[[], None]: ...
    def precompile(self):  # -> None:
        ...

class TritonGPUBenchmarkRequest(GPUDeviceBenchmarkMixin, TritonBenchmarkRequest): ...
class TritonCPUBenchmarkRequest(CPUDeviceBenchmarkMixin, TritonBenchmarkRequest): ...

class CUDABenchmarkRequest(GPUDeviceBenchmarkMixin, BenchmarkRequest):
    def __init__(
        self,
        kernel_name: str,
        input_tensor_meta: TensorMeta | list[TensorMeta],
        output_tensor_meta: TensorMeta | list[TensorMeta],
        extra_args: Iterable[Any],
        source_code: str,
    ) -> None: ...
    def precompile(self):  # -> None:

        ...
    def make_run_fn(self, *input_tensors: torch.Tensor, out: torch.Tensor) -> Callable[[], None]: ...
    def update_workspace_size(self) -> None: ...
    def ensure_dll_loaded(self):  # -> None:
        ...
    def cleanup_run_fn(self) -> None: ...

class CppBenchmarkRequest(CPUDeviceBenchmarkMixin, BenchmarkRequest):
    def __init__(
        self,
        kernel_name: str,
        input_tensor_meta: TensorMeta | list[TensorMeta],
        output_tensor_meta: TensorMeta | list[TensorMeta],
        extra_args: Iterable[Any],
        source_code: str,
    ) -> None: ...
    def precompile(self):  # -> None:
        ...
    def make_run_fn(self, *input_tensors: torch.Tensor, out: torch.Tensor) -> Callable[[], None]: ...
    def cleanup_run_fn(self) -> None: ...

class CuteDSLBenchmarkRequest(GPUDeviceBenchmarkMixin, BenchmarkRequest):
    def __init__(
        self,
        kernel_name: str,
        input_tensor_meta: TensorMeta | list[TensorMeta],
        output_tensor_meta: TensorMeta | list[TensorMeta],
        extra_args: tuple[Any, ...],
        source_code: PartialRender,
    ) -> None: ...
    def make_run_fn(self, *input_tensors: torch.Tensor, out: torch.Tensor) -> Callable[[], None]: ...
    def cleanup_run_fn(self) -> None: ...

@functools.cache
def get_tuning_process_pool() -> TuningProcessPool: ...
def benchmark_in_sub_process(choices: list[TritonTemplateCaller]) -> dict[TritonTemplateCaller, float]: ...
