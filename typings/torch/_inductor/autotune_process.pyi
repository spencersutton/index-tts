import dataclasses
import functools
import torch
from collections.abc import Iterable, Sequence
from typing import Any, Callable, IO, Optional, TYPE_CHECKING, Union, TypeAlias
from torch._inductor import ir
from torch._inductor.select_algorithm import PartialRender, TritonTemplateCaller

if TYPE_CHECKING: ...
CUDA_VISIBLE_DEVICES = ...
autotuning_log = ...

class NonzeroWorkspaceNotSupportedError(Exception): ...

class TuningProcess:
    @staticmethod
    def process_main(read_pipe: IO[bytes], write_pipe: IO[bytes]) -> None: ...
    @staticmethod
    def send(obj: Any, write_pipe: IO[bytes]) -> None: ...
    @staticmethod
    def recv(read_pipe: IO[bytes]) -> Any: ...
    def __init__(self, device: Optional[int]) -> None: ...
    def start(self):  # -> None:

        ...
    def alive(self) -> bool: ...
    def put(self, req: Any) -> None: ...
    def get(self, timeout: float = ...) -> Any: ...
    def shutdown(self, wait: bool = ...) -> None: ...
    def wait(self) -> None: ...
    def close(self) -> None: ...
    def kill(self) -> None: ...

class TuningProcessPool:
    def __init__(self) -> None: ...
    @staticmethod
    def get_device_list() -> Sequence[Optional[int]]: ...
    def shutdown(self) -> None: ...
    def target(self, choice: TritonTemplateCaller) -> float: ...
    def benchmark(self, choices: list[TritonTemplateCaller]) -> dict[TritonTemplateCaller, float]: ...

LayoutOrBuffer: TypeAlias = Union[ir.Layout, ir.Buffer]

@dataclasses.dataclass
class TensorMeta:
    device: torch.device
    dtype: torch.dtype
    sizes: torch._prims_common.ShapeType
    strides: torch._prims_common.StrideType
    offset: int
    name: Optional[str] = ...
    @classmethod
    def from_irnodes(
        cls, irnodes: Union[LayoutOrBuffer, Sequence[LayoutOrBuffer]]
    ) -> Union[TensorMeta, list[TensorMeta]]: ...
    def to_tensor(self) -> torch.Tensor: ...

@dataclasses.dataclass
class BenchmarkRequest:
    def __init__(
        self,
        kernel_name: str,
        input_tensor_meta: Union[TensorMeta, list[TensorMeta]],
        output_tensor_meta: Union[TensorMeta, list[TensorMeta]],
        extra_args: Iterable[Any],
    ) -> None: ...
    def make_run_fn(self, *input_tensors: torch.Tensor, out: torch.Tensor) -> Callable[[], None]: ...
    def cleanup_run_fn(self) -> None: ...
    def do_bench(self, fn, *input_tensors: torch.Tensor, out: Optional[torch.Tensor] = ...) -> float: ...
    def benchmark(self, *input_tensors: torch.Tensor, out: Optional[torch.Tensor] = ...) -> float: ...

class _TestBenchmarkRequest(BenchmarkRequest):
    def __init__(
        self,
        result: float = ...,
        device: Optional[int] = ...,
        sleep: Optional[float] = ...,
        exc: Optional[Exception] = ...,
        crash: bool = ...,
    ) -> None: ...
    def benchmark(self, *input_tensors: torch.Tensor, out: Optional[torch.Tensor] = ...) -> float: ...

class GPUDeviceBenchmarkMixin:
    def do_bench(self, fn, *input_tensors: torch.Tensor, out: Optional[torch.Tensor] = ...) -> float: ...

class CPUDeviceBenchmarkMixin:
    def do_bench(self, fn, *input_tensors: torch.Tensor, out: Optional[torch.Tensor] = ...) -> float: ...

class TritonBenchmarkRequest(BenchmarkRequest):
    def __init__(
        self,
        kernel_name: str,
        input_tensor_meta: Union[TensorMeta, list[TensorMeta]],
        output_tensor_meta: Union[TensorMeta, list[TensorMeta]],
        extra_args: Iterable[Any],
        module_path: str,
        module_cache_key: str,
        num_stages: int,
        num_warps: int,
        num_consumer_groups: int = ...,
        num_buffers_warp_spec: int = ...,
        matrix_instr_nonkdim: int = ...,
        waves_per_eu: int = ...,
        kpack: int = ...,
    ) -> None: ...
    def make_run_fn(self, *input_tensors: torch.Tensor, out: torch.Tensor) -> Callable[[], None]: ...
    def precompile(self):  # -> None:
        ...

class TritonGPUBenchmarkRequest(GPUDeviceBenchmarkMixin, TritonBenchmarkRequest): ...
class TritonCPUBenchmarkRequest(CPUDeviceBenchmarkMixin, TritonBenchmarkRequest): ...

class CUDABenchmarkRequest(GPUDeviceBenchmarkMixin, BenchmarkRequest):
    def __init__(
        self,
        kernel_name: str,
        input_tensor_meta: Union[TensorMeta, list[TensorMeta]],
        output_tensor_meta: Union[TensorMeta, list[TensorMeta]],
        extra_args: Iterable[Any],
        source_code: str,
    ) -> None: ...
    def precompile(self):  # -> None:

        ...
    def make_run_fn(self, *input_tensors: torch.Tensor, out: torch.Tensor) -> Callable[[], None]: ...
    def update_workspace_size(self) -> None: ...
    def ensure_dll_loaded(self):  # -> None:
        ...
    def cleanup_run_fn(self) -> None: ...

class CppBenchmarkRequest(CPUDeviceBenchmarkMixin, BenchmarkRequest):
    def __init__(
        self,
        kernel_name: str,
        input_tensor_meta: Union[TensorMeta, list[TensorMeta]],
        output_tensor_meta: Union[TensorMeta, list[TensorMeta]],
        extra_args: Iterable[Any],
        source_code: str,
    ) -> None: ...
    def precompile(self):  # -> None:
        ...
    def make_run_fn(self, *input_tensors: torch.Tensor, out: torch.Tensor) -> Callable[[], None]: ...
    def cleanup_run_fn(self) -> None: ...

class CuteDSLBenchmarkRequest(GPUDeviceBenchmarkMixin, BenchmarkRequest):
    def __init__(
        self,
        kernel_name: str,
        input_tensor_meta: Union[TensorMeta, list[TensorMeta]],
        output_tensor_meta: Union[TensorMeta, list[TensorMeta]],
        extra_args: tuple[Any, ...],
        source_code: PartialRender,
    ) -> None: ...
    def make_run_fn(self, *input_tensors: torch.Tensor, out: torch.Tensor) -> Callable[[], None]: ...
    def cleanup_run_fn(self) -> None: ...

@functools.cache
def get_tuning_process_pool() -> TuningProcessPool: ...
def benchmark_in_sub_process(choices: list[TritonTemplateCaller]) -> dict[TritonTemplateCaller, float]: ...
