import dataclasses
import functools
import hashlib
import pickle
from collections.abc import Callable, Generator, KeysView, Sequence
from concurrent.futures import Future
from ctypes import CDLL, c_void_p
from functools import lru_cache
from pathlib import Path
from tempfile import _TemporaryFileWrapper
from types import ModuleType
from typing import Any, Self, TypeVar, override

import torch
from torch import Tensor
from torch._inductor import config
from torch._inductor.utils import clear_on_fresh_cache
from torch._subclasses.fake_tensor import TensorMetadata
from torch.compiler._cache import CacheArtifact, CacheArtifactFactory
from torch.export.pt2_archive._package_weights import Weights

from .compile_fx import _CompileFxKwargs
from .graph import GraphLowering
from .ir import ChoiceCaller
from .output_code import CompiledFxGraph, CompiledFxGraphConstants
from .remote_cache import JsonDataTy, RemoteCache
from .runtime.hints import HalideMeta
from .runtime.triton_heuristics import CachingAutotuner
from .utils import InputType

if config.is_fbcode(): ...
T = TypeVar("T")

_IS_WINDOWS = ...
LOCK_TIMEOUT = ...
output_code_log = ...
autotuning_log = ...
log = ...

def use_re_build() -> bool: ...
def get_cpp_wrapper_cubin_path_name() -> str: ...
def get_kernel_bin_format(device: str) -> str: ...

class CacheBase:
    @staticmethod
    @functools.cache
    def get_system() -> dict[str, Any]: ...
    @staticmethod
    @clear_on_fresh_cache
    @functools.cache
    def get_local_cache_path() -> Path: ...
    def __init__(self) -> None: ...
    def get_local_cache(self) -> dict[str, Any]: ...
    def update_local_cache(self, local_cache: dict[str, Any]) -> None: ...

class LocalCache(CacheBase):
    def lookup(self, *keys: str) -> dict[str, Any] | None: ...
    def set_value(self, *keys: str, value: Any) -> None: ...

class PersistentCache(CacheBase):
    def lookup(
        self,
        choices: list[ChoiceCaller],
        op: str,
        inputs: str,
        benchmark: Callable[[Any], dict[ChoiceCaller, float]] | None,
        hint_override: int | None = ...,
    ) -> dict[ChoiceCaller, float]: ...

def get_lock_dir() -> str: ...
def sha256_hash(data: bytes) -> str: ...
def code_hash(code: str | bytes, extra: str | bytes = ...) -> str: ...
def get_path(basename: str, extension: str, specified_dir: str = ...) -> tuple[str, str, str]: ...
def get_hash(content: str | bytes, extra: str = ..., hash_type: str = ...) -> str: ...

class WritableTempFile:
    def __init__(self, mode: str = ..., *, encoding: Any = ..., suffix: Any = ...) -> None: ...
    def __enter__(self) -> _TemporaryFileWrapper[Any]: ...
    def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None: ...

def write(
    content: str | bytes,
    extension: str,
    extra: str = ...,
    hash_type: str = ...,
    specified_dir: str = ...,
    key: str | None = ...,
) -> tuple[str, str]: ...
def write_text(text: str) -> str: ...
def write_atomic(path_: str, content: str | bytes, make_dirs: bool = ..., encode_utf_8: bool = ...) -> None: ...

@dataclasses.dataclass
class TensorMetadataAndValues:
    tensor_metadata: TensorMetadata
    values: list[Any]

def extract_tensor_metadata_for_cache_key(t: Tensor) -> TensorMetadata: ...

class FxGraphCachePickler(pickle.Pickler):
    def __init__(self, gm: torch.fx.GraphModule, has_user_defined_triton_kernels: bool = ...) -> None: ...
    def dumps(self, obj: Any) -> bytes: ...
    def get_hash(self, obj: Any) -> str: ...
    def debug_lines(self, inp: FxGraphHashDetails) -> list[str]: ...

def build_code_hash(roots: list[str] | None, prefix: str, hasher: hashlib._Hash) -> None: ...
def torch_key_cache(func: Callable[[], bytes]) -> Callable[[], bytes]: ...
@torch_key_cache
def torch_key() -> bytes: ...
def get_inductor_root() -> str: ...

@dataclasses.dataclass
class OrderedSetHolder:
    items: list[Any]

class BypassFxGraphCache(Exception): ...

class FxGraphHashDetails:
    EXCLUDED_KWARGS = ...
    def __init__(
        self,
        gm: torch.fx.GraphModule,
        example_inputs: Sequence[InputType],
        fx_kwargs: _CompileFxKwargs,
        inputs_to_check: Sequence[int],
    ) -> None: ...

def compiled_fx_graph_hash(
    gm: torch.fx.GraphModule,
    example_inputs: Sequence[InputType],
    fx_kwargs: _CompileFxKwargs,
    inputs_to_check: Sequence[int],
) -> tuple[str, list[str]]: ...
def add_ephemeral_timeout_increase_for_distributed(time_saved_ns: int) -> int: ...

class GuardedCache[T]:
    @classmethod
    def iterate_over_candidates(
        cls: type[GuardedCache[T]], local: bool, remote_cache: RemoteCache[JsonDataTy] | None, key: str
    ) -> Generator[tuple[T, bytes]]: ...
    @classmethod
    def find_guarded_entry(
        cls: type[GuardedCache[T]],
        key: str,
        local: bool,
        remote_cache: RemoteCache[JsonDataTy] | None,
        evaluate_guards: Callable[[str, list[int] | list[torch.SymInt]], bool],
        hints: list[int],
    ) -> tuple[T | None, bytes | None, dict[str, str]]: ...

@CacheArtifactFactory.register
class InductorCacheArtifact(CacheArtifact):
    @override
    def populate_cache(self) -> None: ...
    @override
    @staticmethod
    def type() -> str: ...

class FxGraphCache(GuardedCache[CompiledFxGraph]):
    @staticmethod
    def cache_hit_post_compile(
        graph: CompiledFxGraph, cache_info: dict[str, Any], constants: CompiledFxGraphConstants
    ) -> tuple[CompiledFxGraph | None, dict[str, Any]]: ...
    @staticmethod
    def prepare_key(
        gm: torch.fx.GraphModule,
        example_inputs: Sequence[InputType],
        fx_kwargs: _CompileFxKwargs,
        inputs_to_check: Sequence[int],
        remote: bool,
    ) -> tuple[tuple[str, list[str]] | None, dict[str, Any]]: ...
    @staticmethod
    def get_remote_cache() -> RemoteCache[JsonDataTy] | None: ...
    @staticmethod
    def load_with_key(
        key: str,
        debug_lines: list[str],
        example_inputs: Sequence[InputType],
        local: bool,
        remote_cache: RemoteCache[JsonDataTy] | None,
        is_backward: bool,
        constants: CompiledFxGraphConstants,
        evaluate_guards: Callable[[str, list[int] | list[torch.SymInt]], bool] | None = ...,
    ) -> tuple[CompiledFxGraph | None, dict[str, Any]]: ...
    @staticmethod
    def clear() -> None: ...

@functools.cache
def split_aot_inductor_output_path(path: str) -> tuple[str, str]: ...

@clear_on_fresh_cache
class CudaKernelParamCache:
    cache: dict[str, dict[str, Any]] = ...
    cache_clear = ...
    @classmethod
    def set(
        cls,
        key: str,
        params: dict[str, str | None],
        cubin: str,
        bin_type: str,
        asm: str | None = ...,
        asm_type: str | None = ...,
    ) -> None: ...
    @classmethod
    def get(cls, key: str) -> dict[str, Any] | None: ...
    @classmethod
    def get_keys(cls) -> KeysView[str]: ...

class AotCodeCompiler:
    @classmethod
    def compile(
        cls,
        graph: GraphLowering,
        wrapper_code: str,
        kernel_code: str,
        serialized_extern_kernel_nodes: str | None,
        *,
        device_type: str,
        additional_files: list[str],
    ) -> list[str | Weights] | str: ...

_libgomp: CDLL | None = ...

def custom_op_wrapper(op: str, *args: Any) -> list[c_void_p] | c_void_p | None: ...

_HEADER_DIR = ...
_HEADER_LOCK_DIR = ...

@clear_on_fresh_cache
class CppCodeCache:
    cache: dict[str, Callable[[], CDLL | ModuleType]] = ...
    cache_clear = ...
    cpp_compile_command_flags: dict[str, Any] = ...
    @classmethod
    def load_async(
        cls,
        main_code: str,
        device_type: str = ...,
        submit_fn: Any = ...,
        extra_flags: Sequence[str] = ...,
        optimized_code: str | None = ...,
    ) -> Any: ...
    @classmethod
    def load(cls, *args: Any, **kwargs: Any) -> Any: ...

@clear_on_fresh_cache
class CppPythonBindingsCodeCache(CppCodeCache):
    cache: dict[str, Callable[[], CDLL | ModuleType]] = ...
    cache_clear = ...
    cpp_compile_command_flags = ...
    entry_function = ...
    call_entry_function = ...
    extra_parse_arg = ...
    suffix_template = ...
    @classmethod
    def load_pybinding_async(
        cls,
        argtypes: Sequence[str],
        main_code: str,
        device_type: str = ...,
        num_outputs: int = ...,
        submit_fn: Any = ...,
        extra_flags: Sequence[str] = ...,
        kernel_code: str | None = ...,
    ) -> Any: ...
    @classmethod
    def load_pybinding(cls, *args: Any, **kwargs: Any) -> Any: ...

@clear_on_fresh_cache
class CppWrapperCodeCache(CppPythonBindingsCodeCache):
    cache: dict[str, Callable[[], CDLL | ModuleType]] = ...
    cache_clear = ...
    cpp_compile_command_flags = ...
    entry_function = ...
    call_entry_function = ...
    extra_parse_arg = ...

@clear_on_fresh_cache
class HalideCodeCache(CppPythonBindingsCodeCache):
    cache: dict[str, Callable[[], ModuleType | CDLL]] = ...
    cache_clear = ...
    _standalone_runtime_path: str | None = ...
    prefix = ...
    glue_template_cpp = ...
    glue_template_cuda = ...
    standalone_runtime_cuda_init = ...
    @classmethod
    @functools.cache
    def config_hash(cls) -> str: ...
    @staticmethod
    @functools.cache
    def find_libautoschedule(name: str) -> str: ...
    @staticmethod
    @functools.cache
    def find_header(name: str) -> str: ...
    @classmethod
    def generate_halide_async(cls, meta: HalideMeta, source_code: str, submit_fn: Any = ...) -> Callable[[], Any]: ...
    @classmethod
    def generate_halide(cls, *args: Any, **kwargs: Any) -> Callable[[], Any]: ...
    @classmethod
    def build_standalone_runtime(cls) -> str: ...

def touch(filename: str) -> None: ...

@clear_on_fresh_cache
class PyCodeCache:
    modules: list[ModuleType] = ...
    modules_no_attr: dict[str, ModuleType] = ...
    linemaps: dict[str, list[tuple[Any, ...]]] = ...
    @classmethod
    def write(cls, source_code: str, extra: str = ...) -> tuple[str, str]: ...
    @classmethod
    def load(cls, source_code: str, extra: str = ...) -> ModuleType: ...
    @classmethod
    def load_by_key_path(
        cls, key: str, path: str, linemap: list[tuple[int, str]] | None = ..., attrs: dict[str, Any] | None = ...
    ) -> ModuleType: ...
    @classmethod
    def cache_clear(cls, purge: bool = ...) -> None: ...
    @classmethod
    @functools.cache
    def stack_frames_for_code(cls, path: str, lineno: int) -> list[dict[str, Any]] | None: ...

@torch_key_cache
def cutlass_key() -> bytes: ...
def cuda_compile_command(
    src_files: list[str], dst_file: str, dst_file_ext: str, extra_args: list[str] | None = ...
) -> str: ...

class DLLWrapper:
    def __init__(self, lib_path: str) -> None: ...
    def close(self) -> None: ...
    def __getattr__(self, name: str) -> Callable[..., None]: ...
    def __enter__(self) -> Self: ...
    def __exit__(self, *args: object) -> None: ...
    def __del__(self) -> None: ...

@lru_cache
def binary_error_path(output_path: str) -> str: ...

@clear_on_fresh_cache
class CUDACodeCache:
    @dataclasses.dataclass
    class CacheEntry:
        input_path: str
        output_path: str
        error_json: str | None = ...

    cache: dict[str, CacheEntry] = ...
    aot_kernels_o: list[str] = ...
    _SOURCE_CODE_SUFFIX = ...
    @staticmethod
    def cache_clear() -> None: ...
    @staticmethod
    @lru_cache(maxsize=4)
    def get_kernel_binary_remote_cache(caching_enabled: bool, caching_available: bool) -> Any | None: ...
    @classmethod
    @lru_cache(None)
    def write(cls, source_code: str, dst_file_ext: str) -> tuple[str, str]: ...
    @classmethod
    def compile(
        cls, source_code: str, dst_file_ext: str, extra_args: list[str] | None = ...
    ) -> tuple[str, str, str]: ...
    @classmethod
    def load(cls, source_code: str, dst_file_ext: str) -> tuple[DLLWrapper, str, str]: ...

@clear_on_fresh_cache
class ROCmCodeCache:
    @dataclasses.dataclass
    class CacheEntry:
        input_path: str
        output_path: str

    cache: dict[str, CacheEntry] = ...
    aot_kernels_o: list[str] = ...
    _SOURCE_CODE_SUFFIX = ...
    _logged_compiler_version = ...
    @staticmethod
    def cache_clear() -> None: ...
    @classmethod
    def write(cls, source_code: str, dst_file_ext: str) -> tuple[str, str]: ...
    @classmethod
    def compile(
        cls, source_code: str, dst_file_ext: str, extra_args: list[str] | None = ...
    ) -> tuple[str, str, str]: ...
    @classmethod
    def load(cls, source_code: str, dst_file_ext: str) -> tuple[DLLWrapper, str, str]: ...

class CodeCacheFuture:
    def result(self) -> Callable[..., Any]: ...

class LambdaFuture(CodeCacheFuture):
    def __init__(self, result_fn: Callable[..., Any], future: Future[Any] | None = ...) -> None: ...
    def result(self) -> Callable[..., Any]: ...

class StaticAutotunerFuture(CodeCacheFuture):
    def __init__(self, static_autotuner: CachingAutotuner) -> None: ...
    def result(self) -> CachingAutotuner: ...
