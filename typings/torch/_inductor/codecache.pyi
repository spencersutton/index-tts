import dataclasses
import functools
import hashlib
import pickle
import torch
from ctypes import CDLL, c_void_p
from functools import lru_cache
from pathlib import Path
from tempfile import _TemporaryFileWrapper
from types import ModuleType
from typing import Any, Callable, Generic, Optional, TYPE_CHECKING, TypeVar, Union
from typing_extensions import Self, override
from torch import SymInt, Tensor
from torch._inductor import config
from torch._inductor.utils import clear_on_fresh_cache
from torch._subclasses.fake_tensor import TensorMetadata
from torch.compiler._cache import CacheArtifact, CacheArtifactFactory
from torch.export.pt2_archive._package_weights import Weights
from .output_code import CompiledFxGraph, CompiledFxGraphConstants
from .remote_cache import JsonDataTy, RemoteCache
from collections.abc import Generator, KeysView, Sequence
from concurrent.futures import Future
from .compile_fx import _CompileFxKwargs
from .graph import GraphLowering
from .ir import ChoiceCaller
from .runtime.hints import HalideMeta
from .runtime.triton_heuristics import CachingAutotuner
from .utils import InputType

if config.is_fbcode(): ...
T = TypeVar("T")
if TYPE_CHECKING: ...
_IS_WINDOWS = ...
LOCK_TIMEOUT = ...
output_code_log = ...
autotuning_log = ...
log = ...

def use_re_build() -> bool: ...
def get_cpp_wrapper_cubin_path_name() -> str: ...
def get_kernel_bin_format(device: str) -> str: ...

class CacheBase:
    @staticmethod
    @functools.cache
    def get_system() -> dict[str, Any]: ...
    @staticmethod
    @clear_on_fresh_cache
    @functools.cache
    def get_local_cache_path() -> Path: ...
    def __init__(self) -> None: ...
    def get_local_cache(self) -> dict[str, Any]: ...
    def update_local_cache(self, local_cache: dict[str, Any]) -> None: ...

class LocalCache(CacheBase):
    def lookup(self, *keys: str) -> Optional[dict[str, Any]]: ...
    def set_value(self, *keys: str, value: Any) -> None: ...

class PersistentCache(CacheBase):
    def lookup(
        self,
        choices: list[ChoiceCaller],
        op: str,
        inputs: str,
        benchmark: Optional[Callable[[Any], dict[ChoiceCaller, float]]],
        hint_override: Optional[int] = ...,
    ) -> dict[ChoiceCaller, float]: ...

def get_lock_dir() -> str: ...
def sha256_hash(data: bytes) -> str: ...
def code_hash(code: Union[str, bytes], extra: Union[str, bytes] = ...) -> str: ...
def get_path(basename: str, extension: str, specified_dir: str = ...) -> tuple[str, str, str]: ...
def get_hash(content: Union[str, bytes], extra: str = ..., hash_type: str = ...) -> str: ...

class WritableTempFile:
    def __init__(self, mode: str = ..., *, encoding: Any = ..., suffix: Any = ...) -> None: ...
    def __enter__(self) -> _TemporaryFileWrapper[Any]: ...
    def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None: ...

def write(
    content: Union[str, bytes],
    extension: str,
    extra: str = ...,
    hash_type: str = ...,
    specified_dir: str = ...,
    key: Optional[str] = ...,
) -> tuple[str, str]: ...
def write_text(text: str) -> str: ...
def write_atomic(path_: str, content: Union[str, bytes], make_dirs: bool = ..., encode_utf_8: bool = ...) -> None: ...

@dataclasses.dataclass
class TensorMetadataAndValues:
    tensor_metadata: TensorMetadata
    values: list[Any]

def extract_tensor_metadata_for_cache_key(t: Tensor) -> TensorMetadata: ...

class FxGraphCachePickler(pickle.Pickler):
    def __init__(self, gm: torch.fx.GraphModule, has_user_defined_triton_kernels: bool = ...) -> None: ...
    def dumps(self, obj: Any) -> bytes: ...
    def get_hash(self, obj: Any) -> str: ...
    def debug_lines(self, inp: FxGraphHashDetails) -> list[str]: ...

def build_code_hash(roots: list[str] | None, prefix: str, hasher: hashlib._Hash) -> None: ...
def torch_key_cache(func: Callable[[], bytes]) -> Callable[[], bytes]: ...
@torch_key_cache
def torch_key() -> bytes: ...
def get_inductor_root() -> str: ...

@dataclasses.dataclass
class OrderedSetHolder:
    items: list[Any]

class BypassFxGraphCache(Exception): ...

class FxGraphHashDetails:
    EXCLUDED_KWARGS = ...
    def __init__(
        self,
        gm: torch.fx.GraphModule,
        example_inputs: Sequence[InputType],
        fx_kwargs: _CompileFxKwargs,
        inputs_to_check: Sequence[int],
    ) -> None: ...

def compiled_fx_graph_hash(
    gm: torch.fx.GraphModule,
    example_inputs: Sequence[InputType],
    fx_kwargs: _CompileFxKwargs,
    inputs_to_check: Sequence[int],
) -> tuple[str, list[str]]: ...
def add_ephemeral_timeout_increase_for_distributed(time_saved_ns: int) -> int: ...

class GuardedCache(Generic[T]):
    @classmethod
    def iterate_over_candidates(
        cls: type[GuardedCache[T]], local: bool, remote_cache: Optional[RemoteCache[JsonDataTy]], key: str
    ) -> Generator[tuple[T, bytes], None, None]: ...
    @classmethod
    def find_guarded_entry(
        cls: type[GuardedCache[T]],
        key: str,
        local: bool,
        remote_cache: Optional[RemoteCache[JsonDataTy]],
        evaluate_guards: Callable[[str, Union[list[int], list[torch.SymInt]]], bool],
        hints: list[int],
    ) -> tuple[Optional[T], Optional[bytes], dict[str, str]]: ...

@CacheArtifactFactory.register
class InductorCacheArtifact(CacheArtifact):
    @override
    def populate_cache(self) -> None: ...
    @override
    @staticmethod
    def type() -> str: ...

class FxGraphCache(GuardedCache[CompiledFxGraph]):
    @staticmethod
    def cache_hit_post_compile(
        graph: CompiledFxGraph, cache_info: dict[str, Any], constants: CompiledFxGraphConstants
    ) -> tuple[Optional[CompiledFxGraph], dict[str, Any]]: ...
    @staticmethod
    def prepare_key(
        gm: torch.fx.GraphModule,
        example_inputs: Sequence[InputType],
        fx_kwargs: _CompileFxKwargs,
        inputs_to_check: Sequence[int],
        remote: bool,
    ) -> tuple[Optional[tuple[str, list[str]]], dict[str, Any]]: ...
    @staticmethod
    def get_remote_cache() -> Optional[RemoteCache[JsonDataTy]]: ...
    @staticmethod
    def load_with_key(
        key: str,
        debug_lines: list[str],
        example_inputs: Sequence[InputType],
        local: bool,
        remote_cache: Optional[RemoteCache[JsonDataTy]],
        is_backward: bool,
        constants: CompiledFxGraphConstants,
        evaluate_guards: Optional[Callable[[str, Union[list[int], list[torch.SymInt]]], bool]] = ...,
    ) -> tuple[Optional[CompiledFxGraph], dict[str, Any]]: ...
    @staticmethod
    def clear() -> None: ...

@functools.cache
def split_aot_inductor_output_path(path: str) -> tuple[str, str]: ...

@clear_on_fresh_cache
class CudaKernelParamCache:
    cache: dict[str, dict[str, Any]] = ...
    cache_clear = ...
    @classmethod
    def set(
        cls,
        key: str,
        params: dict[str, Optional[str]],
        cubin: str,
        bin_type: str,
        asm: Optional[str] = ...,
        asm_type: Optional[str] = ...,
    ) -> None: ...
    @classmethod
    def get(cls, key: str) -> Optional[dict[str, Any]]: ...
    @classmethod
    def get_keys(cls) -> KeysView[str]: ...

class AotCodeCompiler:
    @classmethod
    def compile(
        cls,
        graph: GraphLowering,
        wrapper_code: str,
        kernel_code: str,
        serialized_extern_kernel_nodes: Optional[str],
        *,
        device_type: str,
        additional_files: list[str],
    ) -> Union[list[Union[str, Weights]], str]: ...

_libgomp: Optional[CDLL] = ...

def custom_op_wrapper(op: str, *args: Any) -> Union[list[c_void_p], c_void_p, None]: ...

_HEADER_DIR = ...
_HEADER_LOCK_DIR = ...

@clear_on_fresh_cache
class CppCodeCache:
    cache: dict[str, Callable[[], Union[CDLL, ModuleType]]] = ...
    cache_clear = ...
    cpp_compile_command_flags: dict[str, Any] = ...
    @classmethod
    def load_async(
        cls,
        main_code: str,
        device_type: str = ...,
        submit_fn: Any = ...,
        extra_flags: Sequence[str] = ...,
        optimized_code: Optional[str] = ...,
    ) -> Any: ...
    @classmethod
    def load(cls, *args: Any, **kwargs: Any) -> Any: ...

@clear_on_fresh_cache
class CppPythonBindingsCodeCache(CppCodeCache):
    cache: dict[str, Callable[[], Union[CDLL, ModuleType]]] = ...
    cache_clear = ...
    cpp_compile_command_flags = ...
    entry_function = ...
    call_entry_function = ...
    extra_parse_arg = ...
    suffix_template = ...
    @classmethod
    def load_pybinding_async(
        cls,
        argtypes: Sequence[str],
        main_code: str,
        device_type: str = ...,
        num_outputs: int = ...,
        submit_fn: Any = ...,
        extra_flags: Sequence[str] = ...,
        kernel_code: Optional[str] = ...,
    ) -> Any: ...
    @classmethod
    def load_pybinding(cls, *args: Any, **kwargs: Any) -> Any: ...

@clear_on_fresh_cache
class CppWrapperCodeCache(CppPythonBindingsCodeCache):
    cache: dict[str, Callable[[], Union[CDLL, ModuleType]]] = ...
    cache_clear = ...
    cpp_compile_command_flags = ...
    entry_function = ...
    call_entry_function = ...
    extra_parse_arg = ...

@clear_on_fresh_cache
class HalideCodeCache(CppPythonBindingsCodeCache):
    cache: dict[str, Callable[[], Union[ModuleType, CDLL]]] = ...
    cache_clear = ...
    _standalone_runtime_path: Optional[str] = ...
    prefix = ...
    glue_template_cpp = ...
    glue_template_cuda = ...
    standalone_runtime_cuda_init = ...
    @classmethod
    @functools.cache
    def config_hash(cls) -> str: ...
    @staticmethod
    @functools.cache
    def find_libautoschedule(name: str) -> str: ...
    @staticmethod
    @functools.cache
    def find_header(name: str) -> str: ...
    @classmethod
    def generate_halide_async(cls, meta: HalideMeta, source_code: str, submit_fn: Any = ...) -> Callable[[], Any]: ...
    @classmethod
    def generate_halide(cls, *args: Any, **kwargs: Any) -> Callable[[], Any]: ...
    @classmethod
    def build_standalone_runtime(cls) -> str: ...

def touch(filename: str) -> None: ...

@clear_on_fresh_cache
class PyCodeCache:
    modules: list[ModuleType] = ...
    modules_no_attr: dict[str, ModuleType] = ...
    linemaps: dict[str, list[tuple[Any, ...]]] = ...
    @classmethod
    def write(cls, source_code: str, extra: str = ...) -> tuple[str, str]: ...
    @classmethod
    def load(cls, source_code: str, extra: str = ...) -> ModuleType: ...
    @classmethod
    def load_by_key_path(
        cls, key: str, path: str, linemap: Optional[list[tuple[int, str]]] = ..., attrs: Optional[dict[str, Any]] = ...
    ) -> ModuleType: ...
    @classmethod
    def cache_clear(cls, purge: bool = ...) -> None: ...
    @classmethod
    @functools.cache
    def stack_frames_for_code(cls, path: str, lineno: int) -> Optional[list[dict[str, Any]]]: ...

@torch_key_cache
def cutlass_key() -> bytes: ...
def cuda_compile_command(
    src_files: list[str], dst_file: str, dst_file_ext: str, extra_args: Optional[list[str]] = ...
) -> str: ...

class DLLWrapper:
    def __init__(self, lib_path: str) -> None: ...
    def close(self) -> None: ...
    def __getattr__(self, name: str) -> Callable[..., None]: ...
    def __enter__(self) -> Self: ...
    def __exit__(self, *args: object) -> None: ...
    def __del__(self) -> None: ...

@lru_cache
def binary_error_path(output_path: str) -> str: ...

@clear_on_fresh_cache
class CUDACodeCache:
    @dataclasses.dataclass
    class CacheEntry:
        input_path: str
        output_path: str
        error_json: Optional[str] = ...

    cache: dict[str, CacheEntry] = ...
    aot_kernels_o: list[str] = ...
    _SOURCE_CODE_SUFFIX = ...
    @staticmethod
    def cache_clear() -> None: ...
    @staticmethod
    @lru_cache(maxsize=4)
    def get_kernel_binary_remote_cache(caching_enabled: bool, caching_available: bool) -> Optional[Any]: ...
    @classmethod
    @lru_cache(None)
    def write(cls, source_code: str, dst_file_ext: str) -> tuple[str, str]: ...
    @classmethod
    def compile(
        cls, source_code: str, dst_file_ext: str, extra_args: Optional[list[str]] = ...
    ) -> tuple[str, str, str]: ...
    @classmethod
    def load(cls, source_code: str, dst_file_ext: str) -> tuple[DLLWrapper, str, str]: ...

@clear_on_fresh_cache
class ROCmCodeCache:
    @dataclasses.dataclass
    class CacheEntry:
        input_path: str
        output_path: str

    cache: dict[str, CacheEntry] = ...
    aot_kernels_o: list[str] = ...
    _SOURCE_CODE_SUFFIX = ...
    _logged_compiler_version = ...
    @staticmethod
    def cache_clear() -> None: ...
    @classmethod
    def write(cls, source_code: str, dst_file_ext: str) -> tuple[str, str]: ...
    @classmethod
    def compile(
        cls, source_code: str, dst_file_ext: str, extra_args: Optional[list[str]] = ...
    ) -> tuple[str, str, str]: ...
    @classmethod
    def load(cls, source_code: str, dst_file_ext: str) -> tuple[DLLWrapper, str, str]: ...

class CodeCacheFuture:
    def result(self) -> Callable[..., Any]: ...

class LambdaFuture(CodeCacheFuture):
    def __init__(self, result_fn: Callable[..., Any], future: Optional[Future[Any]] = ...) -> None: ...
    def result(self) -> Callable[..., Any]: ...

class StaticAutotunerFuture(CodeCacheFuture):
    def __init__(self, static_autotuner: CachingAutotuner) -> None: ...
    def result(self) -> CachingAutotuner: ...
