import contextlib
import dataclasses
import enum
import functools
import itertools
from abc import ABC, abstractmethod
from collections.abc import Callable, Iterator, MutableMapping, Sequence
from enum import Enum
from typing import Any, ClassVar, NamedTuple, Self, TypeVar

import sympy
import torch
import torch.fx
from torch._prims_common import ELEMENTWISE_TYPE_PROMOTION_KIND
from torch.fx import GraphModule
from torch.utils._config_module import ConfigModule
from torch.utils._ordered_set import OrderedSet
from torch.utils._sympy.printers import PythonPrinter as _PythonPrinter
from torch.utils._sympy.value_ranges import ValueRanges

from ..custom_graph_pass import CustomGraphModulePass
from ..ir import ChoiceCaller, FixedLayout, IRNode
from ..loop_body import LoopBody
from ..ops_handler import BasicMathOpsMixin, DefaultHandler
from ..scheduler import BaseScheduling, Scheduler, SchedulerNode
from ..shape_propagation import BlockShapeType
from ..utils import DeferredLineBase, IndentedBuffer, ir_dataclass
from ..virtualized import OpsHandler, OpsValue, ReductionType, StoreMode
from .wrapper import PythonWrapperCodegen

type SchedulingConstructor = Callable[[Scheduler | None], BaseScheduling]
type WrapperConstructor = type[PythonWrapperCodegen]
type SymbolLike = str | sympy.Symbol
OpVarT = str
schedule_log = ...
log = ...

def data_type_logger(msg: str) -> None: ...

@dataclasses.dataclass
class FileBackedGraphModule:
    """
    Output of FX wrapper codegen. Exposes the same methods as ModuleType, but these
    map back to a GraphModule instead of Python source.
    """

    gm: GraphModule
    compiled_fn: Callable[..., Any]
    def __post_init__(self) -> None: ...
    @property
    def __file__(self) -> str: ...
    def call(self, args: list[Any]) -> Any: ...
    @property
    def value(self) -> str: ...

class WorkspaceZeroMode(enum.Enum):
    UNINITIALIZED = ...
    ZERO_ON_CALL = ...
    ZERO_PER_GRAPH = ...
    @staticmethod
    def combine(a: WorkspaceZeroMode, b: WorkspaceZeroMode) -> WorkspaceZeroMode: ...
    @staticmethod
    def from_bool(zero_fill: bool) -> WorkspaceZeroMode: ...

class CodegenSymbol(ABC):
    """An IR object possibly corresponding to a variable in the wrapper code."""
    @abstractmethod
    def get_name(self) -> str: ...
    @abstractmethod
    def get_example(self) -> torch.Tensor | sympy.Symbol: ...

@ir_dataclass(frozen=True)
class WorkspaceArg(CodegenSymbol):
    """
    A temporary buffer used for a single kernel, then discarded.

    Not registered as a traditional buffer since there are no users,
    so it would be dead code eliminated.

    Args:
        nbytes: The size of the buffer in bytes.
        zero_fill: Whether the buffer should be initialized to zero.
    """

    count: sympy.Expr
    zero_mode: WorkspaceZeroMode
    device: torch.device
    outer_name: str
    inner_name: str = ...
    dtype: torch.dtype = ...
    @staticmethod
    def unique_name(prefix: str = ...) -> str: ...
    @staticmethod
    def can_join(a: WorkspaceArg, b: WorkspaceArg) -> bool: ...
    @staticmethod
    def join(a: WorkspaceArg, b: WorkspaceArg) -> WorkspaceArg: ...
    @staticmethod
    def maximum(a: WorkspaceArg, b: WorkspaceArg) -> WorkspaceArg: ...
    def get_device(self) -> torch.device: ...

    get_device_or_error = ...
    def get_dtype(self) -> torch.dtype: ...
    def get_example(self) -> torch.Tensor | sympy.Symbol: ...
    def get_layout(self) -> FixedLayout: ...
    @property
    def layout(self) -> FixedLayout: ...

    get_output_spec = ...
    maybe_get_output_spec = ...
    maybe_get_layout = ...
    def get_offset(self) -> sympy.Expr: ...
    def get_size(self) -> list[sympy.Expr]: ...
    def get_stride(self) -> list[sympy.Expr]: ...
    def get_name(self) -> str: ...
    def get_is_pinned(self) -> bool: ...
    def get_inputs_that_alias_output(self) -> list[str]: ...

class TritonScratchWorkspace:
    def __init__(self, size: int, generate_dtype_str: Callable[..., str]) -> None: ...
    def generate_dtype_str(self) -> str: ...

@dataclasses.dataclass
class TensorArg:
    """TensorArg(name: 'str', buffer: 'str', dtype: 'torch.dtype', offset: 'sympy.Expr' = 0, alias_of: 'Optional[str]' = None)"""

    name: str
    buffer: str
    dtype: torch.dtype
    offset: sympy.Expr = ...
    alias_of: str | None = ...

@dataclasses.dataclass
class SizeArg:
    """SizeArg(name: 'str', expr: 'sympy.Expr')"""

    name: str
    expr: sympy.Expr
    @property
    def alias_of(self) -> str | None: ...

@dataclasses.dataclass
class ConstexprArg:
    """ConstexprArg(name: 'str')"""

    name: str

@dataclasses.dataclass
class TMADescriptorArg:
    """TMADescriptorArg(name: 'str', api_type: 'str', block_shape: 'Optional[list[sympy.Expr]]', dtype: 'Optional[torch.dtype]')"""

    name: str
    api_type: str
    block_shape: list[sympy.Expr] | None
    dtype: torch.dtype | None

@dataclasses.dataclass
class DeviceCodegen:
    """DeviceCodegen(scheduling: 'SchedulingConstructor', wrapper_codegen: 'WrapperConstructor', cpp_wrapper_codegen: 'Optional[WrapperConstructor]' = None, fx_wrapper_codegen: 'Optional[WrapperConstructor]' = None)"""

    scheduling: SchedulingConstructor
    wrapper_codegen: WrapperConstructor
    cpp_wrapper_codegen: WrapperConstructor | None = ...
    fx_wrapper_codegen: WrapperConstructor | None = ...

type KernelArgType = WorkspaceArg | TensorArg | SizeArg | TMADescriptorArg | ConstexprArg
device_codegens: dict[str, DeviceCodegen] = ...

class DeviceOpOverrides:
    def import_get_raw_stream_as(self, name: str) -> str: ...
    def set_device(self, device_idx: int) -> str: ...
    def synchronize(self) -> str: ...
    def device_guard(self, device_idx: int) -> str: ...
    def cpp_device_guard(self) -> str: ...
    def cpp_aoti_device_guard(self) -> str: ...
    def cpp_stream_guard(self) -> str: ...
    def cpp_aoti_stream_guard(self) -> str: ...
    def cpp_getStreamFromExternal(self) -> str: ...
    def kernel_header(self) -> str: ...
    def kernel_driver(self) -> str: ...
    def cpp_stream_type(self) -> str: ...
    def aoti_get_stream(self) -> str: ...
    def cpp_kernel_type(self) -> str: ...
    def cpp_device_ptr(self) -> str: ...
    def tma_descriptor_helpers(self) -> str: ...
    def cpp_scratch(
        self, idx: int, workspace: TritonScratchWorkspace, prefix: str | None = ...
    ) -> tuple[list[str], str] | None: ...

device_op_overrides_dict: dict[str, DeviceOpOverrides] = ...
custom_backend_passes: dict[str, CustomGraphModulePass | None] = ...
custom_backend_codegen_configs: dict[str, ConfigModule | None] = ...

def register_backend_for_device(
    device: str,
    device_scheduling: SchedulingConstructor,
    device_wrapper_codegen: WrapperConstructor,
    device_cpp_wrapper_codegen: WrapperConstructor | None = ...,
    device_fx_wrapper_codegen: WrapperConstructor | None = ...,
    device_custom_pass: CustomGraphModulePass | None = ...,
    device_custom_config: ConfigModule | None = ...,
) -> None: ...

class BackendFeature(Enum):
    FOREACH = ...
    BUCKETIZE = ...
    INPLACE_BUFFERS = ...
    MASKED_SCATTER_WITH_INDEX = ...
    SCAN = ...
    SORT = ...
    TUPLE_REDUCTION = ...
    PREFER_STORE_LOOP_ORDER = ...
    TRITON_TEMPLATES = ...
    REDUCE_TO_SINGLE_ELEMENT = ...

def get_backend_features(device: torch.device | str | None) -> OrderedSet[BackendFeature]: ...
def has_backend_feature(device: torch.device | str | None, feature: BackendFeature) -> bool:
    """See also V.graph.has_feature"""

def get_scheduling_for_device(device: str) -> SchedulingConstructor | None: ...
def get_wrapper_codegen_for_device(
    device: str, cpp_wrapper: bool = ..., fx_wrapper: bool = ...
) -> WrapperConstructor | None: ...
def get_custom_backend_pass_for_device(device: str) -> CustomGraphModulePass | None: ...
def get_custom_backend_config_for_device(device: str) -> ConfigModule | None: ...
@functools.cache
def init_backend_registration() -> None:
    """
    Register the backend for different devices, including the scheduling
    for kernel code generation and the host side wrapper code generation.
    """

def index_prevent_reordering(
    index: Sequence[sympy.Expr], index_vars: Sequence[sympy.Expr], sizes: Sequence[sympy.Expr]
) -> list[sympy.Expr]: ...
def register_device_op_overrides(device: str, device_op_overrides: DeviceOpOverrides) -> None: ...
def get_device_op_overrides(device: str) -> DeviceOpOverrides: ...

DTYPE_TO_COMPUTATION_DTYPE: dict[torch.dtype, torch.dtype] = ...

def deduce_output_dtype_by_name(op_name: str, *args: Any, **kwargs: Any) -> torch.dtype | None:
    """Given op name and a list of input dtypes, deduce the output dtype"""

def check_dtype(buffer: IndentedBuffer, var: CSEVariableType, dtype: torch.dtype) -> None: ...

class DataTypePropagation:
    def __init__(self, body: LoopBody) -> None: ...
    def deduce_node_dtype_by_inputs(self, node: torch.fx.Node) -> torch.dtype | None: ...
    def deduce_node_dtype_by_subgraph(self, node: torch.fx.Node) -> torch.dtype: ...
    def deduce_node_dtype(self, node: torch.fx.Node) -> torch.dtype | None: ...
    def propagate_graph(self, graph: torch.fx.Graph) -> torch.dtype | None: ...
    def propagate(self) -> torch.dtype | None: ...
    @classmethod
    def propagate_loopbody(cls, body: LoopBody) -> torch.dtype | None: ...
    @classmethod
    def propagate_scheduler_node(cls, node: SchedulerNode) -> torch.dtype | None: ...

class PythonPrinter(_PythonPrinter):
    def doprint(self, expr: sympy.Expr, *, simplify: bool = ..., p: bool = ...) -> str: ...
    def parenthesize(self, item: sympy.Expr, level: int, strict: bool = ...) -> str: ...

class OpDecompositions:
    """Decomposes inductor ops"""
    @staticmethod
    def identity(value: OpVarT) -> OpVarT: ...
    @staticmethod
    def reciprocal(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def square(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def erfc(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def erfcx(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def expm1(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def log10(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def log2(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def exp2(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def log1p(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def sigmoid(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def relu(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def fma(x: OpVarT, y: OpVarT, z: OpVarT) -> OpVarT: ...
    @staticmethod
    def floor_to_int(a: OpVarT, dtype: torch.dtype) -> OpVarT: ...
    @staticmethod
    def ceil_to_int(a: OpVarT, dtype: torch.dtype) -> OpVarT: ...
    @staticmethod
    def trunc_to_int(a: OpVarT, dtype: torch.dtype) -> OpVarT: ...
    @staticmethod
    def remainder(a: OpVarT, b: OpVarT) -> OpVarT: ...
    @staticmethod
    def round_to_int(a: OpVarT, dtype: torch.dtype) -> OpVarT: ...

_RE_PAREN_NOT_NEEDED = ...

class OpOverrides(BasicMathOpsMixin, OpDecompositions, OpsHandler[Any]):
    @staticmethod
    def paren(string: OpVarT) -> OpVarT: ...
    @staticmethod
    def constant(value: bool | float, dtype: torch.dtype) -> OpVarT: ...
    @staticmethod
    def bitwise_not(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def logical_not(a: OpVarT) -> OpVarT: ...
    @staticmethod
    def bitwise_and(x: OpVarT, y: OpVarT) -> OpVarT: ...
    @staticmethod
    def bitwise_or(x: OpVarT, y: OpVarT) -> OpVarT: ...
    @staticmethod
    def bitwise_xor(x: OpVarT, y: OpVarT) -> OpVarT: ...
    @staticmethod
    def bitwise_left_shift(x: OpVarT, y: OpVarT) -> OpVarT: ...
    @staticmethod
    def bitwise_right_shift(x: OpVarT, y: OpVarT) -> OpVarT: ...
    @staticmethod
    def int_truediv(a: OpVarT, b: OpVarT) -> OpVarT: ...
    @staticmethod
    def load_seed(name: str, offset: OpVarT) -> OpVarT: ...
    def indirect_indexing(
        self, var: OpVarT, size: sympy.Expr | int, check: bool = ..., wrap_neg: bool = ...
    ) -> sympy.Symbol: ...
    def check_bounds(self, expr: sympy.Expr, size: sympy.Expr, lower: bool, upper: bool) -> None: ...
    def load(self, name: str, index: sympy.Expr) -> OpVarT: ...
    def store(self, name: str, index: sympy.Expr, value: OpVarT, mode: StoreMode = ...) -> None: ...
    def store_reduction(self, name: str, index: sympy.Expr, value: OpVarT) -> None: ...
    def reduction(
        self,
        dtype: torch.dtype,
        src_dtype: torch.dtype,
        reduction_type: ReductionType,
        value: OpVarT | tuple[OpVarT, ...],
    ) -> OpVarT | tuple[OpVarT, ...]: ...
    def scan(
        self,
        dtypes: tuple[torch.dtype, ...],
        combine_fn: Callable[[tuple[OpVarT, ...], tuple[OpVarT, ...]], tuple[OpVarT, ...]],
        values: tuple[OpVarT, ...],
    ) -> tuple[OpVarT, ...]: ...
    def sort(
        self, dtypes: tuple[torch.dtype, ...], values: tuple[OpVarT, ...], stable: bool, descending: bool
    ) -> tuple[OpVarT, ...]: ...
    def bucketize(
        self,
        values: OpVarT,
        boundaries: tuple[str, sympy.Expr, sympy.Expr, sympy.Expr],
        boundary_indices: OpVarT,
        indexing_dtype: torch.dtype,
        right: bool,
        sorter: tuple[str, sympy.Expr] | None = ...,
        sorter_indices: OpVarT | None = ...,
    ) -> OpVarT: ...
    def halide_clamp(self, value: OpVarT, size: sympy.Expr, check: bool) -> OpVarT: ...
    def inline_asm_elementwise(
        self,
        *inputs: OpVarT,
        asm: str,
        constraints: str | None = ...,
        dtype: torch.dtype = ...,
        is_pure: bool = ...,
        pack: int = ...,
    ) -> OpVarT: ...
    def output(self, *args: OpVarT) -> None: ...
    def placeholder(self, index: int) -> OpVarT: ...

@dataclasses.dataclass
class OverridesData:
    """OverridesData(name: 'str', cpp: 'Callable[..., str]', triton: 'Optional[Callable[..., str]]' = None, cppvec: 'Optional[Callable[..., str]]' = None, type_promotion_kind: 'ELEMENTWISE_TYPE_PROMOTION_KIND' = <ELEMENTWISE_TYPE_PROMOTION_KIND.DEFAULT: (0,)>, halide: 'Optional[Callable[..., str]]' = None, mps: 'Optional[Callable[..., str]]' = None)"""

    name: str
    cpp: Callable[..., str]
    triton: Callable[..., str] | None = ...
    cppvec: Callable[..., str] | None = ...
    type_promotion_kind: ELEMENTWISE_TYPE_PROMOTION_KIND = ...
    halide: Callable[..., str] | None = ...
    mps: Callable[..., str] | None = ...

pointwise_overrides_data: dict[str, OverridesData] = ...

def is_buffer_removed(name: str) -> bool: ...

class DeferredLine(DeferredLineBase):
    """A line that can be 'unwritten' by adding name to V.graph.removed_buffers"""
    def __init__(self, name: str, line: str) -> None: ...
    def __call__(self) -> str | None: ...

class BracesBuffer(IndentedBuffer):
    def indent(self, offset: int = ...) -> contextlib.AbstractContextManager[None]: ...

class InplacedBuffer(NamedTuple):
    """InplacedBuffer(inner_name, other_names)"""

    inner_name: str
    other_names: list[str]

@dataclasses.dataclass
class ArgName:
    """ArgName(name: 'str', is_constexpr: 'bool' = False)"""

    name: str
    is_constexpr: bool = ...
    def full_name(self) -> str: ...

class RemovedArg: ...

REMOVED = ...

class KernelArgs:
    def __init__(self) -> None: ...
    def input(self, name: str) -> str: ...
    def output(self, name: str) -> str: ...
    def make_inplace(self, input_name: str, output_name: str) -> None: ...
    def workspace(self, nbytes: sympy.Expr, zero_fill: bool) -> tuple[str, int]:
        """
        Allocate or extend a workspace buffer of nbytes bytes.

        This function manages the allocation of a workspace buffer. It either creates
        a new WorkspaceArg or extends an existing one.

        Note:
        - Calling this function will in-place mutate the args by adding or updating
        a WorkspaceArg.
        - The codegen for generating the Python argdefs and call_defs will check
        this field and allocate the buffer accordingly.
        - A new argument "ws_ptr" will be present in the generated code.

        Args:
            nbytes (sympy.Expr): The number of bytes to allocate.
            zero_fill (bool): Whether to initialize the buffer to zero.

        Returns:
            Tuple[str, int]: A tuple containing:
                - "ws_ptr": A string identifier for the workspace pointer.
                - offset: An integer representing the byte offset in the workspace.
        """
    def semaphores(self, min_size: sympy.Expr) -> str:
        """
        Lazily allocate a graph-wide semaphores buffer with at least min_size.  This is a single buffer shared by
        all kernels and zero initialized once at graph start.  Each kernel must leave the buffer zeroed on exit.

        Warning: multiple calls to this function will return the same buffer.

        Args:
            min_size: the number of int32 semaphores required

        Returns:
            name of the semaphores buffer
        """
    def seed_offset(self, name: str, value: int) -> str: ...
    def size(self, name: sympy.Symbol) -> str: ...
    def call_names(self) -> Iterator[str]: ...
    def arg_name(self, name: str) -> str | None:
        """Returns inner name of a given outer name."""
    def wrap_ptr_arg(self, buf: str, dtype: torch.dtype) -> str: ...
    def wrap_size_arg(self, size: SymbolLike) -> str: ...
    def cpp_argdefs(
        self, dtype_to_cpp_type: dict[torch.dtype, str] | None = ...
    ) -> tuple[list[str], list[str], list[str]]: ...
    def python_argdefs(self) -> tuple[list[ArgName], list[str], list[KernelArgType], list[Any]]: ...
    def aliases(self) -> Iterator[tuple[str, str]]: ...
    def is_removed(self, name: str) -> bool: ...
    def live_output_buffers(self) -> OrderedSet[str]: ...

class CSEVariable:
    """
    A CSEVariable is just a name for an expression but it is useful to be able to annotate them on a backend dependent basis.
    To do so, the backends can simply overload `Kernel.create_cse_var`
    The "CSEVariable.update_on_args" method gives you a hook for annotations
    See example of TritonCSEVariable in triton.py
    """
    def __init__(
        self, name: str, bounds: ValueRanges[Any], dtype: torch.dtype | None = ..., shape: BlockShapeType = ...
    ) -> None: ...
    def __hash__(self) -> int: ...
    def __eq__(self, other: object) -> bool: ...
    def update_on_args(self, name: str, args: Any, kwargs: Any) -> None: ...

AugmentedKeyT = TypeVar("AugmentedKeyT", default=str)
CSEVariableType = TypeVar("CSEVariableType", bound=CSEVariable, default=CSEVariable)
type ReductionCacheKey = tuple[torch.dtype, ReductionType, CSEVariable | tuple[CSEVariable, ...]]

class CSE[CSEVariableType: CSEVariable = CSEVariable, AugmentedKeyT = str]:
    """Common subexpression elimination"""
    def __init__(
        self,
        prefix: str = ...,
        suffix: str = ...,
        name_prefix: str = ...,
        iter_buffers: itertools.count[int] | None = ...,
        store_cache: MutableMapping[str, CSEVariableType] | None = ...,
        reduction_cache: MutableMapping[ReductionCacheKey, CSEVariableType] | None = ...,
        varname_map: dict[str, CSEVariableType] | None = ...,
    ) -> None: ...
    def invalidate(self, keep_vars: OrderedSet[CSEVariable]) -> None: ...
    def clone(self) -> Self: ...
    def scoped_copy(self) -> Self:
        """Return a copy of using ScopedDict so changes to *_cache aren't visible in self"""
    def augment_key(self, cache_key: str) -> AugmentedKeyT:
        """Override this method to augment cache key with backend specifics"""
    def put(self, cache_key: str, val: CSEVariableType) -> None: ...
    def contains(self, cache_key: str) -> bool: ...
    def try_get(self, cache_key: str) -> CSEVariableType | None: ...
    def get(self, cache_key: str) -> CSEVariableType: ...
    def generate(
        self,
        buffer: IndentedBuffer,
        expr: str | CSEVariable | OpsValue | IndentedBuffer | DeferredLineBase,
        *,
        bounds: ValueRanges[Any] = ...,
        write: bool = ...,
        assignment: bool = ...,
        dtype: torch.dtype | None = ...,
        shape: BlockShapeType = ...,
    ) -> CSEVariableType: ...
    def newvar(
        self, bounds: ValueRanges[Any] = ..., dtype: torch.dtype | None = ..., shape: BlockShapeType = ...
    ) -> CSEVariableType: ...
    def namedvar(
        self, name: str, bounds: ValueRanges[Any] = ..., dtype: torch.dtype | None = ..., shape: BlockShapeType = ...
    ) -> CSEVariableType: ...

class CodeGen:
    def __init__(self) -> None: ...
    def __enter__(self) -> Self: ...
    def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None: ...

class Kernel[CSEVariableType: CSEVariable = CSEVariable](CodeGen):
    newvar_prefix: str = ...
    suffix: str = ...
    overrides: Callable[[], OpsHandler[Any]] | None = ...
    def __init__(self, args: KernelArgs | None = ..., increase_kernel_count: bool = ...) -> None: ...
    @contextlib.contextmanager
    def set_current_node(self, node: SchedulerNode) -> Iterator[None]: ...
    @contextlib.contextmanager
    def swap_buffers(
        self, lb: IndentedBuffer, cb: IndentedBuffer | None = ..., sb: IndentedBuffer | None = ...
    ) -> Iterator[None]: ...
    def load(self, name: str, index: sympy.Expr) -> CSEVariable: ...
    def indirect_load(self, name: str, index: sympy.Expr) -> CSEVariable:
        """A load the depends on an index we have read"""
    def store_reduction(self, name: str, index: sympy.Expr, value: CSEVariable) -> None: ...
    def store(self, name: str, index: sympy.Expr, value: CSEVariable, mode: StoreMode = ...) -> None: ...
    def reduction(
        self,
        dtype: torch.dtype,
        src_dtype: torch.dtype,
        reduction_type: ReductionType,
        value: CSEVariable | tuple[CSEVariable, ...],
    ) -> CSEVariable | tuple[CSEVariable, ...]: ...
    def scan(
        self,
        dtypes: tuple[torch.dtype, ...],
        combine_fn: Callable[[tuple[CSEVariable, ...], tuple[CSEVariable, ...]], tuple[CSEVariable, ...]],
        values: tuple[CSEVariable, ...],
    ) -> tuple[CSEVariable, ...]: ...
    def sort(
        self, dtypes: tuple[torch.dtype, ...], values: tuple[CSEVariable, ...], stable: bool, descending: bool
    ) -> tuple[CSEVariable, ...]: ...
    def var_ranges(self) -> dict[sympy.Symbol, sympy.Expr]: ...
    def bucketize(
        self,
        values: CSEVariable,
        boundaries: tuple[str, sympy.Expr, sympy.Expr, sympy.Expr],
        boundary_indices: CSEVariable,
        indexing_dtype: torch.dtype,
        right: bool,
        sorter: tuple[str, sympy.Expr] | None = ...,
        sorter_indices: CSEVariable | None = ...,
    ) -> CSEVariable:
        """See [Note: Inductor bucketize op]"""
    @property
    def assert_function(self) -> str: ...
    def indirect_assert(
        self, var: CSEVariable | str, lower: str | None, upper: str | None, mask: CSEVariable | str | None = ...
    ) -> str: ...
    def check_bounds(self, expr: sympy.Expr, size: sympy.Expr, lower: bool, upper: bool) -> None: ...
    def index_to_str(self, index: sympy.Expr) -> str: ...
    def __enter__(self) -> Self: ...
    def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None: ...
    def remove_kernel_local_buffers(self) -> None:
        """
        Any buffers that are both created and have a last use in the
        same kernel can be removed.

        Note that V.graph.scheduler can be None when codegening triton template
        kernels.
        """
    def remove_buffer(self, name: str) -> None: ...
    def remove_inplace_buffer(self, name: str) -> None: ...
    def rename_indexing(self, index: list[sympy.Expr] | tuple[sympy.Expr, ...] | sympy.Expr) -> sympy.Expr: ...
    def create_cse_var(self, *args: Any, **kwargs: Any) -> CSEVariable: ...
    def arg_name(self, node: IRNode) -> str | None:
        """Returns arg name of a given input or output node."""

@dataclasses.dataclass
class OptimizationContext:
    """OptimizationContext(dtype: 'Optional[torch.dtype]' = None, ops_name: 'str' = '')"""

    key: ClassVar[str] = ...
    dtype: torch.dtype | None = ...
    ops_name: str = ...

@functools.cache
def jinja2_env() -> Any: ...

class KernelTemplate:
    """
    Base class for defining kernel templates.

    Children classes: TritonTemplate, CUDATemplate
    """
    @staticmethod
    def indent_except_first(source: str, num_indents: int, indents_spacing: int = ...) -> str: ...
    def __init__(self, name: str) -> None: ...
    @property
    def uid(self) -> str:
        """
        entry point to override for templates to ensure a uid e.g. through a prefix

        the purpose of this is that every KernelTemplate/ExternKernelChoice is unique
        in the system, but reproducible e.g. restarting pytorch should yield the same id
        """
    def choice_or_none(self, **kwargs: Any) -> ChoiceCaller | None:
        """
        Maybe generates a new ChoiceCaller and returns it, or None if generation fails.

        kwargs: Additional kwargs to be passed to self.generate() to generate a new ChoiceCaller.
        """
    def maybe_append_choice(self, choices: list[Any], **kwargs: Any) -> NotImplementedError | None:
        """
        Maybe generates a new ChoiceCaller and appends it into existing choices.
        Returns None if success, otherwise returns the error.

        choices: A list of ChoiceCallers.
        kwargs: Additional kwargs to be passed to self.generate() to generate a new ChoiceCaller.
        """
    def generate(self, **kwargs: Any) -> ChoiceCaller:
        """Generates a ChoiceCaller instance from the given arguments."""

class CSEProxy(DefaultHandler):
    name = ...
    def __init__(self, kernel: Kernel[Any], parent_handler: OpsHandler[Any]) -> None: ...
    def indirect_indexing(
        self, var: CSEVariable, size: sympy.Expr | int, check: bool = ..., wrap_neg: bool = ...
    ) -> sympy.Symbol: ...
    def check_bounds(self, expr: sympy.Expr, size: sympy.Expr, lower: bool, upper: bool) -> None: ...
    def load(self, name: str, index: sympy.Expr) -> CSEVariable: ...
    def store(self, name: str, index: sympy.Expr, value: CSEVariable, mode: StoreMode = ...) -> None: ...
    def store_reduction(self, name: str, index: sympy.Expr, value: CSEVariable) -> None: ...
    def reduction(
        self,
        dtype: torch.dtype,
        src_dtype: torch.dtype,
        reduction_type: ReductionType,
        value: CSEVariable | tuple[CSEVariable, ...],
    ) -> CSEVariable | tuple[CSEVariable, ...]: ...
    def scan(
        self,
        dtypes: tuple[torch.dtype, ...],
        combine_fn: Callable[[tuple[CSEVariable, ...], tuple[CSEVariable, ...]], tuple[CSEVariable, ...]],
        values: tuple[CSEVariable, ...],
    ) -> tuple[CSEVariable, ...]: ...
    def sort(
        self, dtypes: tuple[torch.dtype, ...], values: tuple[CSEVariable, ...], stable: bool, descending: bool
    ) -> tuple[CSEVariable, ...]: ...
    def bucketize(
        self,
        values: CSEVariable,
        boundaries: tuple[str, sympy.Expr, sympy.Expr, sympy.Expr],
        boundary_indices: CSEVariable,
        indexing_dtype: torch.dtype,
        right: bool,
        sorter: tuple[str, sympy.Expr] | None = ...,
        sorter_indices: CSEVariable | None = ...,
    ) -> CSEVariable:
        """
        [Note: Inductor bucketize op]

        Inputs:
        -------
        values: the values to be bucketized.
        boundaries: a tuple containing
          (a) the name of the boundaries tensor (which must be sorted, unless
          the sorting tensor is present),
          (b) the length of the tensor in the last dimension (i.e. the length of
          one set of boundaries),
          (c) the number of elements in the underlying storage (i.e. the length
          of the flattened tensor, ignoring striding), and
          (d) the stride of the tensor in the last dimension.
        boundary_indices: indices into a flattened version of the boundaries
        tensor, of the same size and shape as "values".  Each index points to
        the first element in the set of boundaries to be used for the
        corresponding value.
        indexing_dtype: the dtype to use when indexing into the boundaries
        tensor.  This must be int64 or int32.  This additionally specifies the
        dtype of the return value.
        right: see "Details" below.
        sorter: an optional tuple containing
          (a) the name of an optional sorting tensor, used to access unsorted
          boundaries without reordering the boundaries tensor, and
          (b) the stride of the tensor in the last dimension.
        The values in the sorting tensor are used as indices into the *last*
        dimension of the boundaries tensor, with all other indices matching.
        The size of the sorting and boundaries tensors must be equivalent.
        sorter_indices: must be present if the sorting array is present; see
        "boundary_indices" for the equivalent definition for the boundaries
        tensor.

        Output:
        -------
        The buckets each value belongs in, within a given set of boundaries.  0
        indicates a position before the first boundary, and len(boundaries_set)
        represents a position after the last boundary.

        Details:
        --------
        Given a value and a set of boundaries, calculate the bucket that each
        value belongs to.  This works differently in 1-D and N-D cases.

        for values [[-1, 0, 1, 2], [3, 4, 5, 9]], boundaries [0, 4, 4, 8], right=True
        return =   [[ 0, 1, 1, 1], [1, 3, 3, 4]].

        for values [[-1, 0, 1, 2], [3, 4, 5, 9]], boundaries [[0, 4], [4, 8]], right=True
        return =   [[ 0, 1, 1, 1], [0, 1, 1, 2]]

        Note that in the N-D boundaries case, the shape of "values" and
        "boundaries" must match in every dimension _except_ the last.

        When right == False, bucket i refers to range (boundaries[i], boundaries[i+1]].
        When right == True,  bucket i refers to range [boundaries[i], boundaries[i+1]).

        Boundaries must be non-decreasing, or a sorter must be provided which
        would re-index offsets in a non-decreasing order (e.g. the second output
        of torch.sort(offsets)).  Otherwise, the result is undefined.
        """
