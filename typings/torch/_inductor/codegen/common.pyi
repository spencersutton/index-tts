import contextlib
import dataclasses
import enum
import functools
import itertools
from abc import ABC, abstractmethod
from collections.abc import Callable, Iterator, MutableMapping, Sequence
from enum import Enum
from typing import TYPE_CHECKING, Any, ClassVar, NamedTuple, Self, TypeVar

import sympy
import torch
import torch.fx
from torch._prims_common import ELEMENTWISE_TYPE_PROMOTION_KIND
from torch.fx import GraphModule
from torch.utils._config_module import ConfigModule
from torch.utils._ordered_set import OrderedSet
from torch.utils._sympy.printers import PythonPrinter as _PythonPrinter
from torch.utils._sympy.value_ranges import ValueRanges

from ..custom_graph_pass import CustomGraphModulePass
from ..ir import ChoiceCaller, FixedLayout, IRNode
from ..loop_body import LoopBody
from ..ops_handler import BasicMathOpsMixin, DefaultHandler
from ..scheduler import BaseScheduling, Scheduler, SchedulerNode
from ..shape_propagation import BlockShapeType
from ..utils import DeferredLineBase, IndentedBuffer, ir_dataclass
from ..virtualized import OpsHandler, OpsValue, ReductionType, StoreMode
from .wrapper import PythonWrapperCodegen

if TYPE_CHECKING:
    type SchedulingConstructor = Callable[[Scheduler | None], BaseScheduling]
    type WrapperConstructor = type[PythonWrapperCodegen]
    type SymbolLike = str | sympy.Symbol
    OpVarT = str
schedule_log = ...
log = ...

def data_type_logger(msg: str) -> None: ...

@dataclasses.dataclass
class FileBackedGraphModule:
    gm: GraphModule
    compiled_fn: Callable[..., Any]
    def __post_init__(self) -> None: ...
    @property
    def __file__(self) -> str: ...
    def call(self, args: list[Any]) -> Any: ...
    @property
    def value(self) -> str: ...

class WorkspaceZeroMode(enum.Enum):
    UNINITIALIZED = ...
    ZERO_ON_CALL = ...
    ZERO_PER_GRAPH = ...
    @staticmethod
    def combine(a: WorkspaceZeroMode, b: WorkspaceZeroMode) -> WorkspaceZeroMode: ...
    @staticmethod
    def from_bool(zero_fill: bool) -> WorkspaceZeroMode: ...

class CodegenSymbol(ABC):
    @abstractmethod
    def get_name(self) -> str: ...
    @abstractmethod
    def get_example(self) -> torch.Tensor | sympy.Symbol: ...

@ir_dataclass(frozen=True)
class WorkspaceArg(CodegenSymbol):
    count: sympy.Expr
    zero_mode: WorkspaceZeroMode
    device: torch.device
    outer_name: str
    inner_name: str = ...
    dtype: torch.dtype = ...
    @staticmethod
    def unique_name(prefix: str = ...) -> str: ...
    @staticmethod
    def can_join(a: WorkspaceArg, b: WorkspaceArg) -> bool: ...
    @staticmethod
    def join(a: WorkspaceArg, b: WorkspaceArg) -> WorkspaceArg: ...
    @staticmethod
    def maximum(a: WorkspaceArg, b: WorkspaceArg) -> WorkspaceArg: ...
    def get_device(self) -> torch.device: ...

    get_device_or_error = ...
    def get_dtype(self) -> torch.dtype: ...
    def get_example(self) -> torch.Tensor | sympy.Symbol: ...
    def get_layout(self) -> FixedLayout: ...
    @property
    def layout(self) -> FixedLayout: ...

    get_output_spec = ...
    maybe_get_output_spec = ...
    maybe_get_layout = ...
    def get_offset(self) -> sympy.Expr: ...
    def get_size(self) -> list[sympy.Expr]: ...
    def get_stride(self) -> list[sympy.Expr]: ...
    def get_name(self) -> str: ...
    def get_is_pinned(self) -> bool: ...
    def get_inputs_that_alias_output(self) -> list[str]: ...

class TritonScratchWorkspace:
    def __init__(self, size: int, generate_dtype_str: Callable[..., str]) -> None: ...
    def generate_dtype_str(self) -> str: ...

@dataclasses.dataclass
class TensorArg:
    name: str
    buffer: str
    dtype: torch.dtype
    offset: sympy.Expr = ...
    alias_of: str | None = ...

@dataclasses.dataclass
class SizeArg:
    name: str
    expr: sympy.Expr
    @property
    def alias_of(self) -> str | None: ...

@dataclasses.dataclass
class ConstexprArg:
    name: str

@dataclasses.dataclass
class TMADescriptorArg:
    name: str
    api_type: str
    block_shape: list[sympy.Expr] | None
    dtype: torch.dtype | None

@dataclasses.dataclass
class DeviceCodegen:
    scheduling: SchedulingConstructor
    wrapper_codegen: WrapperConstructor
    cpp_wrapper_codegen: WrapperConstructor | None = ...
    fx_wrapper_codegen: WrapperConstructor | None = ...

type KernelArgType = WorkspaceArg | TensorArg | SizeArg | TMADescriptorArg | ConstexprArg
device_codegens: dict[str, DeviceCodegen] = ...

class DeviceOpOverrides:
    def import_get_raw_stream_as(self, name: str) -> str: ...
    def set_device(self, device_idx: int) -> str: ...
    def synchronize(self) -> str: ...
    def device_guard(self, device_idx: int) -> str: ...
    def cpp_device_guard(self) -> str: ...
    def cpp_aoti_device_guard(self) -> str: ...
    def cpp_stream_guard(self) -> str: ...
    def cpp_aoti_stream_guard(self) -> str: ...
    def cpp_getStreamFromExternal(self) -> str: ...
    def kernel_header(self) -> str: ...
    def kernel_driver(self) -> str: ...
    def cpp_stream_type(self) -> str: ...
    def aoti_get_stream(self) -> str: ...
    def cpp_kernel_type(self) -> str: ...
    def cpp_device_ptr(self) -> str: ...
    def tma_descriptor_helpers(self) -> str: ...
    def cpp_scratch(
        self, idx: int, workspace: TritonScratchWorkspace, prefix: str | None = ...
    ) -> tuple[list[str], str] | None: ...

device_op_overrides_dict: dict[str, DeviceOpOverrides] = ...
custom_backend_passes: dict[str, CustomGraphModulePass | None] = ...
custom_backend_codegen_configs: dict[str, ConfigModule | None] = ...

def register_backend_for_device(
    device: str,
    device_scheduling: SchedulingConstructor,
    device_wrapper_codegen: WrapperConstructor,
    device_cpp_wrapper_codegen: WrapperConstructor | None = ...,
    device_fx_wrapper_codegen: WrapperConstructor | None = ...,
    device_custom_pass: CustomGraphModulePass | None = ...,
    device_custom_config: ConfigModule | None = ...,
) -> None: ...

class BackendFeature(Enum):
    FOREACH = ...
    BUCKETIZE = ...
    INPLACE_BUFFERS = ...
    MASKED_SCATTER_WITH_INDEX = ...
    SCAN = ...
    SORT = ...
    TUPLE_REDUCTION = ...
    PREFER_STORE_LOOP_ORDER = ...
    TRITON_TEMPLATES = ...
    REDUCE_TO_SINGLE_ELEMENT = ...

def get_backend_features(device: torch.device | str | None) -> OrderedSet[BackendFeature]: ...
def has_backend_feature(device: torch.device | str | None, feature: BackendFeature) -> bool: ...
def get_scheduling_for_device(device: str) -> SchedulingConstructor | None: ...
def get_wrapper_codegen_for_device(
    device: str, cpp_wrapper: bool = ..., fx_wrapper: bool = ...
) -> WrapperConstructor | None: ...
def get_custom_backend_pass_for_device(device: str) -> CustomGraphModulePass | None: ...
def get_custom_backend_config_for_device(device: str) -> ConfigModule | None: ...
@functools.cache
def init_backend_registration() -> None: ...
def index_prevent_reordering(
    index: Sequence[sympy.Expr], index_vars: Sequence[sympy.Expr], sizes: Sequence[sympy.Expr]
) -> list[sympy.Expr]: ...
def register_device_op_overrides(device: str, device_op_overrides: DeviceOpOverrides) -> None: ...
def get_device_op_overrides(device: str) -> DeviceOpOverrides: ...

DTYPE_TO_COMPUTATION_DTYPE: dict[torch.dtype, torch.dtype] = ...

def deduce_output_dtype_by_name(op_name: str, *args: Any, **kwargs: Any) -> torch.dtype | None: ...
def check_dtype(buffer: IndentedBuffer, var: CSEVariableType, dtype: torch.dtype) -> None: ...

class DataTypePropagation:
    def __init__(self, body: LoopBody) -> None: ...
    def deduce_node_dtype_by_inputs(self, node: torch.fx.Node) -> torch.dtype | None: ...
    def deduce_node_dtype_by_subgraph(self, node: torch.fx.Node) -> torch.dtype: ...
    def deduce_node_dtype(self, node: torch.fx.Node) -> torch.dtype | None: ...
    def propagate_graph(self, graph: torch.fx.Graph) -> torch.dtype | None: ...
    def propagate(self) -> torch.dtype | None: ...
    @classmethod
    def propagate_loopbody(cls, body: LoopBody) -> torch.dtype | None: ...
    @classmethod
    def propagate_scheduler_node(cls, node: SchedulerNode) -> torch.dtype | None: ...

class PythonPrinter(_PythonPrinter):
    def doprint(self, expr: sympy.Expr, *, simplify: bool = ..., p: bool = ...) -> str: ...
    def parenthesize(self, item: sympy.Expr, level: int, strict: bool = ...) -> str: ...

class OpDecompositions:
    @staticmethod
    def identity(value: OpVarT) -> OpVarT: ...
    @staticmethod
    def reciprocal(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def square(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def erfc(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def erfcx(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def expm1(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def log10(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def log2(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def exp2(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def log1p(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def sigmoid(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def relu(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def fma(x: OpVarT, y: OpVarT, z: OpVarT) -> OpVarT: ...
    @staticmethod
    def floor_to_int(a: OpVarT, dtype: torch.dtype) -> OpVarT: ...
    @staticmethod
    def ceil_to_int(a: OpVarT, dtype: torch.dtype) -> OpVarT: ...
    @staticmethod
    def trunc_to_int(a: OpVarT, dtype: torch.dtype) -> OpVarT: ...
    @staticmethod
    def remainder(a: OpVarT, b: OpVarT) -> OpVarT: ...
    @staticmethod
    def round_to_int(a: OpVarT, dtype: torch.dtype) -> OpVarT: ...

_RE_PAREN_NOT_NEEDED = ...

class OpOverrides(BasicMathOpsMixin, OpDecompositions, OpsHandler[Any]):
    @staticmethod
    def paren(string: OpVarT) -> OpVarT: ...
    @staticmethod
    def constant(value: bool | float, dtype: torch.dtype) -> OpVarT: ...
    @staticmethod
    def bitwise_not(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def logical_not(a: OpVarT) -> OpVarT: ...
    @staticmethod
    def bitwise_and(x: OpVarT, y: OpVarT) -> OpVarT: ...
    @staticmethod
    def bitwise_or(x: OpVarT, y: OpVarT) -> OpVarT: ...
    @staticmethod
    def bitwise_xor(x: OpVarT, y: OpVarT) -> OpVarT: ...
    @staticmethod
    def bitwise_left_shift(x: OpVarT, y: OpVarT) -> OpVarT: ...
    @staticmethod
    def bitwise_right_shift(x: OpVarT, y: OpVarT) -> OpVarT: ...
    @staticmethod
    def int_truediv(a: OpVarT, b: OpVarT) -> OpVarT: ...
    @staticmethod
    def load_seed(name: str, offset: OpVarT) -> OpVarT: ...
    def indirect_indexing(
        self, var: OpVarT, size: sympy.Expr | int, check: bool = ..., wrap_neg: bool = ...
    ) -> sympy.Symbol: ...
    def check_bounds(self, expr: sympy.Expr, size: sympy.Expr, lower: bool, upper: bool) -> None: ...
    def load(self, name: str, index: sympy.Expr) -> OpVarT: ...
    def store(self, name: str, index: sympy.Expr, value: OpVarT, mode: StoreMode = ...) -> None: ...
    def store_reduction(self, name: str, index: sympy.Expr, value: OpVarT) -> None: ...
    def reduction(
        self,
        dtype: torch.dtype,
        src_dtype: torch.dtype,
        reduction_type: ReductionType,
        value: OpVarT | tuple[OpVarT, ...],
    ) -> OpVarT | tuple[OpVarT, ...]: ...
    def scan(
        self,
        dtypes: tuple[torch.dtype, ...],
        combine_fn: Callable[[tuple[OpVarT, ...], tuple[OpVarT, ...]], tuple[OpVarT, ...]],
        values: tuple[OpVarT, ...],
    ) -> tuple[OpVarT, ...]: ...
    def sort(
        self, dtypes: tuple[torch.dtype, ...], values: tuple[OpVarT, ...], stable: bool, descending: bool
    ) -> tuple[OpVarT, ...]: ...
    def bucketize(
        self,
        values: OpVarT,
        boundaries: tuple[str, sympy.Expr, sympy.Expr, sympy.Expr],
        boundary_indices: OpVarT,
        indexing_dtype: torch.dtype,
        right: bool,
        sorter: tuple[str, sympy.Expr] | None = ...,
        sorter_indices: OpVarT | None = ...,
    ) -> OpVarT: ...
    def halide_clamp(self, value: OpVarT, size: sympy.Expr, check: bool) -> OpVarT: ...
    def inline_asm_elementwise(
        self,
        *inputs: OpVarT,
        asm: str,
        constraints: str | None = ...,
        dtype: torch.dtype = ...,
        is_pure: bool = ...,
        pack: int = ...,
    ) -> OpVarT: ...
    def output(self, *args: OpVarT) -> None: ...
    def placeholder(self, index: int) -> OpVarT: ...

@dataclasses.dataclass
class OverridesData:
    name: str
    cpp: Callable[..., str]
    triton: Callable[..., str] | None = ...
    cppvec: Callable[..., str] | None = ...
    type_promotion_kind: ELEMENTWISE_TYPE_PROMOTION_KIND = ...
    halide: Callable[..., str] | None = ...
    mps: Callable[..., str] | None = ...

pointwise_overrides_data: dict[str, OverridesData] = ...

def is_buffer_removed(name: str) -> bool: ...

class DeferredLine(DeferredLineBase):
    def __init__(self, name: str, line: str) -> None: ...
    def __call__(self) -> str | None: ...

class BracesBuffer(IndentedBuffer):
    def indent(self, offset: int = ...) -> contextlib.AbstractContextManager[None]: ...

class InplacedBuffer(NamedTuple):
    inner_name: str
    other_names: list[str]

@dataclasses.dataclass
class ArgName:
    name: str
    is_constexpr: bool = ...
    def full_name(self) -> str: ...

class RemovedArg: ...

REMOVED = ...

class KernelArgs:
    def __init__(self) -> None: ...
    def input(self, name: str) -> str: ...
    def output(self, name: str) -> str: ...
    def make_inplace(self, input_name: str, output_name: str) -> None: ...
    def workspace(self, nbytes: sympy.Expr, zero_fill: bool) -> tuple[str, int]: ...
    def semaphores(self, min_size: sympy.Expr) -> str: ...
    def seed_offset(self, name: str, value: int) -> str: ...
    def size(self, name: sympy.Symbol) -> str: ...
    def call_names(self) -> Iterator[str]: ...
    def arg_name(self, name: str) -> str | None: ...
    def wrap_ptr_arg(self, buf: str, dtype: torch.dtype) -> str: ...
    def wrap_size_arg(self, size: SymbolLike) -> str: ...
    def cpp_argdefs(
        self, dtype_to_cpp_type: dict[torch.dtype, str] | None = ...
    ) -> tuple[list[str], list[str], list[str]]: ...
    def python_argdefs(self) -> tuple[list[ArgName], list[str], list[KernelArgType], list[Any]]: ...
    def aliases(self) -> Iterator[tuple[str, str]]: ...
    def is_removed(self, name: str) -> bool: ...
    def live_output_buffers(self) -> OrderedSet[str]: ...

class CSEVariable:
    def __init__(
        self, name: str, bounds: ValueRanges[Any], dtype: torch.dtype | None = ..., shape: BlockShapeType = ...
    ) -> None: ...
    def __hash__(self) -> int: ...
    def __eq__(self, other: object) -> bool: ...
    def update_on_args(self, name: str, args: Any, kwargs: Any) -> None: ...

AugmentedKeyT = TypeVar("AugmentedKeyT", default=str)
CSEVariableType = TypeVar("CSEVariableType", bound=CSEVariable, default=CSEVariable)
if TYPE_CHECKING:
    type ReductionCacheKey = tuple[torch.dtype, ReductionType, CSEVariable | tuple[CSEVariable, ...]]

class CSE[CSEVariableType: CSEVariable = CSEVariable, AugmentedKeyT = str]:
    def __init__(
        self,
        prefix: str = ...,
        suffix: str = ...,
        name_prefix: str = ...,
        iter_buffers: itertools.count[int] | None = ...,
        store_cache: MutableMapping[str, CSEVariableType] | None = ...,
        reduction_cache: MutableMapping[ReductionCacheKey, CSEVariableType] | None = ...,
        varname_map: dict[str, CSEVariableType] | None = ...,
    ) -> None: ...
    def invalidate(self, keep_vars: OrderedSet[CSEVariable]) -> None: ...
    def clone(self) -> Self: ...
    def scoped_copy(self) -> Self: ...
    def augment_key(self, cache_key: str) -> AugmentedKeyT: ...
    def put(self, cache_key: str, val: CSEVariableType) -> None: ...
    def contains(self, cache_key: str) -> bool: ...
    def try_get(self, cache_key: str) -> CSEVariableType | None: ...
    def get(self, cache_key: str) -> CSEVariableType: ...
    def generate(
        self,
        buffer: IndentedBuffer,
        expr: str | CSEVariable | OpsValue | IndentedBuffer | DeferredLineBase,
        *,
        bounds: ValueRanges[Any] = ...,
        write: bool = ...,
        assignment: bool = ...,
        dtype: torch.dtype | None = ...,
        shape: BlockShapeType = ...,
    ) -> CSEVariableType: ...
    def newvar(
        self, bounds: ValueRanges[Any] = ..., dtype: torch.dtype | None = ..., shape: BlockShapeType = ...
    ) -> CSEVariableType: ...
    def namedvar(
        self, name: str, bounds: ValueRanges[Any] = ..., dtype: torch.dtype | None = ..., shape: BlockShapeType = ...
    ) -> CSEVariableType: ...

class CodeGen:
    def __init__(self) -> None: ...
    def __enter__(self) -> Self: ...
    def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None: ...

class Kernel[CSEVariableType: CSEVariable = CSEVariable](CodeGen):
    newvar_prefix: str = ...
    suffix: str = ...
    overrides: Callable[[], OpsHandler[Any]] | None = ...
    def __init__(self, args: KernelArgs | None = ..., increase_kernel_count: bool = ...) -> None: ...
    @contextlib.contextmanager
    def set_current_node(self, node: SchedulerNode) -> Iterator[None]: ...
    @contextlib.contextmanager
    def swap_buffers(
        self, lb: IndentedBuffer, cb: IndentedBuffer | None = ..., sb: IndentedBuffer | None = ...
    ) -> Iterator[None]: ...
    def load(self, name: str, index: sympy.Expr) -> CSEVariable: ...
    def indirect_load(self, name: str, index: sympy.Expr) -> CSEVariable: ...
    def store_reduction(self, name: str, index: sympy.Expr, value: CSEVariable) -> None: ...
    def store(self, name: str, index: sympy.Expr, value: CSEVariable, mode: StoreMode = ...) -> None: ...
    def reduction(
        self,
        dtype: torch.dtype,
        src_dtype: torch.dtype,
        reduction_type: ReductionType,
        value: CSEVariable | tuple[CSEVariable, ...],
    ) -> CSEVariable | tuple[CSEVariable, ...]: ...
    def scan(
        self,
        dtypes: tuple[torch.dtype, ...],
        combine_fn: Callable[[tuple[CSEVariable, ...], tuple[CSEVariable, ...]], tuple[CSEVariable, ...]],
        values: tuple[CSEVariable, ...],
    ) -> tuple[CSEVariable, ...]: ...
    def sort(
        self, dtypes: tuple[torch.dtype, ...], values: tuple[CSEVariable, ...], stable: bool, descending: bool
    ) -> tuple[CSEVariable, ...]: ...
    def var_ranges(self) -> dict[sympy.Symbol, sympy.Expr]: ...
    def bucketize(
        self,
        values: CSEVariable,
        boundaries: tuple[str, sympy.Expr, sympy.Expr, sympy.Expr],
        boundary_indices: CSEVariable,
        indexing_dtype: torch.dtype,
        right: bool,
        sorter: tuple[str, sympy.Expr] | None = ...,
        sorter_indices: CSEVariable | None = ...,
    ) -> CSEVariable: ...
    @property
    def assert_function(self) -> str: ...
    def indirect_assert(
        self, var: CSEVariable | str, lower: str | None, upper: str | None, mask: CSEVariable | str | None = ...
    ) -> str: ...
    def check_bounds(self, expr: sympy.Expr, size: sympy.Expr, lower: bool, upper: bool) -> None: ...
    def index_to_str(self, index: sympy.Expr) -> str: ...
    def __enter__(self) -> Self: ...
    def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None: ...
    def remove_kernel_local_buffers(self) -> None: ...
    def remove_buffer(self, name: str) -> None: ...
    def remove_inplace_buffer(self, name: str) -> None: ...
    def rename_indexing(self, index: list[sympy.Expr] | tuple[sympy.Expr, ...] | sympy.Expr) -> sympy.Expr: ...
    def create_cse_var(self, *args: Any, **kwargs: Any) -> CSEVariable: ...
    def arg_name(self, node: IRNode) -> str | None: ...

@dataclasses.dataclass
class OptimizationContext:
    key: ClassVar[str] = ...
    dtype: torch.dtype | None = ...
    ops_name: str = ...

@functools.cache
def jinja2_env() -> Any: ...

class KernelTemplate:
    @staticmethod
    def indent_except_first(source: str, num_indents: int, indents_spacing: int = ...) -> str: ...
    def __init__(self, name: str) -> None: ...
    @property
    def uid(self) -> str: ...
    def choice_or_none(self, **kwargs: Any) -> ChoiceCaller | None: ...
    def maybe_append_choice(self, choices: list[Any], **kwargs: Any) -> NotImplementedError | None: ...
    def generate(self, **kwargs: Any) -> ChoiceCaller: ...

class CSEProxy(DefaultHandler):
    name = ...
    def __init__(self, kernel: Kernel[Any], parent_handler: OpsHandler[Any]) -> None: ...
    def indirect_indexing(
        self, var: CSEVariable, size: sympy.Expr | int, check: bool = ..., wrap_neg: bool = ...
    ) -> sympy.Symbol: ...
    def check_bounds(self, expr: sympy.Expr, size: sympy.Expr, lower: bool, upper: bool) -> None: ...
    def load(self, name: str, index: sympy.Expr) -> CSEVariable: ...
    def store(self, name: str, index: sympy.Expr, value: CSEVariable, mode: StoreMode = ...) -> None: ...
    def store_reduction(self, name: str, index: sympy.Expr, value: CSEVariable) -> None: ...
    def reduction(
        self,
        dtype: torch.dtype,
        src_dtype: torch.dtype,
        reduction_type: ReductionType,
        value: CSEVariable | tuple[CSEVariable, ...],
    ) -> CSEVariable | tuple[CSEVariable, ...]: ...
    def scan(
        self,
        dtypes: tuple[torch.dtype, ...],
        combine_fn: Callable[[tuple[CSEVariable, ...], tuple[CSEVariable, ...]], tuple[CSEVariable, ...]],
        values: tuple[CSEVariable, ...],
    ) -> tuple[CSEVariable, ...]: ...
    def sort(
        self, dtypes: tuple[torch.dtype, ...], values: tuple[CSEVariable, ...], stable: bool, descending: bool
    ) -> tuple[CSEVariable, ...]: ...
    def bucketize(
        self,
        values: CSEVariable,
        boundaries: tuple[str, sympy.Expr, sympy.Expr, sympy.Expr],
        boundary_indices: CSEVariable,
        indexing_dtype: torch.dtype,
        right: bool,
        sorter: tuple[str, sympy.Expr] | None = ...,
        sorter_indices: CSEVariable | None = ...,
    ) -> CSEVariable: ...
