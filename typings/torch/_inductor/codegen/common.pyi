import contextlib
import dataclasses
import enum
import functools
import itertools
import sympy
import torch
import torch.fx
from abc import ABC, abstractmethod
from enum import Enum
from typing import Any, Callable, ClassVar, Generic, NamedTuple, Optional, TYPE_CHECKING, Union, TypeAlias
from typing_extensions import Self, TypeVar
from torch._prims_common import ELEMENTWISE_TYPE_PROMOTION_KIND
from torch.utils._config_module import ConfigModule
from torch.utils._ordered_set import OrderedSet
from torch.utils._sympy.printers import PythonPrinter as _PythonPrinter
from torch.utils._sympy.value_ranges import ValueRanges
from ..ops_handler import BasicMathOpsMixin, DefaultHandler
from ..shape_propagation import BlockShapeType
from ..utils import DeferredLineBase, IndentedBuffer, ir_dataclass
from ..virtualized import OpsHandler, OpsValue, ReductionType, StoreMode
from collections.abc import Iterator, MutableMapping, Sequence
from torch.fx import GraphModule
from ..custom_graph_pass import CustomGraphModulePass
from ..ir import ChoiceCaller, FixedLayout, IRNode
from ..loop_body import LoopBody
from ..scheduler import BaseScheduling, Scheduler, SchedulerNode
from .wrapper import PythonWrapperCodegen

if TYPE_CHECKING:
    SchedulingConstructor: TypeAlias = Callable[[Optional[Scheduler]], BaseScheduling]
    WrapperConstructor: TypeAlias = type[PythonWrapperCodegen]
    SymbolLike: TypeAlias = Union[str, sympy.Symbol]
    OpVarT = str
schedule_log = ...
log = ...

def data_type_logger(msg: str) -> None: ...

@dataclasses.dataclass
class FileBackedGraphModule:
    gm: GraphModule
    compiled_fn: Callable[..., Any]
    def __post_init__(self) -> None: ...
    @property
    def __file__(self) -> str: ...
    def call(self, args: list[Any]) -> Any: ...
    @property
    def value(self) -> str: ...

class WorkspaceZeroMode(enum.Enum):
    UNINITIALIZED = ...
    ZERO_ON_CALL = ...
    ZERO_PER_GRAPH = ...
    @staticmethod
    def combine(a: WorkspaceZeroMode, b: WorkspaceZeroMode) -> WorkspaceZeroMode: ...
    @staticmethod
    def from_bool(zero_fill: bool) -> WorkspaceZeroMode: ...

class CodegenSymbol(ABC):
    @abstractmethod
    def get_name(self) -> str: ...
    @abstractmethod
    def get_example(self) -> Union[torch.Tensor, sympy.Symbol]: ...

@ir_dataclass(frozen=True)
class WorkspaceArg(CodegenSymbol):
    count: sympy.Expr
    zero_mode: WorkspaceZeroMode
    device: torch.device
    outer_name: str
    inner_name: str = ...
    dtype: torch.dtype = ...
    @staticmethod
    def unique_name(prefix: str = ...) -> str: ...
    @staticmethod
    def can_join(a: WorkspaceArg, b: WorkspaceArg) -> bool: ...
    @staticmethod
    def join(a: WorkspaceArg, b: WorkspaceArg) -> WorkspaceArg: ...
    @staticmethod
    def maximum(a: WorkspaceArg, b: WorkspaceArg) -> WorkspaceArg: ...
    def get_device(self) -> torch.device: ...

    get_device_or_error = ...
    def get_dtype(self) -> torch.dtype: ...
    def get_example(self) -> Union[torch.Tensor, sympy.Symbol]: ...
    def get_layout(self) -> FixedLayout: ...
    @property
    def layout(self) -> FixedLayout: ...

    get_output_spec = ...
    maybe_get_output_spec = ...
    maybe_get_layout = ...
    def get_offset(self) -> sympy.Expr: ...
    def get_size(self) -> list[sympy.Expr]: ...
    def get_stride(self) -> list[sympy.Expr]: ...
    def get_name(self) -> str: ...
    def get_is_pinned(self) -> bool: ...
    def get_inputs_that_alias_output(self) -> list[str]: ...

class TritonScratchWorkspace:
    def __init__(self, size: int, generate_dtype_str: Callable[..., str]) -> None: ...
    def generate_dtype_str(self) -> str: ...

@dataclasses.dataclass
class TensorArg:
    name: str
    buffer: str
    dtype: torch.dtype
    offset: sympy.Expr = ...
    alias_of: Optional[str] = ...

@dataclasses.dataclass
class SizeArg:
    name: str
    expr: sympy.Expr
    @property
    def alias_of(self) -> Optional[str]: ...

@dataclasses.dataclass
class ConstexprArg:
    name: str

@dataclasses.dataclass
class TMADescriptorArg:
    name: str
    api_type: str
    block_shape: Optional[list[sympy.Expr]]
    dtype: Optional[torch.dtype]

@dataclasses.dataclass
class DeviceCodegen:
    scheduling: SchedulingConstructor
    wrapper_codegen: WrapperConstructor
    cpp_wrapper_codegen: Optional[WrapperConstructor] = ...
    fx_wrapper_codegen: Optional[WrapperConstructor] = ...

KernelArgType: TypeAlias = Union[WorkspaceArg, TensorArg, SizeArg, TMADescriptorArg, ConstexprArg]
device_codegens: dict[str, DeviceCodegen] = ...

class DeviceOpOverrides:
    def import_get_raw_stream_as(self, name: str) -> str: ...
    def set_device(self, device_idx: int) -> str: ...
    def synchronize(self) -> str: ...
    def device_guard(self, device_idx: int) -> str: ...
    def cpp_device_guard(self) -> str: ...
    def cpp_aoti_device_guard(self) -> str: ...
    def cpp_stream_guard(self) -> str: ...
    def cpp_aoti_stream_guard(self) -> str: ...
    def cpp_getStreamFromExternal(self) -> str: ...
    def kernel_header(self) -> str: ...
    def kernel_driver(self) -> str: ...
    def cpp_stream_type(self) -> str: ...
    def aoti_get_stream(self) -> str: ...
    def cpp_kernel_type(self) -> str: ...
    def cpp_device_ptr(self) -> str: ...
    def tma_descriptor_helpers(self) -> str: ...
    def cpp_scratch(
        self, idx: int, workspace: TritonScratchWorkspace, prefix: Optional[str] = ...
    ) -> Optional[tuple[list[str], str]]: ...

device_op_overrides_dict: dict[str, DeviceOpOverrides] = ...
custom_backend_passes: dict[str, Optional[CustomGraphModulePass]] = ...
custom_backend_codegen_configs: dict[str, Optional[ConfigModule]] = ...

def register_backend_for_device(
    device: str,
    device_scheduling: SchedulingConstructor,
    device_wrapper_codegen: WrapperConstructor,
    device_cpp_wrapper_codegen: Optional[WrapperConstructor] = ...,
    device_fx_wrapper_codegen: Optional[WrapperConstructor] = ...,
    device_custom_pass: Optional[CustomGraphModulePass] = ...,
    device_custom_config: Optional[ConfigModule] = ...,
) -> None: ...

class BackendFeature(Enum):
    FOREACH = ...
    BUCKETIZE = ...
    INPLACE_BUFFERS = ...
    MASKED_SCATTER_WITH_INDEX = ...
    SCAN = ...
    SORT = ...
    TUPLE_REDUCTION = ...
    PREFER_STORE_LOOP_ORDER = ...
    TRITON_TEMPLATES = ...
    REDUCE_TO_SINGLE_ELEMENT = ...

def get_backend_features(device: Union[torch.device, str, None]) -> OrderedSet[BackendFeature]: ...
def has_backend_feature(device: Union[torch.device, str, None], feature: BackendFeature) -> bool: ...
def get_scheduling_for_device(device: str) -> Optional[SchedulingConstructor]: ...
def get_wrapper_codegen_for_device(
    device: str, cpp_wrapper: bool = ..., fx_wrapper: bool = ...
) -> Optional[WrapperConstructor]: ...
def get_custom_backend_pass_for_device(device: str) -> Optional[CustomGraphModulePass]: ...
def get_custom_backend_config_for_device(device: str) -> Optional[ConfigModule]: ...
@functools.cache
def init_backend_registration() -> None: ...
def index_prevent_reordering(
    index: Sequence[sympy.Expr], index_vars: Sequence[sympy.Expr], sizes: Sequence[sympy.Expr]
) -> list[sympy.Expr]: ...
def register_device_op_overrides(device: str, device_op_overrides: DeviceOpOverrides) -> None: ...
def get_device_op_overrides(device: str) -> DeviceOpOverrides: ...

DTYPE_TO_COMPUTATION_DTYPE: dict[torch.dtype, torch.dtype] = ...

def deduce_output_dtype_by_name(op_name: str, *args: Any, **kwargs: Any) -> Optional[torch.dtype]: ...
def check_dtype(buffer: IndentedBuffer, var: CSEVariableType, dtype: torch.dtype) -> None: ...

class DataTypePropagation:
    def __init__(self, body: LoopBody) -> None: ...
    def deduce_node_dtype_by_inputs(self, node: torch.fx.Node) -> Optional[torch.dtype]: ...
    def deduce_node_dtype_by_subgraph(self, node: torch.fx.Node) -> torch.dtype: ...
    def deduce_node_dtype(self, node: torch.fx.Node) -> Optional[torch.dtype]: ...
    def propagate_graph(self, graph: torch.fx.Graph) -> Optional[torch.dtype]: ...
    def propagate(self) -> Optional[torch.dtype]: ...
    @classmethod
    def propagate_loopbody(cls, body: LoopBody) -> Optional[torch.dtype]: ...
    @classmethod
    def propagate_scheduler_node(cls, node: SchedulerNode) -> Optional[torch.dtype]: ...

class PythonPrinter(_PythonPrinter):
    def doprint(self, expr: sympy.Expr, *, simplify: bool = ..., p: bool = ...) -> str: ...
    def parenthesize(self, item: sympy.Expr, level: int, strict: bool = ...) -> str: ...

class OpDecompositions:
    @staticmethod
    def identity(value: OpVarT) -> OpVarT: ...
    @staticmethod
    def reciprocal(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def square(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def erfc(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def erfcx(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def expm1(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def log10(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def log2(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def exp2(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def log1p(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def sigmoid(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def relu(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def fma(x: OpVarT, y: OpVarT, z: OpVarT) -> OpVarT: ...
    @staticmethod
    def floor_to_int(a: OpVarT, dtype: torch.dtype) -> OpVarT: ...
    @staticmethod
    def ceil_to_int(a: OpVarT, dtype: torch.dtype) -> OpVarT: ...
    @staticmethod
    def trunc_to_int(a: OpVarT, dtype: torch.dtype) -> OpVarT: ...
    @staticmethod
    def remainder(a: OpVarT, b: OpVarT) -> OpVarT: ...
    @staticmethod
    def round_to_int(a: OpVarT, dtype: torch.dtype) -> OpVarT: ...

_RE_PAREN_NOT_NEEDED = ...

class OpOverrides(BasicMathOpsMixin, OpDecompositions, OpsHandler[Any]):
    @staticmethod
    def paren(string: OpVarT) -> OpVarT: ...
    @staticmethod
    def constant(value: Union[bool, float], dtype: torch.dtype) -> OpVarT: ...
    @staticmethod
    def bitwise_not(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def logical_not(a: OpVarT) -> OpVarT: ...
    @staticmethod
    def bitwise_and(x: OpVarT, y: OpVarT) -> OpVarT: ...
    @staticmethod
    def bitwise_or(x: OpVarT, y: OpVarT) -> OpVarT: ...
    @staticmethod
    def bitwise_xor(x: OpVarT, y: OpVarT) -> OpVarT: ...
    @staticmethod
    def bitwise_left_shift(x: OpVarT, y: OpVarT) -> OpVarT: ...
    @staticmethod
    def bitwise_right_shift(x: OpVarT, y: OpVarT) -> OpVarT: ...
    @staticmethod
    def int_truediv(a: OpVarT, b: OpVarT) -> OpVarT: ...
    @staticmethod
    def load_seed(name: str, offset: OpVarT) -> OpVarT: ...
    def indirect_indexing(
        self, var: OpVarT, size: Union[sympy.Expr, int], check: bool = ..., wrap_neg: bool = ...
    ) -> sympy.Symbol: ...
    def check_bounds(self, expr: sympy.Expr, size: sympy.Expr, lower: bool, upper: bool) -> None: ...
    def load(self, name: str, index: sympy.Expr) -> OpVarT: ...
    def store(self, name: str, index: sympy.Expr, value: OpVarT, mode: StoreMode = ...) -> None: ...
    def store_reduction(self, name: str, index: sympy.Expr, value: OpVarT) -> None: ...
    def reduction(
        self,
        dtype: torch.dtype,
        src_dtype: torch.dtype,
        reduction_type: ReductionType,
        value: Union[OpVarT, tuple[OpVarT, ...]],
    ) -> Union[OpVarT, tuple[OpVarT, ...]]: ...
    def scan(
        self,
        dtypes: tuple[torch.dtype, ...],
        combine_fn: Callable[
            [tuple[OpVarT, ...], tuple[OpVarT, ...]],
            tuple[OpVarT, ...],
        ],
        values: tuple[OpVarT, ...],
    ) -> tuple[OpVarT, ...]: ...
    def sort(
        self, dtypes: tuple[torch.dtype, ...], values: tuple[OpVarT, ...], stable: bool, descending: bool
    ) -> tuple[OpVarT, ...]: ...
    def bucketize(
        self,
        values: OpVarT,
        boundaries: tuple[str, sympy.Expr, sympy.Expr, sympy.Expr],
        boundary_indices: OpVarT,
        indexing_dtype: torch.dtype,
        right: bool,
        sorter: Optional[tuple[str, sympy.Expr]] = ...,
        sorter_indices: Optional[OpVarT] = ...,
    ) -> OpVarT: ...
    def halide_clamp(self, value: OpVarT, size: sympy.Expr, check: bool) -> OpVarT: ...
    def inline_asm_elementwise(
        self,
        *inputs: OpVarT,
        asm: str,
        constraints: Optional[str] = ...,
        dtype: torch.dtype = ...,
        is_pure: bool = ...,
        pack: int = ...,
    ) -> OpVarT: ...
    def output(self, *args: OpVarT) -> None: ...
    def placeholder(self, index: int) -> OpVarT: ...

@dataclasses.dataclass
class OverridesData:
    name: str
    cpp: Callable[..., str]
    triton: Optional[Callable[..., str]] = ...
    cppvec: Optional[Callable[..., str]] = ...
    type_promotion_kind: ELEMENTWISE_TYPE_PROMOTION_KIND = ...
    halide: Optional[Callable[..., str]] = ...
    mps: Optional[Callable[..., str]] = ...

pointwise_overrides_data: dict[str, OverridesData] = ...

def is_buffer_removed(name: str) -> bool: ...

class DeferredLine(DeferredLineBase):
    def __init__(self, name: str, line: str) -> None: ...
    def __call__(self) -> Optional[str]: ...

class BracesBuffer(IndentedBuffer):
    def indent(self, offset: int = ...) -> contextlib.AbstractContextManager[None]: ...

class InplacedBuffer(NamedTuple):
    inner_name: str
    other_names: list[str]

@dataclasses.dataclass
class ArgName:
    name: str
    is_constexpr: bool = ...
    def full_name(self) -> str: ...

class RemovedArg: ...

REMOVED = ...

class KernelArgs:
    def __init__(self) -> None: ...
    def input(self, name: str) -> str: ...
    def output(self, name: str) -> str: ...
    def make_inplace(self, input_name: str, output_name: str) -> None: ...
    def workspace(self, nbytes: sympy.Expr, zero_fill: bool) -> tuple[str, int]: ...
    def semaphores(self, min_size: sympy.Expr) -> str: ...
    def seed_offset(self, name: str, value: int) -> str: ...
    def size(self, name: sympy.Symbol) -> str: ...
    def call_names(self) -> Iterator[str]: ...
    def arg_name(self, name: str) -> Optional[str]: ...
    def wrap_ptr_arg(self, buf: str, dtype: torch.dtype) -> str: ...
    def wrap_size_arg(self, size: SymbolLike) -> str: ...
    def cpp_argdefs(
        self, dtype_to_cpp_type: Optional[dict[torch.dtype, str]] = ...
    ) -> tuple[list[str], list[str], list[str]]: ...
    def python_argdefs(self) -> tuple[list[ArgName], list[str], list[KernelArgType], list[Any]]: ...
    def aliases(self) -> Iterator[tuple[str, str]]: ...
    def is_removed(self, name: str) -> bool: ...
    def live_output_buffers(self) -> OrderedSet[str]: ...

class CSEVariable:
    def __init__(
        self, name: str, bounds: ValueRanges[Any], dtype: Optional[torch.dtype] = ..., shape: BlockShapeType = ...
    ) -> None: ...
    def __hash__(self) -> int: ...
    def __eq__(self, other: object) -> bool: ...
    def update_on_args(self, name: str, args: Any, kwargs: Any) -> None: ...

AugmentedKeyT = TypeVar("AugmentedKeyT", default=str)
CSEVariableType = TypeVar("CSEVariableType", bound=CSEVariable, default=CSEVariable)
if TYPE_CHECKING:
    ReductionCacheKey: TypeAlias = tuple[
        torch.dtype,
        ReductionType,
        Union[CSEVariable, tuple[CSEVariable, ...]],
    ]

class CSE(Generic[CSEVariableType, AugmentedKeyT]):
    def __init__(
        self,
        prefix: str = ...,
        suffix: str = ...,
        name_prefix: str = ...,
        iter_buffers: Optional[itertools.count[int]] = ...,
        store_cache: Optional[MutableMapping[str, CSEVariableType]] = ...,
        reduction_cache: Optional[MutableMapping[ReductionCacheKey, CSEVariableType]] = ...,
        varname_map: Optional[dict[str, CSEVariableType]] = ...,
    ) -> None: ...
    def invalidate(self, keep_vars: OrderedSet[CSEVariable]) -> None: ...
    def clone(self) -> Self: ...
    def scoped_copy(self) -> Self: ...
    def augment_key(self, cache_key: str) -> AugmentedKeyT: ...
    def put(self, cache_key: str, val: CSEVariableType) -> None: ...
    def contains(self, cache_key: str) -> bool: ...
    def try_get(self, cache_key: str) -> Optional[CSEVariableType]: ...
    def get(self, cache_key: str) -> CSEVariableType: ...
    def generate(
        self,
        buffer: IndentedBuffer,
        expr: Union[str, CSEVariable, OpsValue, IndentedBuffer, DeferredLineBase],
        *,
        bounds: ValueRanges[Any] = ...,
        write: bool = ...,
        assignment: bool = ...,
        dtype: Optional[torch.dtype] = ...,
        shape: BlockShapeType = ...,
    ) -> CSEVariableType: ...
    def newvar(
        self, bounds: ValueRanges[Any] = ..., dtype: Optional[torch.dtype] = ..., shape: BlockShapeType = ...
    ) -> CSEVariableType: ...
    def namedvar(
        self, name: str, bounds: ValueRanges[Any] = ..., dtype: Optional[torch.dtype] = ..., shape: BlockShapeType = ...
    ) -> CSEVariableType: ...

class CodeGen:
    def __init__(self) -> None: ...
    def __enter__(self) -> Self: ...
    def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None: ...

class Kernel(CodeGen, Generic[CSEVariableType]):
    newvar_prefix: str = ...
    suffix: str = ...
    overrides: Optional[Callable[[], OpsHandler[Any]]] = ...
    def __init__(self, args: Optional[KernelArgs] = ..., increase_kernel_count: bool = ...) -> None: ...
    @contextlib.contextmanager
    def set_current_node(self, node: SchedulerNode) -> Iterator[None]: ...
    @contextlib.contextmanager
    def swap_buffers(
        self, lb: IndentedBuffer, cb: Optional[IndentedBuffer] = ..., sb: Optional[IndentedBuffer] = ...
    ) -> Iterator[None]: ...
    def load(self, name: str, index: sympy.Expr) -> CSEVariable: ...
    def indirect_load(self, name: str, index: sympy.Expr) -> CSEVariable: ...
    def store_reduction(self, name: str, index: sympy.Expr, value: CSEVariable) -> None: ...
    def store(self, name: str, index: sympy.Expr, value: CSEVariable, mode: StoreMode = ...) -> None: ...
    def reduction(
        self,
        dtype: torch.dtype,
        src_dtype: torch.dtype,
        reduction_type: ReductionType,
        value: Union[CSEVariable, tuple[CSEVariable, ...]],
    ) -> Union[CSEVariable, tuple[CSEVariable, ...]]: ...
    def scan(
        self,
        dtypes: tuple[torch.dtype, ...],
        combine_fn: Callable[[tuple[CSEVariable, ...], tuple[CSEVariable, ...]], tuple[CSEVariable, ...]],
        values: tuple[CSEVariable, ...],
    ) -> tuple[CSEVariable, ...]: ...
    def sort(
        self, dtypes: tuple[torch.dtype, ...], values: tuple[CSEVariable, ...], stable: bool, descending: bool
    ) -> tuple[CSEVariable, ...]: ...
    def var_ranges(self) -> dict[sympy.Symbol, sympy.Expr]: ...
    def bucketize(
        self,
        values: CSEVariable,
        boundaries: tuple[str, sympy.Expr, sympy.Expr, sympy.Expr],
        boundary_indices: CSEVariable,
        indexing_dtype: torch.dtype,
        right: bool,
        sorter: Optional[tuple[str, sympy.Expr]] = ...,
        sorter_indices: Optional[CSEVariable] = ...,
    ) -> CSEVariable: ...
    @property
    def assert_function(self) -> str: ...
    def indirect_assert(
        self,
        var: Union[CSEVariable, str],
        lower: Optional[str],
        upper: Optional[str],
        mask: Optional[Union[CSEVariable, str]] = ...,
    ) -> str: ...
    def check_bounds(self, expr: sympy.Expr, size: sympy.Expr, lower: bool, upper: bool) -> None: ...
    def index_to_str(self, index: sympy.Expr) -> str: ...
    def __enter__(self) -> Self: ...
    def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None: ...
    def remove_kernel_local_buffers(self) -> None: ...
    def remove_buffer(self, name: str) -> None: ...
    def remove_inplace_buffer(self, name: str) -> None: ...
    def rename_indexing(self, index: Union[list[sympy.Expr], tuple[sympy.Expr, ...], sympy.Expr]) -> sympy.Expr: ...
    def create_cse_var(self, *args: Any, **kwargs: Any) -> CSEVariable: ...
    def arg_name(self, node: IRNode) -> Optional[str]: ...

@dataclasses.dataclass
class OptimizationContext:
    key: ClassVar[str] = ...
    dtype: Optional[torch.dtype] = ...
    ops_name: str = ...

@functools.cache
def jinja2_env() -> Any: ...

class KernelTemplate:
    @staticmethod
    def indent_except_first(source: str, num_indents: int, indents_spacing: int = ...) -> str: ...
    def __init__(self, name: str) -> None: ...
    @property
    def uid(self) -> str: ...
    def choice_or_none(self, **kwargs: Any) -> Optional[ChoiceCaller]: ...
    def maybe_append_choice(self, choices: list[Any], **kwargs: Any) -> Optional[NotImplementedError]: ...
    def generate(self, **kwargs: Any) -> ChoiceCaller: ...

class CSEProxy(DefaultHandler):
    name = ...
    def __init__(self, kernel: Kernel[Any], parent_handler: OpsHandler[Any]) -> None: ...
    def indirect_indexing(
        self, var: CSEVariable, size: Union[sympy.Expr, int], check: bool = ..., wrap_neg: bool = ...
    ) -> sympy.Symbol: ...
    def check_bounds(self, expr: sympy.Expr, size: sympy.Expr, lower: bool, upper: bool) -> None: ...
    def load(self, name: str, index: sympy.Expr) -> CSEVariable: ...
    def store(self, name: str, index: sympy.Expr, value: CSEVariable, mode: StoreMode = ...) -> None: ...
    def store_reduction(self, name: str, index: sympy.Expr, value: CSEVariable) -> None: ...
    def reduction(
        self,
        dtype: torch.dtype,
        src_dtype: torch.dtype,
        reduction_type: ReductionType,
        value: Union[CSEVariable, tuple[CSEVariable, ...]],
    ) -> Union[CSEVariable, tuple[CSEVariable, ...]]: ...
    def scan(
        self,
        dtypes: tuple[torch.dtype, ...],
        combine_fn: Callable[
            [tuple[CSEVariable, ...], tuple[CSEVariable, ...]],
            tuple[CSEVariable, ...],
        ],
        values: tuple[CSEVariable, ...],
    ) -> tuple[CSEVariable, ...]: ...
    def sort(
        self, dtypes: tuple[torch.dtype, ...], values: tuple[CSEVariable, ...], stable: bool, descending: bool
    ) -> tuple[CSEVariable, ...]: ...
    def bucketize(
        self,
        values: CSEVariable,
        boundaries: tuple[str, sympy.Expr, sympy.Expr, sympy.Expr],
        boundary_indices: CSEVariable,
        indexing_dtype: torch.dtype,
        right: bool,
        sorter: Optional[tuple[str, sympy.Expr]] = ...,
        sorter_indices: Optional[CSEVariable] = ...,
    ) -> CSEVariable: ...
