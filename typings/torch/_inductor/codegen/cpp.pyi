import contextlib
import dataclasses
import functools
from collections.abc import Sequence
from enum import Enum

import sympy
import torch
from torch.utils._ordered_set import OrderedSet

from ..loop_body import LoopBody
from ..scheduler import BaseSchedulerNode, BaseScheduling, FusedSchedulerNode, Scheduler, SchedulerNode
from ..utils import cache_on_self
from .common import BackendFeature, BracesBuffer, CSEVariable, IndentedBuffer, Kernel, OpOverrides
from .cpp_utils import CppCSEVariable

_IS_WINDOWS = ...

@functools.cache
def get_export_declaration(): ...

schedule_log = ...
NATIVE_OMP_RTYPES = ...
RTYPE_TO_CPP = ...
VECTORIZABLE_RTYPES = ...
PYTHON_TO_CPP = ...
CONTAINER_PYTHON_TO_CPP = ...
DTYPE_LOWP_FP = ...
VECTORIZABLE_DTYPES: list[torch.dtype] = ...
MASKED_VECTORIZABLE_DTYPES: list[torch.dtype] = ...

def reduction_init(reduction_type, dtype): ...
def reduction_acc_type(reduction_type, dtype): ...
def reduction_combine(
    reduction_type, var, next_value, helper_val=..., index: sympy.Symbol | None = ..., src_dtype=...
): ...
def reduction_project(reduction_type, acc): ...
def move_code_under_inner_loop(
    code: IndentedBuffer, iter_var: sympy.Expr, new_iter_var: str, loop_start: sympy.Expr, loop_end: sympy.Expr
) -> BracesBuffer:
    r"""
    f(iter_var) is transformed to f(new_iter_var) under the inner loop
      \/
    for (new_iter_var = loop_start; new_iter_var < loop_end; new_iter_var++) {
        f(new_iter_var)
    }
    Please be careful while using this function,
    as the variable defined in f(iter_var) will be invalid outside the for loop.
    For example:
    auto tmp0 = in_ptr[x0]; ->
    for (new_x0 = start; new_x0 < end; new_x0++){
        auto tmp0 = in_ptr[new_x0];
    }
    The tmp0 is invalid outside the loop.
    """

def reduction_prefix_array(
    acc_var: str | CSEVariable, acc_type: str, reduction_type: str, dtype: torch.dtype, len: str | int, init_fn
):
    """
    MSVC don't support dynamic array(VLA). So we use std::unique_ptr here.
    Ref: https://stackoverflow.com/questions/56555406/creating-dynamic-sized-array-using-msvc-c-compiler
    MSVC is the only one compiler without VLA. support. Since MSVC can't get good performance here.
    We just use unique_ptr make it works on MSVC.
    For other compilers, we continue to use VLA to get best performance.
    """

def replace_acc_name(buffer: IndentedBuffer, name: str, new_name: str): ...
def replace_cascade_sum_with_add(buffer: IndentedBuffer):
    """Replaces `acc = cascade_sum_combine(value, ...)` with `acc = acc + value;`"""

@functools.lru_cache
def stride_at(index: sympy.Expr, var: sympy.Symbol): ...
@functools.lru_cache
def simplify_index_in_vec_range(index: sympy.Expr, var: sympy.Expr, vec_length: int):
    """
    Simplifies the index expression within the range of a vectorized loop.
    Given a vectorized loop variable `var` in the range of a loop with `vec_length`,
    this function transforms the `index` into an equivalent form. It handles
    simplifications for cases where `var` can be expressed as `vec_length * a + b`,
    where `b` ranges from 0 to `vec_length - 1`. The function reduces occurrences
    of `FloorDiv` and `ModularIndexing` in the `index` with best-effort optimizations.

    NOTE:
    The simplified index expression is intended for analysis purposes only, not
    for code generation. It replaces `FloorDiv` and `ModularIndexing` with free variables
    which are not dependent on the loop variable `var` in the vectorized range. Check
    https://github.com/pytorch/pytorch/pull/117221#discussion_r1449746217 for more details.

    Examples:
    1. If `var` is `x3` and `vec_length` is 16, and `x3 = 16*a + b`, then
       `FloorDiv(x3, div)` or `ModularIndexing(x3, div, mod)` becomes a free variable
       when `div` is divisible by 16.
    2. `ModularIndexing(x3, 1, mod)` can be simplified to `x3 + c` where `c` is a free
       variable when `mod` is divisible by 16.
    """

@functools.lru_cache
def stride_at_vec_range(index: sympy.Expr, var: sympy.Symbol, vec_length: int | None = ...): ...

@dataclasses.dataclass
class ParallelDepth:
    """
    A class representing parallel depth.
    Includes the starting depth of parallelism and the depth of parallelism.
    """

    parallel_depth: int
    start_depth: int

class OuterLoopFusedSchedulerNode(FusedSchedulerNode):
    @classmethod
    def fuse(cls, node1: BaseSchedulerNode, node2: BaseSchedulerNode, outer_loop_fusion_depth): ...
    def __init__(
        self, scheduler: Scheduler, outer_fused_nodes: list[FusedSchedulerNode | SchedulerNode], outer_loop_fusion_depth
    ) -> None: ...
    def get_outer_nodes(self): ...
    def check_outer_fusion_loop_level_attr(self, cpp_kernel_proxy_list, outer_loop_fusion_depth): ...
    def merge_outer_fusion_kernels(self, cpp_kernel_proxy_list): ...

class RecordOptimizationContext:
    def __init__(self, func_name: str = ...) -> None: ...
    def __enter__(self): ...
    def __exit__(self, exc_type, exc_val, exc_tb): ...
    def get_opt_ctx(self): ...
    def get_fx_node(self): ...

def decltype_promoted(*args): ...

class CppOverrides(OpOverrides):
    """Map element-wise ops to C++"""
    @staticmethod
    def add(a, b): ...
    @staticmethod
    def sub(a, b): ...
    @staticmethod
    def mul(a, b): ...
    @staticmethod
    def to_dtype(x, dtype, src_dtype=..., use_compute_types=...): ...
    @staticmethod
    def to_dtype_bitcast(x, dtype, src_dtype): ...
    @staticmethod
    def abs(x): ...
    @staticmethod
    def sin(x): ...
    @staticmethod
    def cos(x): ...
    @staticmethod
    def neg(x): ...
    @staticmethod
    def exp(x): ...
    @staticmethod
    def exp2(x): ...
    @staticmethod
    def expm1(x): ...
    @staticmethod
    def erf(x): ...
    @staticmethod
    def erfc(x): ...
    @staticmethod
    def erfinv(x): ...
    @staticmethod
    def sqrt(x): ...
    @staticmethod
    def rsqrt(x): ...
    @staticmethod
    def log1p(x): ...
    @staticmethod
    def tan(x): ...
    @staticmethod
    def tanh(x): ...
    @staticmethod
    def signbit(x):
        """
        On windows std::signbit only support float type.
        Ref: https://learn.microsoft.com/en-us/cpp/c-runtime-library/reference/signbit?view=msvc-170
        """
    @staticmethod
    def pow(a, b): ...
    @staticmethod
    def log(x): ...
    @staticmethod
    def round(x): ...
    @staticmethod
    def floor(x): ...
    @staticmethod
    def floordiv(a, b): ...
    @staticmethod
    def ceil(x): ...
    @staticmethod
    def trunc(x): ...
    @staticmethod
    def truncdiv(a, b): ...
    @staticmethod
    def fmod(a, b): ...
    @staticmethod
    def isinf(x): ...
    @staticmethod
    def isnan(x): ...
    @staticmethod
    def lgamma(x): ...
    @staticmethod
    def acos(x): ...
    @staticmethod
    def acosh(x): ...
    @staticmethod
    def cosh(x): ...
    @staticmethod
    def sinh(x): ...
    @staticmethod
    def asin(x): ...
    @staticmethod
    def asinh(x): ...
    @staticmethod
    def atan2(x, y): ...
    @staticmethod
    def atan(x): ...
    @staticmethod
    def atanh(x): ...
    @staticmethod
    def copysign(x, y): ...
    @staticmethod
    def frexp(x): ...
    @staticmethod
    def hypot(x, y): ...
    @staticmethod
    def log10(x): ...
    @staticmethod
    def log2(x): ...
    @staticmethod
    def nextafter(x, y): ...
    @staticmethod
    def relu(x): ...
    @staticmethod
    def minimum(a, b): ...
    @staticmethod
    def maximum(a, b): ...
    @staticmethod
    def where(a, b, c): ...
    @staticmethod
    def mod(a, b): ...
    @staticmethod
    def constant(val, dtype): ...
    @staticmethod
    def index_expr(expr, dtype): ...
    @staticmethod
    def masked(mask, body, other): ...
    @staticmethod
    def logical_and(a, b): ...
    @staticmethod
    def logical_not(a): ...
    @staticmethod
    def logical_or(a, b): ...
    @staticmethod
    def logical_xor(a, b): ...
    @staticmethod
    def bitwise_and(a, b): ...
    @staticmethod
    def bitwise_not(a): ...
    @staticmethod
    def bitwise_or(a, b): ...
    @staticmethod
    def bitwise_xor(a, b): ...
    @staticmethod
    def bitwise_left_shift(a, b): ...
    @staticmethod
    def bitwise_right_shift(a, b): ...
    @staticmethod
    def rand(seed: sympy.Expr, offset: sympy.Expr): ...
    @staticmethod
    def randn(seed: sympy.Expr, offset: sympy.Expr): ...
    @staticmethod
    def randint64(seed: sympy.Expr, offset: sympy.Expr, low, high): ...
    @staticmethod
    def sigmoid(x): ...
    @staticmethod
    def sign(x): ...
    @staticmethod
    def device_assert_async(cond, msg): ...

class CppVecOverrides(CppOverrides):
    """Map element-wise ops to aten vectorization C++"""
    def __new__(cls, *args, **kargs): ...
    @staticmethod
    def add(a, b): ...
    @staticmethod
    def sub(a, b): ...
    @staticmethod
    def mul(a, b): ...
    @staticmethod
    def truediv(a, b): ...
    @staticmethod
    def abs(x): ...
    @staticmethod
    def sin(x): ...
    @staticmethod
    def cos(x): ...
    @staticmethod
    def exp(x): ...
    @staticmethod
    def exp2(x): ...
    @staticmethod
    def expm1(x): ...
    @staticmethod
    def erf(x): ...
    @staticmethod
    def erfc(x): ...
    @staticmethod
    def erfinv(x): ...
    @staticmethod
    def sqrt(x): ...
    @staticmethod
    def eq(x, y): ...
    @staticmethod
    def ne(x, y): ...
    @staticmethod
    def lt(x, y): ...
    @staticmethod
    def gt(x, y): ...
    @staticmethod
    def le(x, y): ...
    @staticmethod
    def ge(x, y): ...
    @staticmethod
    def and_(x, y): ...
    @staticmethod
    def rsqrt(x): ...
    @staticmethod
    def pow(a, b): ...
    @staticmethod
    def log(x): ...
    @staticmethod
    def round(x): ...
    @staticmethod
    def floor(x): ...
    @staticmethod
    def ceil(x): ...
    @staticmethod
    def trunc(x): ...
    @staticmethod
    def fmod(a, b): ...
    @staticmethod
    def lgamma(x): ...
    @staticmethod
    def logical_and(a, b): ...
    @staticmethod
    def logical_not(a): ...
    @staticmethod
    def logical_or(a, b): ...
    @staticmethod
    def logical_xor(a, b): ...
    @staticmethod
    def bitwise_and(a, b): ...
    @staticmethod
    def bitwise_not(a): ...
    @staticmethod
    def bitwise_or(a, b): ...
    @staticmethod
    def bitwise_xor(a, b): ...
    @staticmethod
    def bitwise_left_shift(a, b): ...
    @staticmethod
    def bitwise_right_shift(a, b): ...
    @staticmethod
    def load_seed(name, offset): ...
    @staticmethod
    def rand(seed, offset): ...
    @staticmethod
    def randn(seed, offset): ...
    @staticmethod
    def randint64(seed, offset, low, high): ...
    @staticmethod
    def remainder(a, b): ...
    @staticmethod
    def tan(a): ...
    @staticmethod
    def tanh(a): ...
    @staticmethod
    def reciprocal(a): ...
    @staticmethod
    def atan(x): ...
    @staticmethod
    def acos(x): ...
    @staticmethod
    def asin(x): ...
    @staticmethod
    def cosh(x): ...
    @staticmethod
    def sinh(x): ...
    @staticmethod
    def log10(x): ...
    @staticmethod
    def log2(x): ...
    @staticmethod
    def nextafter(x, y): ...
    @staticmethod
    def copysign(a, b): ...
    @staticmethod
    def atan2(a, b): ...
    @staticmethod
    def hypot(a, b): ...
    @staticmethod
    def atanh(x): ...
    @staticmethod
    def asinh(x): ...
    @staticmethod
    def acosh(x): ...
    @staticmethod
    def relu(x): ...
    @staticmethod
    def sigmoid(x): ...
    @staticmethod
    def neg(x): ...
    @staticmethod
    def floordiv(a, b): ...
    @staticmethod
    def truncdiv(a, b): ...
    @staticmethod
    def minimum(a, b): ...
    @staticmethod
    def maximum(a, b): ...
    @staticmethod
    def square(a): ...
    @staticmethod
    def where(a, b, c): ...
    @staticmethod
    def sign(x): ...
    @staticmethod
    def to_dtype(x, dtype, src_dtype=..., use_compute_dtypes=...): ...
    @staticmethod
    def log1p(x): ...
    @staticmethod
    def masked(mask, body, other): ...
    @staticmethod
    def index_expr(expr, dtype): ...
    @staticmethod
    def frexp(x): ...

class CppTile2DOverrides(CppVecOverrides):
    @staticmethod
    def index_expr(expr, dtype): ...

class CppKernel(Kernel):
    """
    Base class for C++ kernel code generation in PyTorch Inductor.
    This class is responsible for generating C++ code from the intermediate representation.

    Args:
        args: Kernel arguments used for code generation
        num_threads: Number of threads for parallel execution
    """

    overrides = CppOverrides
    sexpr = ...
    newvar_prefix = ...
    suffix = ...
    def __init__(self, args, num_threads) -> None: ...
    def update_stores_with_parallel_reduction(self): ...
    def gen_body(self, code: BracesBuffer | None = ...): ...
    @contextlib.contextmanager
    def masked(self, mask):
        """Context manager to add an additional mask to loads and stores."""
    def scale_index_with_offset(self, index: sympy.Expr, scale=..., itervar_idx=..., offset=...): ...
    def index_to_str(self, index: sympy.Expr) -> str:
        """
        Convert an index expr to a string that can be used in cpp code.
        e.g. a sympy expression "s2" may actually appear as "ks1" in the cpp kernel.
        """
    def index_indirect_depends_on(self, index: sympy.Expr, itervar: sympy.Symbol):
        """Check if an index has free symbol CppCSEVariable that depends on `itervar`."""
    def index_depends_on(self, index: sympy.Expr, itervar: sympy.Symbol): ...
    def var_ranges(self): ...
    def check_bounds(self, expr: sympy.Expr, size: sympy.Expr, lower: bool, upper: bool): ...
    def load(self, name: str, index: sympy.Expr): ...
    def store(self, name, index, value, mode=...): ...
    def finalize_reduction_prefix(self, size: int | None = ...): ...
    def need_use_acc_helper(self, reduction_type, dtype, use_scalar): ...
    def need_use_acc_helper(self, reduction_type, dtype, use_scalar): ...
    def reduction(self, dtype, src_dtype, reduction_type, value): ...
    def store_reduction(self, name, index, value): ...
    def set_ranges(self, lengths, reduction_lengths): ...
    def size_hint(self): ...
    def codegen_loops_impl(self, loop_nest, code, worksharing): ...
    def codegen_loops(self, code, worksharing): ...
    @property
    def assert_function(self) -> str: ...
    def decide_parallel_depth(self, max_parallel_depth, threads): ...
    @contextlib.contextmanager
    def write_to_suffix(self): ...
    def create_cse_var(self, *args, **kwargs): ...
    def get_to_dtype_expr(self, src, dtype, src_dtype): ...
    def cache_dtype_convert(self, dst, dst_dtype, src, src_dtype): ...
    def codegen_conditions(self, code: BracesBuffer, prefix: str | None = ..., var: sympy.Symbol | None = ...): ...

class CppVecKernel(CppKernel):
    overrides = CppVecOverrides
    def __init__(self, args, num_threads, tiling_factor, tiling_idx, tail_size=...) -> None: ...
    def load(self, name: str, index: sympy.Expr): ...
    def store(self, name, index, value, mode=...): ...
    def reduction(self, dtype, src_dtype, reduction_type, value):
        """
        Perform vectorized reduction operation.

        This method handles vectorized reduction for different reduction types.
        It manages special cases for low-precision floating point types and
        employs precision improvement techniques for certain reduction operations.

        Args:
            dtype: The output data type for the reduction result
            src_dtype: The source data type of the input value
            reduction_type: Type of reduction operation (sum, min, max, etc.)
            value: The input value to reduce

        Returns:
            The result of the reduction operation
        """
    def store_reduction(self, name, index, value): ...
    def broadcast(self, scalar_var: CppCSEVariable) -> CppCSEVariable: ...
    def arange(self, index: CppCSEVariable, stride: sympy.Symbol) -> CppCSEVariable: ...
    def reduction_init_vec(self, reduction_type, dtype): ...
    def reduction_acc_type_vec(self, reduction_type, dtype): ...
    def reduction_combine_vec(
        self,
        reduction_type,
        var,
        next_value,
        helper_val=...,
        index: sympy.Symbol | None = ...,
        horizontal_reduction: bool | None = ...,
        src_dtype: torch.dtype | None = ...,
    ): ...
    def indirect_assert(self, var, lower, upper, mask=...): ...
    def get_to_dtype_expr(self, src, dtype, src_dtype): ...

class CppTile2DKernel(CppVecKernel):
    """
    A vector kernel that handles the 2d tiles with the tile size defined in `tiling_factor` on
    the inner-most loop level and one of the outer loop level (`outer_tiling_idx`). When the data
    tile is accessed in a contiguous way from the outer loop axis, a transposition is applied on the
    tile to make the access contiguous from the inner-most loop axis. Then, the same vectorization
    logic from its parent `CppVecKernel` is leveraged for load/store/compute. The transposed tile load
    and store are generated into kernel.preloads and kernel.poststores buffers.

    The loop structure looks like below:
    for ...
      for i_outer ...
        for ...
          for inner_most ...
            // generated by CppTile2DKernel
            float tmp0[16*16]; at::vec::transpose_mxn<...>(tmp0, in_ptr0 + ..., ...); // into kernel.preloads
            float tmp1[16*16]; // into kernel.preloads
            for i_inner ... { // the kernel inner loop
              vectorized loads/compute/stores (e.g., load tmp0, store tmp1) // into kernel.loads/compute/stores
            }
            at::vec::transpose_mxn(out_ptr0 + ..., tmp1, ...) // into kernel.poststores
          for inner_most ... (tail)
            // generated by CppVecKernel
            ...
      for i_outer ... (tail)
        for ...
          for ...
            // generated by CppKernel
            ...
    """

    overrides = CppTile2DOverrides
    def __init__(
        self, args, num_threads, tiling_factor, tiling_indices, inner_tail_size=..., outer_tail_size=...
    ) -> None: ...
    def inner_itervar(self): ...
    def need_vec_transpose(self, index): ...
    def gen_transposed_tile_load_store(self, name, var, index, is_store, store_mode=...): ...
    def load(self, name: str, index: sympy.Expr): ...
    def store(self, name, index, value, mode=...): ...
    def codegen_inner_loops(self, code): ...
    def set_ranges(self, group, reduction_group): ...
    def transform_indexing(self, index: sympy.Expr) -> sympy.Expr: ...

def get_loop_body_lowp_fp(_body: LoopBody) -> tuple[torch.dtype | None, bool]:
    """
    Returns the low precision data type (torch.float16/torch.bfloat16) contained in the nodes
    and if all the nodes can codegen with this data type without converting to float.
    Otherwise returns None and True.
    """

class TilingSelect:
    """
    Implement the heuristic to select the tiling factors and tiling indices.
    In the future, we can implement advanced heuristic in a subclass.
    """
    def __init__(self) -> None: ...
    def select_tiling(self, fn_list, var_sizes_list) -> tuple[list[int], list[int]]: ...

class CppKernelProxy(CppKernel):
    kernel_cls: type[CppKernel] = ...
    vec_kernel_cls: type[CppVecKernel] = ...
    tile2d_kernel_cls: type[CppTile2DKernel] = ...
    def __init__(self, kernel_group) -> None: ...
    def data_type_propagation(self, nodes): ...
    def is_lowp_fp_scheduler(self, scheduler_node: SchedulerNode): ...
    def legalize_lowp_fp_dtype_loopbody(self, loop_body: LoopBody): ...
    def legalize_lowp_fp_dtype(self, nodes): ...
    def codegen_functions(self, fn_list, var_sizes_list): ...
    def codegen_loop_bodies(self, loop_bodies, var_sizes_list): ...
    def codegen_nodes(self, nodes: list[SchedulerNode]): ...
    def codegen_loops(self, code, worksharing): ...
    def update_stores_with_parallel_reduction(self): ...
    def gen_body(self, code: BracesBuffer | None = ...): ...
    def aggregate_reduction_buffers(self, inner_loop_reduction_outer_not: bool, outer_loop: LoopLevel | None):
        """
        CppKernel/CppVecKernel/CppTile2dKernel have reduction buffers themselves.
        Here, we decide how to aggregate them together and place new reduction buffers
        under CppKernelProxy.
        """

class OuterLoopFusedKernel(CppKernel):
    def __init__(self, kernel_group) -> None: ...
    def decide_parallel_depth(self, max_parallel_depth, threads): ...

class ReasonFusedNodes(Enum):
    SAME_VARS_REDUCE = ...
    COMPATIBLE_REDUCTION = ...
    COMPATIBLE_RANGES_NO_REDUCTION = ...

class CppScheduling(BaseScheduling):
    kernel_proxy_cls: type[CppKernelProxy] = ...
    MAX_FUSED_KERNEL_ARGS_NUM = ...
    backend_features = ...
    @classmethod
    def get_backend_features(cls, device: torch.device) -> OrderedSet[BackendFeature]: ...
    def __init__(self, scheduler) -> None: ...
    def group_fn(self, sizes): ...
    def reset_kernel_group(self): ...
    def fuse(self, node1, node2): ...
    def can_fuse_horizontal(self, node1, node2): ...
    def can_fuse_multi_outputs_template(self, node1: BaseSchedulerNode, node2: BaseSchedulerNode) -> bool: ...
    def can_fuse_vertical_outer_loop(self, node1, node2): ...
    def get_fusion_pair_priority(self, node1, node2): ...
    def can_fuse_vertical(self, node1, node2): ...
    def try_loop_split(self, nodes: list[SchedulerNode]):
        """
        Apply loop split optimization.
        When one of the indexing_exprs contains a division, we eliminate the division by splitting the loop
        to avoid non-contiguous loads, subject to the following conditions:
            1. No reduction and no mudular index for all nodes.
            2. The indexing_exprs of all nodes contain only one (or more, but all the same) division,
               where the divisor is an integer and not too small (the divisor > 8), the dividend is
               one of the iter_vars, and this var, i.e. the dimension that needs to be split, is
               contiguous in all other indexing_exprs.

        For example, if the node's var_ranges: {z0: 2, z1: 9216, z2: 960} and indexing_exprs:
        {'index0': 8847360*z0 + 960*z1 + z2, 'index1': 32*z0 + (z2//30), 'index2': z2},
        we will split z2 -> 30*z2 + z3, then the node's var_ranges will be changed to
        {z0: 2, z1: 9216, z2: 32, z3: 30} and indexing_exprs will be changed to
        {'index0': 8847360*z0 + 960*z1 + 30*z2 + z3, 'index1': 32*z0 + z2, 'index2': 30*z2 + z3}.
        """
    def codegen_outer_loop_node(self, node: OuterLoopFusedSchedulerNode):
        """
        Generate the code for the outer loop fused scheduler node.
        1. Codegen with fused outer loop: depends on the analysis of
            the outer loop fused scheduler node, with or without the local buffer.
        2. If failed, fallback to standard codegen.
        """
    def codegen_node(self, node: OuterLoopFusedSchedulerNode | FusedSchedulerNode | SchedulerNode):
        """Turn an set of pre-fused nodes into a C++ kernel."""
    def is_cpp_template(self, node: BaseSchedulerNode) -> bool: ...
    def codegen_template(
        self,
        template_node: BaseSchedulerNode,
        epilogue_nodes: Sequence[BaseSchedulerNode],
        prologue_nodes: Sequence[BaseSchedulerNode],
    ):
        """Codegen a CPP template, possibly with fused epilogues"""
    def ready_to_flush(self): ...
    def codegen_sync(self): ...
    def define_kernel(self, src_code, nodes, kernel_args=...): ...
    def flush(self): ...

class KernelGroup:
    def __init__(self) -> None: ...
    def new_kernel(self, cls, *args): ...
    def finalize_kernel(self, new_kernel, nodes): ...
    def get_num_args(self): ...
    def codegen_group(self, name=...) -> str: ...
    def call_kernel(self, wrapper, kernel_name, debug_handle: int | None = ...): ...

class WorkSharing:
    def __init__(self, code) -> None: ...
    def parallel(self, threads): ...
    def single(self): ...
    def close(self): ...
    def __enter__(self): ...
    def __exit__(self, exc_type, exc_val, exc_tb): ...

@dataclasses.dataclass
class LoopLevel:
    """LoopLevel(var: Optional[sympy.core.expr.Expr] = None, size: Optional[sympy.core.expr.Expr] = None, offset: sympy.core.expr.Expr = 0, tiled_size: sympy.core.expr.Expr = 0, steps: sympy.core.expr.Expr = 1, parallel: int = 0, simd_omp: bool = False, simd_vec: bool = False, collapsed: bool = False, is_reduction: bool = False)"""

    var: sympy.Expr | None = ...
    size: sympy.Expr | None = ...
    offset: sympy.Expr = ...
    tiled_size: sympy.Expr = ...
    steps: sympy.Expr = ...
    parallel: int = ...
    simd_omp: bool = ...
    simd_vec: bool = ...
    collapsed: bool = ...
    is_reduction: bool = ...
    def __post_init__(self): ...
    def tile(self, factor): ...
    def lines(self): ...

@dataclasses.dataclass
class LoopNest:
    """
    A loop-nest-like structure. It is built with the `build` method
    as a loop nest and then will perform loop-tiling at some depth.

    A typical case is for vectorization, where we typically do loop-tiling
    at the innermost loop level. A more complicated case is when we do
    2D tiling at both the innermost and outer levels.
    """

    loops: list[LoopLevel] | None = ...
    kernel: CppKernel | None = ...
    @staticmethod
    def build(kernel: CppKernel):
        """Build a LoopNest with the given `kernel` as the leaf"""
    def __bool__(self) -> bool: ...
    @cache_on_self
    def max_parallel_depth(self):
        """
        Maximal allowed depth for parallelism: All reduction or non-reduction levels.
        When the range of the first inner loop beyond the maximum parallel depth is much
        larger than the range of all outer loops within the maximum parallel depth,
        change the starting depth of parallelism to the first inner loop and recalculate
        the maximum parallel depth.
        """
    def mark_parallel(self, par_depth): ...
    def tile(self, depth, factor):
        """
        Do loop-tiling at the `depth` level with `factor`.
            for (x0 = 0; x0 < x0_end; x0++)
            ->
            for (x0 = 0; x0 < x0_end; x0 += factor)
        See details in Note [tiled_size].
        """
    def get_kernel(self) -> CppKernel: ...
    def set_kernel(self, kernel): ...
    def from_loop_level(self, level: int): ...
