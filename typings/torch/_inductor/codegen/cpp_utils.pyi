from collections.abc import Callable
from typing import Any

import sympy
import torch
from torch.utils._sympy.printers import CppPrinter as _CppPrinter
from torch.utils._sympy.value_ranges import ValueRanges

from .. import ir
from ..scheduler import BaseSchedulerNode
from ..shape_propagation import BlockShapeType
from ..utils import IndentedBuffer
from ..virtualized import V
from .common import CSEVariable, KernelArgs

DTYPE_TO_CPP = ...
DTYPE_TO_ATEN = ...
DEVICE_TO_ATEN = ...
LAYOUT_TO_ATEN = ...
DEVICE_TO_INT = ...
_IS_WINDOWS = ...
INDEX_TYPE = ...
GemmBlocking = ...

def get_promote_dtype(args): ...
def promote_args(new_args): ...

class CppCSEVariable(CSEVariable):
    def __init__(
        self, name, bounds: ValueRanges[Any], dtype: torch.dtype | None = ..., shape: BlockShapeType = ...
    ) -> None: ...
    def update_on_args(self, name, args, kwargs): ...
    def depends_on(self, itervar: sympy.Symbol): ...

class CppPrinter(_CppPrinter):
    def doprint(self, expr, *, simplify: bool = ..., p=...): ...
    def parenthesize(self, item: sympy.Expr, level: int, strict: bool = ...) -> str: ...

cexpr = ...

def cexpr_index(index): ...
def value_to_cpp(value, cpp_type): ...
def rewrite_index_for_function(
    localize_buffer_handler: LocalizeBufferHandler, index: sympy.Expr, global_buf_name: str
): ...
def rewrite_index_for_nodes(
    localize_buffer_handler: LocalizeBufferHandler, index: sympy.Expr, global_buf_name: str
): ...

class LocalizeBufferHandler(V.WrapperHandler):
    def __init__(
        self,
        inner,
        global_to_local: dict[str, ir.Buffer],
        rewrite_index: Callable[[LocalizeBufferHandler, sympy.Expr, str], sympy.Expr],
    ) -> None: ...
    def localize(self, name: str, index: sympy.Expr): ...
    def load(self, name: str, index: sympy.Expr): ...
    def store(self, name, index, value, mode=...): ...
    def store_reduction(self, name, index, value): ...

class LocalBufferContext:
    """
    This class creates a context that helps to generate code involving Inductor IR with
    function local buffers. These buffers are constructed during the codegen process and
    are used to store intermediate results such as local accumulators. We do not want to
    add them to `V.graph` since they are not global and we do not want to add them as
    function arguments either. So we patch the codegen processes under this scope to support
    these buffers without exposure to the outside world.
    """
    def __init__(self, kernel_args: KernelArgs) -> None: ...
    def __enter__(self): ...
    def __exit__(self, exc_type, exc_val, exc_tb): ...
    def add_local_buffer(self, local_buffer: ir.Buffer, global_buffers: list[ir.Buffer] | None = ...): ...
    def localize_function(
        self,
        fn: Callable[..., Any],
        rewrite_index: Callable[[LocalizeBufferHandler, sympy.Expr, str], sympy.Expr] = ...,
    ): ...
    def localize_nodes(
        self,
        nodes: list[ir.IRNode],
        rewrite_index: Callable[[LocalizeBufferHandler, sympy.Expr, str], sympy.Expr] = ...,
    ) -> list[ir.IRNode]:
        """
        Given `local_buf` and `global_buf` registered in current `LocalBufferContext`
        though the method of `add_local_buffer`, localizes the `global_buf` to `local_buf`
        for the given `nodes` and returns a new list of IR nodes that work on `local_buf`
        instead of `global_buf`, i.e., all the loads and stores are redirected to
        `local_buf`. This helps the fused loops to work on smaller-sized local buffers
        for better data locality.

        The the data access of `local_buf` is assumed to be contiguous with the
        same order as the `global_buf`.
        """

def unify_mask_base_type(buffer: IndentedBuffer, vars: tuple[CSEVariable, ...], dtype=...):
    """
    Given list of cse variables,
    Cast each to new mask base dtype and return casted cse variable.
    """

def may_unify_binary_op_mask_type(a, b):
    """Given two cse variables, when dtype is bool, unify them to the same mask dtype and return casted cse variable."""

def codegen_rand(offset, code, rand_function, dst_dtype=...): ...
def get_gemm_template_output_and_compute_dtype(input_dtype): ...
def create_epilogue_with_attr(input_buffer, attr, **kwargs): ...
def template_fusion_with_epilogues_supported(
    template: BaseSchedulerNode, epilogues: list[BaseSchedulerNode]
) -> tuple[bool, bool]: ...
