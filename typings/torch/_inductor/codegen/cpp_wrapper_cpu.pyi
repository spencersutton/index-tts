import functools
from collections.abc import Callable, Sequence
from typing import Any, Protocol

import sympy
import torch
import torch._ops
from torch.utils._ordered_set import OrderedSet

from .. import ir
from ..utils import DeferredLineBase, LineContext
from .common import IndentedBuffer
from .wrapper import PythonWrapperCodegen

type _OUTPUT_ARGS_TYPE = list[str | None | list[str | None]]

class HasWriteLine(Protocol):
    def writeline(self, line: LineContext | DeferredLineBase | str) -> None: ...

class CppWrapperCpu(PythonWrapperCodegen):
    """Generates cpp wrapper for running on CPU and calls cpp kernels"""
    def __init__(self) -> None: ...
    @staticmethod
    def create(
        is_subgraph: bool,
        subgraph_name: str | None,
        parent_wrapper: PythonWrapperCodegen | None,
        partition_signatures: ir.GraphPartitionSignature | None = ...,
    ): ...
    def write_constant(self, name, hashed): ...
    @staticmethod
    def get_device_include_path(device: str) -> str: ...
    def add_device_include(self, device: str) -> None: ...
    def write_header(self): ...
    def mark_output_type(self): ...
    def write_prefix(self): ...
    def write_input_output_info(self, info_kind: str, idx: int, name: str): ...
    def codegen_input_symbol_assignment(self, name: str, value: ir.TensorBox, bound_vars: OrderedSet[sympy.Symbol]): ...
    def generate_input_output_runtime_checks(self):
        """
        In debug_compile mode, we generate checks to ensure the dtype/shape/stride/device of each
        real input/output tensor match ones provided at compile time via sample
        input/output.
        """
    def write_wrapper_decl(self): ...
    def codegen_tensor_dtype_var_decl(self, code: IndentedBuffer, name): ...
    def codegen_input_size_var_decl(self, code: IndentedBuffer, name): ...
    def codegen_input_stride_var_decl(self, code: IndentedBuffer, name): ...
    def codegen_input_device_type_var_decl(self, code: IndentedBuffer, name): ...
    def codegen_additional_funcs(self): ...
    def codegen_model_kernels(self): ...

    MSVC_C2026_MAX_STRING_LENGTH = ...
    def codegen_write_arg_with_large_length_string(
        self, arg_name: str, arg_str_val: str, max_truncate_length: int = ...
    ): ...
    def codegen_model_constructor(self):
        """
        // Generated code example
        AOTInductorModel::AOTInductorModel()
            : AOTInductorModelBase(4, 1) {
        inputs_info_[0].name = "input0";
        inputs_info_[0].dtype = "torch.float16";
        ...
        constants_info_[0].name = "L__self___weight";
        constants_info_[0].dtype = at::kFloat;
        constants_info_[0].offset = 0;
        constants_info_[0].data_size = 8192;
        constants_info_[0].shape = {64, 32};
        constants_info_[0].stride = {32, 1};
        ...
        outputs_info_[0].name = "output0";
        outputs_info_[0].dtype = "torch.float16";
        }
        """
    def codegen_const_run_driver(self):
        """
        // Generated code example
        std::unordered_map<std::string, AtenTensorHandle> AOTInductorModel::const_run_impl(
            DeviceStreamType stream,
            AOTIProxyExecutorHandle proxy_executor,
            bool initialization
        ) {
            std::unordered_map<std::string, AtenTensorHandle> folded_constants_map;
            std::vector<AtenTensorHandle> output_handles;
            // build up output_handles over here.
            _const_run_impl(output_handles, stream, proxy_executor);
            // build up folded_constants_map
            return folded_constants_map;
        }
        """
    def generate(self, is_inference): ...
    def finalize_prefix(self): ...
    def codegen_scalar_to_tensor(self, output: str): ...
    def codegen_tensor_item(self, dtype: torch.dtype, tensor: str, scalar: str, indented_buffer=...): ...
    def generate_return(self, output_refs: list[str]): ...
    def generate_before_suffix(self, result): ...
    def generate_end(self, result):
        """Generates the end of the code block, and any code needed to call it."""
    @staticmethod
    def get_c_shim_func_name(kernel: str, device: str) -> str: ...
    def generate_c_shim_extern_kernel_call(
        self,
        kernel: str,
        args: list[str],
        device: str,
        *,
        debug_args: list[str] | None = ...,
        debug_handle: int | None = ...,
    ) -> None:
        """
        debug_args kwarg allows CppWrapperCpuArrayRef to pass in wrapped arguments in
        place of args while preserving debug printer output.
        """
    def generate_c_shim_extern_kernel_alloc(self, extern_kernel: ir.ExternKernelAlloc, args: list[str]) -> None: ...
    def generate_c_shim_fallback_kernel(self, fallback_kernel: ir.FallbackKernel, args: list[str]) -> None: ...
    def generate_scatter_fallback(
        self, output, inputs, cpp_kernel_name, python_kernel_name, src_is_tensor, reduce, kwargs
    ): ...
    def generate_index_put_fallback(self, kernel, x, indices, values, accumulate): ...
    def add_benchmark_harness(self, output): ...
    def codegen_cpp_sizevar(self, x: sympy.Expr, *, simplify: bool = ...) -> str: ...
    def codegen_sizevar(self, x: sympy.Expr) -> str: ...
    def codegen_tuple_access(self, basename: str, name: str, index: str) -> str: ...
    def codegen_shape_tuple(self, shape: Sequence[sympy.Expr]) -> str: ...
    def ensure_size_computed(self, sym: sympy.Symbol): ...
    def codegen_dynamic_scalar(self, node): ...
    def codegen_dynamic_select_index(self, node): ...
    def make_buffer_free(self, buffer): ...
    def make_free_by_names(self, names_to_del: list[str]): ...
    def codegen_exact_buffer_reuse(self, old_name: str, new_name: str, del_line: str): ...
    def generate_profiler_mark_wrapper_call(self, stack): ...
    def generate_start_graph(self): ...
    def generate_end_graph(self): ...
    def generate_inf_and_nan_checker(self, nodes): ...
    def codegen_device(self, device): ...
    def codegen_dtype(self, dtype): ...
    def codegen_layout(self, layout): ...
    def codegen_memory_format(self, memory_format): ...
    @functools.cache
    def codegen_int_array_var(
        self, int_array: str, writeline: Callable[..., None], known_statically=..., graph=...
    ): ...
    def make_buffer_allocation(self, buffer): ...
    def make_allocation(self, name, device, dtype, shape, stride, allocation_shape=..., is_pinned=...): ...
    def codegen_alloc_from_pool(self, name, offset, dtype, shape, stride) -> tuple[str, list[str]]: ...
    def codegen_reinterpret_view(self, data, size, stride, offset, writeline: Callable[..., None], dtype=...) -> str:
        """
        Returns a newly-created, temporary RAII tensor handle containing the
        reinterpreted tensor data.  Callers of this function are responsible for saving
        the handle if persistent access is needed.
        """
    def codegen_device_copy(self, src, dst, non_blocking: bool | str):
        """
        This function is overridden by cpp_wrapper_cpu_array_ref, so we don't need to
        handle cases where dst is not an AtenTensorHandle.
        """
    def codegen_multi_output(self, node: ir.MultiOutput): ...
    def codegen_subgraph_prefix(self, subgraph, outer_inputs, outer_outputs): ...
    def codegen_subgraph_suffix(self, subgraph, outer_inputs, outer_outputs): ...
    def codegen_invoke_subgraph(self, invoke_subgraph): ...
    def codegen_conditional(self, conditional): ...
    def codegen_subgraph(self, subgraph, outer_inputs, outer_outputs): ...
    def codegen_while_loop(self, while_loop, stack_output=...): ...
    def generate_extern_kernel_args_decl_if_needed(
        self,
        op_overload: torch._ops.OpOverload | torch._ops.HigherOrderOperator,
        raw_args: Sequence[Any],
        output_args: _OUTPUT_ARGS_TYPE,
        raw_outputs: Sequence[ir.Buffer],
    ):
        """
        Generates declarations for external kernel arguments if needed, based on the provided
        operator and its arguments. It processes both input and output arguments, categorizing
        them into tensor and integer arguments for further code generation.
        """
    def generate_fallback_kernel_with_runtime_lookup(
        self,
        buf_name: str,
        python_kernel_name: str,
        get_args: Callable[[], Sequence[str]],
        op_overload: torch._ops.OpOverload | torch._ops.HigherOrderOperator,
        raw_args: Sequence[Any],
        outputs: Sequence[ir.Buffer],
    ) -> None:
        """
        Generate a call to a kernel not contained in the C-shim.  This results in
        different code paths for AOT Inductor vs cpp_wrapper Inductor mode.
        """
    def generate_scoped_gil_acquire(self, declarations_before_scope, lines_in_scope): ...
    def load_custom_op_wrapper(self): ...
    def generate_float_value(self, val): ...
    def generate_py_arg(self, py_args_var, idx, raw_arg, arg_type): ...
    def generate_fallback_kernel_with_runtime_lookup_nopython(
        self,
        get_args: Callable[[], Sequence[str]],
        op_overload: torch._ops.OpOverload,
        output_args: Sequence[str | None],
        raw_outputs: Sequence[ir.Buffer],
    ) -> None:
        """
        Generate fallback kernel calls with runtime (non-AOT) dispatch.  This can
        only be called in cpp_wrapper mode, and assumes that the input is a non-None
        OpOverload.

        In the future, we may switch over to directly calling c10::Dispatcher if we need
        to support more datatypes.
        """
    def generate_fallback_kernel_with_runtime_lookup_python(
        self,
        buf_name: str,
        python_kernel_name: str,
        op_overload: torch._ops.OpOverload,
        raw_args: Sequence[Any],
        output_args: Sequence[str | None],
        raw_outputs: Sequence[ir.Buffer],
    ) -> None:
        """
        Generate fallback kernel calls with runtime (non-AOT) dispatch.  This can
        only be called in cpp_wrapper mode, and assumes that the input is a non-None
        OpOverload.

        This function calls into Python to dispatch, which allows it to handle datatypes
        that cannot be contained in StableIValue, at the cost of some performance.
        """
    def generate_fallback_kernel_with_runtime_lookup_aot(
        self,
        op_overload: torch._ops.OpOverload | torch._ops.HigherOrderOperator,
        raw_args: Sequence[Any],
        output_args: _OUTPUT_ARGS_TYPE,
        raw_outputs: Sequence[ir.Buffer],
    ) -> None: ...
    def generate_reset_kernel_saved_flags(self): ...
    def generate_save_uncompiled_kernels(self): ...
    def c_type_for_prim_type(self, val, type_) -> str: ...
    def val_to_arg_str_for_prim_type(self, val, type_) -> str: ...
    def val_to_arg_str(self, val, type_=...) -> str: ...
    def create_tmp_raii_handle_var_if_needed(self, handle: str, writer: HasWriteLine | list[str] | None = ...) -> str:
        """
        If the input handle is an rvalue RAII tensor, creates an lvalue variable for
        it in writer.  Returns a variable name that can be used to access handle.
        """
