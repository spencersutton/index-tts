import functools
from collections.abc import Callable, Sequence
from typing import TYPE_CHECKING, Any, Protocol

import sympy
import torch
import torch._ops
from torch.utils._ordered_set import OrderedSet

from .. import ir
from ..utils import DeferredLineBase, LineContext
from .common import IndentedBuffer
from .wrapper import PythonWrapperCodegen

if TYPE_CHECKING:
    type _OUTPUT_ARGS_TYPE = list[str | None | list[str | None]]

class HasWriteLine(Protocol):
    def writeline(self, line: LineContext | DeferredLineBase | str) -> None: ...

class CppWrapperCpu(PythonWrapperCodegen):
    def __init__(self) -> None: ...
    @staticmethod
    def create(
        is_subgraph: bool,
        subgraph_name: str | None,
        parent_wrapper: PythonWrapperCodegen | None,
        partition_signatures: ir.GraphPartitionSignature | None = ...,
    ): ...
    def write_constant(self, name, hashed): ...
    @staticmethod
    def get_device_include_path(device: str) -> str: ...
    def add_device_include(self, device: str) -> None: ...
    def write_header(self): ...
    def mark_output_type(self): ...
    def write_prefix(self): ...
    def write_input_output_info(self, info_kind: str, idx: int, name: str): ...
    def codegen_input_symbol_assignment(self, name: str, value: ir.TensorBox, bound_vars: OrderedSet[sympy.Symbol]): ...
    def generate_input_output_runtime_checks(self): ...
    def write_wrapper_decl(self): ...
    def codegen_tensor_dtype_var_decl(self, code: IndentedBuffer, name): ...
    def codegen_input_size_var_decl(self, code: IndentedBuffer, name): ...
    def codegen_input_stride_var_decl(self, code: IndentedBuffer, name): ...
    def codegen_input_device_type_var_decl(self, code: IndentedBuffer, name): ...
    def codegen_additional_funcs(self): ...
    def codegen_model_kernels(self): ...

    MSVC_C2026_MAX_STRING_LENGTH = ...
    def codegen_write_arg_with_large_length_string(
        self, arg_name: str, arg_str_val: str, max_truncate_length: int = ...
    ): ...
    def codegen_model_constructor(self): ...
    def codegen_const_run_driver(self): ...
    def generate(self, is_inference): ...
    def finalize_prefix(self): ...
    def codegen_scalar_to_tensor(self, output: str): ...
    def codegen_tensor_item(self, dtype: torch.dtype, tensor: str, scalar: str, indented_buffer=...): ...
    def generate_return(self, output_refs: list[str]): ...
    def generate_before_suffix(self, result): ...
    def generate_end(self, result): ...
    @staticmethod
    def get_c_shim_func_name(kernel: str, device: str) -> str: ...
    def generate_c_shim_extern_kernel_call(
        self,
        kernel: str,
        args: list[str],
        device: str,
        *,
        debug_args: list[str] | None = ...,
        debug_handle: int | None = ...,
    ) -> None: ...
    def generate_c_shim_extern_kernel_alloc(self, extern_kernel: ir.ExternKernelAlloc, args: list[str]) -> None: ...
    def generate_c_shim_fallback_kernel(self, fallback_kernel: ir.FallbackKernel, args: list[str]) -> None: ...
    def generate_scatter_fallback(
        self, output, inputs, cpp_kernel_name, python_kernel_name, src_is_tensor, reduce, kwargs
    ): ...
    def generate_index_put_fallback(self, kernel, x, indices, values, accumulate): ...
    def add_benchmark_harness(self, output): ...
    def codegen_cpp_sizevar(self, x: sympy.Expr, *, simplify: bool = ...) -> str: ...
    def codegen_sizevar(self, x: sympy.Expr) -> str: ...
    def codegen_tuple_access(self, basename: str, name: str, index: str) -> str: ...
    def codegen_shape_tuple(self, shape: Sequence[sympy.Expr]) -> str: ...
    def ensure_size_computed(self, sym: sympy.Symbol): ...
    def codegen_dynamic_scalar(self, node): ...
    def codegen_dynamic_select_index(self, node): ...
    def make_buffer_free(self, buffer): ...
    def make_free_by_names(self, names_to_del: list[str]): ...
    def codegen_exact_buffer_reuse(self, old_name: str, new_name: str, del_line: str): ...
    def generate_profiler_mark_wrapper_call(self, stack): ...
    def generate_start_graph(self): ...
    def generate_end_graph(self): ...
    def generate_inf_and_nan_checker(self, nodes): ...
    def codegen_device(self, device): ...
    def codegen_dtype(self, dtype): ...
    def codegen_layout(self, layout): ...
    def codegen_memory_format(self, memory_format): ...
    @functools.cache
    def codegen_int_array_var(
        self, int_array: str, writeline: Callable[..., None], known_statically=..., graph=...
    ): ...
    def make_buffer_allocation(self, buffer): ...
    def make_allocation(self, name, device, dtype, shape, stride, allocation_shape=..., is_pinned=...): ...
    def codegen_alloc_from_pool(self, name, offset, dtype, shape, stride) -> tuple[str, list[str]]: ...
    def codegen_reinterpret_view(
        self, data, size, stride, offset, writeline: Callable[..., None], dtype=...
    ) -> str: ...
    def codegen_device_copy(self, src, dst, non_blocking: bool | str): ...
    def codegen_multi_output(self, node: ir.MultiOutput): ...
    def codegen_subgraph_prefix(self, subgraph, outer_inputs, outer_outputs): ...
    def codegen_subgraph_suffix(self, subgraph, outer_inputs, outer_outputs): ...
    def codegen_invoke_subgraph(self, invoke_subgraph): ...
    def codegen_conditional(self, conditional): ...
    def codegen_subgraph(self, subgraph, outer_inputs, outer_outputs): ...
    def codegen_while_loop(self, while_loop, stack_output=...): ...
    def generate_extern_kernel_args_decl_if_needed(
        self,
        op_overload: torch._ops.OpOverload | torch._ops.HigherOrderOperator,
        raw_args: Sequence[Any],
        output_args: _OUTPUT_ARGS_TYPE,
        raw_outputs: Sequence[ir.Buffer],
    ): ...
    def generate_fallback_kernel_with_runtime_lookup(
        self,
        buf_name: str,
        python_kernel_name: str,
        get_args: Callable[[], Sequence[str]],
        op_overload: torch._ops.OpOverload | torch._ops.HigherOrderOperator,
        raw_args: Sequence[Any],
        outputs: Sequence[ir.Buffer],
    ) -> None: ...
    def generate_scoped_gil_acquire(self, declarations_before_scope, lines_in_scope): ...
    def load_custom_op_wrapper(self): ...
    def generate_float_value(self, val): ...
    def generate_py_arg(self, py_args_var, idx, raw_arg, arg_type): ...
    def generate_fallback_kernel_with_runtime_lookup_nopython(
        self,
        get_args: Callable[[], Sequence[str]],
        op_overload: torch._ops.OpOverload,
        output_args: Sequence[str | None],
        raw_outputs: Sequence[ir.Buffer],
    ) -> None: ...
    def generate_fallback_kernel_with_runtime_lookup_python(
        self,
        buf_name: str,
        python_kernel_name: str,
        op_overload: torch._ops.OpOverload,
        raw_args: Sequence[Any],
        output_args: Sequence[str | None],
        raw_outputs: Sequence[ir.Buffer],
    ) -> None: ...
    def generate_fallback_kernel_with_runtime_lookup_aot(
        self,
        op_overload: torch._ops.OpOverload | torch._ops.HigherOrderOperator,
        raw_args: Sequence[Any],
        output_args: _OUTPUT_ARGS_TYPE,
        raw_outputs: Sequence[ir.Buffer],
    ) -> None: ...
    def generate_reset_kernel_saved_flags(self): ...
    def generate_save_uncompiled_kernels(self): ...
    def c_type_for_prim_type(self, val, type_) -> str: ...
    def val_to_arg_str_for_prim_type(self, val, type_) -> str: ...
    def val_to_arg_str(self, val, type_=...) -> str: ...
    def create_tmp_raii_handle_var_if_needed(
        self, handle: str, writer: HasWriteLine | list[str] | None = ...
    ) -> str: ...
