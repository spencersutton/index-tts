import functools
from collections.abc import Callable
from dataclasses import dataclass
from typing import TYPE_CHECKING, Any, Literal, Optional, TypeAlias, Union

from sympy import Expr
from torch._inductor.codegen.cuda.cuda_template import CUDATemplate
from torch._inductor.scheduler import BaseSchedulerNode

from ...autotune_process import CUDABenchmarkRequest
from ...ir import (
    Buffer,
    ChoiceCaller,
    CUDATemplateBuffer,
    IRNode,
    Layout,
    PrimitiveInfoType,
    ShapeAsConstantBuffer,
    TensorBox,
)
from ..common import CSEVariable, Kernel, OpOverrides
from .cuda_template import ArgInfo

if TYPE_CHECKING: ...
if TYPE_CHECKING: ...
log = ...
cexpr = ...
type ValidLayoutSymbols = Literal["M", "N", "K", "B", "lda", "ldb", "ldc", "ldd"]
type ValidLayoutAttrs = Literal["size", "stride"]

@dataclass(frozen=True)
class LayoutArg:
    node: IRNode
    symbol: ValidLayoutSymbols
    attr: ValidLayoutAttrs
    dim: int
    def matches(self, node, attr, dim) -> bool: ...

class CUDAKernel(Kernel):
    overrides = OpOverrides
    def __init__(self, *args, **kwargs) -> None: ...
    def find_symbol(self, node: IRNode, attr: ValidLayoutAttrs, dim: int) -> str | None: ...
    def find_layout_arg(self, node: IRNode, attr: ValidLayoutAttrs, dim: int) -> LayoutArg | None: ...
    def add_layout_arg(self, symbol: ValidLayoutSymbols, node: IRNode, attr: ValidLayoutAttrs, dim: int):  # -> None:
        ...
    def init_layout_args(self) -> None: ...
    def get_layout_args(self) -> tuple[Expr | int, ...]: ...
    def get_dynamic_shape_args(self) -> list[Expr | int]: ...
    def get_offset_args(self) -> list[Expr]: ...
    @staticmethod
    def find_ld_idx(node: IRNode) -> int: ...

class CUDATemplateKernel(CUDAKernel):
    _EXTRA_CPP_ARGS = ...
    def __init__(self, kernel_name: str, runtime_arg_info: list[ArgInfo], runtime_arg_values: list[Any]) -> None: ...
    def check_not_null(self, node: IRNode) -> str: ...
    def get_signature(self) -> str: ...
    def def_kernel(
        self,
        inputs: list[IRNode],
        outputs: list[IRNode],
        names_str: str = ...,
        input_reorder: list[int] | None = ...,
    ) -> str: ...
    def call_kernel(self, name: str, node: CUDATemplateBuffer) -> None: ...
    def dtype(self, node: IRNode) -> str | None: ...
    def cutlass_dtype(self, node: IRNode, default_dtype=...) -> str | None: ...
    def max_valid_index(self, node: IRNode, default=...):  # -> Literal[0]:
        ...
    def ptr(self, node: IRNode) -> str: ...
    def size(self, node: IRNode, start_index: int, end_index: int | None = ..., default_value: int = ...) -> str: ...
    def stride(self, node: IRNode, index: int, default_value: int = ...) -> str: ...
    def batch_stride(self, node: IRNode, default_value: int = ...) -> str: ...
    def row_or_column_stride(self, node: IRNode, default_value: int = ...) -> str: ...
    def load(self, name: str, index: Expr, mode: Any = ...) -> CSEVariable: ...
    def store(self, name: str, index: Expr, value: Any, mode: Any = ...) -> None: ...

class CUDATemplateCaller(ChoiceCaller):
    def __init__(
        self,
        name: str,
        category: str,
        input_nodes: list[Buffer],
        layout: Layout,
        make_kernel_render: Callable[
            [CUDATemplateBuffer, list[BaseSchedulerNode] | None],
            tuple[CUDATemplateKernel, functools.partial[str]],
        ],
        bmreq: CUDABenchmarkRequest,
        supports_epilogue_fusion: bool,
        template: CUDATemplate,
        info_kwargs: dict[str, PrimitiveInfoType | list[PrimitiveInfoType]] | None,
        description: str,
    ) -> None: ...
    def precompile(self) -> None: ...
    def benchmark(self, *args, out) -> float: ...
    def call_name(self) -> str: ...
    def kernel_hash_key(self) -> str: ...
    def hash_key(self) -> str: ...
    def info_dict(self) -> dict[str, PrimitiveInfoType | list[PrimitiveInfoType]]: ...
    def output_node(self) -> TensorBox | ShapeAsConstantBuffer: ...
