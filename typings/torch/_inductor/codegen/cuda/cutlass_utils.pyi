import atexit
import functools
import torch
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Optional
from torch.utils._ordered_set import OrderedSet
from ...ir import Layout

log = ...
CUTLASS_OPERATION_KIND: str = ...
ACCUMULATOR_DTYPES: OrderedSet[torch.dtype] = ...
XW_DTYPES: OrderedSet[torch.dtype] = ...

@atexit.register
def move_cutlass_compiled_cache() -> None: ...
@functools.cache
def try_import_cutlass() -> bool: ...

@dataclass
class CUTLASSArgs:
    architectures: str | None = ...
    cuda_version: str | None = ...
    instantiation_level: str | None = ...
    operations: str | None = ...
    build_dir = ...
    curr_build_dir = ...
    generator_target = ...
    kernels = ...
    ignore_kernels = ...
    exclude_kernels = ...
    kernel_filter_file: None = ...
    selected_kernel_list: None = ...
    interface_dir: None = ...
    filter_by_cc = ...
    disable_full_archs_compilation = ...
    def __post_init__(self):  # -> None:
        ...

def gen_ops() -> dict[Any, Any]: ...

DTYPE_TO_CUTLASS_TYPE = ...

@functools.lru_cache(32)
def torch_dtype_to_cutlass_type(torch_dtype: torch.dtype) -> cutlass_library.library.DataType: ...
@functools.lru_cache(32)
def dtype_match(torch_dtype: torch.dtype | None, cutlass_dtype: cutlass_library.library.DataType) -> bool: ...
def get_accumulator_dtype(input_torch_dtypes: list[torch.dtype]) -> torch.dtype | None: ...
@functools.lru_cache(32)
def get_alignments(torch_dtype: torch.dtype) -> list[int]: ...
def get_max_alignment(inductor_layout: Layout) -> int: ...

class CUDACompileSourceCapturingContext:
    def __init__(self) -> None: ...
    def __enter__(self, *args, **kwargs):  # -> Self:
        ...
    def __exit__(self, *args, **kwargs):  # -> None:
        ...

def cuda_standalone_runner_compile_command(srcpath: Path, exepath: Path):  # -> str:
    ...
