import dataclasses
from collections.abc import Callable, Sequence
from typing import Any

import sympy
import torch

from ...utils._ordered_set import OrderedSet
from ...utils._sympy.value_ranges import ValueRanges
from ..ops_handler import ReductionType, StoreMode
from ..runtime.hints import HalideMeta
from ..shape_propagation import BlockShapeType
from .common import BackendFeature, CSEVariable, OpOverrides, PythonPrinter
from .simd import SIMDKernel, SIMDScheduling

log = ...

def halide_constant(val): ...

class Unsupported(RuntimeError):
    def __init__(self, thing) -> None: ...

class HalidePrinter(PythonPrinter):
    @staticmethod
    def cast_index(expr): ...
    @staticmethod
    def cast_float(expr): ...

    _print_FloorToInt = ...
    _print_TruncToInt = ...
    _print_RoundToInt = ...

texpr = ...
pexpr = ...
_halide_type = ...

def halide_type(dtype): ...
def halide_acc_type(dtype): ...

class HalideOverrides(OpOverrides):
    @staticmethod
    def to_dtype(x, dtype: torch.dtype, src_dtype: torch.dtype | None = ..., use_compute_types=...): ...
    @staticmethod
    def to_dtype_bitcast(x, dtype: torch.dtype, src_dtype: torch.dtype): ...
    @classmethod
    def constant(cls, value, dtype): ...
    @staticmethod
    def abs(x): ...
    @staticmethod
    def exp(x): ...
    @staticmethod
    def sqrt(x): ...
    @staticmethod
    def minimum(a, b): ...
    @staticmethod
    def maximum(a, b): ...
    @staticmethod
    def where(a, b, c): ...
    @staticmethod
    def cos(x): ...
    @staticmethod
    def sin(x): ...
    @staticmethod
    def lgamma(x): ...
    @staticmethod
    def erf(x): ...
    @staticmethod
    def cosh(x): ...
    @staticmethod
    def sinh(x): ...
    @staticmethod
    def acos(x): ...
    @staticmethod
    def acosh(x): ...
    @staticmethod
    def asin(x): ...
    @staticmethod
    def asinh(x): ...
    @staticmethod
    def atan2(x, y): ...
    @staticmethod
    def atan(x): ...
    @staticmethod
    def atanh(x): ...
    @staticmethod
    def copysign(x, y): ...
    @staticmethod
    def erfinv(x): ...
    @staticmethod
    def hypot(x, y): ...
    @staticmethod
    def nextafter(x, y): ...
    @staticmethod
    def logical_and(a, b): ...
    @staticmethod
    def logical_not(a): ...
    @staticmethod
    def logical_or(a, b): ...
    @staticmethod
    def logical_xor(a, b): ...
    @staticmethod
    def bitwise_and(a, b): ...
    @staticmethod
    def bitwise_not(a): ...
    @staticmethod
    def bitwise_or(a, b): ...
    @staticmethod
    def bitwise_xor(a, b): ...
    @staticmethod
    def bitwise_left_shift(a, b): ...
    @staticmethod
    def bitwise_right_shift(a, b): ...
    @staticmethod
    def rand(seed, offset): ...
    @staticmethod
    def randn(seed, offset): ...
    @staticmethod
    def randint64(seed, offset, low, high): ...
    @staticmethod
    def load_seed(name, offset): ...
    @staticmethod
    def rsqrt(x): ...
    @staticmethod
    def tan(x): ...
    @staticmethod
    def tanh(x): ...
    @staticmethod
    def signbit(x): ...
    @staticmethod
    def fmod(a, b): ...
    @staticmethod
    def pow(a, b): ...
    @staticmethod
    def log(x): ...
    @staticmethod
    def log2(x): ...
    @staticmethod
    def isinf(x): ...
    @staticmethod
    def isnan(x): ...
    @staticmethod
    def round(x): ...
    @staticmethod
    def floor(x): ...
    @staticmethod
    def int_truediv(a, b): ...
    @staticmethod
    def floordiv(a, b): ...
    @classmethod
    def sign(cls, x): ...
    @staticmethod
    def trunc(x): ...
    @staticmethod
    def truncdiv(a, b): ...
    @staticmethod
    def ceil(x): ...
    @staticmethod
    def relu(x): ...
    @classmethod
    def index_expr(cls, expr, dtype): ...
    @classmethod
    def indirect_indexing(cls, index_var, size, check=..., wrap_neg=...): ...
    @classmethod
    def halide_clamp(cls, value, size, check): ...
    @staticmethod
    def masked(mask, body, other): ...
    @staticmethod
    def frexp(x): ...
    @staticmethod
    def device_assert_async(cond, msg): ...

class HalideCSEVariable(CSEVariable):
    undefined_re = ...
    def __init__(
        self, name, bounds: ValueRanges[Any], dtype: torch.dtype | None = ..., shape: BlockShapeType = ...
    ) -> None: ...
    def update_on_args(self, name, args, kwargs): ...
    def index_str(self, dims): ...
    def subs_str(self, replacements): ...

@dataclasses.dataclass
class DimensionInfo:
    """DimensionInfo(expr, size, stride) -> 'None'"""

    expr: sympy.Expr | None
    size: sympy.Expr
    stride: sympy.Expr
    def __init__(self, expr, size, stride) -> None: ...
    def index_str(self, replacements=..., zero_vars=...): ...

def eq(left, right): ...
def lt(left, right): ...

class HalideKernel(SIMDKernel):
    overrides = HalideOverrides
    kexpr: Callable[[sympy.Expr], str] = ...
    def __init__(self, tiling: dict[str, sympy.Expr], **kwargs) -> None: ...
    def dtype_to_str(self, dtype: torch.dtype) -> str: ...
    def create_cse_var(self, name, bounds=..., dtype=..., shape=...): ...
    def finalize_indexing(self, indices: Sequence[sympy.Expr]):
        """
        Hook called right before codegen with every index that will be
        used in the fused kernel.

        This populates self.halide_vars/index_replacements/reduction_renames which is an alternate indexing
        scheme that avoids using divide and modulus.  Instead of xindex/yindex/rindex
        we base indexing on a larger number of vars whose product combines to those.

        This function populates self.halide_vars, self.index_replacements, and self.reduction_renames
        """
    def setup_dom_indexing(self):
        """RDom based indexing uses explicit iteration ranges for Func updates"""
    def codegen_rdom(self, name, vars): ...
    def prepare_indexing(self, index: sympy.Expr): ...
    def sym_size(self, sym):
        """The size of an index symbol"""
    def indexing_to_dimensions(self, var: str, index: sympy.Expr, is_store: bool):
        """Convert address-based indexing into dimensions using self.halide_vars"""
    def install_dims(self, var, dims, offset, is_store):
        """Try to set self.buffer_dimensions[var], return True on success"""
    def apply_offset_to_dimension(self, dims, offset): ...
    def used_dims_from_index(self, index: sympy.Expr):
        """Detect which range trees are used to populate HalideCSEVariable.used_dims"""
    def sort_used_dims(self, used_dims): ...
    def make_index_str(self, dims, replacements=..., zero_vars=...): ...
    def load(self, name: str, index: sympy.Expr):
        """Codegen a load from an InputBuffer"""
    def lookup_cse_var(self, name: str): ...
    def store(self, name: str, index: sympy.Expr, value: CSEVariable, mode: StoreMode = ...) -> None:
        """Codegen a store to an OutputBuffer"""
    def reduction(
        self,
        dtype: torch.dtype,
        src_dtype: torch.dtype,
        reduction_type: ReductionType,
        value: CSEVariable | tuple[CSEVariable, ...],
    ) -> CSEVariable | tuple[CSEVariable, ...]:
        """Codegen a reduction operation"""
    def welford_combine_impl(self, mean, m2, weight): ...
    def scan(
        self,
        dtypes: tuple[torch.dtype, ...],
        combine_fn: Callable[[tuple[CSEVariable, ...], tuple[CSEVariable, ...]], tuple[CSEVariable, ...]],
        values_orig: tuple[CSEVariable, ...],
    ) -> tuple[CSEVariable, ...]: ...
    def genfunc(self, line, used_dims, *, bounds=..., shape: BlockShapeType = ...) -> HalideCSEVariable: ...
    def newfunc(self, used_dims, *, shape: BlockShapeType = ...) -> HalideCSEVariable: ...
    def halide_buffer_numel(self, name: str):
        """
        We map all tensors to 1D buffers in Halide since Halide has trouble representing some strides that PyTorch
        supports.  If there are gaps in the underlying layout the numel we pass to Halide includes the gaps while
        PyTorch's numel excludes them.
        """
    def halide_argdefs(self):
        """Halide requires scalar inputs before outputs, so need to reorder args."""
    def halide_kernel_meta(self) -> HalideMeta:
        """Compute metadata required by codecache.py"""
    def codegen_kernel(self, name=...):
        """Called at the end to generate a final kernel string"""
    def call_kernel(self, name: str, node=...):
        """Codegen a call to this kernel"""
    def generate_assert(self, check): ...
    def check_bounds(self, expr: sympy.Expr, size: sympy.Expr, lower: bool, upper: bool): ...

class HalideScheduling(SIMDScheduling):
    kernel_type = ...
    @classmethod
    def get_backend_features(cls, device: torch.device) -> OrderedSet[BackendFeature]: ...
    def define_kernel(self, src_code, node_schedule, kernel):
        """Codegen kernel definition to go in output wrapper code"""
