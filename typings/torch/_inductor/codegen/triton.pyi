import dataclasses
from collections.abc import Callable, Sequence
from functools import lru_cache
from typing import Any, TypeVar, Union

import sympy
import torch
from torch._inductor.dtype_propagation import DtypePropagationOpsHandler
from torch.utils._ordered_set import OrderedSet

from ...utils._sympy.symbol import SymT
from ...utils._sympy.value_ranges import ValueRanges
from ..ir import IRNode
from ..scheduler import BaseSchedulerNode, Scheduler
from ..utils import cache_on_self
from ..virtualized import ReductionType, StoreMode
from .common import CSE, BlockShapeType, CSEVariable, IndentedBuffer, OpOverrides, PythonPrinter
from .simd import IterationRanges, IterationRangesEntry, IterationRangesRoot, SIMDKernel, SIMDScheduling
from .simd_kernel_features import SIMDKernelFeatures

_T = TypeVar("_T")
log = ...
perf_hint_log = ...
schedule_log = ...
fusion_log = ...
async_compile = ...

class OpDtypeSupport:
    supported_dtypes: dict[str, OrderedSet[torch.dtype]] = ...
    convert_outputs: dict[str, bool] = ...
    @classmethod
    def register_upcast(cls, func: Callable[..., str], convert_output: bool) -> None: ...

@lru_cache(None)
def gen_attr_descriptor_import() -> str: ...
@lru_cache(None)
def gen_common_triton_imports() -> str: ...

class TritonSymbols:
    reduction_types = ...
    block_types = ...
    block_offsets = ...
    block_sizes = ...
    @classmethod
    def get_block_size(cls, tree: IterationRanges) -> sympy.Symbol: ...
    @classmethod
    def get_block_offset(cls, tree: IterationRanges) -> sympy.Symbol: ...

@dataclasses.dataclass
class IndexingOptions:
    index_str: str
    mask_vars: OrderedSet[str]
    expand_str: str | None
    _has_rindex: bool
    index: sympy.Expr
    expand_shape: Sequence[int | str] | None
    def has_mask(self) -> bool: ...
    def has_indirect(self) -> bool: ...
    def has_rindex(self) -> bool: ...
    def has_tmpmask(self) -> bool: ...
    def has_rmask(self) -> bool: ...
    @property
    def mask_str(self) -> str: ...

@dataclasses.dataclass
class BlockDescriptorOptions:
    params: BlockParameters
    constant_offset: sympy.Expr
    order: list[int]
    mask_vars: OrderedSet[str]
    broadcast_shape: Sequence[sympy.Expr]
    broadcasting_dims: list[bool]
    final_shape: Sequence[sympy.Expr]
    _boundary_check: list[int] | None = ...
    @property
    def shape(self) -> list[sympy.Expr]: ...
    @property
    def block_shape(self) -> list[sympy.Expr]: ...
    @property
    def strides(self) -> list[sympy.Expr]: ...
    @property
    def offsets(self) -> list[sympy.Expr]: ...
    @classmethod
    def create(
        cls,
        *,
        params: BlockParameters,
        constant_offset: sympy.Expr,
        range_trees: list[IterationRangesRoot],
        mask_vars: OrderedSet[str],
        get_max_block: Callable[[str], int],
    ) -> BlockDescriptorOptions: ...
    def replace_offset(self, expr: sympy.Expr, replacement: sympy.Expr, symt: SymT) -> sympy.Expr: ...
    def remove_roffsets(self, expr: sympy.Expr) -> sympy.Expr: ...
    def compute_boundary_check(
        self, get_max_block: Callable[[str], int], range_trees: list[IterationRangesRoot]
    ) -> None: ...
    def boundary_check(self) -> list[int]: ...
    def has_indirect(self) -> bool: ...
    def has_rindex(self) -> bool: ...
    def has_rmask(self) -> bool: ...
    def has_tmpmask(self) -> bool: ...
    def has_mask(self) -> bool: ...
    def codegen_broadcast_and_reshape(
        self, value: str, initial_shape: Sequence[sympy.Expr], final_shape: Sequence[sympy.Expr], allow_implicit: bool
    ) -> str: ...

@dataclasses.dataclass
class TensorDescriptorOptions(BlockDescriptorOptions):
    def format(self, name: str, roffset=...) -> str: ...

@dataclasses.dataclass
class BlockPtrOptions(BlockDescriptorOptions):
    def replace_offset(self, expr: sympy.Expr, replacement: sympy.Expr, symt: SymT) -> sympy.Expr: ...
    def remove_roffsets(self, expr: sympy.Expr) -> sympy.Expr: ...
    def format(self, name: str, roffset=...) -> str: ...
    def advance_roffset(self, symt: SymT) -> sympy.Expr: ...

def triton_reshape(value: str, old_shape: Sequence[sympy.Expr], new_shape: Sequence[sympy.Expr]) -> str: ...

class TritonPrinter(PythonPrinter): ...

texpr = ...

def triton_compute_type(dtype: torch.dtype) -> str: ...
def triton_store_type(dtype: torch.dtype) -> str: ...
def upcast_acc_dtype(dtype: torch.dtype) -> torch.dtype: ...
def triton_acc_type(dtype: torch.dtype) -> str: ...
def low_precision_fp(dtype: torch.dtype) -> bool: ...
def low_precision_fp_var(var: CSEVariable | Any) -> bool: ...

class TritonCSEVariable(CSEVariable):
    def __init__(
        self, name: str, bounds: ValueRanges[Any], dtype: torch.dtype, shape: BlockShapeType = ...
    ) -> None: ...
    def update_on_args(self, name, args, kwargs): ...

def get_dtype_handler() -> DtypePropagationOpsHandler: ...
def maybe_upcast_float32(convert_output: bool = ...) -> Callable[[_T], _T]: ...

class TritonOverrides(OpOverrides):
    _LOG_2_E = ...
    @staticmethod
    def to_dtype(x, dtype: torch.dtype, src_dtype: torch.dtype | None = ..., use_compute_types=...): ...
    @staticmethod
    def to_dtype_bitcast(x, dtype: torch.dtype, src_dtype: torch.dtype): ...
    @classmethod
    def constant(cls, value, dtype): ...
    @staticmethod
    @maybe_upcast_float32()
    def abs(x): ...
    @staticmethod
    def truediv(x, y): ...
    @staticmethod
    def mod(x, y): ...
    @staticmethod
    @maybe_upcast_float32()
    def exp(x): ...
    @staticmethod
    @maybe_upcast_float32()
    def exp2(x): ...
    @staticmethod
    @maybe_upcast_float32()
    def expm1(x): ...
    @staticmethod
    @maybe_upcast_float32()
    def sqrt(x): ...
    @staticmethod
    def relu(x): ...
    @staticmethod
    def minimum(a, b): ...
    @staticmethod
    def maximum(a, b): ...
    @staticmethod
    def where(a, b, c): ...
    @staticmethod
    def inline_asm_elementwise(*inputs, asm, constraints=..., dtype=..., is_pure=..., pack=...): ...
    @staticmethod
    @maybe_upcast_float32()
    def cos(x): ...
    @staticmethod
    @maybe_upcast_float32()
    def sin(x): ...
    @classmethod
    def index_expr(cls, expr, dtype): ...
    @staticmethod
    def masked(mask, body, other): ...
    @staticmethod
    @maybe_upcast_float32()
    def lgamma(x): ...
    @staticmethod
    @maybe_upcast_float32()
    def erf(x): ...
    @staticmethod
    @maybe_upcast_float32()
    def cosh(x): ...
    @staticmethod
    @maybe_upcast_float32()
    def sinh(x): ...
    @staticmethod
    @maybe_upcast_float32()
    def acos(x): ...
    @staticmethod
    @maybe_upcast_float32()
    def acosh(x): ...
    @staticmethod
    @maybe_upcast_float32()
    def asin(x): ...
    @staticmethod
    @maybe_upcast_float32()
    def asinh(x): ...
    @staticmethod
    @maybe_upcast_float32()
    def atan2(x, y): ...
    @staticmethod
    @maybe_upcast_float32()
    def atan(x): ...
    @staticmethod
    @maybe_upcast_float32()
    def atanh(x): ...
    @staticmethod
    @maybe_upcast_float32()
    def copysign(x, y): ...
    @staticmethod
    @maybe_upcast_float32()
    def erfc(x): ...
    @staticmethod
    @maybe_upcast_float32()
    def erfinv(x): ...
    @staticmethod
    @maybe_upcast_float32()
    def hypot(x, y): ...
    @staticmethod
    @maybe_upcast_float32()
    def log10(x): ...
    @staticmethod
    @maybe_upcast_float32()
    def log2(x): ...
    @staticmethod
    @maybe_upcast_float32()
    def nextafter(x, y): ...
    @staticmethod
    def logical_and(a, b): ...
    @staticmethod
    def logical_not(a): ...
    @staticmethod
    def logical_or(a, b): ...
    @staticmethod
    def logical_xor(a, b): ...
    @staticmethod
    def bitwise_and(a, b): ...
    @staticmethod
    def bitwise_not(a): ...
    @staticmethod
    def bitwise_or(a, b): ...
    @staticmethod
    def bitwise_xor(a, b): ...
    @staticmethod
    def bitwise_left_shift(a, b): ...
    @staticmethod
    def bitwise_right_shift(a, b): ...
    @staticmethod
    def rand(seed, offset): ...
    @staticmethod
    def randn(seed, offset): ...
    @staticmethod
    def randint64(seed, offset, low, high): ...
    @staticmethod
    def load_seed(name, offset): ...
    @staticmethod
    @maybe_upcast_float32()
    def rsqrt(x): ...
    @staticmethod
    @maybe_upcast_float32()
    def log1p(x): ...
    @staticmethod
    @maybe_upcast_float32()
    def tan(x): ...
    @staticmethod
    @maybe_upcast_float32()
    def tanh(x): ...
    @staticmethod
    @maybe_upcast_float32()
    def sigmoid(x): ...
    @staticmethod
    def signbit(x): ...
    @staticmethod
    @maybe_upcast_float32()
    def fmod(a, b): ...
    @staticmethod
    @maybe_upcast_float32()
    def pow(a, b): ...
    @staticmethod
    @maybe_upcast_float32()
    def log(x): ...
    @staticmethod
    @maybe_upcast_float32(convert_output=False)
    def isinf(x): ...
    @staticmethod
    @maybe_upcast_float32(convert_output=False)
    def isnan(x): ...
    @staticmethod
    @maybe_upcast_float32()
    def round(x): ...
    @staticmethod
    @maybe_upcast_float32()
    def floor(x): ...
    @staticmethod
    def floordiv(a, b): ...
    @staticmethod
    def sign(x): ...
    @staticmethod
    @maybe_upcast_float32()
    def trunc(x): ...
    @staticmethod
    def truncdiv(a, b): ...
    @staticmethod
    @maybe_upcast_float32()
    def ceil(x): ...

class TritonKernelOverrides(TritonOverrides):
    def __init__(self, *args, **kwargs) -> None: ...
    @classmethod
    def constant(cls, value, dtype): ...
    @classmethod
    def index_expr(cls, expr, dtype): ...
    @staticmethod
    def masked(mask, body, other): ...
    @staticmethod
    def load_seed(name, offset): ...
    @staticmethod
    def frexp(x): ...
    @staticmethod
    def device_assert_async(cond, msg): ...

class HelperFunctions:
    _templates_seen: dict[str, str]
    finalized_helpers: list[str]
    def __init__(self) -> None: ...
    def add(self, template_code: str, *, base_name=...) -> str: ...
    def __iter__(self): ...
    def __getitem__(self, idx): ...

@dataclasses.dataclass
class BlockParameters:
    shape: list[sympy.Expr] = ...
    block_shape: list[sympy.Expr] = ...
    strides: list[sympy.Expr] = ...
    offsets: list[sympy.Expr] = ...
    def __add__(self, other: BlockParameters) -> BlockParameters: ...

class CooperativeReductionWorkspaceCache:
    def __init__(self, args) -> None: ...
    def allocate(self, nbytes: sympy.Expr): ...
    def on_loop_end(self): ...
    def increment_store_count(self): ...

@dataclasses.dataclass
class FixedTritonConfig:
    config: dict[str, int]
    def __getitem__(self, item): ...
    def __contains__(self, item) -> bool: ...

class TritonCSE(CSE[TritonCSEVariable, Union[str, tuple[str, str]]]):
    def augment_key(self, cache_key: str) -> str | tuple[str, str]: ...

@dataclasses.dataclass
class TMACompatibilityChecker:
    kernel: TritonKernel
    dtype: torch.dtype
    for_store: bool
    def __post_init__(self): ...
    def can_use_tma(self) -> bool: ...
    def are_block_parameters_compatible(self, block_params: BlockParameters) -> bool: ...

@dataclasses.dataclass
class TMACompatibilityChecker:
    kernel: TritonKernel
    dtype: torch.dtype
    for_store: bool
    def __post_init__(self): ...
    def can_use_tma(self) -> bool: ...
    def are_block_parameters_compatible(self, block_params: BlockParameters) -> bool: ...

class TritonKernel(SIMDKernel[TritonCSEVariable]):
    overrides = TritonKernelOverrides
    helper_functions: HelperFunctions
    kexpr: Callable[[sympy.Expr], str] = ...
    allow_block_ptr = ...
    tma_compatibility_checker_cls = TMACompatibilityChecker
    def __init__(
        self,
        tiling: dict[str, sympy.Expr],
        min_elem_per_thread=...,
        optimize_mask=...,
        fixed_config: FixedTritonConfig | None = ...,
        hint_override: int | None = ...,
        **kwargs,
    ) -> None: ...
    def dtype_to_str(self, dtype: torch.dtype) -> str: ...
    def should_use_cooperative_reduction(self) -> bool: ...
    def init_cooperative_reduction(self): ...
    def init_cooperative_reduction_mask(self): ...
    def codegen_range_tree(self): ...
    def need_numel_args(self): ...
    def should_use_persistent_reduction(self) -> bool: ...
    def want_no_x_dim(self): ...
    @property
    def assert_function(self) -> str: ...
    def indexing(
        self,
        index: sympy.Expr,
        *,
        copy_shape=...,
        dense_indexing=...,
        override_mask=...,
        block_ptr=...,
        tma_compatibility_checker: TMACompatibilityChecker | None = ...,
    ): ...
    def codegen_block_ptr(
        self, name: str, var: str, indexing: BlockPtrOptions | TensorDescriptorOptions, other=...
    ) -> tuple[str, str]: ...
    def codegen_block_ptr_store_line(self, name, indexing, block_ptr, value, other=...): ...
    def check_bounds(self, expr: sympy.Expr, size: sympy.Expr, lower: bool, upper: bool): ...
    def get_load_buffer(self, indexing): ...
    def load(self, name: str, index: sympy.Expr): ...
    def store(self, name: str, index: sympy.Expr, value: CSEVariable, mode: StoreMode = ...) -> None: ...
    def guard_cooperative_store(self, name, buffer): ...
    def bucketize(
        self,
        values: CSEVariable,
        boundaries: tuple[str, sympy.Expr, sympy.Expr, sympy.Expr],
        boundary_indices: CSEVariable,
        indexing_dtype: torch.dtype,
        right: bool,
        sorter: tuple[str, sympy.Expr] | None = ...,
        sorter_indices: CSEVariable | None = ...,
    ) -> CSEVariable: ...
    def reduction_resize(self, value) -> str: ...
    def reduction_resize_and_shape(self, value, shape) -> tuple[str, BlockShapeType]: ...
    def reduction_collapse_dims(self, buffer, value: CSEVariable, dtype: torch.dtype) -> CSEVariable: ...
    def reduction(
        self,
        dtype: torch.dtype,
        src_dtype: torch.dtype,
        reduction_type: ReductionType,
        value: CSEVariable | tuple[CSEVariable, ...],
    ) -> CSEVariable | tuple[CSEVariable, ...]: ...
    def welford_reduce(self, result_var, reduction_type, value, where_cond, acc_type, dtype): ...
    def welford_reduce_final_reduction(
        self, buffer, result_mean, result_m2, result_weight, mean, m2, weight, dim, dtype
    ): ...
    def online_softmax_reduce_final_reduction(self, buffer, result_max, result_sum, peer_max, peer_sum, dim, dtype): ...
    def max_rsplit(self): ...
    def codegen_cooperative_reduction_peer_combine(self, result_var, dtype, default_val) -> CSEVariable: ...
    def store_reduction(self, name: str, index: sympy.Expr, value: CSEVariable | tuple[CSEVariable, ...]): ...
    def scan(
        self,
        dtypes: tuple[torch.dtype, ...],
        combine_fn: Callable[[tuple[CSEVariable, ...], tuple[CSEVariable, ...]], tuple[CSEVariable, ...]],
        values: tuple[CSEVariable, ...],
    ) -> tuple[CSEVariable, ...]: ...
    def sort(
        self, dtypes: tuple[torch.dtype, ...], values: tuple[CSEVariable, ...], stable: bool, descending: bool
    ) -> tuple[CSEVariable, ...]: ...
    def codegen_body(self): ...
    def kernel_benchmark_extra_args(self) -> list[str]: ...
    def codegen_kernel_benchmark(self, num_gb): ...
    def imports_for_benchmark_kernel(self): ...
    @staticmethod
    def inductor_meta_common(): ...
    def codegen_kernel(self, name=...) -> str: ...
    @staticmethod
    def has_persistent_RBLOCK(rnumel): ...
    def codegen_static_numels(self, code): ...
    def add_numel_to_call_args(self, name, call_args, arg_types): ...
    def call_kernel(self, name: str, node: IRNode | None = ...): ...
    def codegen_nan_check(self) -> None: ...
    def create_cse_var(self, *args, **kwargs) -> TritonCSEVariable: ...
    def codegen_iteration_ranges_entry(self, entry: IterationRangesEntry): ...
    def iteration_ranges_ranges_code(self, entry: IterationRangesRoot) -> str: ...
    def iteration_ranges_scalar_code(self, entry: IterationRangesRoot, value: Any) -> str: ...
    def iteration_ranges_get_pid(self, entry: IterationRangesRoot) -> str: ...
    def needs_yz_grid_overflow(self, entry: IterationRangesRoot) -> bool: ...
    def max_block(self, prefix: str) -> int: ...
    def filter_masks(self, mask_vars: OrderedSet[str]) -> None: ...
    @cache_on_self
    def get_reduction_prefixes(self) -> list[str]: ...
    def codegen_reduction_numels(self, buffer: IndentedBuffer) -> None: ...
    def codegen_reduction_indices(self, buffer: IndentedBuffer) -> None: ...
    def iteration_ranges_codegen_header(self, entry: IterationRangesRoot, code: IndentedBuffer) -> None: ...

class TritonScheduling(SIMDScheduling):
    kernel_type: type[Any] = ...
    backend_features = ...
    def __init__(self, scheduler: Scheduler | None) -> None: ...
    @classmethod
    def get_backend_features(cls, device: torch.device): ...
    def codegen_comment(self, node_schedule): ...
    def define_kernel(self, src_code, node_schedule, kernel): ...
    def benchmark_fused_nodes(self, nodes, n_spills_threshold=...) -> tuple[float, str]: ...
    def benchmark_codegened_module(
        self, mod, n_spills_threshold=..., node_names: OrderedSet[str] | None = ...
    ) -> tuple[float, str]: ...
    def create_kernel_choices(
        self, kernel_features: SIMDKernelFeatures, kernel_args: list[Any], kernel_kwargs: dict[str, Any]
    ) -> list[TritonKernel]: ...
    def add_multi_kernel_choices(
        self, kernel: TritonKernel, kernel_args: list[Any], kernel_kwargs: dict[str, Any]
    ) -> list[TritonKernel]: ...
    def benchmark_combo_kernel(self, node_list): ...

def debug_triton_code(node: BaseSchedulerNode) -> list[str]: ...
