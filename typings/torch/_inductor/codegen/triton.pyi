import dataclasses
import sympy
import torch
from collections.abc import Sequence
from functools import lru_cache
from typing import Any, Callable, Optional, TYPE_CHECKING, TypeVar, Union
from torch.utils._ordered_set import OrderedSet
from ...utils._sympy.symbol import SymT
from ...utils._sympy.value_ranges import ValueRanges
from .. import config
from ..scheduler import BaseSchedulerNode, Scheduler
from ..utils import cache_on_self
from ..virtualized import ReductionType, StoreMode
from .common import BlockShapeType, CSE, CSEVariable, IndentedBuffer, OpOverrides, PythonPrinter
from .simd import IterationRanges, IterationRangesEntry, IterationRangesRoot, SIMDKernel, SIMDScheduling
from torch._inductor.dtype_propagation import DtypePropagationOpsHandler
from ..ir import IRNode
from .simd_kernel_features import SIMDKernelFeatures

if TYPE_CHECKING:
    _T = TypeVar("_T")
log = ...
perf_hint_log = ...
schedule_log = ...
fusion_log = ...
async_compile = ...

class OpDtypeSupport:
    supported_dtypes: dict[str, OrderedSet[torch.dtype]] = ...
    convert_outputs: dict[str, bool] = ...
    @classmethod
    def register_upcast(cls, func: Callable[..., str], convert_output: bool) -> None: ...

@lru_cache(None)
def gen_attr_descriptor_import() -> str: ...
@lru_cache(None)
def gen_common_triton_imports() -> str: ...

class TritonSymbols:
    reduction_types = ...
    block_types = ...
    block_offsets = ...
    block_sizes = ...
    @classmethod
    def get_block_size(cls, tree: IterationRanges) -> sympy.Symbol: ...
    @classmethod
    def get_block_offset(cls, tree: IterationRanges) -> sympy.Symbol: ...

@dataclasses.dataclass
class IndexingOptions:
    index_str: str
    mask_vars: OrderedSet[str]
    expand_str: Optional[str]
    _has_rindex: bool
    index: sympy.Expr
    expand_shape: Optional[Sequence[Union[int, str]]]
    def has_mask(self) -> bool: ...
    def has_indirect(self) -> bool: ...
    def has_rindex(self) -> bool: ...
    def has_tmpmask(self) -> bool: ...
    def has_rmask(self) -> bool: ...
    @property
    def mask_str(self) -> str: ...

@dataclasses.dataclass
class BlockDescriptorOptions:
    params: BlockParameters
    constant_offset: sympy.Expr
    order: list[int]
    mask_vars: OrderedSet[str]
    broadcast_shape: Sequence[sympy.Expr]
    broadcasting_dims: list[bool]
    final_shape: Sequence[sympy.Expr]
    _boundary_check: Optional[list[int]] = ...
    @property
    def shape(self) -> list[sympy.Expr]: ...
    @property
    def block_shape(self) -> list[sympy.Expr]: ...
    @property
    def strides(self) -> list[sympy.Expr]: ...
    @property
    def offsets(self) -> list[sympy.Expr]: ...
    @classmethod
    def create(
        cls,
        *,
        params: BlockParameters,
        constant_offset: sympy.Expr,
        range_trees: list[IterationRangesRoot],
        mask_vars: OrderedSet[str],
        get_max_block: Callable[[str], int],
    ) -> BlockDescriptorOptions: ...
    def replace_offset(self, expr: sympy.Expr, replacement: sympy.Expr, symt: SymT) -> sympy.Expr: ...
    def remove_roffsets(self, expr: sympy.Expr) -> sympy.Expr: ...
    def compute_boundary_check(
        self, get_max_block: Callable[[str], int], range_trees: list[IterationRangesRoot]
    ) -> None: ...
    def boundary_check(self) -> list[int]: ...
    def has_indirect(self) -> bool: ...
    def has_rindex(self) -> bool: ...
    def has_rmask(self) -> bool: ...
    def has_tmpmask(self) -> bool: ...
    def has_mask(self) -> bool: ...
    def codegen_broadcast_and_reshape(
        self, value: str, initial_shape: Sequence[sympy.Expr], final_shape: Sequence[sympy.Expr], allow_implicit: bool
    ) -> str: ...

@dataclasses.dataclass
class TensorDescriptorOptions(BlockDescriptorOptions):
    def format(self, name: str, roffset=...) -> str: ...

@dataclasses.dataclass
class BlockPtrOptions(BlockDescriptorOptions):
    def replace_offset(self, expr: sympy.Expr, replacement: sympy.Expr, symt: SymT) -> sympy.Expr: ...
    def remove_roffsets(self, expr: sympy.Expr) -> sympy.Expr: ...
    def format(self, name: str, roffset=...) -> str: ...
    def advance_roffset(self, symt: SymT) -> sympy.Expr: ...

def triton_reshape(value: str, old_shape: Sequence[sympy.Expr], new_shape: Sequence[sympy.Expr]) -> str: ...

class TritonPrinter(PythonPrinter): ...

texpr = ...

def triton_compute_type(dtype: torch.dtype) -> str: ...
def triton_store_type(dtype: torch.dtype) -> str: ...
def upcast_acc_dtype(dtype: torch.dtype) -> torch.dtype: ...
def triton_acc_type(dtype: torch.dtype) -> str: ...
def low_precision_fp(dtype: torch.dtype) -> bool: ...
def low_precision_fp_var(var: Union[CSEVariable, Any]) -> bool: ...

class TritonCSEVariable(CSEVariable):
    def __init__(
        self, name: str, bounds: ValueRanges[Any], dtype: torch.dtype, shape: BlockShapeType = ...
    ) -> None: ...
    def update_on_args(self, name, args, kwargs):  # -> None:
        ...

def get_dtype_handler() -> DtypePropagationOpsHandler: ...
def maybe_upcast_float32(convert_output: bool = ...) -> Callable[[_T], _T]: ...

class TritonOverrides(OpOverrides):
    _LOG_2_E = ...
    @staticmethod
    def to_dtype(x, dtype: torch.dtype, src_dtype: Optional[torch.dtype] = ..., use_compute_types=...):  # -> str:
        ...
    @staticmethod
    def to_dtype_bitcast(x, dtype: torch.dtype, src_dtype: torch.dtype):  # -> str:
        ...
    @classmethod
    def constant(cls, value, dtype):  # -> str:
        ...
    @staticmethod
    @maybe_upcast_float32()
    def abs(x):  # -> str:
        ...
    @staticmethod
    def truediv(x, y):  # -> str:
        ...
    @staticmethod
    def mod(x, y):  # -> str:
        ...
    @staticmethod
    @maybe_upcast_float32()
    def exp(x):  # -> str:

        ...
    @staticmethod
    @maybe_upcast_float32()
    def exp2(x):  # -> str:
        ...
    @staticmethod
    @maybe_upcast_float32()
    def expm1(x):  # -> str:
        ...
    @staticmethod
    @maybe_upcast_float32()
    def sqrt(x):  # -> str:
        ...
    @staticmethod
    def relu(x):  # -> str | Any:
        ...
    @staticmethod
    def minimum(a, b):  # -> str:
        ...
    @staticmethod
    def maximum(a, b):  # -> str:
        ...
    @staticmethod
    def where(a, b, c):  # -> str:
        ...
    @staticmethod
    def inline_asm_elementwise(*inputs, asm, constraints=..., dtype=..., is_pure=..., pack=...):  # -> str:
        ...
    @staticmethod
    @maybe_upcast_float32()
    def cos(x):  # -> str:
        ...
    @staticmethod
    @maybe_upcast_float32()
    def sin(x):  # -> str:
        ...
    @classmethod
    def index_expr(cls, expr, dtype): ...
    @staticmethod
    def masked(mask, body, other): ...
    @staticmethod
    @maybe_upcast_float32()
    def lgamma(x):  # -> str:
        ...
    @staticmethod
    @maybe_upcast_float32()
    def erf(x):  # -> str:
        ...
    @staticmethod
    @maybe_upcast_float32()
    def cosh(x):  # -> str:
        ...
    @staticmethod
    @maybe_upcast_float32()
    def sinh(x):  # -> str:
        ...
    @staticmethod
    @maybe_upcast_float32()
    def acos(x):  # -> str:
        ...
    @staticmethod
    @maybe_upcast_float32()
    def acosh(x):  # -> str:
        ...
    @staticmethod
    @maybe_upcast_float32()
    def asin(x):  # -> str:
        ...
    @staticmethod
    @maybe_upcast_float32()
    def asinh(x):  # -> str:
        ...
    @staticmethod
    @maybe_upcast_float32()
    def atan2(x, y):  # -> str:
        ...
    @staticmethod
    @maybe_upcast_float32()
    def atan(x):  # -> str:
        ...
    @staticmethod
    @maybe_upcast_float32()
    def atanh(x):  # -> str:
        ...
    @staticmethod
    @maybe_upcast_float32()
    def copysign(x, y):  # -> str:
        ...
    @staticmethod
    @maybe_upcast_float32()
    def erfc(x):  # -> str:
        ...
    @staticmethod
    @maybe_upcast_float32()
    def erfinv(x):  # -> str:
        ...
    @staticmethod
    @maybe_upcast_float32()
    def hypot(x, y):  # -> str:
        ...
    @staticmethod
    @maybe_upcast_float32()
    def log10(x):  # -> str:
        ...
    @staticmethod
    @maybe_upcast_float32()
    def log2(x):  # -> str:
        ...
    @staticmethod
    @maybe_upcast_float32()
    def nextafter(x, y):  # -> str:
        ...
    @staticmethod
    def logical_and(a, b):  # -> str:
        ...
    @staticmethod
    def logical_not(a):  # -> str:
        ...
    @staticmethod
    def logical_or(a, b):  # -> str:
        ...
    @staticmethod
    def logical_xor(a, b):  # -> str:
        ...
    @staticmethod
    def bitwise_and(a, b):  # -> str:
        ...
    @staticmethod
    def bitwise_not(a):  # -> str:
        ...
    @staticmethod
    def bitwise_or(a, b):  # -> str:
        ...
    @staticmethod
    def bitwise_xor(a, b):  # -> str:
        ...
    @staticmethod
    def bitwise_left_shift(a, b):  # -> str:
        ...
    @staticmethod
    def bitwise_right_shift(a, b):  # -> str:
        ...
    @staticmethod
    def rand(seed, offset):  # -> str:
        ...
    @staticmethod
    def randn(seed, offset):  # -> str:
        ...
    @staticmethod
    def randint64(seed, offset, low, high):  # -> str:
        ...
    @staticmethod
    def load_seed(name, offset): ...
    @staticmethod
    @maybe_upcast_float32()
    def rsqrt(x):  # -> str:
        ...
    @staticmethod
    @maybe_upcast_float32()
    def log1p(x):  # -> str:
        ...
    @staticmethod
    @maybe_upcast_float32()
    def tan(x):  # -> str:
        ...
    @staticmethod
    @maybe_upcast_float32()
    def tanh(x):  # -> str:
        ...
    @staticmethod
    @maybe_upcast_float32()
    def sigmoid(x):  # -> str:
        ...
    @staticmethod
    def signbit(x):  # -> str:
        ...
    @staticmethod
    @maybe_upcast_float32()
    def fmod(a, b):  # -> str:
        ...
    @staticmethod
    @maybe_upcast_float32()
    def pow(a, b):  # -> str:
        ...
    @staticmethod
    @maybe_upcast_float32()
    def log(x):  # -> str:
        ...
    @staticmethod
    @maybe_upcast_float32(convert_output=False)
    def isinf(x):  # -> str:
        ...
    @staticmethod
    @maybe_upcast_float32(convert_output=False)
    def isnan(x):  # -> str:
        ...
    @staticmethod
    @maybe_upcast_float32()
    def round(x):  # -> str:
        ...
    @staticmethod
    @maybe_upcast_float32()
    def floor(x):  # -> str:
        ...
    @staticmethod
    def floordiv(a, b):  # -> str:
        ...
    @staticmethod
    def sign(x):  # -> str:
        ...
    @staticmethod
    @maybe_upcast_float32()
    def trunc(x):  # -> str:
        ...
    @staticmethod
    def truncdiv(a, b):  # -> str:
        ...
    @staticmethod
    @maybe_upcast_float32()
    def ceil(x):  # -> str:
        ...

class TritonKernelOverrides(TritonOverrides):
    def __init__(self, *args, **kwargs) -> None: ...
    @classmethod
    def constant(cls, value, dtype):  # -> str:
        ...
    @classmethod
    def index_expr(cls, expr, dtype): ...
    @staticmethod
    def masked(mask, body, other):  # -> Any:
        ...
    @staticmethod
    def load_seed(name, offset):  # -> str:
        ...
    @staticmethod
    def frexp(x):  # -> tuple[Any, Any]:
        ...
    @staticmethod
    def device_assert_async(cond, msg):  # -> str:
        ...

class HelperFunctions:
    _templates_seen: dict[str, str]
    finalized_helpers: list[str]
    def __init__(self) -> None: ...
    def add(self, template_code: str, *, base_name=...) -> str: ...
    def __iter__(self):  # -> Iterator[str]:
        ...
    def __getitem__(self, idx): ...

@dataclasses.dataclass
class BlockParameters:
    shape: list[sympy.Expr] = ...
    block_shape: list[sympy.Expr] = ...
    strides: list[sympy.Expr] = ...
    offsets: list[sympy.Expr] = ...
    def __add__(self, other: BlockParameters) -> BlockParameters: ...

class CooperativeReductionWorkspaceCache:
    def __init__(self, args) -> None: ...
    def allocate(self, nbytes: sympy.Expr):  # -> tuple[Any, Any]:
        ...
    def on_loop_end(self):  # -> None:
        ...
    def increment_store_count(self):  # -> int:
        ...

@dataclasses.dataclass
class FixedTritonConfig:
    config: dict[str, int]
    def __getitem__(self, item):  # -> int:
        ...
    def __contains__(self, item):  # -> bool:
        ...

class TritonCSE(CSE[TritonCSEVariable, Union[str, tuple[str, str]]]):
    def augment_key(self, cache_key: str) -> Union[str, tuple[str, str]]: ...

@dataclasses.dataclass
class TMACompatibilityChecker:
    kernel: TritonKernel
    dtype: torch.dtype
    for_store: bool
    def __post_init__(self):  # -> None:
        ...
    def can_use_tma(self) -> bool: ...
    def are_block_parameters_compatible(self, block_params: BlockParameters) -> bool: ...

class TritonKernel(SIMDKernel[TritonCSEVariable]):
    overrides = TritonKernelOverrides
    helper_functions: HelperFunctions
    kexpr: Callable[[sympy.Expr], str] = ...
    allow_block_ptr = ...
    tma_compatibility_checker_cls = TMACompatibilityChecker
    def __init__(
        self,
        tiling: dict[str, sympy.Expr],
        min_elem_per_thread=...,
        optimize_mask=...,
        fixed_config: Optional[FixedTritonConfig] = ...,
        hint_override: Optional[int] = ...,
        **kwargs,
    ) -> None: ...
    def dtype_to_str(self, dtype: torch.dtype) -> str: ...
    def should_use_cooperative_reduction(self) -> bool: ...
    def init_cooperative_reduction(self):  # -> None:

        ...
    def init_cooperative_reduction_mask(self):  # -> None:
        ...
    def codegen_range_tree(self):  # -> None:
        ...
    def need_numel_args(self):  # -> Literal[True]:

        ...
    def should_use_persistent_reduction(self) -> bool: ...
    def want_no_x_dim(self):  # -> bool | None:
        ...
    @property
    def assert_function(self) -> str: ...
    def indexing(
        self,
        index: sympy.Expr,
        *,
        copy_shape=...,
        dense_indexing=...,
        override_mask=...,
        block_ptr=...,
        tma_compatibility_checker: Optional[TMACompatibilityChecker] = ...,
    ):  # -> BlockDescriptorOptions | IndexingOptions:

        ...
    def codegen_block_ptr(
        self, name: str, var: str, indexing: Union[BlockPtrOptions, TensorDescriptorOptions], other=...
    ) -> tuple[str, str]: ...
    def codegen_block_ptr_store_line(self, name, indexing, block_ptr, value, other=...):  # -> str:
        ...
    def check_bounds(self, expr: sympy.Expr, size: sympy.Expr, lower: bool, upper: bool):  # -> None:
        ...
    def get_load_buffer(self, indexing):  # -> IndentedBuffer:
        ...
    def load(self, name: str, index: sympy.Expr):  # -> TritonCSEVariable:

        ...
    def store(self, name: str, index: sympy.Expr, value: CSEVariable, mode: StoreMode = ...) -> None: ...
    def guard_cooperative_store(self, name, buffer): ...
    def bucketize(
        self,
        values: CSEVariable,
        boundaries: tuple[str, sympy.Expr, sympy.Expr, sympy.Expr],
        boundary_indices: CSEVariable,
        indexing_dtype: torch.dtype,
        right: bool,
        sorter: Optional[tuple[str, sympy.Expr]] = ...,
        sorter_indices: Optional[CSEVariable] = ...,
    ) -> CSEVariable: ...
    def reduction_resize(self, value) -> str: ...
    def reduction_resize_and_shape(self, value, shape) -> tuple[str, BlockShapeType]: ...
    def reduction_collapse_dims(self, buffer, value: CSEVariable, dtype: torch.dtype) -> CSEVariable: ...
    def reduction(
        self,
        dtype: torch.dtype,
        src_dtype: torch.dtype,
        reduction_type: ReductionType,
        value: Union[CSEVariable, tuple[CSEVariable, ...]],
    ) -> Union[CSEVariable, tuple[CSEVariable, ...]]: ...
    def welford_reduce(self, result_var, reduction_type, value, where_cond, acc_type, dtype):  # -> tuple[Any, ...]:

        ...
    def welford_reduce_final_reduction(
        self, buffer, result_mean, result_m2, result_weight, mean, m2, weight, dim, dtype
    ):  # -> tuple[Any, ...]:

        ...
    def online_softmax_reduce_final_reduction(
        self, buffer, result_max, result_sum, peer_max, peer_sum, dim, dtype
    ):  # -> tuple[Any, Any]:
        ...
    def max_rsplit(self):  # -> int:
        ...
    def codegen_cooperative_reduction_peer_combine(self, result_var, dtype, default_val) -> CSEVariable: ...
    def store_reduction(
        self, name: str, index: sympy.Expr, value: Union[CSEVariable, tuple[CSEVariable, ...]]
    ):  # -> None:
        ...
    def scan(
        self,
        dtypes: tuple[torch.dtype, ...],
        combine_fn: Callable[[tuple[CSEVariable, ...], tuple[CSEVariable, ...]], tuple[CSEVariable, ...]],
        values: tuple[CSEVariable, ...],
    ) -> tuple[CSEVariable, ...]: ...
    def sort(
        self, dtypes: tuple[torch.dtype, ...], values: tuple[CSEVariable, ...], stable: bool, descending: bool
    ) -> tuple[CSEVariable, ...]: ...
    def codegen_body(self):  # -> None:

        ...
    def kernel_benchmark_extra_args(self) -> list[str]: ...
    def codegen_kernel_benchmark(self, num_gb):  # -> IndentedBuffer:
        ...
    def imports_for_benchmark_kernel(self):  # -> str:
        ...
    @staticmethod
    def inductor_meta_common():  # -> dict[str, Any | bool | int | None]:
        ...
    def codegen_kernel(self, name=...) -> str: ...
    @staticmethod
    def has_persistent_RBLOCK(rnumel):  # -> bool:
        ...
    def codegen_static_numels(self, code):  # -> None:

        ...
    def add_numel_to_call_args(self, name, call_args, arg_types):  # -> None:
        ...
    def call_kernel(self, name: str, node: Optional[IRNode] = ...):  # -> None:
        ...
    def codegen_nan_check(self) -> None: ...
    def create_cse_var(self, *args, **kwargs) -> TritonCSEVariable: ...
    def codegen_iteration_ranges_entry(self, entry: IterationRangesEntry):  # -> None:
        ...
    def iteration_ranges_ranges_code(self, entry: IterationRangesRoot) -> str: ...
    def iteration_ranges_scalar_code(self, entry: IterationRangesRoot, value: Any) -> str: ...
    def iteration_ranges_get_pid(self, entry: IterationRangesRoot) -> str: ...
    def needs_yz_grid_overflow(self, entry: IterationRangesRoot) -> bool: ...
    def max_block(self, prefix: str) -> int: ...
    def filter_masks(self, mask_vars: OrderedSet[str]) -> None: ...
    @cache_on_self
    def get_reduction_prefixes(self) -> list[str]: ...
    def codegen_reduction_numels(self, buffer: IndentedBuffer) -> None: ...
    def codegen_reduction_indices(self, buffer: IndentedBuffer) -> None: ...
    def iteration_ranges_codegen_header(self, entry: IterationRangesRoot, code: IndentedBuffer) -> None: ...

class TritonScheduling(SIMDScheduling):
    kernel_type: type[Any] = ...
    backend_features = ...
    def __init__(self, scheduler: Optional[Scheduler]) -> None: ...
    @classmethod
    def get_backend_features(cls, device: torch.device):  # -> OrderedSet[BackendFeature]:
        ...
    def codegen_comment(self, node_schedule):  # -> None:
        ...
    def define_kernel(self, src_code, node_schedule, kernel):  # -> str:
        ...
    def benchmark_fused_nodes(self, nodes, n_spills_threshold=...) -> tuple[float, str]: ...
    def benchmark_codegened_module(
        self, mod, n_spills_threshold=..., node_names: Optional[OrderedSet[str]] = ...
    ) -> tuple[float, str]: ...
    def create_kernel_choices(
        self, kernel_features: SIMDKernelFeatures, kernel_args: list[Any], kernel_kwargs: dict[str, Any]
    ) -> list[TritonKernel]: ...
    def add_multi_kernel_choices(
        self, kernel: TritonKernel, kernel_args: list[Any], kernel_kwargs: dict[str, Any]
    ) -> list[TritonKernel]: ...
    def benchmark_combo_kernel(self, node_list):  # -> tuple[float | Literal[0], float, list[Any]]:
        ...

def debug_triton_code(node: BaseSchedulerNode) -> list[str]: ...
