import sympy
from torch._inductor.codegen.triton import TritonKernel

class TritonSplitScanKernel(TritonKernel):
    """
    Generates a triton kernel that supports ops.scan calls while also splitting
    the reduction dimension over multiple triton programs.

    For this kernel, loop numels will always take the form ``(xdim, rdim)``
    and the grid has the shape ``(CeilDiv(rdim, RBLOCK), xdim)``. Communication
    between blocks occurs within a global memory workspace buffer, which
    must be zero-filled before launching the kernel.

    Note that generation for ``ops.reduction`` is not supported.

    For details of the communication strategy, see
    https://research.nvidia.com/publication/2016-03_single-pass-parallel-prefix-scan-decoupled-look-back
    """
    def __init__(self, tiling: dict[str, sympy.Expr], pid_cache=..., fixed_config=..., **kwargs) -> None: ...
    def should_use_persistent_reduction(self) -> bool: ...
    def should_use_cooperative_reduction(self) -> bool: ...
    def initialize_range_tree(self, pid_cache): ...
    def reduction(self, dtype, src_dtype, reduction_type, value): ...
    def scan(self, dtypes, combine_fn, values):
        """Perform an associative scan on 'values'."""
