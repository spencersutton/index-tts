import contextlib
import dataclasses
from collections.abc import Callable, Iterator, Sequence
from typing import Any

import sympy
import torch
import torch._ops
import torch.utils._pytree as pytree
import triton
from sympy import Expr
from torch.utils._ordered_set import OrderedSet

from .. import ir
from ..graph import GraphLowering
from ..ir import IRNode
from ..utils import IndentedBuffer, LineContext, cache_on_self
from .common import CodeGen, WorkspaceArg
from .wrapper_fxir import FxConverter

log = ...
pexpr = ...
type ReuseKey = tuple[torch.device, torch.dtype, str, bool]
type BufferLike = ir.Buffer | WorkspaceArg
type FxConversionFunc = Callable[[WrapperLine], None]

def buffer_reuse_key(node: BufferLike) -> ReuseKey: ...
def can_match_buffer_size(input_buf: BufferLike, output_buf: BufferLike): ...

type TritonMetaParams = dict[str, int]
type TritonGrid = tuple[int | sympy.Expr, ...] | Callable[[TritonMetaParams], tuple[int, ...]]

def user_defined_kernel_grid_fn_code(
    name: str,
    configs: list[triton.Config],
    grids: list[TritonGrid],
    wrapper: PythonWrapperCodegen | None = ...,
    original_fxnode_name: str | None = ...,
) -> tuple[str, str]: ...
def user_defined_triton_kernel_transitive_closure_source_code(kernel) -> str:
    """
    Given a triton kernel function pointer collect the transitive closure of
    its dependencies
    """

@dataclasses.dataclass
class SymbolicCallArg:
    """SymbolicCallArg(inner: 'sympy.Symbol', inner_expr: 'sympy.Expr')"""

    inner: sympy.Symbol
    inner_expr: sympy.Expr

class MemoryPlanningState:
    def __init__(self) -> None: ...
    def __contains__(self, key: ReuseKey) -> bool: ...
    def pop(self, key: ReuseKey) -> FreeIfNotReusedLine: ...
    def push(self, key: ReuseKey, item: FreeIfNotReusedLine) -> None: ...

class WrapperLine:
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class EnterSubgraphLine(WrapperLine):
    """EnterSubgraphLine(wrapper: 'PythonWrapperCodegen', graph: 'GraphLowering')"""

    wrapper: PythonWrapperCodegen
    graph: GraphLowering
    def __post_init__(self) -> None: ...
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class CommentLine(WrapperLine):
    """CommentLine(line: 'LineContext')"""

    line: LineContext
    def codegen(self, code: IndentedBuffer) -> None: ...
    @staticmethod
    def codegen_fx(converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class ExitSubgraphLine(WrapperLine):
    """ExitSubgraphLine(wrapper: 'PythonWrapperCodegen')"""

    wrapper: PythonWrapperCodegen
    def __post_init__(self) -> None: ...
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class EnterDeviceContextManagerLine(WrapperLine):
    """EnterDeviceContextManagerLine(device_idx: 'int', last_seen_device_guard_index: 'Optional[int]')"""

    device_idx: int
    last_seen_device_guard_index: int | None
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

class ExitDeviceContextManagerLine(WrapperLine):
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class ExternKernelAllocLine(WrapperLine):
    """ExternKernelAllocLine(wrapper: 'PythonWrapperCodegen', node: 'ir.ExternKernelAlloc')"""

    wrapper: PythonWrapperCodegen
    node: ir.ExternKernelAlloc
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class ExternKernelOutLine(WrapperLine):
    """ExternKernelOutLine(wrapper: 'PythonWrapperCodegen', node: 'ir.ExternKernelOut')"""

    wrapper: PythonWrapperCodegen
    node: ir.ExternKernelOut
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class FreeLine(WrapperLine):
    """FreeLine(wrapper: 'PythonWrapperCodegen', node: 'Union[BufferLike, ir.TorchBindObject]')"""

    wrapper: PythonWrapperCodegen
    node: BufferLike | ir.TorchBindObject
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class KernelCallLine(WrapperLine):
    """KernelCallLine(wrapper: 'PythonWrapperCodegen', kernel_name: 'str', call_args: 'tuple[Any, ...]', raw_keys: 'tuple[Any, ...]', raw_args: 'tuple[Any, ...]', arg_types: 'list[str]', triton: 'bool', triton_meta: 'dict[str, Any]', device: 'torch.device', graph_name: 'str', original_fxnode_name: 'str')"""

    wrapper: PythonWrapperCodegen
    kernel_name: str
    call_args: tuple[Any, ...]
    raw_keys: tuple[Any, ...]
    raw_args: tuple[Any, ...]
    arg_types: list[str]
    triton: bool
    triton_meta: dict[str, Any]
    device: torch.device
    graph_name: str
    original_fxnode_name: str
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class KernelDefinitionLine(WrapperLine):
    """KernelDefinitionLine(wrapper: 'PythonWrapperCodegen', kernel_name: 'str', kernel_body: 'str', metadata: 'Optional[str]' = None, gpu: 'bool' = True, cpp_definition: 'Optional[str]' = None)"""

    wrapper: PythonWrapperCodegen
    kernel_name: str
    kernel_body: str
    metadata: str | None = ...
    gpu: bool = ...
    cpp_definition: str | None = ...
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class MemoryPlanningLine(WrapperLine):
    """MemoryPlanningLine(wrapper: 'PythonWrapperCodegen')"""

    wrapper: PythonWrapperCodegen
    def plan(self, state: MemoryPlanningState) -> MemoryPlanningLine:
        """First pass to find reuse"""
    def codegen(self, code: IndentedBuffer) -> None:
        """Second pass to output code"""

class EfficientPeakEstimate:
    def __init__(self) -> None: ...
    def peak_between(self, line_a: FreeIfNotReusedLine, line_b: AllocateLine): ...
    def update_peak_between(self, line_a: FreeIfNotReusedLine, line_b: AllocateLine): ...

@dataclasses.dataclass
class AllocateLine(MemoryPlanningLine):
    """AllocateLine(wrapper: 'PythonWrapperCodegen', node: 'BufferLike')"""

    node: BufferLike
    def __post_init__(self): ...
    def should_reuse_buffer(self, free_line: FreeIfNotReusedLine, size: int) -> bool: ...
    def plan(self, state: MemoryPlanningState) -> MemoryPlanningLine: ...
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class FreeIfNotReusedLine(MemoryPlanningLine):
    """FreeIfNotReusedLine(wrapper: 'PythonWrapperCodegen', node: 'BufferLike', is_reused: 'bool' = False)"""

    node: BufferLike
    is_reused: bool = ...
    def __post_init__(self): ...
    def plan(self, state: MemoryPlanningState) -> MemoryPlanningLine: ...
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class ReinterpretLine(MemoryPlanningLine):
    """ReinterpretLine(wrapper: 'PythonWrapperCodegen', node: 'BufferLike', reused_as: 'BufferLike', layout: 'ir.Layout')"""

    node: BufferLike
    reused_as: BufferLike
    layout: ir.Layout
    def plan(self, state: MemoryPlanningState) -> MemoryPlanningLine: ...
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class ReuseLine(MemoryPlanningLine):
    """ReuseLine(wrapper: 'PythonWrapperCodegen', node: 'BufferLike', reused_as: 'BufferLike', delete_old: 'bool' = True)"""

    node: BufferLike
    reused_as: BufferLike
    delete_old: bool = ...
    def plan(self, state: MemoryPlanningState) -> MemoryPlanningLine: ...
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

class NullLine(MemoryPlanningLine):
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class CommBufferLine(WrapperLine):
    """CommBufferLine(wrapper: 'PythonWrapperCodegen', node: 'ir.Buffer')"""

    wrapper: PythonWrapperCodegen
    node: ir.Buffer
    @property
    def size(self) -> int: ...
    @property
    def comm_buffer_type(self) -> ir.CommBufferType: ...
    @property
    def group_name(self) -> str: ...

@dataclasses.dataclass
class CommBufferAllocateLine(CommBufferLine):
    """CommBufferAllocateLine(wrapper: 'PythonWrapperCodegen', node: 'ir.Buffer')"""
    def codegen(self, code: IndentedBuffer) -> None: ...
    @staticmethod
    def make_allocation_line(comm_buffer_type, group_name, wrapper, name, device, dtype, shape, stride): ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class CommBufferFreeLine(CommBufferLine):
    """CommBufferFreeLine(wrapper: 'PythonWrapperCodegen', node: 'ir.Buffer')"""
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class MultiOutputLine(WrapperLine):
    """Given a MultiOutputLayout buffer, indexes actual buffer(s) from the result."""

    wrapper: PythonWrapperCodegen
    result_name: str
    arg_name: str
    indices: Sequence[Any]
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class SymbolicCallArgLine(WrapperLine):
    """SymbolicCallArgLine(wrapper: 'PythonWrapperCodegen', arg: 'SymbolicCallArg', graph: 'GraphLowering')"""

    wrapper: PythonWrapperCodegen
    arg: SymbolicCallArg
    graph: GraphLowering
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

BufferName = str
type Line = MemoryPlanningLine | LineContext

class PythonWrapperCodegen(CodeGen):
    """Generate outer wrapper in Python that calls the kernels."""

    supports_caching = ...
    def __init__(self) -> None: ...
    @staticmethod
    def create(
        is_subgraph: bool,
        subgraph_name: str | None,
        parent_wrapper: PythonWrapperCodegen | None,
        partition_signatures: ir.GraphPartitionSignature | None = ...,
    ): ...
    def set_launcher_fn_name(self) -> None: ...
    def write_constant(self, name: str, hashed: str) -> None: ...
    def write_header(self) -> None: ...
    def include_extra_header(self, header: str): ...
    def write_kernel_autotune_defs_header(self) -> None: ...
    @cache_on_self
    def write_triton_header_once(self) -> None: ...
    def write_get_raw_stream_header(self) -> None: ...
    @cache_on_self
    def write_get_raw_stream_header_once(self) -> None: ...
    def add_meta_once(self, meta: TritonMetaParams) -> str: ...
    @cache_on_self
    def get_output_refs(self) -> list[str]: ...
    def mark_output_type(self) -> None: ...
    def get_graph_inputs(self) -> dict[str, ir.TensorBox | ir.TorchBindObject | sympy.Expr]: ...
    def get_graph_outputs(self) -> list[IRNode]: ...
    def codegen_input_size_asserts(self) -> None: ...
    def codegen_input_nan_asserts(self) -> None: ...
    def write_async_compile_wait(self) -> None: ...
    def write_args(self, input_names: list[str]): ...
    def write_launcher_fn_call_get_indent(self) -> int: ...
    def get_graph_input_names(self) -> list[str]: ...
    def write_prefix(self) -> None: ...
    def codegen_input_size_and_nan_asserts(self) -> None: ...
    def write_get_raw_stream(self, device_idx: int, graph_name: str) -> str: ...
    def get_codegened_graph(self): ...
    def push_codegened_graph(self, graph): ...
    def pop_codegened_graph(self): ...
    def push_computed_sizes(self, computed_sizes): ...
    def pop_computed_sizes(self): ...
    def next_kernel_suffix(self) -> str: ...
    def codegen_device_guard_enter(self, device_idx: int) -> None: ...
    def codegen_device_guard_exit(self) -> None: ...
    def generate_return(self, output_refs: list[str]) -> None: ...
    def generate_before_suffix(self, result: IndentedBuffer) -> None: ...
    def generate_after_suffix(self, result: IndentedBuffer) -> None: ...
    def generate_end(self, result: IndentedBuffer) -> None: ...
    def generate_fallback_kernel(self, node: ir.FallbackKernel) -> None: ...
    def generate_extern_kernel_alloc(self, node: ir.ExternKernelAlloc): ...
    def generate_extern_kernel_out(self, node: ir.ExternKernelOut) -> None: ...
    def generate_tma_descriptor(self, desc): ...
    def generate_scatter_fallback(
        self, output, inputs, cpp_kernel_name, python_kernel_name, src_is_tensor, reduce, kwargs
    ): ...
    def generate_index_put_fallback(self, kernel, x, indices, values, accumulate): ...
    def generate_fallback_kernel_with_runtime_lookup(
        self,
        buf_name: str,
        python_kernel_name: str,
        get_args: Callable[[], Sequence[str]],
        op_overload: torch._ops.OpOverload | torch._ops.HigherOrderOperator,
        raw_args: Sequence[Any],
        outputs: Sequence[ir.Buffer],
    ) -> None: ...
    def generate(self, is_inference): ...
    def get_wrapper_call_indent(self) -> int: ...
    @contextlib.contextmanager
    def set_writeline(self, new: Callable[..., None]) -> Iterator[Callable[..., None]]: ...
    def generate_and_run_autotune_block(self):
        """
        Compose self.kernel_autotune_defs and self.kernel_autotune_calls into a single block of
        code and execute it to trigger Triton kernel compilation and auto-tuning
        """
    def memory_plan(self): ...
    def memory_plan_reuse(self): ...
    def run_wrapper_ir_passes(self, is_inference: bool): ...
    def codegen_input_symbol_assignment(self, name: str, value: ir.TensorBox, bound_vars: OrderedSet[sympy.Symbol]): ...
    def codegen_inputs(self):
        """Assign all symbolic shapes to locals"""
    def ensure_size_computed(self, sym: sympy.Symbol): ...
    def finalize_prefix(self): ...
    def codegen_cpp_sizevar(self, x: Expr, *, simplify: bool = ...) -> str: ...
    def codegen_python_sizevar(self, x: Expr, *, simplify: bool = ...) -> str: ...
    def codegen_sizevar(self, x: Expr) -> str: ...
    def codegen_tuple_access(self, basename: str, name: str, index: str) -> str: ...
    def codegen_python_shape_tuple(self, shape: Sequence[Expr]) -> str: ...
    def codegen_shape_tuple(self, shape: Sequence[Expr]) -> str: ...
    def codegen_alloc_from_pool(self, name, offset, dtype, shape, stride) -> tuple[str, list[str]]: ...
    def codegen_reinterpret_view(
        self, data, size, stride, offset, writeline: Callable[..., None], dtype=...
    ) -> str: ...
    def codegen_device_copy(self, src, dst, non_blocking: bool | str): ...
    def codegen_multi_output(self, node: ir.MultiOutput): ...
    def codegen_dynamic_select_index(self, node): ...
    def codegen_dynamic_scalar(self, node): ...
    def benchmark_compiled_module(self, output): ...
    def add_benchmark_harness(self, output):
        """Append a benchmark harness to generated code for debugging"""
    def define_kernel(
        self,
        kernel_name: str,
        kernel_body: str,
        metadata: str | None = ...,
        gpu: bool = ...,
        cpp_definition: str | None = ...,
    ): ...
    def define_subgraph_launcher_fn(self, fn_code: str): ...
    def define_user_defined_triton_kernel(
        self, kernel, configs, kwargs, restore_value_args, reset_to_zero_args, grids: list[list[int | sympy.Expr]]
    ): ...
    def generate_numel_expr(self, kernel_name: str, tree, suffix: str | None = ...): ...
    def generate_workspace_allocation(self, ws: WorkspaceArg): ...
    def generate_workspace_deallocation(self, ws: WorkspaceArg): ...
    def make_zero_buffer(self, name): ...
    def wrap_kernel_call(self, name, call_args): ...
    def generate_profiler_mark_wrapper_call(self, stack): ...
    def generate_start_graph(self): ...
    def generate_end_graph(self): ...
    def generate_reset_kernel_saved_flags(self): ...
    def generate_save_uncompiled_kernels(self):
        """
        Precompile and save the CUBINs of the Triton kernels that haven't
        been precompiled and saved as a side effect of running the generated
        JIT model (Python wrapper). This can happen when the model contains
        control flow: only one pass through the control flow operators covers
        the kernels that are saved, the remaining kernels are not launched,
        hence not saved. The main purpose of this codegen is to compile and
        save the Triton kernels outside the active control flow path for
        subsequent AOTInductor code generation and compilation.
        """
    def prepare_triton_kernel_call(self, call_args): ...
    def generate_example_arg_value(self, arg, arg_type, raw_arg=...): ...
    def generate_kernel_call(
        self,
        kernel_name: str,
        call_args,
        *,
        device=...,
        triton=...,
        arg_types=...,
        raw_keys=...,
        raw_args=...,
        triton_meta=...,
        original_fxnode_name=...,
        debug_handle: int | None = ...,
    ):
        """
        Generates kernel call code.

        triton: Defines whether the backend uses Triton for codegen. Otherwise it uses the CUDA language when gpu=True,
                and C++ when gpu=False.
        """
    def writeline(self, line): ...
    def writelines(self, lines): ...
    def enter_context(self, ctx): ...
    def val_to_arg_str(self, s, type_=...): ...
    def make_buffer_allocation(self, buffer: BufferLike): ...
    @cache_on_self
    def write_memory_track_allocation_once(self): ...
    def make_allocation(self, name, device, dtype, shape, stride, allocation_shape=..., is_pinned=...): ...
    def make_comment(self, line): ...
    def make_tensor_alias(self, new_name, old_name, comment=...): ...
    def make_buffer_free(self, buffer: BufferLike | ir.TorchBindObject): ...
    def make_free_by_names(self, names_to_del: list[str]): ...
    def codegen_exact_buffer_reuse(self, old_name: str, new_name: str, del_line: str): ...
    def write_provenance_debug_handle(self, kernel_name, debug_handle: int | None = ...): ...
    def make_buffer_reuse(self, old: BufferLike, new: BufferLike, delete_old: bool): ...
    def codegen_deferred_allocation(self, name: str, view: ir.ReinterpretView) -> None: ...
    def codegen_allocation(self, buffer: ir.Buffer): ...
    def codegen_free(self, buffer): ...
    def can_reuse(self, input_buffer, output_buffer=...): ...
    def did_reuse(self, buffer, reused_buffer): ...
    def codegen_inplace_reuse(self, input_buffer: ir.Buffer, output_buffer: ir.Buffer): ...
    def codegen_unbacked_symbol_decl(self, symbol): ...
    def codegen_unbacked_symbol_defs_for_outputs(
        self, output_name: str, outputs: Any, unbacked_bindings: dict[sympy.Symbol, pytree.KeyPath] | None
    ) -> None: ...
    def codegen_subgraph_by_inlining(self, subgraph, outer_inputs, outer_outputs): ...
    def codegen_partition_call(self, partition_id: int, partition_signatures: ir.GraphPartitionSignature):
        """Generate code to call a graph partition"""
    def set_all_partition_names(self, num_partitions: int): ...
    def codegen_subgraph_call_with_flattened_outputs(self, subgraph, outer_inputs, outer_flattened_outputs): ...
    def codegen_subgraph_call(self, subgraph, outer_inputs, outer_buffer_name): ...
    def codegen_subgraph_common(self, subgraph): ...
    def codegen_subgraph_with_flattened_outputs(self, subgraph, outer_inputs, outer_flattened_outputs): ...
    def codegen_subgraph(self, subgraph, outer_inputs, outer_buffer_name): ...
    def codegen_invoke_subgraph(self, invoke_subgraph): ...
    def codegen_conditional(self, conditional): ...
    def codegen_while_loop(self, while_loop, stack_output):
        """while_loop is codegened as a host side while_loop"""
    @staticmethod
    def statically_known_int_or_none(x): ...
    @staticmethod
    def statically_known_list_of_ints_or_none(lst): ...
    @staticmethod
    def is_statically_known_list_of_ints(lst): ...
    @staticmethod
    def static_shape_for_buffer_or_none(buffer): ...
    @staticmethod
    def can_prove_buffer_has_static_shape(buffer): ...

class SubgraphPythonWrapperCodegen(PythonWrapperCodegen):
    """
    A wrapper codegen that generates code for a subgraph. For most of the
    methods, we rely on the implementation in the PythonWrapperCodegen. But we
    override a few functions to produce cleaner code (like avoiding writing
    imports twice in the output code)
    """
    def __init__(
        self,
        subgraph_name: str,
        parent_wrapper: PythonWrapperCodegen,
        partition_signatures: ir.GraphPartitionSignature | None = ...,
    ) -> None: ...
    def set_launcher_fn_name(self) -> None: ...
    def write_header(self) -> None: ...
    def add_benchmark_harness(self, output): ...
    def benchmark_compiled_module(self, output): ...
    def write_async_compile_wait(self): ...
    def next_kernel_suffix(self) -> str: ...
    def generate_after_suffix(self, result: IndentedBuffer) -> None: ...
    def write_launcher_fn_call_get_indent(self) -> int: ...
    def get_wrapper_call_indent(self) -> int: ...
    def get_graph_inputs(self) -> dict[str, ir.TensorBox | ir.TorchBindObject | sympy.Expr]: ...
    def get_graph_input_names(self) -> list[str]: ...
    def get_graph_outputs(self) -> list[IRNode]: ...
    def codegen_allocation(self, buffer: ir.Buffer): ...
    @cache_on_self
    def write_triton_header_once(self) -> None: ...
    @cache_on_self
    def write_get_raw_stream_header_once(self) -> None: ...
