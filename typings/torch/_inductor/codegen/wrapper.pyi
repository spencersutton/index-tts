import contextlib
import dataclasses
import sympy
import torch
import torch._ops
import torch.utils._pytree as pytree
import triton
from typing import Any, Optional, TYPE_CHECKING, Union, TypeAlias
from collections.abc import Callable
from sympy import Expr
from torch.utils._ordered_set import OrderedSet
from .. import ir
from ..ir import IRNode, ReinterpretView
from ..utils import IndentedBuffer, LineContext, cache_on_self
from .common import CodeGen, WorkspaceArg
from collections.abc import Iterator, Sequence
from ..graph import GraphLowering
from .wrapper_fxir import FxConverter

if TYPE_CHECKING: ...
log = ...
pexpr = ...
type ReuseKey = tuple[torch.device, torch.dtype, str, bool]
type BufferLike = ir.Buffer | WorkspaceArg
type FxConversionFunc = Callable[[WrapperLine], None]

def buffer_reuse_key(node: BufferLike) -> ReuseKey: ...
def can_match_buffer_size(input_buf: BufferLike, output_buf: BufferLike):  # -> bool:
    ...

type TritonMetaParams = dict[str, int]
type TritonGrid = tuple[int | sympy.Expr, ...] | Callable[[TritonMetaParams], tuple[int, ...]]

def user_defined_kernel_grid_fn_code(
    name: str,
    configs: list[triton.Config],
    grids: list[TritonGrid],
    wrapper: PythonWrapperCodegen | None = ...,
    original_fxnode_name: str | None = ...,
) -> tuple[str, str]: ...
def user_defined_triton_kernel_transitive_closure_source_code(kernel) -> str: ...

@dataclasses.dataclass
class SymbolicCallArg:
    inner: sympy.Symbol
    inner_expr: sympy.Expr

class MemoryPlanningState:
    def __init__(self) -> None: ...
    def __contains__(self, key: ReuseKey) -> bool: ...
    def pop(self, key: ReuseKey) -> FreeIfNotReusedLine: ...
    def push(self, key: ReuseKey, item: FreeIfNotReusedLine) -> None: ...

class WrapperLine:
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class EnterSubgraphLine(WrapperLine):
    wrapper: PythonWrapperCodegen
    graph: GraphLowering
    def __post_init__(self) -> None: ...
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class CommentLine(WrapperLine):
    line: LineContext
    def codegen(self, code: IndentedBuffer) -> None: ...
    @staticmethod
    def codegen_fx(converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class ExitSubgraphLine(WrapperLine):
    wrapper: PythonWrapperCodegen
    def __post_init__(self) -> None: ...
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class EnterDeviceContextManagerLine(WrapperLine):
    device_idx: int
    last_seen_device_guard_index: int | None
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

class ExitDeviceContextManagerLine(WrapperLine):
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class ExternKernelAllocLine(WrapperLine):
    wrapper: PythonWrapperCodegen
    node: ir.ExternKernelAlloc
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class ExternKernelOutLine(WrapperLine):
    wrapper: PythonWrapperCodegen
    node: ir.ExternKernelOut
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class FreeLine(WrapperLine):
    wrapper: PythonWrapperCodegen
    node: BufferLike | ir.TorchBindObject
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class KernelCallLine(WrapperLine):
    wrapper: PythonWrapperCodegen
    kernel_name: str
    call_args: tuple[Any, ...]
    raw_keys: tuple[Any, ...]
    raw_args: tuple[Any, ...]
    arg_types: list[str]
    triton: bool
    triton_meta: dict[str, Any]
    device: torch.device
    graph_name: str
    original_fxnode_name: str
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class KernelDefinitionLine(WrapperLine):
    wrapper: PythonWrapperCodegen
    kernel_name: str
    kernel_body: str
    metadata: str | None = ...
    gpu: bool = ...
    cpp_definition: str | None = ...
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class MemoryPlanningLine(WrapperLine):
    wrapper: PythonWrapperCodegen
    def plan(self, state: MemoryPlanningState) -> MemoryPlanningLine: ...
    def codegen(self, code: IndentedBuffer) -> None: ...

class EfficientPeakEstimate:
    def __init__(self) -> None: ...
    def peak_between(self, line_a: FreeIfNotReusedLine, line_b: AllocateLine):  # -> int:
        ...
    def update_peak_between(self, line_a: FreeIfNotReusedLine, line_b: AllocateLine):  # -> None:
        ...

@dataclasses.dataclass
class AllocateLine(MemoryPlanningLine):
    node: BufferLike
    def __post_init__(self):  # -> None:
        ...
    def should_reuse_buffer(self, free_line: FreeIfNotReusedLine, size: int) -> bool: ...
    def plan(self, state: MemoryPlanningState) -> MemoryPlanningLine: ...
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class FreeIfNotReusedLine(MemoryPlanningLine):
    node: BufferLike
    is_reused: bool = ...
    def __post_init__(self):  # -> None:
        ...
    def plan(self, state: MemoryPlanningState) -> MemoryPlanningLine: ...
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class ReinterpretLine(MemoryPlanningLine):
    node: BufferLike
    reused_as: BufferLike
    layout: ir.Layout
    def plan(self, state: MemoryPlanningState) -> MemoryPlanningLine: ...
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class ReuseLine(MemoryPlanningLine):
    node: BufferLike
    reused_as: BufferLike
    delete_old: bool = ...
    def plan(self, state: MemoryPlanningState) -> MemoryPlanningLine: ...
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

class NullLine(MemoryPlanningLine):
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class CommBufferLine(WrapperLine):
    wrapper: PythonWrapperCodegen
    node: ir.Buffer
    @property
    def size(self) -> int: ...
    @property
    def comm_buffer_type(self) -> ir.CommBufferType: ...
    @property
    def group_name(self) -> str: ...

@dataclasses.dataclass
class CommBufferAllocateLine(CommBufferLine):
    def codegen(self, code: IndentedBuffer) -> None: ...
    @staticmethod
    def make_allocation_line(comm_buffer_type, group_name, wrapper, name, device, dtype, shape, stride):  # -> str:
        ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class CommBufferFreeLine(CommBufferLine):
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class MultiOutputLine(WrapperLine):
    wrapper: PythonWrapperCodegen
    result_name: str
    arg_name: str
    indices: Sequence[Any]
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class SymbolicCallArgLine(WrapperLine):
    wrapper: PythonWrapperCodegen
    arg: SymbolicCallArg
    graph: GraphLowering
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

BufferName = str
type Line = MemoryPlanningLine | LineContext

class PythonWrapperCodegen(CodeGen):
    supports_caching = ...
    def __init__(self) -> None: ...
    @staticmethod
    def create(
        is_subgraph: bool,
        subgraph_name: str | None,
        parent_wrapper: PythonWrapperCodegen | None,
        partition_signatures: ir.GraphPartitionSignature | None = ...,
    ):  # -> SubgraphPythonWrapperCodegen | PythonWrapperCodegen:
        ...
    def set_launcher_fn_name(self) -> None: ...
    def write_constant(self, name: str, hashed: str) -> None: ...
    def write_header(self) -> None: ...
    def include_extra_header(self, header: str):  # -> None:
        ...
    def write_kernel_autotune_defs_header(self) -> None: ...
    @cache_on_self
    def write_triton_header_once(self) -> None: ...
    def write_get_raw_stream_header(self) -> None: ...
    @cache_on_self
    def write_get_raw_stream_header_once(self) -> None: ...
    def add_meta_once(self, meta: TritonMetaParams) -> str: ...
    @cache_on_self
    def get_output_refs(self) -> list[str]: ...
    def mark_output_type(self) -> None: ...
    def get_graph_inputs(self) -> dict[str, ir.TensorBox | ir.TorchBindObject | sympy.Expr]: ...
    def get_graph_outputs(self) -> list[IRNode]: ...
    def codegen_input_size_asserts(self) -> None: ...
    def codegen_input_nan_asserts(self) -> None: ...
    def write_async_compile_wait(self) -> None: ...
    def write_args(self, input_names: list[str]):  # -> None:
        ...
    def write_launcher_fn_call_get_indent(self) -> int: ...
    def get_graph_input_names(self) -> list[str]: ...
    def write_prefix(self) -> None: ...
    def codegen_input_size_and_nan_asserts(self) -> None: ...
    def write_get_raw_stream(self, device_idx: int, graph_name: str) -> str: ...
    def get_codegened_graph(self): ...
    def push_codegened_graph(self, graph):  # -> None:
        ...
    def pop_codegened_graph(self): ...
    def push_computed_sizes(self, computed_sizes):  # -> None:
        ...
    def pop_computed_sizes(self): ...
    def next_kernel_suffix(self) -> str: ...
    def codegen_device_guard_enter(self, device_idx: int) -> None: ...
    def codegen_device_guard_exit(self) -> None: ...
    def generate_return(self, output_refs: list[str]) -> None: ...
    def generate_before_suffix(self, result: IndentedBuffer) -> None: ...
    def generate_after_suffix(self, result: IndentedBuffer) -> None: ...
    def generate_end(self, result: IndentedBuffer) -> None: ...
    def generate_fallback_kernel(self, node: ir.FallbackKernel) -> None: ...
    def generate_extern_kernel_alloc(self, node: ir.ExternKernelAlloc):  # -> None:
        ...
    def generate_extern_kernel_out(self, node: ir.ExternKernelOut) -> None: ...
    def generate_tma_descriptor(self, desc):  # -> None:
        ...
    def generate_scatter_fallback(
        self, output, inputs, cpp_kernel_name, python_kernel_name, src_is_tensor, reduce, kwargs
    ):  # -> None:
        ...
    def generate_index_put_fallback(self, kernel, x, indices, values, accumulate):  # -> None:
        ...
    def generate_fallback_kernel_with_runtime_lookup(
        self,
        buf_name: str,
        python_kernel_name: str,
        get_args: Callable[[], Sequence[str]],
        op_overload: torch._ops.OpOverload | torch._ops.HigherOrderOperator,
        raw_args: Sequence[Any],
        outputs: Sequence[ir.Buffer],
    ) -> None: ...
    def generate(self, is_inference):  # -> tuple[ValueWithLineMap, ValueWithLineMap]:
        ...
    def get_wrapper_call_indent(self) -> int: ...
    @contextlib.contextmanager
    def set_writeline(self, new: Callable[..., None]) -> Iterator[Callable[..., None]]: ...
    def generate_and_run_autotune_block(self):  # -> None:

        ...
    def memory_plan(self):  # -> None:
        ...
    def memory_plan_reuse(self):  # -> None:
        ...
    def run_wrapper_ir_passes(self, is_inference: bool):  # -> None:
        ...
    def codegen_input_symbol_assignment(
        self, name: str, value: ir.TensorBox, bound_vars: OrderedSet[sympy.Symbol]
    ):  # -> None:
        ...
    def codegen_inputs(self):  # -> None:

        ...
    def ensure_size_computed(self, sym: sympy.Symbol):  # -> None:
        ...
    def finalize_prefix(self):  # -> None:
        ...
    def codegen_cpp_sizevar(self, x: Expr, *, simplify: bool = ...) -> str: ...
    def codegen_python_sizevar(self, x: Expr, *, simplify: bool = ...) -> str: ...
    def codegen_sizevar(self, x: Expr) -> str: ...
    def codegen_tuple_access(self, basename: str, name: str, index: str) -> str: ...
    def codegen_python_shape_tuple(self, shape: Sequence[Expr]) -> str: ...
    def codegen_shape_tuple(self, shape: Sequence[Expr]) -> str: ...
    def codegen_alloc_from_pool(self, name, offset, dtype, shape, stride) -> tuple[str, list[str]]: ...
    def codegen_reinterpret_view(
        self, data, size, stride, offset, writeline: Callable[..., None], dtype=...
    ) -> str: ...
    def codegen_device_copy(self, src, dst, non_blocking: bool | str):  # -> None:
        ...
    def codegen_multi_output(self, node: ir.MultiOutput):  # -> None:
        ...
    def codegen_dynamic_select_index(self, node):  # -> None:
        ...
    def codegen_dynamic_scalar(self, node):  # -> None:
        ...
    def benchmark_compiled_module(self, output):  # -> None:
        ...
    def add_benchmark_harness(self, output):  # -> None:

        ...
    def define_kernel(
        self,
        kernel_name: str,
        kernel_body: str,
        metadata: str | None = ...,
        gpu: bool = ...,
        cpp_definition: str | None = ...,
    ):  # -> None:
        ...
    def define_subgraph_launcher_fn(self, fn_code: str):  # -> None:
        ...
    def define_user_defined_triton_kernel(
        self, kernel, configs, kwargs, restore_value_args, reset_to_zero_args, grids: list[list[int | sympy.Expr]]
    ):  # -> tuple[str, Any, list[Any]] | tuple[str, dict[str, Any], list[Any]]:
        ...
    def generate_numel_expr(self, kernel_name: str, tree, suffix: str | None = ...):  # -> SymbolicCallArg:
        ...
    def generate_workspace_allocation(self, ws: WorkspaceArg):  # -> None:
        ...
    def generate_workspace_deallocation(self, ws: WorkspaceArg):  # -> None:
        ...
    def make_zero_buffer(self, name):  # -> str:
        ...
    def wrap_kernel_call(self, name, call_args):  # -> str:
        ...
    def generate_profiler_mark_wrapper_call(self, stack):  # -> None:
        ...
    def generate_start_graph(self):  # -> None:
        ...
    def generate_end_graph(self):  # -> None:
        ...
    def generate_reset_kernel_saved_flags(self):  # -> None:
        ...
    def generate_save_uncompiled_kernels(self):  # -> None:

        ...
    def prepare_triton_kernel_call(self, call_args):  # -> list[str]:
        ...
    def generate_example_arg_value(self, arg, arg_type, raw_arg=...):  # -> str:
        ...
    def generate_kernel_call(
        self,
        kernel_name: str,
        call_args,
        *,
        device=...,
        triton=...,
        arg_types=...,
        raw_keys=...,
        raw_args=...,
        triton_meta=...,
        original_fxnode_name=...,
        debug_handle: int | None = ...,
    ):  # -> None:

        ...
    def writeline(self, line):  # -> None:
        ...
    def writelines(self, lines):  # -> None:
        ...
    def enter_context(self, ctx):  # -> None:
        ...
    def val_to_arg_str(self, s, type_=...):  # -> str:
        ...
    def make_buffer_allocation(self, buffer: BufferLike):  # -> str:
        ...
    @cache_on_self
    def write_memory_track_allocation_once(self):  # -> None:
        ...
    def make_allocation(self, name, device, dtype, shape, stride, allocation_shape=..., is_pinned=...):  # -> str:
        ...
    def make_comment(self, line):  # -> None:
        ...
    def make_tensor_alias(self, new_name, old_name, comment=...):  # -> str:
        ...
    def make_buffer_free(self, buffer: BufferLike | ir.TorchBindObject):  # -> str:
        ...
    def make_free_by_names(self, names_to_del: list[str]):  # -> str:
        ...
    def codegen_exact_buffer_reuse(self, old_name: str, new_name: str, del_line: str):  # -> str:
        ...
    def write_provenance_debug_handle(self, kernel_name, debug_handle: int | None = ...):  # -> None:
        ...
    def make_buffer_reuse(self, old: BufferLike, new: BufferLike, delete_old: bool):  # -> str:
        ...
    def codegen_deferred_allocation(self, name: str, view: ir.ReinterpretView) -> None: ...
    def codegen_allocation(self, buffer: ir.Buffer):  # -> None:
        ...
    def codegen_free(self, buffer):  # -> None:
        ...
    def can_reuse(self, input_buffer, output_buffer=...):  # -> bool:
        ...
    def did_reuse(self, buffer, reused_buffer):  # -> Literal[False]:
        ...
    def codegen_inplace_reuse(self, input_buffer: ir.Buffer, output_buffer: ir.Buffer):  # -> None:
        ...
    def codegen_unbacked_symbol_decl(self, symbol):  # -> str:
        ...
    def codegen_unbacked_symbol_defs_for_outputs(
        self, output_name: str, outputs: Any, unbacked_bindings: dict[sympy.Symbol, pytree.KeyPath] | None
    ) -> None: ...
    def codegen_subgraph_by_inlining(self, subgraph, outer_inputs, outer_outputs):  # -> None:
        ...
    def codegen_partition_call(self, partition_id: int, partition_signatures: ir.GraphPartitionSignature):  # -> None:

        ...
    def set_all_partition_names(self, num_partitions: int):  # -> None:
        ...
    def codegen_subgraph_call_with_flattened_outputs(self, subgraph, outer_inputs, outer_flattened_outputs):  # -> None:
        ...
    def codegen_subgraph_call(self, subgraph, outer_inputs, outer_buffer_name):  # -> None:
        ...
    def codegen_subgraph_common(self, subgraph):  # -> None:
        ...
    def codegen_subgraph_with_flattened_outputs(self, subgraph, outer_inputs, outer_flattened_outputs):  # -> None:
        ...
    def codegen_subgraph(self, subgraph, outer_inputs, outer_buffer_name):  # -> None:
        ...
    def codegen_invoke_subgraph(self, invoke_subgraph):  # -> None:
        ...
    def codegen_conditional(self, conditional):  # -> None:
        ...
    def codegen_while_loop(self, while_loop, stack_output):  # -> None:

        ...
    @staticmethod
    def statically_known_int_or_none(x):  # -> int | None:
        ...
    @staticmethod
    def statically_known_list_of_ints_or_none(lst):  # -> list[Any] | None:
        ...
    @staticmethod
    def is_statically_known_list_of_ints(lst):  # -> bool:
        ...
    @staticmethod
    def static_shape_for_buffer_or_none(buffer):  # -> list[Any] | None:
        ...
    @staticmethod
    def can_prove_buffer_has_static_shape(buffer):  # -> bool:
        ...

class SubgraphPythonWrapperCodegen(PythonWrapperCodegen):
    def __init__(
        self,
        subgraph_name: str,
        parent_wrapper: PythonWrapperCodegen,
        partition_signatures: ir.GraphPartitionSignature | None = ...,
    ) -> None: ...
    def set_launcher_fn_name(self) -> None: ...
    def write_header(self) -> None: ...
    def add_benchmark_harness(self, output):  # -> None:
        ...
    def benchmark_compiled_module(self, output):  # -> None:
        ...
    def write_async_compile_wait(self):  # -> None:
        ...
    def next_kernel_suffix(self) -> str: ...
    def generate_after_suffix(self, result: IndentedBuffer) -> None: ...
    def write_launcher_fn_call_get_indent(self) -> int: ...
    def get_wrapper_call_indent(self) -> int: ...
    def get_graph_inputs(self) -> dict[str, ir.TensorBox | ir.TorchBindObject | sympy.Expr]: ...
    def get_graph_input_names(self) -> list[str]: ...
    def get_graph_outputs(self) -> list[IRNode]: ...
    def codegen_allocation(self, buffer: ir.Buffer):  # -> None:
        ...
    @cache_on_self
    def write_triton_header_once(self) -> None: ...
    @cache_on_self
    def write_get_raw_stream_header_once(self) -> None: ...
