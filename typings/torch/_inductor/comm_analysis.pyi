import functools
from enum import IntEnum

from . import ir

log = ...

class NCCL_COLL(IntEnum):
    ALL_REDUCE = ...
    ALL_GATHER = ...
    REDUCE_SCATTER = ...
    ALL_TO_ALL = ...

class NVIDIA_GPU_TYPE(IntEnum):
    VOLTA = ...
    AMPERE = ...
    HOPPER = ...

@functools.lru_cache
def get_gpu_type() -> NVIDIA_GPU_TYPE: ...
def get_collective_type(node: ir.IRNode) -> NCCL_COLL: ...
def get_collective_input_size_bytes(node: ir.IRNode) -> int: ...
def get_collective_group_size(node: ir.IRNode) -> int: ...

class NCCL_HW(IntEnum):
    NVLINK = ...
    PCI = ...
    NET = ...

class NCCL_ALGO(IntEnum):
    TREE = ...
    RING = ...

class NCCL_PROTO(IntEnum):
    LL = ...

baseLat = ...
hwLat = ...
llMaxBws = ...

def estimate_nccl_collective_runtime_nccl_estimator(snode) -> float | None: ...
def estimate_nccl_collective_runtime(node: ir.IRNode) -> float:
    """
    Returns estimated NCCL collective runtime in milliseconds (ms).

    The following heuristics are copied from https://github.com/NVIDIA/nccl/blob/master/src/graph/tuning.cc.
    We aim to estimate the runtime as accurately as possible.

    Assumptions:
    - only ring algorithm (NCCL_ALGO_RING) is used
    - only Low-Latency protocol (NCCL_PROTO_LL) is used, i.e. Simple or LL128 is not used
    - 8 gpus per node  # TODO: Need to find a way to get accurate "gpus per node" and "# nodes" info.
    - collective is one of: allreduce, reducescatter, allgather
    """
