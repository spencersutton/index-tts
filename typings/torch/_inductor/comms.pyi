from dataclasses import dataclass
from typing import TYPE_CHECKING, Optional, Union

import torch
from torch._inductor.scheduler import BaseSchedulerNode

from .ir import IRNode, Operation
from .scheduler import SchedulerBuffer

if TYPE_CHECKING: ...
log = ...
overlap_log = ...
if TYPE_CHECKING: ...

def align_runtime_estimations_across_all_distributed_ranks(snodes: list[BaseSchedulerNode]):  # -> None:
    ...
def sink_waits(snodes: list[BaseSchedulerNode]) -> list[BaseSchedulerNode]: ...
def raise_comms(snodes: list[BaseSchedulerNode]) -> list[BaseSchedulerNode]: ...
def reorder_compute_for_overlap(snodes: list[BaseSchedulerNode]) -> list[BaseSchedulerNode]: ...
def reorder_communication_preserving_peak_memory(snodes: list[BaseSchedulerNode]) -> list[BaseSchedulerNode]: ...

@dataclass
class ReorderInfo:
    initial_exposed: float = ...
    final_exposed: float = ...
    limiting_factor: str = ...
    moves: int = ...
    grouped: int = ...
    grouped_info: str = ...
    @property
    def improvement(self):  # -> float:
        ...

def is_gemm_like(node: IRNode | Operation | None) -> bool: ...
def contains_gemm_like(snode: BaseSchedulerNode) -> bool: ...
def decide_global_ordering_of_comms(
    nodes: list[BaseSchedulerNode], name_to_buf, name_to_fused_node
) -> list[BaseSchedulerNode]: ...

@dataclass
class SinkWaitInfo:
    grouped: int = ...
    grouped_info: str = ...
    moves: int = ...
    moves_info: str = ...
    limiting_factor: str = ...

def sink_waits_iterative(snodes: list[BaseSchedulerNode]) -> list[BaseSchedulerNode]: ...
def estimate_op_runtime(snode: BaseSchedulerNode) -> float: ...
def node_summary(snode):  # -> str:
    ...
def visualize_overlap(order):  # -> None:
    ...
def reorder_compute_and_comm_for_overlap(snodes: list[BaseSchedulerNode]) -> list[BaseSchedulerNode]: ...
def remove_fsdp2_unsharded_param_graph_input_usage(graph: torch.fx.Graph):  # -> None:

    ...
def reinplace_fsdp_all_gather(graph: torch.fx.Graph) -> None: ...
def get_op_idx(snode):  # -> int:
    ...
def enforce_comm_ordering_for_fsdp(
    snodes: list[torch._inductor.scheduler.BaseSchedulerNode],
    name_to_buf: dict[str, torch._inductor.scheduler.SchedulerBuffer],
    name_to_fused_node: dict[str, BaseSchedulerNode],
) -> list[torch._inductor.scheduler.BaseSchedulerNode]: ...
