import contextlib
import enum
import types
from abc import ABC, abstractmethod
from collections.abc import Callable, Generator, Sequence
from contextlib import AbstractContextManager
from dataclasses import dataclass
from typing import TYPE_CHECKING, Any, ParamSpec, Protocol, TypeVar, Unpack, override

import torch.fx
from torch._inductor.cudagraph_utils import BoxedDeviceIndex, PlaceholderInfo
from torch._inductor.output_code import OutputCode
from torch._inductor.utils import BoxedBool, InputType
from torch._ops import OpOverload
from torch.export.pt2_archive._package_weights import Weights
from torch.fx import GraphModule
from typing_extensions import TypedDict

from .ir import ExternKernelNode

_P = ParamSpec("_P")
_T = TypeVar("_T")
if TYPE_CHECKING or not config.is_fbcode():
    def time_and_log(attr: str) -> Callable[[Callable[_P, _T]], Callable[_P, _T]]: ...
    def log_optimus_to_scuba(*args: object, **kwargs: object) -> None: ...

class FxCompileMode(enum.Enum):
    NORMAL = ...
    SERIALIZE = ...
    SUBPROCESS = ...

@dataclass
class FxCompileConfig:
    """FxCompileConfig(mode: 'FxCompileMode', use_async: 'bool', use_progressive: 'bool')"""

    mode: FxCompileMode
    use_async: bool
    use_progressive: bool

_fx_compile_config = ...
fx_compile_mode = ...
fx_compile_async = ...
fx_compile_progressive = ...
log = ...
perf_hint_log = ...
pre_grad_graphs_log = ...
post_grad_graphs_log = ...
static_inputs_log = ...
inductor_metrics_log = ...

def get_static_input_idxs(num_fixed: int) -> list[int]: ...
def record_original_output_strides(gm: GraphModule) -> None: ...
def split_const_gm(
    gm: GraphModule,
    skip_constructor: bool = ...,
    lifted_constant_names: list[str] | None = ...,
    skip_folding_node_fn: Callable[[torch.fx.Node], bool] | None = ...,
) -> tuple[GraphModule, dict[str, int]]:
    """
    This function takes an GraphModule input "gm".
    The gm will be split into 2 components,
      1) const_gm, which consists the subgraph of gm that can be constant folded.
      2) gm (being inplace modified,) which returns the graph after constant folding.

    If an additional "lifted_constants" argument is passed in, we will assume the gm has
    been lifted and run the transformation accordingly.

    When a "skip_folding_node_fn" callback is passed, we will skip constant folding on
    the nodes for which the callback returns True.

    const_output_index is a mapping of corresponding node name from gm to the
    output index of const_gm.
    Returns (const_gm, const_output_index)
    """

def is_tf32_warning_applicable(gm: GraphModule) -> bool: ...
def maybe_disable_comprehensive_padding(example_inputs: Sequence[InputType]) -> AbstractContextManager[None, None]:
    """
    For CPU backend, enable comprehensive padding causes some unit tests
    fail due to changing number of generated kernels. Skip for now.
    """

def maybe_disable_graph_partition(cpp_wrapper: bool, aot_mode: bool) -> AbstractContextManager[None, None]:
    """graph partition does not support cpp_wrapper and aot_mode yet."""

def fake_tensor_prop(
    gm: GraphModule, example_inputs: Sequence[InputType], force_allow_non_fake_inputs: bool = ...
) -> torch._subclasses.FakeTensorMode:
    """
    If we can not detect fake mode from the context of inputs, create one.

    The created fake mode will be returned.
    """

def get_patched_config_dict(config_patches: str | dict[str, Any] | None = ...) -> dict[str, Any]: ...
@contextlib.contextmanager
def with_fresh_cache_if_config() -> Generator[None]: ...

class _CompileFxKwargs(TypedDict, total=False):
    cudagraphs: BoxedBool | None
    static_input_idxs: Sequence[int]
    is_backward: bool
    graph_id: int | None
    cpp_wrapper: bool
    aot_mode: bool
    is_inference: bool
    layout_opt: bool | None
    extern_node_serializer: Callable[[list[ExternKernelNode]], Any] | None
    boxed_forward_device_index: BoxedDeviceIndex | None
    fx_wrapper: bool

class _CompileFxCallable(Protocol):
    def __call__(
        self, gm: GraphModule, example_inputs: Sequence[InputType], **kwargs: Unpack[_CompileFxKwargs]
    ) -> OutputCode: ...

def compile_fx_inner(
    gm: GraphModule, example_inputs: Sequence[InputType], **kwargs: Unpack[_CompileFxKwargs]
) -> OutputCode: ...

class _FxCompileStat:
    codegen_and_compile: int = ...

class FxCompile(ABC):
    """
    An FxCompile represents a mechanism that can turn a GraphModule into an
    OutputCode.
    """

    _compile_stats: dict[type[FxCompile], _FxCompileStat] = ...
    @abstractmethod
    def codegen_and_compile(
        self,
        gm: GraphModule,
        example_inputs: Sequence[InputType],
        inputs_to_check: Sequence[int],
        graph_kwargs: _CompileFxKwargs,
    ) -> OutputCode: ...

class _InProcessFxCompile(FxCompile):
    @override
    def codegen_and_compile(
        self,
        gm: GraphModule,
        example_inputs: Sequence[InputType],
        inputs_to_check: Sequence[int],
        graph_kwargs: _CompileFxKwargs,
    ) -> OutputCode:
        """Generates the OutputCode from the GraphModule and example_inputs."""

def fx_codegen_and_compile(
    gm: GraphModule,
    example_inputs: Sequence[InputType],
    inputs_to_check: Sequence[int],
    **graph_kwargs: Unpack[_CompileFxKwargs],
) -> OutputCode: ...
def get_input_idxs_to_check(inputs: Sequence[InputType], static_input_idxs: Sequence[int]) -> Sequence[int]:
    """
    This function runs at compile time, and generates a list of indices for which we
    might need to do a copy to preserve alignment requirements.
    """

def cudagraphify(
    model: Callable[..., Any],
    static_input_idxs: Sequence[int] = ...,
    *,
    device_index: int,
    stack_traces: list[str | None],
    is_backward: bool,
    is_inference: bool,
    constants: tuple[torch.Tensor, ...] = ...,
    placeholders: Sequence[PlaceholderInfo] = ...,
    mutated_input_idxs: tuple[int, ...] = ...,
) -> Callable[..., Any]: ...
def static_input(x: torch.Tensor) -> torch.Tensor:
    """Copy and input while preserving strides"""

def index_expanded_dims_and_copy_(dst: torch.Tensor, src: torch.Tensor, expanded_dims: list[int]) -> None:
    """Index into expanded dimensions of both dst and src then copy_"""

def cudagraphify_impl(
    model: Callable[..., Any], inputs: list[torch.Tensor], static_input_idxs: Sequence[int] = ...
) -> Callable[[list[InputType]], Any]:
    """Assumes inputs[static_input_idxs[i]] are always the same memory address"""

def compile_fx_aot(
    model_: GraphModule,
    example_inputs_: list[InputType],
    inner_compile: _CompileFxCallable = ...,
    config_patches: dict[str, Any] | None = ...,
) -> list[str | Weights] | str | GraphModule: ...

_graph_counter = ...

def fw_compiler_freezing(
    aot_autograd_model: GraphModule,
    aot_example_inputs: Sequence[InputType],
    dynamo_model: GraphModule,
    num_example_inputs: int,
    inner_compile: Callable[..., Any],
    cudagraphs: BoxedBool,
    graph_id: int,
    forward_device: BoxedDeviceIndex,
) -> Callable[[list[object]], Sequence[torch.Tensor]]: ...
def get_cpp_wrapper_config() -> dict[str, object]: ...
def get_cuda_device_context(gm: torch.fx.GraphModule) -> AbstractContextManager[None]:
    """Returns a cuda device context manager if there is a single device in the graph"""

def partition_fn(
    gm: GraphModule, joint_inputs: Sequence[object], **kwargs: object
) -> tuple[GraphModule, GraphModule]: ...
def get_num_model_outputs(model: GraphModule) -> int: ...

@dataclass(frozen=True)
class CompilerConfigExtra:
    """CompilerConfigExtra(cudagraphs: 'BoxedBool', graph_id: 'int', forward_device: 'BoxedDeviceIndex')"""

    cudagraphs: BoxedBool
    graph_id: int
    forward_device: BoxedDeviceIndex

def create_compiler_config_extra(config: types.ModuleType) -> CompilerConfigExtra: ...
def compile_fx_forward(
    gm: GraphModule,
    example_inputs: Sequence[InputType],
    num_orig_model_outputs: int,
    num_example_inputs: int,
    compiler_config_extra: CompilerConfigExtra,
    inner_compile: Callable[..., OutputCode] = ...,
    is_inference: bool = ...,
) -> OutputCode:
    """
    Compile the forward graph of the given graph module.

    Args:
        gm: The graph module to compile.
        example_inputs: The example inputs to use for compilation.
        num_orig_model_outputs: The number of model outputs from the original dynamo graph.
        num_example_inputs: The number of example inputs from the original dynamo graph.
        compiler_config_extra: Extra configuration for the compiler.
        inner_compile: The inner compile function to use.
        is_inference: Whether this is an inference graph.
    """

def compile_fx_backward(
    gm: GraphModule,
    example_inputs: Sequence[InputType],
    compiler_config_extra: CompilerConfigExtra,
    inner_compile: Callable[..., OutputCode] = ...,
) -> OutputCode:
    """
    Compile the backward graph of the given graph module.

    Args:
        gm: The graph module to compile.
        example_inputs: The example inputs to use for compilation.
        compiler_config_extra: Extra configuration for the compiler.
        inner_compile: The inner compile function to use.
    """

def run_pre_grad_passes(model_: GraphModule, example_inputs_: Sequence[InputType]) -> GraphModule: ...
def compile_fx(
    model_: GraphModule,
    example_inputs_: Sequence[InputType],
    inner_compile: Callable[..., OutputCode] = ...,
    config_patches: dict[str, Any] | None = ...,
    decompositions: dict[OpOverload, Callable[..., Any]] | None = ...,
    ignore_shape_env: bool = ...,
) -> Callable[[list[object]], Sequence[torch.Tensor]] | str | list[str] | Weights:
    """
    Main entry point for compiling given FX graph.  Despite the fact that this
    lives in :mod:`torch._inductor`, this function is responsible for calling
    into AOT Autograd (and we will eventually get a callback to
    ``inner_compile`` to perform actual compilation.  In other words, this
    function orchestrates end-to-end compilation for the inductor backend when
    you use :func:`torch.compile`.

    NB: This function TAKES OWNERSHIP of the input ``model_`` and can potentially
    mutate it!  Make a copy if you need to preserve the original GraphModule.
    """

def graph_returns_tuple(gm: GraphModule) -> bool:
    """True if a FX graph returns a tuple"""

def make_graph_return_tuple(
    gm: GraphModule, inputs: Sequence[InputType], compile_gm: Callable[..., Any]
) -> Callable[..., Any]:
    """
    Mutate gm so it returns a tuple.  This is only needed for graphs
    not created by torchdynamo that return non-tuples.
    """

def handle_dynamo_export_graph(
    gm: GraphModule, inputs: Sequence[InputType], compile_gm: Callable[..., Any]
) -> Callable[..., Any]:
    """
    `torch._dynamo.export` embeds pytrees in the FX graph codegen object,
    convert that to a normal FX graph so inductor can compile it.
    """
