from collections.abc import Callable
from typing import Any, Literal

import torch
import torch._inductor.custom_graph_pass
from torch._environment import is_fbcode
from torch.utils._config_typing import *

inplace_padding = ...
can_inplace_pad_graph_input = ...

def fx_graph_remote_cache_default() -> bool | None: ...
def vec_isa_ok_default() -> bool | None: ...
def autotune_remote_cache_default() -> bool | None: ...
def bundled_autotune_remote_cache_default() -> bool | None: ...
def bundle_triton_into_fx_graph_cache_default() -> bool | None: ...
def static_cuda_launcher_default() -> bool: ...
def prologue_fusion_enabled() -> bool: ...

enable_auto_functionalized_v2 = ...
debug = ...
disable_progress = ...
verbose_progress = ...
worker_log_path = ...
precompilation_timeout_seconds: int = ...
fx_graph_cache: bool = ...
remote_gemm_autotune_cache: bool = ...
fx_graph_remote_cache: bool | None = ...
bundle_triton_into_fx_graph_cache: bool | None = ...
non_blocking_remote_cache_write: bool = ...
autotune_local_cache: bool = ...
autotune_remote_cache: bool | None = ...
bundled_autotune_remote_cache: bool | None = ...
force_disable_caches: bool = ...
unsafe_skip_cache_dynamic_shape_guards: bool = ...
unsafe_marked_cacheable_functions: dict[str, str] = ...
sleep_sec_TESTING_ONLY: int | None = ...
triton_kernel_default_layout_constraint: Literal["needs_fixed_stride_order", "flexible_layout"] = ...
cpp_wrapper: bool = ...
cpp_wrapper_build_separate: bool = ...
fx_wrapper: bool = ...
cpp_cache_precompile_headers: bool = ...
online_softmax = ...
dce = ...
static_weight_shapes = ...
size_asserts = ...
nan_asserts = ...
scalar_asserts = ...
alignment_asserts = ...
pick_loop_orders = ...
inplace_buffers = ...
allow_buffer_reuse = ...
memory_planning = ...
use_fast_math = ...
bfloat16_atomic_adds_enabled = ...
memory_pool: Literal["none", "intermediates", "outputs", "combined"] = ...
benchmark_harness = ...
epilogue_fusion = ...
prologue_fusion = ...
epilogue_fusion_first = ...
pattern_matcher = ...
b2b_gemm_pass = ...
post_grad_custom_pre_pass: torch._inductor.custom_graph_pass.CustomGraphPassType = ...
post_grad_custom_post_pass: torch._inductor.custom_graph_pass.CustomGraphPassType = ...
custom_partitioner_fn: torch._inductor.custom_graph_pass.CustomPartitionerFnType = ...
joint_custom_pre_pass: torch._inductor.custom_graph_pass.CustomGraphPassType = ...
joint_custom_post_pass: torch._inductor.custom_graph_pass.CustomGraphPassType = ...
pre_grad_custom_pass: Callable[[torch.fx.graph.Graph], None] | None = ...
_pre_fusion_custom_pass: (
    Callable[[list[torch._inductor.scheduler.BaseSchedulerNode]], list[torch._inductor.scheduler.BaseSchedulerNode]]
    | None
) = ...
_post_fusion_custom_pass: (
    Callable[[list[torch._inductor.scheduler.BaseSchedulerNode]], list[torch._inductor.scheduler.BaseSchedulerNode]]
    | None
) = ...
split_cat_fx_passes = ...
efficient_conv_bn_eval_fx_passes = ...
is_predispatch = ...
group_fusion = ...
batch_fusion = ...
pre_grad_fusion_options: dict[str, dict[str, Any]] = ...
post_grad_fusion_options: dict[str, dict[str, Any]] = ...
reorder_for_locality = ...
dynamic_scale_rblock = ...
force_fuse_int_mm_with_mul = ...
use_mixed_mm = ...
fx_passes_numeric_check: dict[str, Any] = ...
mixed_mm_choice: Literal["default", "triton", "aten", "heuristic"] = ...
reorder_for_compute_comm_overlap = ...
reorder_for_compute_comm_overlap_passes: list[
    str
    | Callable[[list[torch._inductor.scheduler.BaseSchedulerNode]], list[torch._inductor.scheduler.BaseSchedulerNode]]
] = ...
reorder_prefetch_limit: int | None = ...
reorder_for_peak_memory = ...
reorder_iterative_debug_memory_recompute: bool = ...
reorder_iterative_debug_limit_to_reorder: int | None = ...
sink_waits_iterative_debug_limit_to_sink: int | None = ...
bucket_all_gathers_fx: Literal["none", "all", "only_fsdp"] = ...
bucket_all_gathers_fx_bucket_size_determinator: Callable[[int], int] | None = ...
bucket_reduce_scatters_fx: Literal["none", "all"] = ...
bucket_reduce_scatters_fx_bucket_size_determinator: Callable[[int], int] | None = ...
estimate_op_runtime = ...
runtime_estimations_mms_benchmark: bool = ...
intra_node_bw = ...
inter_node_bw = ...
use_experimental_benchmarker: bool = ...
max_autotune = ...
max_autotune_pointwise = ...
max_autotune_gemm = ...
autotune_num_choices_displayed: int | None = ...
max_autotune_report_choices_stats = ...
max_autotune_prune_choices_based_on_shared_mem = ...
graph_partition: bool = ...
custom_should_partition_ops: list[str] = ...
force_same_precision: bool = ...
multi_kernel_hints: list[int] = ...
max_autotune_gemm_backends = ...
max_autotune_conv_backends = ...
max_autotune_gemm_search_space: Literal["DEFAULT", "EXHAUSTIVE"] = ...
max_autotune_flex_search_space: Literal["DEFAULT", "EXHAUSTIVE"] = ...
autotune_fallback_to_aten = ...
unbacked_symint_fallback = ...
search_autotune_cache = ...
save_args = ...
autotune_in_subproc = ...
max_autotune_subproc_result_timeout_seconds = ...
max_autotune_subproc_graceful_timeout_seconds = ...
max_autotune_subproc_terminate_timeout_seconds = ...
autotune_multi_device = ...
coordinate_descent_tuning = ...
coordinate_descent_check_all_directions = ...
coordinate_descent_search_radius = ...
autoheuristic_collect = ...
autoheuristic_use = ...
run_jit_post_compile_hook = ...

def run_autoheuristic(name: str) -> bool: ...
def collect_autoheuristic(name: str) -> bool: ...
def use_autoheuristic(name: str) -> bool: ...

autoheuristic_log_path = ...
layout_opt_default = ...
layout_optimization = ...
force_layout_optimization = ...
keep_output_stride = ...
warn_mix_layout = ...
realize_reads_threshold = ...
realize_opcount_threshold = ...
realize_acc_reads_threshold = ...
realize_acc_reads_size_threshold: int | None = ...
fallback_random = ...
implicit_fallbacks = ...
assume_unaligned_fallback_output = ...
aggressive_fusion = ...
debug_fusion: bool = ...
benchmark_fusion: bool = ...
enabled_metric_tables = ...
loop_ordering_after_fusion: bool = ...
score_fusion_memory_threshold = ...
benchmark_epilogue_fusion = ...
max_epilogue_benchmarked_choices = ...
max_fusion_size = ...
max_fusion_buffer_group_pairwise_attempts = ...
max_pointwise_cat_inputs = ...
force_pointwise_cat = ...
unroll_reductions_threshold = ...
comment_origin = ...
conv_1x1_as_mm = ...
split_reductions = ...
min_num_split = ...
benchmark_kernel = ...
constant_and_index_propagation = ...
always_keep_tensor_constants = ...
assert_indirect_indexing = ...
compute_all_bounds = ...
combo_kernels = ...
benchmark_combo_kernel = ...
combo_kernels_autotune = ...
combo_kernel_allow_mixed_sizes = ...
combo_kernel_foreach_dynamic_shapes = ...
joint_graph_constant_folding = ...
debug_index_asserts = ...
emulate_precision_casts = ...
is_nightly_or_source = ...
developer_warnings = ...
optimize_scatter_upon_const_tensor = ...
add_pre_grad_passes: str | None = ...
remove_pre_grad_passes: str | None = ...

def decide_worker_start_method() -> str: ...

worker_start_method: str = ...
small_memory_access_threshold: int = ...
worker_suppress_logging: bool = ...
log_tlparse: bool = ...
_fuse_ddp_communication = ...
_fuse_ddp_bucket_size = ...
_fuse_ddp_communication_passes: list[Callable[..., None] | str] = ...
_micro_pipeline_tp: bool = ...

class _collective:
    """
    Shim to redirect to main config.
    `config.triton.cudagraphs` maps to _config["triton.cudagraphs"]
    """

    auto_select: bool = ...
    one_shot_all_reduce_threshold_bytes: int = ...

def parallel_compile_enabled_internally() -> bool:
    """
    TODO: Remove when parallel compiled is fully enabled internally. For rollout, use a
    knob to enable / disable. The justknob should not be performed at import, however.
    So for fbcode, we assign compile_threads to 'None' below and initialize lazily in
    async_compile.py.
    """

def decide_compile_threads() -> int:
    """
    Here are the precedence to decide compile_threads
    1. User can override it by TORCHINDUCTOR_COMPILE_THREADS.  One may want to disable async compiling by
       setting this to 1 to make pdb happy.
    2. Set to 1 if it's win32 platform
    3. decide by the number of CPU cores
    """

compile_threads: int | None = ...
quiesce_async_compile_pool: bool = ...
use_static_cuda_launcher: bool = ...
static_launch_user_defined_triton_kernels: bool = ...
strict_static_cuda_launcher: bool = ...
global_cache_dir: str | None
if is_fbcode(): ...
else:
    global_cache_dir = ...
kernel_name_max_ops = ...
shape_padding = ...
comprehensive_padding = ...
pad_channels_last = ...
pad_dynamic_shapes = ...
disable_padding_cpu = ...
expand_dimension_for_pointwise_nodes = ...
padding_alignment_bytes = ...
padding_stride_threshold = ...
pad_outputs = ...
bw_outputs_user_visible = ...
force_shape_pad: bool = ...
permute_fusion = ...
profiler_mark_wrapper_call = ...
generate_intermediate_hooks = ...
debug_ir_traceback = ...
_raise_error_for_testing = ...
_profile_var = ...
profile_bandwidth = ...
profile_bandwidth_regex = ...
profile_bandwidth_output: str | None = ...
profile_bandwidth_with_do_bench_using_profiling = ...
disable_cpp_codegen = ...
freezing: bool = ...
freezing_discard_parameters: bool = ...
decompose_mem_bound_mm: bool = ...
assume_aligned_inputs: bool = ...
unsafe_ignore_unsupported_triton_autotune_args: bool = ...
check_stack_no_cycles_TESTING_ONLY: bool = ...
always_complex_memory_overlap_TESTING_ONLY: bool = ...
enable_linear_binary_folding = ...
annotate_training: bool = ...
enable_caching_generated_triton_templates: bool = ...
autotune_lookup_table: dict[str, dict[str, Any]] = ...

def get_worker_log_path() -> str | None: ...

torchinductor_worker_logpath: str = ...

class cpp:
    """
    Shim to redirect to main config.
    `config.triton.cudagraphs` maps to _config["triton.cudagraphs"]
    """

    threads = ...
    no_redundant_loops = ...
    dynamic_threads = ...
    simdlen: int | None = ...
    min_chunk_size = ...
    cxx: tuple[None, str] = ...
    enable_kernel_profile = ...
    weight_prepack = ...
    inject_relu_bug_TESTING_ONLY: str | None = ...
    inject_log1p_bug_TESTING_ONLY: str | None = ...
    vec_isa_ok: bool | None = ...
    descriptive_names: Literal["torch", "original_aten", "inductor_node"] = ...
    max_horizontal_fusion_size = ...
    fallback_scatter_reduce_sum = ...
    enable_unsafe_math_opt_flag = ...
    enable_floating_point_contract_flag = ...
    enable_tiling_heuristics = ...
    enable_grouped_gemm_template = ...
    gemm_max_k_slices = ...
    gemm_cache_blocking = ...
    gemm_thread_factors = ...
    enable_loop_tail_vec = ...
    enable_concat_linear = ...
    use_decompose_tanh = ...
    use_small_dequant_buffer = ...
    force_inline_kernel = ...
    use_constexpr_for_int_array = ...

class triton:
    """
    Shim to redirect to main config.
    `config.triton.cudagraphs` maps to _config["triton.cudagraphs"]
    """

    cudagraphs = ...
    cudagraph_trees = ...
    cudagraph_skip_dynamic_graphs = ...
    cudagraph_capture_sizes: tuple[int | tuple[int, ...]] | None = ...
    slow_path_cudagraph_asserts = ...
    cudagraph_trees_history_recording = ...
    cudagraph_support_input_mutation = ...
    cudagraph_unexpected_rerecord_limit = ...
    cudagraph_dynamic_shape_warn_limit: int | None = ...
    force_cudagraph_sync = ...
    force_cudagraphs_warmup = ...
    cudagraph_or_error: bool = ...
    fast_path_cudagraph_asserts = ...
    skip_cudagraph_warmup = ...
    debug_sync_graph = ...
    debug_sync_kernel = ...
    dense_indexing = ...
    coalesce_tiling_analysis: bool = ...
    max_tiles: int | None = ...
    prefer_nd_tiling: bool = ...
    autotune_pointwise = ...
    autotune_cublasLt = ...
    autotune_at_compile_time: bool | None = ...
    autotune_with_sample_inputs: bool = ...
    tile_reductions: bool = ...
    tiling_prevents_pointwise_fusion = ...
    tiling_prevents_reduction_fusion = ...
    unique_kernel_names = ...
    unique_user_kernel_names = ...
    descriptive_names: Literal["torch", "original_aten", "inductor_node"] = ...
    persistent_reductions = ...
    cooperative_reductions = ...
    force_cooperative_reductions = ...
    multi_kernel: Literal[0, 1, 2, 3] = ...
    divisible_by_16 = ...
    min_split_scan_rblock = ...
    store_cubin = ...
    spill_threshold: int = ...
    use_block_ptr = ...
    use_tensor_descriptor = ...
    inject_relu_bug_TESTING_ONLY: str | None = ...
    codegen_upcast_to_fp32 = ...
    enable_persistent_tma_matmul = ...
    skip_l1_cache = ...
    disallow_failing_autotune_kernels_TESTING_ONLY = ...
    num_decompose_k_splits = ...
    decompose_k_threshold = ...

class aot_inductor:
    """
    Shim to redirect to main config.
    `config.triton.cudagraphs` maps to _config["triton.cudagraphs"]
    """

    output_path = ...
    debug_compile = ...
    compile_wrapper_opt_level = ...
    debug_intermediate_value_printer: Literal["0", "1", "2", "3"] = ...
    filtered_kernel_names = ...
    serialized_in_spec = ...
    serialized_out_spec = ...
    use_runtime_constant_folding: bool = ...
    force_mmap_weights: bool = ...
    use_consts_asm_build = ...
    package: bool = ...
    package_cpp_only: bool | None = ...
    metadata: dict[str, str] = ...
    raise_error_on_ignored_optimization: bool = ...
    dump_aoti_minifier: bool = ...
    repro_level: int = ...
    presets: dict[str, Any] = ...
    allow_stack_allocation: bool = ...
    use_minimal_arrayref_interface: bool = ...
    weight_use_caching_allocator: bool = ...
    package_constants_in_so: bool = ...
    package_constants_on_disk: bool = ...
    precompile_headers: bool = ...
    embed_kernel_binary: bool | None = ...
    emit_multi_arch_kernel: bool | None = ...
    model_name_for_generated_files: str | None = ...
    custom_ops_to_c_shims: dict[torch._ops.OpOverload, list[str]] = ...
    custom_op_libs: list[str] | None = ...
    compile_standalone: bool = ...
    enable_lto = ...

class cuda:
    """
    Shim to redirect to main config.
    `config.triton.cudagraphs` maps to _config["triton.cudagraphs"]
    """

    arch: str | None = ...
    version: str | None = ...
    compile_opt_level: Literal["-O0", "-O1", "-O2", "-O3", "-OS"] = ...
    enable_cuda_lto = ...
    enable_ptxas_info = ...
    enable_debug_info = ...
    use_fast_math = ...
    cutlass_dir = ...
    cutlass_max_profiling_configs: int | None = ...
    cutlass_max_profiling_swizzle_options: list[int] = ...
    cutlass_epilogue_fusion_enabled = ...
    cutlass_tma_only = ...
    cuda_cxx: str | None = ...
    cutlass_backend_min_gemm_size: int = ...
    generate_test_runner: bool = ...
    cutlass_op_allowlist_regex: str | None = ...
    cutlass_op_denylist_regex: str | None = ...
    cutlass_instantiation_level: str = ...
    cutlass_presets: str | None = ...
    cutlass_hash_with_compile_cmd: bool = ...
    cutlass_prescreening: bool = ...
    cutlass_enabled_ops: str = ...
    use_binary_remote_cache: bool = ...
    upload_to_binary_remote_cache: bool = ...
    binary_remote_cache_force_write: bool = ...
    enable_caching_codegen: bool = ...

class rocm:
    """
    Shim to redirect to main config.
    `config.triton.cudagraphs` maps to _config["triton.cudagraphs"]
    """

    arch: list[str] = ...
    ck_supported_arch: list[Literal["gfx90a", "gfx942", "gfx950"]] = ...
    compile_opt_level: Literal["-O0", "-O1", "-O2", "-O3", "-Os", "-Oz", "-Omin", "-Ofast", "-Omax"] = ...
    is_debug = ...
    save_temps = ...
    use_fast_math = ...
    flush_denormals = ...
    print_kernel_resource_usage = ...
    rocm_home: str | None = ...
    ck_dir = ...
    generate_test_runner: bool = ...
    n_max_profiling_configs: int | None = ...
    ck_max_profiling_configs: int | None = ...
    ck_tile_max_profiling_configs: int | None = ...
    use_preselected_instances: bool = ...
    kBatch_sweep: list[int] | None = ...
    split_k_threshold: int = ...
    contiguous_threshold: int = ...

cpu_backend: Literal["cpp", "triton", "halide"] = ...
cuda_backend: Literal["triton", "halide"] = ...

class halide:
    """
    Shim to redirect to main config.
    `config.triton.cudagraphs` maps to _config["triton.cudagraphs"]
    """

    cpu_target = ...
    gpu_target = ...
    scheduler_cuda: Literal["Anderson2021", "Li2018", "Adams2019", "Mullapudi2016"] = ...
    scheduler_cpu: Literal["Anderson2021", "Li2018", "Adams2019", "Mullapudi2016"] = ...
    asserts = ...
    debug = ...
    scan_kernels = ...

class trace:
    """
    Shim to redirect to main config.
    `config.triton.cudagraphs` maps to _config["triton.cudagraphs"]
    """

    enabled = ...
    save_real_tensors = ...
    debug_dir: str | None = ...
    debug_log = ...
    info_log = ...
    fx_graph = ...
    fx_graph_transformed = ...
    ir_pre_fusion = ...
    ir_post_fusion = ...
    output_code = ...
    graph_diagram = ...
    draw_orig_fx_graph = ...
    dot_graph_shape = ...
    log_url_for_graph_xform = ...
    compile_profile = ...
    upload_tar: Callable[[str], None] | None = ...
    log_autotuning_results = ...
    provenance_tracking_level: int = ...

_save_config_ignore: list[str] = ...
_cache_config_ignore_prefix: list[str] = ...
external_matmul: list[Callable[[torch.Tensor, torch.Tensor, torch.Tensor], None]] = ...

class test_configs:
    """
    Shim to redirect to main config.
    `config.triton.cudagraphs` maps to _config["triton.cudagraphs"]
    """

    force_extern_kernel_in_multi_template: bool = ...
    max_mm_configs: int | None = ...
    runtime_triton_dtype_assert = ...
    static_cpp_dtype_assert = ...
    autotune_choice_name_regex: str | None = ...
    autotune_choice_desc_regex: str | None = ...
    graphsafe_rng_func_ignores_fallback_random = ...
    track_memory_lifecycle: Literal["assert", "log"] | None = ...
    use_libtorch = ...
