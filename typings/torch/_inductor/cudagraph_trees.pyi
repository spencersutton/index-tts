import contextlib
import dataclasses
from collections.abc import Callable, Generator, Iterator, Sequence
from contextlib import AbstractContextManager
from enum import Enum
from typing import Any, TypeVar

from torch import Tensor
from torch._guards import CompileId
from torch._inductor.cudagraph_utils import (
    CheckInvariantStatus,
    FunctionID,
    ModelType,
    OutputType,
    PlaceholderInfo,
    WrappedFunction,
)
from torch._inductor.utils import InputType
from torch.cuda import _POOL_HANDLE
from torch.multiprocessing.reductions import StorageWeakRef
from torch.storage import UntypedStorage

StorageWeakRefPointer = int
StorageDataPtr = int
NBytes = int
S = TypeVar("S", bound=StorageWeakRefWrapper)
if torch.backends.cuda.is_built(): ...
else:
    class AllocatorState: ...

log = ...

@dataclasses.dataclass(frozen=True)
class GraphID:
    id: int

def clear_cublass_cache() -> None: ...
@contextlib.contextmanager
def clear_cublas_manager() -> Generator[None]: ...
@contextlib.contextmanager
def disable_conv_cache_emptying() -> Generator[None]: ...
@contextlib.contextmanager
def enable_history_recording() -> Generator[None]: ...
def get_history_recording() -> AbstractContextManager[None]: ...

class TreeManagerContainer:
    def __init__(self, device_index: int) -> None: ...
    def finalize_cudagraphify_fn(self) -> None: ...
    def add_strong_reference(self, fn: Callable[..., Any]) -> None: ...
    def get_tree_manager(self) -> CUDAGraphTreeManager: ...

local = ...

class MarkStepBox:
    mark_step_counter = ...

def mark_step_begin() -> None: ...
def reset_cudagraph_trees() -> None: ...
def get_obj(local: Any, attr_name: str) -> Any: ...
def get_container(device_index: int) -> TreeManagerContainer: ...
def get_manager(device_index: int, create_if_none_exists: bool = ...) -> CUDAGraphTreeManager | None: ...
def is_cudagraph_capture_sizes(int_key: int | tuple[int, ...]) -> bool: ...
def cudagraphify_impl(
    model: ModelType, inputs: list[InputType], static_input_idxs: Sequence[int], *args: Any, **kwargs: Any
) -> ModelType: ...
@contextlib.contextmanager
def dynamo_timed_cudagraph(name: str, compile_id: CompileId | None, mode: CompilationMode | None) -> Generator[Any]: ...
def cudagraphify(
    model: ModelType,
    inputs: list[InputType],
    static_input_idxs: Sequence[int] = ...,
    *,
    device_index: int,
    is_backward: bool,
    is_inference: bool,
    stack_traces: StackTraces | None = ...,
    constants: tuple[torch.Tensor, ...] = ...,
    placeholders: tuple[PlaceholderInfo, ...] = ...,
    mutated_input_idxs: tuple[int, ...] = ...,
    compile_id: CompileId | None = ...,
) -> tuple[ModelType, OutputType]: ...

class StorageWeakRefWrapper:
    __slots__ = ...
    storage_ref: StorageWeakRef | None
    def __init__(self, inp: Tensor | UntypedStorage, extra_ref_check: Callable[[], bool] | None = ...) -> None: ...
    @classmethod
    def from_weakref_and_data_ptr(
        cls: type[StorageWeakRefWrapper], cdata: Any, data_ptr: int, extra_ref_check: Callable[[], bool] | None = ...
    ) -> StorageWeakRefWrapper: ...
    def __call__(self) -> StorageWeakRefPointer | None: ...
    def swap_weakref(self, cdata: Any) -> None: ...
    def data_ptr(self) -> int: ...
    def remove_extra_reference(self) -> None: ...
    def expired(self) -> bool: ...

def is_live(weak_ref: StorageWeakRefWrapper | None) -> bool: ...
def maybe_deref(weak_ref: StorageWeakRefWrapper | None) -> tuple[StorageWeakRefPointer, int] | None: ...
def map_to_ref(t: Tensor | None) -> StorageWeakRefWrapper | None: ...

type PathOutputIndex = tuple[int, int]
type PathLiveness = list[list[bool]]
type StackTraces = list[str | None]

class CUDAWarmupNode:
    def __init__(
        self,
        wrapped_function: WrappedFunction,
        parent: CUDAGraphNode | CUDAWarmupNode | None,
        cuda_graphs_pool: tuple[int, int],
        existing_cuda_graph: torch.cuda.CUDAGraph | None,
        device_index: int,
        stack_traces: StackTraces | None,
        stream: torch.cuda.Stream,
        already_warm: bool,
        id: GraphID,
    ) -> None: ...
    def run(self, new_inputs: Any) -> OutputType: ...
    def path_live_weakrefs(self) -> Iterator[StorageWeakRefWrapper]: ...
    def all_outputs_are_dead(self) -> bool: ...

InputList = list
OutputList = list
LevelList = list

class OutputAliasInfo: ...
class _UnaliasedStorage(OutputAliasInfo): ...

UnaliasedStorage = ...

class AliasesPriorGraphOutput(OutputAliasInfo):
    __slots__ = ...
    index: PathOutputIndex
    def __init__(self, index: PathOutputIndex) -> None: ...

class AliasesNewOutput(OutputAliasInfo):
    __slots__ = ...
    index: int
    def __init__(self, index: int) -> None: ...

class CUDAGraphNode:
    def __init__(
        self,
        wrapped_function: WrappedFunction,
        id: GraphID,
        parent: CUDAGraphNode | None,
        inputs: list[InputType],
        cuda_graphs_pool: _POOL_HANDLE,
        device_index: int,
        stack_traces: StackTraces | None,
        stream: torch.cuda.Stream,
        mode: CompilationMode | None,
        compile_id: CompileId | None,
    ) -> None: ...
    def check_static_inputs_are_stable(self, new_inputs: list[InputType]) -> None: ...
    def run_first_inputs(self, new_inputs: list[InputType]) -> OutputType: ...
    def run(self, new_inputs: list[InputType]) -> OutputType: ...
    def reconstruct_outputs(self) -> OutputType: ...
    def prepare_alias_info_for_tensor_construction(
        self, out_alias_info: OutputAliasInfo | None, metadata: dict[str, Any] | int | None
    ) -> UntypedStorage | None | int: ...
    def prepare_storages_for_construction(self) -> list[UntypedStorage | None | int]: ...
    def run_graph(self) -> None: ...
    def all_outputs_are_dead(self) -> bool: ...
    def get_output_refcount(self, index: int) -> int: ...
    @property
    def parent(self) -> CUDAGraphNode | None: ...
    def add_child(self, function_id: FunctionID, node: CUDAGraphNode) -> None: ...
    def debug_assert_invariants(
        self, expected_liveness: list[list[bool]], newly_dead: list[PathOutputIndex]
    ) -> None: ...
    def debug_check_invariants_before_invocation(self) -> None: ...
    def debug_check_invariants_after_invocation(self) -> None: ...
    def data_ptrs_dead_since_invocation(self) -> list[int]: ...
    def path_live_weakrefs(self) -> Iterator[StorageWeakRefWrapper]: ...
    def remove_node_cached_tensors(self) -> None: ...
    def remove_path_cached_tensors(self) -> None: ...
    def clear_path_state(self) -> None: ...
    def create_storage(self, metadata: dict[str, Any]) -> torch.types.Storage: ...
    def check_invariants(self, inputs: list[InputType]) -> tuple[CheckInvariantStatus, Callable[..., str]]: ...
    def num_descendants(self) -> int: ...

def get_cudagraph_segments(pool_id: tuple[int, int]) -> Any: ...
def get_block_addrs(pool_id: tuple[int, int], live_only: bool = ...) -> list[int]: ...
def format_tb(frames: list[Any]) -> str: ...
def check_memory_pool(
    device: int, pool_id: tuple[int, int], live_storages_ptrs: list[StorageWeakRefWrapper]
) -> None: ...

class ExecutionState(Enum):
    NONE = ...
    WARMUP = ...
    RECORDING = ...
    EXECUTION = ...

class CompilationMode(Enum):
    FORWARD = ...
    BACKWARD = ...
    INFERENCE = ...

class CUDAGraphTreeManager:
    def __init__(self, device_index: int) -> None: ...
    def run(self, new_inputs: list[InputType], function_id: FunctionID) -> OutputType: ...
    def set_to_running_backward(self) -> None: ...
    def new_warmup_node_id(self) -> GraphID: ...
    def exceed_rerecord_limit(self, node_id: GraphID | None, function_id: FunctionID) -> bool: ...
    def shutdown(self) -> None: ...
    def record_function(self, new_inputs: list[InputType], function_id: FunctionID) -> OutputType: ...
    def execute_node(self, node: CUDAGraphNode, new_inputs: list[InputType]) -> OutputType: ...
    def run_eager(self, new_inputs: list[InputType], function_id: FunctionID) -> OutputType: ...
    def new_graph_id(self) -> GraphID: ...
    def new_func_id(self) -> FunctionID: ...
    def add_function(
        self,
        model: ModelType,
        inputs: list[InputType],
        static_input_idxs: Sequence[int],
        stack_traces: StackTraces | None,
        mode: CompilationMode,
        constants: tuple[torch.Tensor, ...],
        placeholders: tuple[PlaceholderInfo, ...],
        mutated_input_idxs: tuple[int, ...],
        compile_id: CompileId | None,
    ) -> tuple[ModelType, OutputType]: ...
    @property
    def in_recording(self) -> bool: ...
    @property
    def in_warmup(self) -> bool: ...
    def get_roots(self) -> Iterator[CUDAGraphNode]: ...
    @property
    def current_node(self) -> CUDAGraphNode | CUDAWarmupNode | None: ...
    @current_node.setter
    def current_node(self, value: CUDAGraphNode | CUDAWarmupNode | None) -> None: ...
    def update_generation(self) -> None: ...
    @staticmethod
    def get_curr_generation() -> int: ...
    @staticmethod
    def user_invoked_mark_step() -> bool: ...
    def can_start_new_generation(self) -> bool: ...
    def in_new_torch_compile_invocation(self) -> bool: ...
    def try_end_curr_recording(self, function_id: FunctionID) -> None: ...
    def try_end_curr_execution(self) -> None: ...
    def try_end_curr_warmup(self, function_id: FunctionID) -> None: ...
    def check_warn_on_unable_to_start_executing(self, function_id: FunctionID) -> None: ...
    @staticmethod
    def format_dealloc_msg(stack_trace: str | None) -> str: ...
    def dealloc_current_path_weakrefs(self) -> None: ...
    def clear_current_path_state_and_set_to_none(self) -> None: ...
    def apply_checkpoint_execution_state_in_allocator(self) -> None: ...
    def live_cudagraph_pool_storages_in_curr_execution(self) -> list[StorageWeakRefPointer]: ...
