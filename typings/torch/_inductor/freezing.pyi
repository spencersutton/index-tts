from typing import Any, Optional

import torch

aten = ...
prims = ...
log = ...

def replace_params_with_constants(
    gm: torch.fx.GraphModule, flat_params: list[Any], fw_metadata: torch._functorch.aot_autograd.ViewAndMutationMeta
) -> list[int]: ...
def freeze(
    dynamo_gm: torch.fx.GraphModule,
    aot_autograd_gm: torch.fx.GraphModule,
    example_inputs: list[torch._subclasses.FakeTensor],
) -> tuple[torch.fx.GraphModule, list[int]]: ...

class ErasedTensor(torch.Tensor):
    @staticmethod
    def __new__(cls, elem, name, owning_mod):  # -> Self:
        ...
    def __init__(self, elem, name: str | None, mod) -> None: ...
    @classmethod
    def __torch_dispatch__(cls, func, types, args=..., kwargs=...): ...

def invalidate_eager_modules():  # -> None:
    ...
def discard_traced_gm_params(mod: torch.fx.GraphModule):  # -> None:
    ...
def enforce_output_layout(gm: torch.fx.GraphModule):  # -> None:

    ...
def enforce_as_strided_input_layout(gm: torch.fx.GraphModule):  # -> None:

    ...
def convert_conv_weights_to_channels_last(gm: torch.fx.GraphModule):  # -> None:

    ...
