import torch

from ..ir import ShapeAsConstantBuffer, Subgraph, TensorBox
from ..pattern_matcher import Arg, CallFunction, Match, register_graph_pattern
from ..select_algorithm import SymbolicGridFn

B2B_GEMM_PASS = ...

@SymbolicGridFn
def b2b_gemm_grid(M, P, meta, *, cdiv): ...

b2b_gemm_left_template = ...
b2b_gemm_right_template = ...

def load_ratio_left(M: int, N: int, O: int, P: int, m: int, n: int, o: int, p: int) -> float:
    """
    compute the ratio of estimated numbers of loads in baseline and b2bgemm
    M, N, O, P are matrix sizes
    m, n, o, p are block sizes
    |       | baseline (lower bound)        | b2bgemm
    | load  | M * N + N * O + M * O + O * P | M / m * P / p * O / o * (o * p + N / n * (m * n + n * o))
    | store | M * O + M * P                 | M * P
    b2bgemm is always better on stores, but for loads we need to find out beneficial cases using this function
    """

def load_ratio_right(M: int, N: int, O: int, P: int, m: int, n: int, o: int, p: int) -> float:
    """
    compute the ratio of estimated numbers of loads in baseline and b2bgemm
    M, N, O, P are matrix sizes
    m, n, o, p are block sizes
    |       | baseline (lower bound)        | b2bgemm
    | load  | N * O + O * P + M * N + N * P | M / m * P / p * N / n * (m * n + O / o * (n * o + o * p))
    | store | N * P + M * P                 | M * P
    b2bgemm is always better on stores, but for loads we need to find out beneficial cases using this function
    """

b2b_gemm_configs = ...

def is_b2b_gemm_good_on(
    is_left_assoc: bool, A_node: torch.fx.Node, B_node: torch.fx.Node, C_node: torch.fx.Node
) -> bool:
    """checks whether the sizes are good for b2b_gemm"""

def unoptimized_b2b_gemm(
    is_left_assoc: bool, subgraph: Subgraph, A: torch.Tensor, B: torch.Tensor, C: torch.Tensor, *, out: torch.Tensor
) -> torch.Tensor:
    """The unoptimized version is used as a fallback when the b2b_gemm kernel is not beneficial."""

unoptimized_choice = ...

def build_subgraph_buffer(args: list[TensorBox], subgraph: Subgraph):
    """
    This function is adapted from ../kernel/flex_attention.py.
    The goal is to take in the required args and produce the subgraph buffer
    The subgraph buffer is a ComputedBuffer that will be inlined into the triton template

    Args:
        args: The args that are passed into the subgraph
        subgraph: The Subgraph ir for which to produce the output node
    """

def create_placeholder(name: str, dtype: torch.dtype, device: torch.device) -> TensorBox | ShapeAsConstantBuffer:
    """Creates a placeholder input buffers for producing subgraph_output"""

def tuned_b2b_gemm(
    is_left_assoc: bool,
    subgraph: Subgraph,
    A: torch._inductor.ir.TensorBox,
    B: torch._inductor.ir.TensorBox,
    C: torch._inductor.ir.TensorBox,
    *,
    layout=...,
) -> torch._inductor.ir.TensorBox: ...
@register_graph_pattern(CallFunction(torch.ops.aten.mm, Arg(), Arg()), pass_dict=B2B_GEMM_PASS)
def b2b_gemm_handler(match: Match, mat1: torch.fx.Node, mat2: torch.fx.Node) -> None: ...
