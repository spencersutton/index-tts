import torch
from collections.abc import Sequence
from typing import Optional
from ..pattern_matcher import init_once_fakemode

log = ...
efficient_conv_bn_eval_pass = ...
fuse_split_linear_add_pass = ...
fuse_chunk_squeeze_cat_pass = ...
remove_reshape_pass = ...
normalization_pass_aten = ...
merge_splits_pass_aten = ...
split_cat_pass_aten = ...
unbind_stack_pass_aten = ...
merge_getitem_cat_pass_aten = ...
merge_stack_tahn_unbind_pass_aten = ...
mutate_cat_pass_aten = ...
remove_split_with_size_one_pass_aten = ...

def save_inductor_dict(pass_to_compare=...):  # -> dict[str | Any, int]:
    ...
def is_same_dict(inductor_dict, optimus_dict):  # -> bool:
    ...
def shape_prop(mod) -> None: ...
def normalize_node_kwargs_pass(graph):  # -> None:
    ...
def fuse_parallel_linear_pass(graph):  # -> None:
    ...
def remove_split_ops(graph, shape_prop):  # -> None:
    ...
def remove_split_ops_pass(graph):  # -> None:
    ...
def fuse_chunk_reshape_unsqueeze_concat_pass(graph):  # -> None:
    ...
def fuse_chunk_reshape_concat_pass(graph):  # -> None:
    ...
def remove_noop_pass(graph):  # -> None:
    ...
def stack_to_unsqueeze_pass(graph):  # -> None:
    ...
def merge_concats_pass(graph):  # -> None:
    ...
def relu_nan_to_num(graph):  # -> None:
    ...
def fuse_split_getitem_squeeze_cat(graph):  # -> None:
    ...
def use_triton_dot_compress(graph):  # -> None:
    ...
def use_triton_lce_replace_simple_LCE_helper(gm, shape_prop):  # -> None:
    ...
def use_triton_lce_replace_simple_LCE(graph):  # -> None:
    ...
def use_triton_lce_replace_normal_LCE_helper(gm, shape_prop):  # -> None:
    ...
def use_triton_lce_replace_normal_LCE(graph):  # -> None:
    ...
def use_matmul_lce_replace_normal_LCE(graph):  # -> None:
    ...
def use_matmul_fuse_lce_replace_first_LCE(graph):  # -> None:
    ...
@init_once_fakemode
def lazy_init():  # -> None:
    ...
def pre_grad_passes(
    gm: torch.fx.GraphModule,
    example_inputs: Sequence[object] = ...,
    add_passes: str | None = ...,
    remove_passes: str | None = ...,
) -> torch.fx.GraphModule: ...
def fuse_fx(gm: torch.fx.GraphModule, example_inputs) -> torch.fx.GraphModule: ...
def fetch_attr(target: str, mod):  # -> Any:
    ...
def remove_identity(gm: torch.fx.GraphModule) -> torch.fx.GraphModule: ...
def fuse_conv_bn(gm: torch.fx.GraphModule, inplace=...) -> torch.fx.GraphModule: ...

class NormalizedLinearNode:
    def __init__(self, node: torch.fx.Node) -> None: ...
    def get_input(self) -> torch.fx.Node: ...
    def get_weight(self) -> torch.fx.Node: ...
    def get_bias(self) -> torch.fx.Node: ...

class NormalizedMatmulNode:
    def __init__(self, node: torch.fx.Node) -> None: ...
    def get_input(self) -> torch.fx.Node: ...
    def get_other(self) -> torch.fx.Node: ...

def check_permute(node: torch.fx.Node) -> bool: ...
def sink_cat_after_pointwise(module: torch.fx.GraphModule) -> torch.fx.GraphModule: ...
def linear_permute_fusion(module: torch.fx.GraphModule) -> torch.fx.GraphModule: ...
def linear_transpose(input: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor | None) -> torch.Tensor: ...
def permute_linear_fusion(module: torch.fx.GraphModule) -> torch.fx.GraphModule: ...
def permute_matmul_fusion(module: torch.fx.GraphModule) -> torch.fx.GraphModule: ...
def transpose_linear(input: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor | None) -> torch.Tensor: ...
def transpose_matmul(A: torch.Tensor, B: torch.Tensor, Atrans: bool, Btrans: bool) -> torch.Tensor: ...
