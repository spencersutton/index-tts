import torch

aten = ...
prims = ...
quantized_decomposed = ...
quantized = ...
_PER_TENSOR_QUANTIZE_OPS = ...
_VIEW_OPS = ...

def get_dequantize_per_tensor_activation_pattern(is_tensor_overload=...): ...

dequantize_per_channel_weight_pattern = ...
dequantize_per_channel_to_bf16_weight_pattern = ...
dequantize_per_channel_clone_weight_pattern = ...
dequantize_per_channel_to_bf16_clone_weight_pattern = ...

def get_qconv_pt2e_pattern(users=...): ...
def get_qconv2d_binary_pt2e_pattern(users=...): ...
def get_qlinear_pt2e_pattern(x_scale_zp_are_tensors, users=...): ...
def get_qlinear_binary_pt2e_pattern(x_scale_zp_are_tensors, users=...): ...

dequantize_accum_pattern = ...

def generate_pattern_with_binary(
    binary_post_op, computation_call, extra_input_pattern, dtype_convert=..., swap_inputs=...
): ...
def generate_pattern_with_unary(computation_call, unary_post_op): ...
def generate_pattern_with_output_quant(computation_call, with_dtype_convert=...): ...

_raw_dequantize_per_tensor_activation_pattern = ...

class PostOpAttr:
    def __init__(
        self, binary_op_name: str = ..., alpha=..., unary_op_name: str = ..., scalars_attr=..., algorithm_attr=...
    ) -> None: ...

def concat_linear_woq_int4(gm: torch.fx.GraphModule):
    """
    Concat Linear optimization pass for WOQ int4
    This pass fuses the original pattern:
    def ...
        return (woq_int4(x, w1, group_size, scale_zp1), woq_int4(x, w2, group_size, scale_zp1) ...)
    into a single operation:
    def ...
        concat_res = woq_int4(x, concat_w, group_size, concat_scale_zp)
        return split(concat_res, split_size_list)
    """

def quant_lift_up(graph_module: torch.fx.GraphModule):
    """
    Lift up the quant node before view like nodes. It can benefit performance
    of Attention like block. For example, we have the pattern as:

             DQ
    DQ       LINEAR
    LINEAR   VIEW
    VIEW     PERMUTE
    PERMUTE  TRANSPOSE
    Q        Q
    DQ       DQ
       Matmul
        DIV
        ADD
      SOFTMAX

    We want to lift up the the quant nodes from matmul before view like nodes
    as the output of Linear node.

             DQ
    DQ       LINEAR
    LINEAR   Q
    Q        VIEW
    VIEW     PERMUTE
    PERMUTE  TRANSPOSE
    DQ       DQ
       Matmul
        DIV
        ADD
      SOFTMAX

    It produces a DQ->LINEAR->Q pattern which can be fused by backend.
    """
