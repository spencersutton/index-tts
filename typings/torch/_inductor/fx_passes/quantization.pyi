import torch

aten = ...
prims = ...
quantized_decomposed = ...
quantized = ...
_PER_TENSOR_QUANTIZE_OPS = ...
_VIEW_OPS = ...

def get_dequantize_per_tensor_activation_pattern(is_tensor_overload=...):  # -> CallFunction:
    ...

dequantize_per_channel_weight_pattern = ...
dequantize_per_channel_to_bf16_weight_pattern = ...
dequantize_per_channel_clone_weight_pattern = ...
dequantize_per_channel_to_bf16_clone_weight_pattern = ...

def get_qconv_pt2e_pattern(users=...):  # -> CallFunction:
    ...
def get_qconv2d_binary_pt2e_pattern(users=...):  # -> CallFunction:
    ...
def get_qlinear_pt2e_pattern(x_scale_zp_are_tensors, users=...):  # -> CallFunction:
    ...
def get_qlinear_binary_pt2e_pattern(x_scale_zp_are_tensors, users=...):  # -> CallFunction:
    ...

dequantize_accum_pattern = ...

def generate_pattern_with_binary(
    binary_post_op, computation_call, extra_input_pattern, dtype_convert=..., swap_inputs=...
):  # -> CallFunction:
    ...
def generate_pattern_with_unary(computation_call, unary_post_op):  # -> CallFunction:
    ...
def generate_pattern_with_output_quant(computation_call, with_dtype_convert=...):  # -> CallFunction:
    ...

_raw_dequantize_per_tensor_activation_pattern = ...

class PostOpAttr:
    def __init__(
        self, binary_op_name: str = ..., alpha=..., unary_op_name: str = ..., scalars_attr=..., algorithm_attr=...
    ) -> None: ...

def concat_linear_woq_int4(gm: torch.fx.GraphModule):  # -> None:

    ...
def quant_lift_up(graph_module: torch.fx.GraphModule):  # -> None:

    ...
