from collections.abc import Callable
from dataclasses import dataclass
from typing import Any

import torch

log = ...
aten = ...

@dataclass(frozen=True)
class InplaceableOp:
    """InplaceableOp(inplace_op: Callable[..., Any], mutated_arg: int, extra_check: Callable[[torch.fx.node.Node], bool] = <function InplaceableOp.<lambda> at 0x140875120>)"""

    inplace_op: Callable[..., Any]
    mutated_arg: int
    extra_check: Callable[[torch.fx.Node], bool] = ...

_SCATTER_OP_TO_VIEW = ...
_VIEW_OP_TO_SCATTER = ...

def graph_call_function(graph: torch.fx.Graph, fn, *args, **kwargs): ...

@dataclass
class ViewOp:
    """ViewOp(target: torch._ops.OpOverload, args: tuple[typing.Any, ...], kwargs: dict[str, typing.Any])"""

    target: torch._ops.OpOverload
    args: tuple[Any, ...]
    kwargs: dict[str, Any]

_ALWAYS_MUTATING_SCATTER_OPS = ...

def scatter_always_uses_mutation(node: torch.fx.Node) -> bool: ...
def should_reinplace_scatter(node: torch.fx.Node) -> bool:
    """
    Choose between mutating and functional scatter decompositions

    Reinplacing view scatter ops can be pessimising as it blocks fusion with the
    input or output tensor computations. However, it is still profitable if the
    input and output would have been realized anyway.
    """

def decompose_generalized_scatter(graph: torch.fx.Graph) -> None:
    """Replace _generalized_scatter with normal aten ops"""

def canonicalize_view_scatter_ops(graph: torch.fx.Graph) -> None:
    """
    This canonicalizes view scatter ops into a generalized form, defined as:
      def scatter(inp, src, views):
        tmp = inp.clone()
        for view in views:
          tmp = view(tmp)
        tmp.copy_(src)

    We also fuse consecutive view scatter ops of the form
        a = scatter(view2(self), src, [view1])
        b = scatter(self, a, [view2])
    which can be rewritten as
        b = scatter(self, src, [view2, view1])
        a = view2(b)

    This is both more efficient as we only do a single scatter, and also
    easier to reinplace since there is only one use of `self`
    """

inplaceable_ops: dict[Callable[..., Any], InplaceableOp] = ...
c10d_functional = ...
inplaceable_collective_ops: dict[Callable[..., Any], InplaceableOp] = ...
inplaceable_foreach_ops: dict[torch._ops.OpOverload, InplaceableOp] = ...
inplaceable_triton_ops = ...
META_ONLY_OPS = ...

def reinplace_inplaceable_ops_core(graph: torch.fx.Graph) -> None:
    """
    Reinplaces in-placeable operations.
    If there are no uses of a view of the mutated arg after the current node,
    it is possible to inplace the op.
    This above algorithm could be justified by observing side effects. While
    we traverse the graph in forwards direction, only latter nodes could view
    side effects of the current node. If the current node is not used later as
    well as no view of this node is used later in the graph, then it is safe to
    inplace as there would be no way to observe the side effects.
    This condition is slightly different for graph inputs where they can only
    be inplaced if the above condition is true and there's a copy_ in the
    epilogue that signals that the caller wants to observe the mutation.

    Unlike JIT Inductor, AOTInductor currently unlifts weights and buffers from
    input args, so instead of checking mutation on placeholder, AOTInductor
    checks mutation on get_attr. This is subject to change in future.
    """

def reinplace_inplaceable_ops(
    fake_tensor_updater: torch._inductor.fx_utils.FakeTensorUpdater, graph: torch.fx.Graph
) -> None: ...
