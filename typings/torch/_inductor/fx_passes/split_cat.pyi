import operator
from collections.abc import Callable
from typing import Any

import torch
from torch.utils._ordered_set import OrderedSet

from ..pattern_matcher import (
    MULTIPLE,
    CallFunction,
    CallFunctionVarArgs,
    CallMethodVarArgs,
    Ignored,
    KeywordArg,
    ListOf,
    Match,
    MatchContext,
    PatternMatcherPass,
    RepeatedExpr,
    register_graph_pattern,
)

log = ...
type _Arguments = tuple[torch.fx.node.Argument, ...]
type _TransformParam = tuple[_Arguments | None, _Arguments | None, _Arguments | None, _Arguments | None]
type _Range = tuple[int, int]
PRE_GRAD_PATTERNS: dict[str, PatternMatcherPass] = ...
POST_GRAD_PATTERNS: dict[str, PatternMatcherPass] = ...
pre_grad_pass_names = ...
post_grad_pass_names = ...
backend = ...

def construct_pattern_matcher_pass(pass_name: str):
    """Return the specific pattern_matcher_pass given the pass name."""

def normalize_split_base(
    match: Match, _get_split_args: Callable[[torch.fx.Node], tuple[torch.fx.Node | None, Any | None, int | None]]
):
    """
    Normalize split with split_size into split_with_sizes, so that we only deal with one type of split in
    subsequent optimizations
    """

@register_graph_pattern(
    CallFunctionVarArgs(torch.split, users=MULTIPLE), pass_dict=construct_pattern_matcher_pass("normalization_pass")
)
@register_graph_pattern(
    CallMethodVarArgs("split", users=MULTIPLE), pass_dict=construct_pattern_matcher_pass("normalization_pass")
)
def normalize_split_default(match: Match, *args, **kwargs): ...
@register_graph_pattern(
    CallFunctionVarArgs(torch.split, users=MULTIPLE),
    pass_dict=construct_pattern_matcher_pass("remove_split_with_size_one_pass"),
)
@register_graph_pattern(
    CallMethodVarArgs("split", users=MULTIPLE),
    pass_dict=construct_pattern_matcher_pass("remove_split_with_size_one_pass"),
)
def remove_split_with_size_one(match: Match, *args, **kwargs): ...
@register_graph_pattern(
    CallFunctionVarArgs(torch.unbind, users=MULTIPLE), pass_dict=construct_pattern_matcher_pass("normalization_pass")
)
@register_graph_pattern(
    CallMethodVarArgs("unbind", users=MULTIPLE), pass_dict=construct_pattern_matcher_pass("normalization_pass")
)
def normalize_unbind_default(match: Match, *args, **kwargs): ...
@register_graph_pattern(
    CallFunctionVarArgs([torch.cat, torch.concat], users=MULTIPLE),
    pass_dict=construct_pattern_matcher_pass("normalization_pass"),
)
def normalize_cat_default(match: Match, *args, **kwargs): ...
@register_graph_pattern(
    CallFunctionVarArgs(torch.stack, users=MULTIPLE), pass_dict=construct_pattern_matcher_pass("normalization_pass")
)
def normalize_stack_default(match: Match, *args, **kwargs): ...
def find_next_users(split_node: torch.fx.Node) -> list[torch.fx.Node]: ...
@register_graph_pattern(
    CallMethodVarArgs("squeeze", users=MULTIPLE), pass_dict=construct_pattern_matcher_pass("normalization_pass")
)
def normalize_squeeze_default(match: Match, *args, **kwargs): ...
@register_graph_pattern(
    CallMethodVarArgs("reshape", users=MULTIPLE), pass_dict=construct_pattern_matcher_pass("normalization_pass")
)
def normalize_reshape_default(match: Match, *args, **kwargs): ...
@register_graph_pattern(
    CallMethodVarArgs("clamp", users=MULTIPLE), pass_dict=construct_pattern_matcher_pass("normalization_pass")
)
@register_graph_pattern(
    CallFunctionVarArgs(torch.clamp, users=MULTIPLE), pass_dict=construct_pattern_matcher_pass("normalization_pass")
)
def normalize_clamp_default(match: Match, *args, **kwargs): ...
@register_graph_pattern(
    CallMethodVarArgs("detach", users=MULTIPLE), pass_dict=construct_pattern_matcher_pass("normalization_pass")
)
def normalize_detach_default(match: Match, *args, **kwargs): ...

class TorchSplit(CallFunction):
    """
    Matches a call to torch.split if it is in a normalized form. Ensures that all users of
    splits are unique getitems.
    """
    def __init__(self, arg, sizes, func=...) -> None: ...

@register_graph_pattern(
    TorchSplit(
        CallFunction(
            operator.getitem, TorchSplit(KeywordArg("first_split_input"), KeywordArg("first_split_sections")), Ignored()
        ),
        KeywordArg("next_split_sections"),
    ),
    pass_dict=construct_pattern_matcher_pass("merge_splits_pass"),
)
def merge_splits(
    match: Match,
    first_split_input: torch.fx.Node,
    first_split_sections: list[int],
    next_split_sections: list[int],
    dim: int,
): ...

class SplitCatSimplifier:
    """
    Helper class to simplify split-cat pattern. In simple cases, both split and cat node can be removed in a "split->cat"
    pattern. However, there are various cases where they can't and we need to simplify split/ add transforms before cat.
    Some such cases are:
        1. Final node has additional args (not coming from the initial split)
        2. Shuffling of args between split/cat
        3. Some final nodes are non-(cat/stack)
        4. Split-dim != cat-dim (but equal split)

    Note that any combination of the above cases can happen.

    To deal with 1, 2, & 3 - we iterate over all users of split. And figure out common "ranges" that can be merged.
    Then, we simplify the split accordingly. In the best case, split can be entirely removed.

    To deal with 4, we add some transformations (unflatten + movedim) (See `get_transform_params`).

    Finally, depending on final node being cat or stack, unsqueeze/flatten needs to be added.
    """
    def simplify(self, graph: torch.fx.Graph, split_node: torch.fx.Node, split_sections: list[int]): ...
    def get_user_input_list(
        self, split_node: torch.fx.Node, next_users: list[torch.fx.Node]
    ) -> list[list[torch.fx.Node | _Range]]:
        """
        Returns list of inputs to the following user nodes, in order. The outer list represents the user node. The inner
        list represents the inputs to that particular node. This list can either contain
          - a tuple representing the ranges of get_items that should go into the cat (closed interval)
          - torch.fx.Node representing "other" inputs (which are not coming from our split)
        """
    def get_merged_user_inputs(
        self, split_node: torch.fx.Node, cat_node: torch.fx.Node
    ) -> list[torch.fx.Node | _Range]: ...
    def get_non_cat_node_input(self, split_node: torch.fx.Node, node: torch.fx.Node) -> list[_Range]:
        """Get input for a non cat node in the same format as `get_merged_user_inputs`"""
    def merge_consecutive_inputs(self, inputs: list[torch.fx.Node | int]) -> list[torch.fx.Node | _Range]:
        """
        Merge consecutive inputs going into a user node.

        For e.g.
        [arg0, 0, 1, 2, arg1] -> [arg0, (0, 2), arg1]
        """
    def get_simplified_split_ranges(
        self, split_sections, next_users, user_inputs_list: list[list[torch.fx.Node | _Range]]
    ) -> list[_Range] | None: ...
    def has_non_overlapping_ranges(self, ranges: list[_Range]) -> bool: ...
    def fill_gaps(self, ranges: list[_Range], min_: int, max_: int) -> list[_Range]: ...
    def get_transform_params(
        self,
        split_node: torch.fx.Node,
        next_users: list[torch.fx.Node],
        user_inputs_list: list[list[torch.fx.Node | _Range]],
    ) -> list[list[_TransformParam]] | None:
        """
        Figure out what transforms are needed for each input to each cat node.

        We replace a split node with an unflatten followed by a movedim
        """
    def replace_split(
        self,
        graph: torch.fx.Graph,
        split_node: torch.fx.Node,
        split_sections: list[int],
        user_inputs_list: list[list[torch.fx.Node | _Range]],
        split_ranges: list[_Range],
    ) -> list[list[torch.fx.Node]]:
        """
        Replace the split node. It can either remove the split node if len(split_ranges) == 1, or simplify it
        into a split with lesser sections if len(split_ranges) > 1.

        Returns the new `user_inputs_list`, with tuples replaced with new getitems from the newer split node.
        """
    def replace_cat(
        self,
        graph: torch.fx.Graph,
        split_node: torch.fx.Node,
        next_users: list[torch.fx.Node],
        user_inputs_list_new,
        transform_params_list: list[list[_TransformParam]],
    ): ...
    def erase_old_nodes(self, graph: torch.fx.Graph, split_node: torch.fx.Node, next_users: list[torch.fx.Node]): ...

class UnbindCatRemover(SplitCatSimplifier):
    """
    Helper class to merge Unbind->Cat/Stack. Many of the cases are similar to SplitCatSimplifier.

    Unbind can't be simplified like splits. So, we can only remove the unbind node. Other than this,
    other cases like multiple users, additional args, dim mismatch are similar to `SplitCatSimplifier`,
    hence we extend that class.
    """
    def remove_unbind(self, graph: torch.fx.Graph, unbind_node: torch.fx.Node): ...
    def get_simplified_split_ranges(
        self,
        split_sections: list[int],
        next_users: list[torch.fx.Node],
        user_inputs_list: list[list[torch.fx.Node | _Range]],
    ) -> list[_Range] | None: ...
    def get_transform_params(
        self,
        split_node: torch.fx.Node,
        next_users: list[torch.fx.Node],
        user_inputs_list: list[list[torch.fx.Node | _Range]],
    ) -> list[list[_TransformParam]] | None:
        """
        Figure out what transforms are needed for each input to each cat node.

        Here is the rough transforms we apply:

        x -> unbind -> stack => x -> movedim

        x -> unbind -> cat => x -> movedim -> flatten

        When cat/stack nodes have additional args:

             addn ---|              addn -> unsqueeze ---|
        x -> unbind -> stack  =>           x -> movedim  -> cat

             addn ---|                            addn ---|
        x -> unbind -> cat  =>   x -> movedim -> flatten  -> cat

        (Note application of these depends on the dims as well)
        """

class GetItem(CallFunction):
    def __init__(self, arg, index, _users=...) -> None: ...
    def find_anchor_nodes(self, ctx: MatchContext, searched: OrderedSet[torch.fx.Node]): ...

@register_graph_pattern(
    RepeatedExpr(
        CallFunction(
            torch.squeeze,
            GetItem(TorchSplit(KeywordArg("split_input"), KeywordArg("split_sizes")), Ignored()),
            KeywordArg("dim"),
            _users=MULTIPLE,
        )
    ),
    pass_dict=construct_pattern_matcher_pass("split_cat_pass"),
)
@register_graph_pattern(
    RepeatedExpr(
        CallFunction(
            torch.squeeze,
            GetItem(TorchSplit(KeywordArg("split_input"), KeywordArg("split_sizes")), Ignored()),
            dim=KeywordArg("dim"),
            _users=MULTIPLE,
        )
    ),
    pass_dict=construct_pattern_matcher_pass("split_cat_pass"),
)
def merge_split_squeeze(match: Match, split_input: torch.fx.Node, split_sizes: list[int], dim: int): ...

getitem_unbind = ...

@register_graph_pattern(
    CallFunction([torch.stack, torch.cat], getitem_unbind, Ignored(), _users=MULTIPLE),
    pass_dict=construct_pattern_matcher_pass("unbind_stack_pass"),
)
@register_graph_pattern(
    CallFunction([torch.stack, torch.cat], getitem_unbind, dim=Ignored(), _users=MULTIPLE),
    pass_dict=construct_pattern_matcher_pass("unbind_stack_pass"),
)
@register_graph_pattern(
    CallFunction([torch.stack, torch.cat], tensors=getitem_unbind, dim=Ignored(), _users=MULTIPLE),
    pass_dict=construct_pattern_matcher_pass("unbind_stack_pass"),
)
def merge_unbind_stack(match: Match, unbind_input: torch.fx.Node, dim: int): ...

getitem_split = ...
reshape_getitem_split = ...

@register_graph_pattern(
    CallFunction([torch.stack, torch.cat], tensors=getitem_split, dim=Ignored(), _users=MULTIPLE),
    pass_dict=construct_pattern_matcher_pass("split_cat_pass"),
)
@register_graph_pattern(
    CallFunction([torch.stack, torch.cat], getitem_split, dim=Ignored(), _users=MULTIPLE),
    pass_dict=construct_pattern_matcher_pass("split_cat_pass"),
)
@register_graph_pattern(
    CallFunction([torch.stack, torch.cat], getitem_split, Ignored(), _users=MULTIPLE),
    pass_dict=construct_pattern_matcher_pass("split_cat_pass"),
)
def simplify_split_cat(match: Match, split_sections: list[int], dim: int): ...
def has_same_parent_node(node: torch.fx.Node): ...
def remove_zeros(split_sections: list[int]):
    """
    Remove zeros from the list and get the index mapping dict from getitem
    in split node to getitem in new split node
    """

def is_sorted_and_consecutive(arr: list[int]) -> bool: ...
def calculate_fused_tensor_size(split_node: torch.fx.Node, indices: list[int]) -> int:
    """Calculate the fused tensor size in the indices"""

@register_graph_pattern(
    CallFunction(torch.cat, getitem_split, dim=Ignored(), _users=MULTIPLE),
    pass_dict=construct_pattern_matcher_pass("merge_getitem_cat_pass"),
)
def merge_getitem_cat(match: Match, split_sections: list[int], dim: int): ...
@register_graph_pattern(
    CallFunction(torch.cat, getitem_split, dim=Ignored(), _users=MULTIPLE),
    pass_dict=construct_pattern_matcher_pass("mutate_cat_pass"),
)
def mutate_cat_node(match: Match, split_sections: list[int], dim: int): ...

getitem_split_aten = ...

@register_graph_pattern(
    CallFunctionVarArgs(torch.ops.aten.split.Tensor, users=MULTIPLE),
    pass_dict=construct_pattern_matcher_pass("normalization_aten_pass"),
)
def normalize_split_default_aten(match: Match, *args, **kwargs): ...
@register_graph_pattern(
    CallFunctionVarArgs(torch.ops.aten.split_with_sizes.default, users=MULTIPLE),
    pass_dict=construct_pattern_matcher_pass("normalization_aten_pass"),
)
def normalize_split_with_size_default_aten(match: Match, *args, **kwargs): ...
@register_graph_pattern(
    CallFunction(torch.ops.aten.cat.default, getitem_split_aten, dim=Ignored(), _users=MULTIPLE),
    pass_dict=construct_pattern_matcher_pass("split_cat_aten_pass"),
)
def merge_split_cat_aten(match: Match, *args, **kwargs): ...
@register_graph_pattern(
    CallFunction(
        torch.ops.aten.cat.default,
        ListOf(CallFunctionVarArgs(torch.ops.aten.select.int, users=MULTIPLE), partial=True),
        dim=Ignored(),
        _users=MULTIPLE,
    ),
    pass_dict=construct_pattern_matcher_pass("select_cat_aten_pass"),
)
def merge_select_cat_aten(match: Match, *args, **kwargs): ...
@register_graph_pattern(
    CallFunctionVarArgs(torch.ops.aten.cat.default, users=MULTIPLE),
    pass_dict=construct_pattern_matcher_pass("normalization_aten_pass"),
)
def normalize_cat_default_aten(match: Match, *args, **kwargs): ...
@register_graph_pattern(
    CallFunction(torch.ops.aten.cat, ListOf(CallFunctionVarArgs(torch.ops.aten.unsqueeze)), _users=MULTIPLE),
    pass_dict=construct_pattern_matcher_pass("unbind_stack_aten_pass"),
)
def merge_unbind_stack_aten(match: Match, *args, **kwargs): ...
def divide_into_consecutive_sublists(indices: list[int]) -> list[list[int]]: ...
def update_args_from_split_getitem(
    graph: torch.fx.Graph,
    node: torch.fx.Node,
    getitem_indices: list[int],
    parents_seen: list[torch.fx.Node],
    new_cat_args: list[torch.fx.Node],
    new_cat_args_meta: list[torch.fx.Node],
    idx_to_getitems: dict[int, torch.fx.Node],
    threshold_to_cat: int = ...,
): ...
def reshape_cat_node(
    graph: torch.fx.Graph,
    cat_node: torch.fx.Node,
    unbind_input: torch.fx.Node,
    cat_dim: int,
    unbind_dim: int,
    cat_shape: torch.Size,
) -> torch.fx.Node: ...
def update_args_from_unbind_getitem(
    graph: torch.fx.Graph,
    node: torch.fx.Node,
    getitem_indices: list[int],
    parents_seen: list[torch.fx.Node],
    new_cat_args: list[torch.fx.Node],
    new_cat_args_meta: list[torch.fx.Node],
    idx_to_getitems: dict[int, torch.fx.Node],
    threshold_to_cat: int = ...,
): ...
def construct_cat_args(
    graph: torch.fx.Graph,
    cat_or_stack_node: torch.fx.Node,
    inputs: list[torch.fx.Node],
    split_or_unbind_node: torch.fx.Node,
    threshold_to_cat: int = ...,
    run_update_func: Callable = ...,
) -> tuple[list[torch.fx.Node], list[torch.Tensor]]: ...
def remove_split_unbind_children(graph: torch.fx.Graph, inputs: list[torch.fx.Node]): ...
@register_graph_pattern(
    CallFunction(torch.cat, getitem_split, dim=Ignored(), _users=MULTIPLE),
    pass_dict=construct_pattern_matcher_pass("split_cat_to_slices_pass"),
)
def split_cat_to_slices(match: Match, split_sections: list[int], dim: int): ...
@register_graph_pattern(
    CallFunction(torch.cat, getitem_unbind, dim=Ignored(), _users=MULTIPLE),
    pass_dict=construct_pattern_matcher_pass("unbind_cat_to_view_pass"),
)
def unbind_cat_to_view(match: Match, unbind_input: torch.fx.Node, dim: int): ...
def reshape_cat_node_to_stack(
    graph: torch.fx.Graph, cat_node: torch.fx.Node, stack_node: torch.fx.Node, split_or_unbind_dim: int
) -> None: ...
def convert_reshape_cat_arg_to_stack(
    graph: torch.fx.Graph,
    cat_node: torch.fx.Node,
    stack_node: torch.fx.Node,
    stack_node_shape: torch.Size,
    stack_dim: int,
    split_dim: int,
) -> torch.fx.Node: ...
@register_graph_pattern(
    CallFunction(torch.stack, getitem_split, dim=Ignored(), _users=MULTIPLE),
    pass_dict=construct_pattern_matcher_pass("split_stack_to_cats_pass"),
)
def split_stack_to_cats(match: Match, split_sections: list[int], dim: int): ...
@register_graph_pattern(
    CallFunction(torch.stack, getitem_unbind, dim=Ignored(), _users=MULTIPLE),
    pass_dict=construct_pattern_matcher_pass("unbind_stack_to_slices_pass"),
)
def unbind_stack_to_slices(match: Match, unbind_input: torch.fx.Node, dim: int): ...
def get_view_shape_list(cat_arg: torch.fx.Node, stack_dim: int) -> list[int]: ...
@register_graph_pattern(
    CallFunction(torch.stack, reshape_getitem_split, dim=Ignored(), _users=MULTIPLE),
    pass_dict=construct_pattern_matcher_pass("move_reshape_out_of_split_stack_pass"),
)
def move_reshape_out_of_split_stack(match: Match, *args, **kwargs): ...

view_getitem_split_aten = ...

@register_graph_pattern(
    CallFunction(torch.ops.aten.cat.default, view_getitem_split_aten, dim=Ignored(), _users=MULTIPLE),
    pass_dict=construct_pattern_matcher_pass("move_view_after_cat_aten_pass"),
)
def move_view_after_cat(match: Match, *args, **kwargs): ...
def match_einsum_strings(s: str) -> bool:
    """
    This function takes a string s as input, where s is in the format "3 letter string,
    4 letter string -> 3 letter string".
    It checks if the strings match the rule and returns True if they do, False otherwise.

    The rule is:
    - The three strings have the same first two characters.
    - The first two strings have the same third character.
    - The second and third strings have the same last character.
    """

@register_graph_pattern(
    CallFunctionVarArgs(torch.functional.einsum, users=MULTIPLE),
    pass_dict=construct_pattern_matcher_pass("einsum_to_pointwise_pass"),
)
def replace_einsum_to_pointwise(match: Match, *args, **kwargs): ...
