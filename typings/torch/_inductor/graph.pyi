import contextlib
from collections.abc import Callable, Iterable, Iterator, Sequence
from contextlib import contextmanager
from types import ModuleType
from typing import TYPE_CHECKING, Any, NoReturn

import sympy
import torch
import torch.fx
from sympy import Expr
from torch import Tensor, device
from torch.fx import GraphModule
from torch.fx.experimental.symbolic_shapes import ShapeEnv
from torch.fx.graph import Graph
from torch.fx.node import Node
from torch.utils._ordered_set import OrderedSet

from . import config, ir
from .codegen.common import BackendFeature, FileBackedGraphModule, WorkspaceArg
from .codegen.wrapper import PythonWrapperCodegen
from .dependencies import Dep
from .ir import Constant, GraphPartitionSignature, ShapeAsConstantBuffer, TensorBox, TorchBindObject
from .scheduler import BaseSchedulerNode
from .utils import ValueWithLineMap

if TYPE_CHECKING:
    type CompiledModule = ModuleType | FileBackedGraphModule
log = ...
perf_hint_log = ...
aten = ...
_post_grad_graph_counter = ...
if config.is_fbcode(): ...
else:
    def log_module_code(*args: Any, **kwargs: Any) -> None: ...

def may_get_constant_buffer_dtype(constant_buffer: sympy.Expr) -> torch.dtype | None: ...
def is_magic_method(op: Any) -> bool: ...
def getattr_recursive(obj: GraphModule, target: str) -> Tensor | torch._C.ScriptObject | GraphModule: ...
def get_user_visible_output_strides(g: Graph) -> dict[Node, tuple[int, ...]]: ...
def mark_nodes_dislike_padding(g: Graph, user_visible_output_strides: dict[Node, tuple[int, ...]]) -> None: ...

class GraphLowering(torch.fx.Interpreter):
    graph_outputs: list[ir.IRNode]
    def __init__(
        self,
        gm: torch.fx.GraphModule,
        example_inputs: Sequence[object] | None = ...,
        shape_env: ShapeEnv | None = ...,
        graph_id: int | None = ...,
        cpp_wrapper: bool = ...,
        aot_mode: bool = ...,
        layout_opt: bool | None = ...,
        extern_node_serializer: Callable[[list[ir.ExternKernelNode]], Any] | None = ...,
        is_inference: bool = ...,
        is_backward: bool = ...,
        is_const_graph: bool = ...,
        const_output_index: dict[str, int] | None = ...,
        const_wrapper_code: str | None = ...,
        const_kernel_code: str | None = ...,
        const_module: GraphLowering | None = ...,
        name: str | None = ...,
        inputs_to_check: Sequence[int] | None = ...,
        fx_wrapper: bool = ...,
    ) -> None: ...
    def freeze_runtime_asserts(self) -> None: ...
    def symbolic_sizes_strides(self, ex: torch.Tensor) -> tuple[Sequence[int | Expr], Sequence[int | Expr]]: ...
    def static_sizes_strides(self, ex: torch.Tensor) -> tuple[list[sympy.Expr], list[sympy.Expr]]: ...
    def get_allocation_size(
        self, node: ir.TensorBox | ir.StorageBox | ir.Buffer | WorkspaceArg | ir.TorchBindObject
    ) -> Sequence[Expr]: ...
    def get_allocation_storage_size(self, node: ir.Buffer | WorkspaceArg | ir.TorchBindObject) -> Expr: ...
    def has_feature(self, device: torch._inductor.ir.IRNode | device | None, feature: BackendFeature) -> bool: ...
    def get_dep_size_hint(self, dep: Dep) -> int: ...
    def get_current_device_or_throw(self) -> torch.device: ...
    @contextlib.contextmanager
    def set_current_device(self, device: torch.device) -> Iterator[None]: ...
    def get_training_phase(self) -> str: ...
    @staticmethod
    def decide_layout_opt(gm: GraphModule, *, is_inference: bool) -> bool: ...
    def qualify_name(self, name: str) -> str: ...
    def make_subgraph(
        self, gm: torch.fx.GraphModule, example_inputs: list[torch.Tensor], subgraph_name: str
    ) -> SubgraphLowering: ...
    def find_nodes_prefer_channels_last(self) -> OrderedSet[Node]: ...
    def warn_fallback(self, name: str) -> None: ...
    def add_device_info(self, device: torch.device) -> None: ...
    @property
    def fake_mode(self) -> torch._subclasses.fake_tensor.FakeTensorMode: ...
    def try_get_buffer(self, buffer_name: str) -> ir.TensorBox | ir.Buffer | ir.TorchBindObject | None: ...
    def add_symbol_graph_input(self, symbol: sympy.Expr) -> None: ...
    def get_buffer(self, buffer_name: str) -> ir.TensorBox | ir.Buffer | ir.TorchBindObject: ...
    def get_dtype(self, buffer_name: str) -> torch.dtype: ...
    def get_numel(self, buffer_name: str) -> int | Expr: ...
    def run(self, *args: Any) -> Any: ...
    def register_operation(self, op: ir.Operation) -> str: ...
    def register_buffer(self, buffer: ir.Buffer, *, set_name: bool = ...) -> str: ...
    def register_operation_list(self, operation_names: list[str]) -> str: ...
    def register_users_of(self, node_output: Iterable[ir.IRNode] | ir.IRNode) -> None: ...
    def mark_buffer_mutated(self, name: str) -> None: ...
    def get_original_value_of_constant(self, name: str) -> torch.Tensor: ...
    def allocate_non_dup_const_name(self, name: str | None, data: Tensor) -> str: ...
    def add_tensor_constant(self, data: Tensor, name: str | None = ...) -> TensorBox | ir.ShapeAsConstantBuffer: ...
    def constant_name(self, name: str, device_override: torch.device | None) -> str: ...
    def placeholder(self, target: str, args: tuple[object], kwargs: dict[str, object]) -> Expr | TensorBox | None: ...
    def call_function(self, target: Callable, args: Any, kwargs: dict[str, Any]) -> Any: ...
    @staticmethod
    def can_inline_constant(t: torch.Tensor) -> bool: ...
    def get_attr(
        self, target: str, args: tuple[()], kwargs: dict[str, object]
    ) -> Constant | TensorBox | ShapeAsConstantBuffer | ir.Subgraph | TorchBindObject: ...
    def call_module(self, target: Any, args: Any, kwargs: Any) -> NoReturn: ...
    def call_method(self, target: Any, args: Any, kwargs: Any) -> NoReturn: ...
    def output(self, target: str, args: tuple[object], kwargs: dict[str, object]) -> None: ...
    def finalize(self) -> None: ...
    @contextmanager
    def set_current_node(self, node: torch.fx.Node): ...
    @contextmanager
    def set_current_wrapper_code(self) -> Iterator[None]: ...
    def propagate_mutation(
        self,
        fx_node: torch.fx.Node,
        old_args: tuple[Any],
        old_kwargs: dict[str, Any],
        new_args: tuple[Any],
        new_kwargs: dict[str, Any],
    ) -> None: ...
    def run_node(self, n: torch.fx.Node) -> object: ...
    def create_deferred_runtime_asserts(
        self, n: torch.fx.Node, new_unbacked_defs: OrderedSet[sympy.Symbol]
    ) -> None: ...
    def validate_can_generate_cpp_wrapper(self) -> None: ...
    def init_wrapper_code(
        self,
        is_subgraph: bool = ...,
        subgraph_name: str | None = ...,
        parent_wrapper_code: PythonWrapperCodegen | None = ...,
        partition_signatures: GraphPartitionSignature | None = ...,
    ) -> None: ...
    def extract_autotune_inputs(self, example_inputs: list[int | float | torch.Tensor]) -> None: ...
    def codegen_with_cpp_wrapper(self) -> tuple[ValueWithLineMap, ValueWithLineMap]: ...
    def codegen(self) -> tuple[ValueWithLineMap, ValueWithLineMap]: ...
    def codegen_subgraph(self, parent_graph: GraphLowering) -> None: ...
    def count_bytes(self) -> tuple[int, list[tuple[BaseSchedulerNode, int]], list[tuple[BaseSchedulerNode, float]]]: ...

    save_output_code: Callable[[str], None] | None = ...
    def compile_to_module(self) -> CompiledModule: ...
    def get_output_names(self) -> list[str]: ...
    def is_unspec_arg(self, name: str) -> bool: ...

class SubgraphLowering(GraphLowering):
    def __init__(self, parent: GraphLowering, *args: Any, **kwargs: Any) -> None: ...
    def init_wrapper_code(
        self,
        is_subgraph: bool = ...,
        subgraph_name: str | None = ...,
        parent_wrapper_code: PythonWrapperCodegen | None = ...,
        partition_signatures: GraphPartitionSignature | None = ...,
    ) -> None: ...
