import contextlib
import dataclasses
from collections.abc import Callable, Container, Generator, Iterable, Iterator, Sequence
from enum import Enum
from typing import TYPE_CHECKING, Any, ClassVar, Literal, Never, ParamSpec, Self, TypeIs, TypeVar, overload, override

import sympy
import torch._export.serde.schema as export_schema
import torch.fx
import torch.utils._pytree as pytree
from sympy import Expr, Integer, Symbol
from torch._library.fake_class_registry import FakeScriptObject
from torch.fx.experimental.symbolic_shapes import IterateExprs, ShapeEnv, SympyBoolean
from torch.fx.node import Node
from torch.utils._ordered_set import OrderedSet

from . import dependencies
from .codegen.common import CodegenSymbol, Kernel
from .codegen.cuda.cuda_template import CUDATemplate
from .codegen.wrapper import PythonWrapperCodegen
from .dependencies import Dep
from .graph import GraphLowering
from .loop_body import LoopBody
from .ops_handler import OpCountResult, ReductionType, StoreMode
from .runtime.hints import ReductionHint
from .utils import IndentedBuffer, cache_on_self, cache_on_self_and_args, ir_dataclass
from .virtualized import OpsValue

triton_version = ...
has_triton = ...
_P = ParamSpec("_P")
_T = TypeVar("_T")
_U = TypeVar("_U")
_V = TypeVar("_V")
type _IntLike = int | Expr
type _NumLike = int | float | Expr
type _OpOverloads = torch._ops.OpOverload | torch._ops.HigherOrderOperator
log = ...
indent = ...
aten = ...
autotune_warmup = ...
autotune_rep = ...
type _NodeOrNodes = (
    int
    | TensorBox
    | dict[str, TensorBox]
    | Symbol
    | IRNode
    | Sequence[int | dict[str, TensorBox] | TensorBox | Symbol | IRNode | None]
)

@dataclasses.dataclass(frozen=True)
class GraphPartitionSignature:
    symbol_inputs: OrderedSet[sympy.Symbol]
    input_nodes: dict[str, IRNode | sympy.Expr | TorchBindObject]
    output_nodes: list[IRNode]
    input_deallocation: dict[str, bool]
    skip_cudagraph: bool
    constant_names: list[str]

def validate_ir(node_or_nodes: _NodeOrNodes | None) -> None: ...
def ops_wrapper(name: str) -> Callable[..., OpsValue]: ...
def inverse_reorder(order: Sequence[int]) -> Callable[[Sequence[_T]], Sequence[_T]]: ...
def same_reorder(order: Sequence[int]) -> Callable[[Sequence[_T]], Sequence[_T]]: ...
def fuse_reindexing(
    reindex1: Callable[[Sequence[_U]], Sequence[_V]], reindex2: Callable[[Sequence[_T]], Sequence[_U]]
) -> Callable[[Sequence[_T]], Sequence[_V]]: ...

NHWC_STRIDE_ORDER = ...
NHWDC_STRIDE_ORDER = ...

def get_fill_order(seq: Sequence[int | torch.SymInt | Expr], shape_env: ShapeEnv | None = ...) -> Sequence[int]: ...
def stride_order2fill_order(order: Sequence[int | Integer]) -> Sequence[int]: ...
def get_stride_order(seq: Sequence[int | torch.SymInt | Expr], shape_env: ShapeEnv | None = ...) -> Sequence[int]: ...
@overload
def ir_node_to_tensor(x: None, guard_shape: bool = ...) -> None: ...
@overload
def ir_node_to_tensor(x: IRNode, guard_shape: bool = ...) -> torch.Tensor: ...
def ir_node_to_tensor(x: IRNode | None, guard_shape: bool = ...) -> torch.Tensor | None: ...
def may_convert_to_optional[T](value: Sequence[_T] | None) -> Sequence[_T | None] | None: ...
def get_device_type(x: IRNode | OutputSpec | torch.device | None | str) -> str | None: ...
def is_triton(x: IRNode | torch.device | None | str) -> bool: ...
def is_cpu(x: IRNode | torch.device | None | str) -> bool: ...
def is_aligned_realized_tensor_hint(x: Buffer | TensorBox, alignment: int) -> bool: ...
def significant_strides_equal(
    strides1: Sequence[_IntLike], strides2: Sequence[_IntLike], shape: Sequence[_IntLike]
) -> bool: ...
def try_match_insignificant_strides(tensor: IRNode, strides: Sequence[int | torch.SymInt]) -> IRNode: ...
def gm_original_output_strides(gm: torch.fx.GraphModule) -> None: ...
def get_symbolic_inputs(inputs: Sequence[IRNode]) -> list[Expr]: ...

class IRNode:
    _current_origins: ClassVar[OrderedSet[Any]] = ...
    origins: OrderedSet[Any] = ...
    traceback: list[str] | None = ...
    origin_node: torch.fx.Node | None = ...
    @staticmethod
    @contextlib.contextmanager
    def current_origins(origins: OrderedSet[Node]) -> Generator[None]: ...
    @staticmethod
    def is_realized_node(node: IRNode) -> bool: ...
    def __post_init__(self) -> None: ...
    def get_read_names(self) -> OrderedSet[str]: ...
    def get_traceback(self) -> list[str] | None: ...
    def get_origin_node(self) -> torch.fx.Node | None: ...
    def get_defining_op(self) -> Operation | None: ...
    def get_stack_traces(self) -> OrderedSet[str]: ...
    def common_repr(self, shorten: bool = ...) -> Sequence[str]: ...
    def str_helper(self, lines: Sequence[object], shorten: bool = ..., multiline: bool = ...) -> str: ...
    def get_dtype(self) -> torch.dtype: ...
    def maybe_get_dtype(self) -> torch.dtype | None: ...
    def get_layout(self) -> Layout: ...
    def maybe_get_layout(self) -> Layout | None: ...
    def get_output_spec(self) -> OutputSpec: ...
    def maybe_get_output_spec(self) -> OutputSpec | None: ...
    def has_tensor_output(self) -> bool: ...
    def get_size(self) -> Sequence[Expr]: ...
    def maybe_get_size(self) -> Sequence[_IntLike] | None: ...
    @property
    def shape(self) -> _IntLike | sympy.Rel | Sequence[_IntLike]: ...
    def get_numel(self) -> Expr: ...
    def is_zero_elements(self) -> bool: ...
    def realize(self) -> str | None: ...
    def codegen_reference(self, writer: IndentedBuffer | None = ...) -> str: ...
    def get_device(self) -> torch.device | None: ...
    def get_device_or_error(self) -> torch.device: ...
    def has_exceeded_max_reads(self) -> bool: ...
    def make_loader(self) -> Callable[[Sequence[Expr]], OpsValue]: ...
    def make_indexer(self) -> Callable[[Sequence[Expr]], Expr]: ...
    def get_stride(self) -> Sequence[_IntLike]: ...
    def maybe_get_stride(self) -> Sequence[_IntLike] | None: ...
    def get_name(self) -> str: ...
    def maybe_get_name(self) -> str | None: ...
    def is_input_buffer(self) -> bool: ...
    def has_large_inner_fn(self, threshold: int | None = ...) -> bool: ...
    def mark_reuse(self, users: int) -> None: ...
    def realize_hint(self) -> None: ...
    def unwrap_view(self) -> IRNode: ...
    def freeze_layout(self) -> None: ...
    def freeze_layout_with_stride_order(self, order: Sequence[int], allow_padding: bool = ...) -> None: ...
    def freeze_layout_with_fill_order(self, order: Sequence[int]) -> None: ...
    def freeze_layout_with_same_order(self, stride: Sequence[_IntLike]) -> None: ...
    def freeze_layout_with_exact_strides(
        self, exact_strides: Sequence[_IntLike], allow_padding: bool = ...
    ) -> None: ...
    def get_read_writes(self) -> dependencies.ReadWrites: ...
    def get_reads(self) -> OrderedSet[Dep]: ...
    def num_reads(self) -> int: ...
    def get_storage_numel(self) -> _IntLike: ...
    def get_free_symbol_uses(self, unbacked_only: bool = ...) -> OrderedSet[sympy.Symbol]: ...
    def get_reduction_type(self) -> str | None: ...
    def get_reduction_size(self) -> Sequence[Expr]: ...
    def is_extern(self) -> bool: ...
    def is_no_op(self) -> bool: ...
    def constant_to_device(self, device: torch.device) -> IRNode: ...
    def get_mutation_names(self) -> Sequence[str]: ...
    def get_operation_name(self) -> str: ...
    def get_inputs_that_alias_output(self) -> Sequence[str]: ...

    if TYPE_CHECKING:
        @property
        def dtype(self) -> torch.dtype: ...

@ir_dataclass(frozen=False)
class Operation:
    def __post_init__(self) -> None: ...
    def get_device(self) -> torch.device | None: ...
    def get_origin_node(self) -> torch.fx.Node | None: ...
    def get_origins(self) -> OrderedSet[Any]: ...
    def get_operation_name(self) -> str: ...
    def is_extern(self) -> bool: ...
    def is_no_op(self) -> bool: ...
    def get_read_writes(self) -> dependencies.ReadWrites: ...
    def is_user_of(self, name: str) -> bool: ...
    def get_read_names(self) -> OrderedSet[str]: ...
    def get_reads(self) -> OrderedSet[Dep]: ...
    def get_outputs(self) -> list[Buffer]: ...
    def get_unbacked_symbol_defs(self) -> OrderedSet[sympy.Symbol]: ...
    def get_free_symbol_uses(self, unbacked_only: bool = ...) -> OrderedSet[sympy.Symbol]: ...
    def get_workspace_size(self) -> int: ...

@ir_dataclass
class Loops(IRNode):
    device: torch.device
    dtype: torch.dtype
    inner_fn: Callable[..., Any]
    ranges: Sequence[_IntLike]
    @cache_on_self_and_args("Loops")
    def get_free_symbol_uses(self, unbacked_only: bool = ...) -> OrderedSet[sympy.Symbol]: ...
    def __post_init__(self) -> None: ...

    __repr__ = ...
    def get_device(self) -> torch.device | None: ...
    def get_origin_node(self) -> torch.fx.Node | None: ...
    def get_size(self) -> Sequence[Expr]: ...
    def get_pointwise_size(self) -> Sequence[Expr]: ...
    @classmethod
    def create(cls, *args: Any, **kwargs: Any) -> TensorBox | ShapeAsConstantBuffer: ...
    @cache_on_self
    def inner_fn_opcount(self) -> OpCountResult: ...
    def inner_fn_args(self) -> Sequence[Sequence[_IntLike]]: ...
    @cache_on_self
    def inner_fn_str(self) -> str: ...
    def has_large_inner_fn(self, threshold: int | None = ...) -> bool: ...
    def inner_fn_free_symbols(self, unbacked_only: bool = ...) -> OrderedSet[Symbol]: ...
    def get_reads(self) -> OrderedSet[Dep]: ...
    def get_read_names(self) -> OrderedSet[str]: ...
    def num_reads(self) -> int: ...
    def get_reduction_size(self) -> Sequence[Expr]: ...
    def get_reduction_type(self) -> str | None: ...
    def constant_to_device(self, device: torch.device) -> IRNode: ...

def nop_loader_fn(idx: Expr | Sequence[Expr], *, dtype: torch.dtype) -> OpsValue: ...

@ir_dataclass
class Pointwise(Loops):
    def make_loader(self) -> Callable[[Sequence[Expr]], OpsValue]: ...
    def get_reduction_size(self) -> Sequence[sympy.Expr]: ...
    def get_reduction_type(self) -> str | None: ...
    def store_output(
        self, output_name: str | None, indexer: Callable[[Sequence[Expr]], Never], vars: Sequence[Expr]
    ) -> None: ...
    def constant_to_device(self, device: torch.device) -> IRNode: ...

@ir_dataclass
class Scatter(Pointwise):
    output_indexer: Callable[[Sequence[Expr]], Expr]
    scatter_mode: StoreMode = ...
    def constant_to_device(self, device: torch.device) -> IRNode: ...
    def store_output(
        self, output_name: str | None, indexer: Callable[[Sequence[Expr]], Never], vars: Sequence[Expr]
    ) -> Any: ...

REDUCTION_COMBINE_FN: dict[str, Callable[..., OpsValue]] = ...

def get_reduction_combine_fn(
    reduction_type: str, dtype: torch.dtype, arg_break_ties_left: bool = ...
) -> Callable[..., object]: ...

@ir_dataclass
class Reduction(Loops):
    reduction_ranges: Sequence[_IntLike]
    reduction_type: ReductionType
    src_dtype: torch.dtype
    reduction_hint: ReductionHint

    __repr__ = ...
    @cache_on_self_and_args("Reduction")
    def get_free_symbol_uses(self, unbacked_only: bool = ...) -> OrderedSet[Symbol]: ...
    def get_reduction_size(self) -> Sequence[Expr]: ...
    def get_reduction_type(self) -> str | None: ...
    def store_reduction(
        self,
        output_name: str | None,
        indexer: Callable[[Sequence[Expr]], Never],
        vars: Sequence[Expr],
        reduction_vars: Sequence[Symbol],
    ) -> None: ...
    def index_length(self) -> int: ...
    def inner_fn_args(self) -> Sequence[Sequence[Expr]]: ...
    def inner_fn_free_symbols(self, unbacked_only: bool = ...) -> OrderedSet[Symbol]: ...
    def constant_to_device(self, device: torch.device) -> IRNode: ...
    @staticmethod
    def num_splits(
        device: torch.device,
        dst_dtype: torch.dtype,
        src_dtype: torch.dtype,
        inner_fn: Callable[_P, OpsValue],
        ranges: Sequence[_IntLike],
        reduction_ranges: Sequence[_IntLike],
        reduction_type: ReductionType | Literal["scan"],
        reduction_numel: Expr,
        input_node: IRNode | None = ...,
    ) -> tuple[ReductionHint, _IntLike]: ...
    @classmethod
    def create(
        cls,
        device: torch.device,
        dst_dtype: torch.dtype,
        src_dtype: torch.dtype,
        inner_fn: Callable[..., Any],
        ranges: Sequence[Expr],
        reduction_ranges: Sequence[Expr],
        reduction_type: ReductionType,
        reduction_hint: ReductionHint = ...,
        input_node: IRNode | None = ...,
    ) -> TensorBox | ShapeAsConstantBuffer: ...
    @staticmethod
    def default_accumulator(reduction_type: str, dtype: torch.dtype) -> _NumLike | Sequence[_NumLike]: ...
    @staticmethod
    def default_value(reduction_type: str, dtype: torch.dtype) -> _NumLike | Sequence[_NumLike]: ...
    @classmethod
    def check_for_split_dense_dim_reindexing(
        cls, reduction_numel: _IntLike, input_node: IRNode | None
    ) -> int | None: ...
    @classmethod
    def create_multilayer_helper(
        cls,
        device: torch.device,
        dst_dtype: torch.dtype,
        src_dtype: torch.dtype,
        wrapper_fn: Callable[..., Any],
        original_ranges: Sequence[Expr],
        original_reduction_ranges: Sequence[Expr],
        new_ranges: list[Expr],
        new_reduction_ranges: list[Integer],
        reduction_type: ReductionType,
        split: _IntLike,
        reduction_hint: ReductionHint,
    ) -> TensorBox | ShapeAsConstantBuffer: ...
    @classmethod
    def create_multilayer(
        cls,
        device: torch.device,
        dst_dtype: torch.dtype,
        src_dtype: torch.dtype,
        inner_fn: Callable[..., Any],
        ranges: Sequence[Expr],
        reduction_ranges: Sequence[Expr],
        reduction_type: ReductionType,
        split: _IntLike,
        reduction_hint: ReductionHint,
        input_node: IRNode | None = ...,
    ) -> TensorBox | ShapeAsConstantBuffer: ...
    @classmethod
    def create_multilayer_existing_ranges(
        cls,
        device: torch.device,
        dst_dtype: torch.dtype,
        src_dtype: torch.dtype,
        inner_fn: Callable[..., Any],
        original_ranges: Sequence[Expr],
        original_reduction_ranges: Sequence[Expr],
        new_ranges: list[Integer],
        new_reduction_ranges: list[Integer],
        reduction_type: ReductionType,
        reduction_hint: ReductionHint,
    ) -> TensorBox | ShapeAsConstantBuffer: ...

type INNER_FN_TY = Callable[[Sequence[Expr], Sequence[Expr]], OpsValue]

class MultiOutputReduction(Reduction):
    output_index: int
    def __init__(
        self,
        device: torch.device,
        dst_dtype: torch.dtype,
        inner_fns: INNER_FN_TY | Sequence[INNER_FN_TY],
        ranges: Sequence[Integer],
        reduction_ranges: Sequence[Integer],
        reduction_type: ReductionType,
        src_dtype: torch.dtype,
        reduction_hint: ReductionHint,
        output_index: int,
    ) -> None: ...
    def store_reduction(
        self,
        output_name: str | None,
        indexer: Callable[[Sequence[Expr]], Never],
        vars: Sequence[Expr],
        reduction_vars: Sequence[Symbol],
    ) -> Any: ...

class OnlineSoftmaxReduction(MultiOutputReduction):
    @classmethod
    def create(
        cls,
        device: torch.device,
        dst_dtype: torch.dtype,
        src_dtype: torch.dtype,
        inner_fn: Callable[..., Any],
        ranges: Sequence[Expr],
        reduction_ranges: Sequence[Expr],
        num_output: int,
        reduction_hint: ReductionHint = ...,
        input_node: IRNode | None = ...,
    ) -> Sequence[TensorBox | ShapeAsConstantBuffer]: ...

class WelfordReduction(MultiOutputReduction):
    @classmethod
    def create(
        cls,
        device: torch.device,
        dtype: torch.dtype,
        inner_fns: Sequence[Callable[..., Any]],
        ranges: list[Integer],
        reduction_ranges: list[Integer],
        reduction_type: ReductionType,
        reduction_hint: ReductionHint = ...,
    ) -> Sequence[TensorBox | ShapeAsConstantBuffer]: ...
    @staticmethod
    def default_value(reduction_type: str, dtype: torch.dtype) -> _NumLike | Sequence[_NumLike]: ...
    @classmethod
    def create_multilayer(
        cls,
        device: torch.device,
        dtype: torch.dtype,
        inner_fns: Sequence[Callable[..., Any]],
        ranges: list[Integer],
        reduction_ranges: list[Integer],
        reduction_type: ReductionType,
        split: _IntLike,
        reduction_hint: ReductionHint,
    ) -> Sequence[TensorBox | ShapeAsConstantBuffer]: ...

@ir_dataclass
class Scan(Loops):
    scan_ranges: list[Integer]
    size: list[Integer]
    combine_fn: Callable[[tuple[Any, ...], tuple[Any, ...]], tuple[Any, ...]]
    reindex: Callable[[Sequence[_IntLike], Sequence[_IntLike]], Sequence[_IntLike]]
    reduction_hint: ReductionHint
    output_index: int
    dtypes: tuple[torch.dtype, ...]
    inner_fns: tuple[Callable[..., Any], ...]
    @cache_on_self_and_args("Scan")
    def get_free_symbol_uses(self, unbacked_only: bool = ...) -> OrderedSet[Symbol]: ...
    def __post_init__(self) -> None: ...
    def store_reduction(
        self,
        output_name: str | None,
        indexer: Callable[[Sequence[_IntLike]], Never],
        vars: Sequence[Expr],
        scan_vars: Sequence[Symbol],
    ) -> Any: ...
    def get_reduction_type(self) -> str | None: ...
    def get_reduction_size(self) -> Sequence[Expr]: ...
    def get_size(self) -> Sequence[Expr]: ...
    def get_pointwise_size(self) -> Sequence[Expr]: ...
    def index_length(self) -> int: ...
    def inner_fn_args(self) -> Sequence[Sequence[_IntLike]]: ...
    def inner_fn_free_symbols(self, unbacked_only: bool = ...) -> OrderedSet[Symbol]: ...
    @classmethod
    def create(
        cls,
        device: torch.device,
        dtypes: tuple[torch.dtype, ...],
        inner_fns: tuple[Callable[[Sequence[Expr]], Any], ...],
        size: list[Integer],
        axis: int,
        combine_fn: Callable[[tuple[Any, ...], tuple[Any, ...]], tuple[Any, ...]],
        reduction_hint: ReductionHint = ...,
        *,
        can_fallback_to_aten: bool = ...,
        **kwargs: Any,
    ) -> Sequence[TensorBox | ShapeAsConstantBuffer | None]: ...
    @classmethod
    def num_splits(
        cls,
        device: torch.device,
        dtype: torch.dtype,
        inner_fn: Callable[[Sequence[Expr]], OpsValue],
        axis: int,
        pointwise_ranges: list[Integer],
        scan_ranges: list[Integer],
        combine_fn: Callable[[tuple[Any, ...], tuple[Any, ...]], tuple[Any, ...]],
        scan_numel: Expr,
    ) -> tuple[ReductionHint, _IntLike]: ...

@ir_dataclass
class SplitScan(Scan): ...

@ir_dataclass
class Sort(Loops):
    sort_ranges: list[Integer]
    size: list[Integer]
    reindex: Callable[[Sequence[Expr], Sequence[Expr]], Sequence[Expr]]
    reduction_hint: ReductionHint
    output_index: int
    dtypes: tuple[torch.dtype, ...]
    inner_fns: tuple[Callable[..., Any], ...]
    stable: bool
    descending: bool
    @cache_on_self_and_args("Sort")
    def get_free_symbol_uses(self, unbacked_only: bool = ...) -> OrderedSet[Symbol]: ...
    def __post_init__(self) -> None: ...
    def store_reduction(
        self,
        output_name: str | None,
        indexer: Callable[[Sequence[Expr]], Expr],
        vars: Sequence[Expr],
        reduction_vars: Sequence[Expr],
    ) -> Any: ...
    def get_reduction_type(self) -> str | None: ...
    def get_reduction_size(self) -> Sequence[Expr]: ...
    def get_size(self) -> Sequence[Expr]: ...
    def get_pointwise_size(self) -> Sequence[Expr]: ...
    def index_length(self) -> int: ...
    def inner_fn_args(self) -> Sequence[Sequence[Expr]]: ...
    def inner_fn_free_symbols(self, unbacked_only: bool = ...) -> OrderedSet[Symbol]: ...
    @classmethod
    def create(
        cls,
        device: torch.device,
        dtypes: tuple[torch.dtype, ...],
        inner_fns: tuple[Callable[[list[Expr]], Any], ...],
        size: list[Integer],
        axis: int,
        stable: bool,
        descending: bool,
        reduction_hint: ReductionHint = ...,
        **kwargs: Any,
    ) -> Sequence[TensorBox | ShapeAsConstantBuffer | None]: ...

def is_storage_and_layout(x: IRNode) -> bool: ...
def is_contiguous_storage_and_layout(x: IRNode) -> bool: ...
def as_storage_and_layout(
    x: IRNode,
    freeze: bool = ...,
    want_contiguous: bool = ...,
    stride_order: Sequence[int | Integer] | None = ...,
    allow_padding: bool = ...,
    exact_strides: Sequence[int | Integer] | None = ...,
) -> tuple[StorageBox, Layout]: ...
def is_stride_order_storage_and_layout(x: IRNode, stride_order: Sequence[int | Integer]) -> bool: ...
def is_unaligned(node: IRNode) -> bool: ...

@ir_dataclass
class BaseView(IRNode):
    data: IRNode
    @cache_on_self_and_args("BaseView")
    def get_free_symbol_uses(self, unbacked_only: bool = ...) -> OrderedSet[Symbol]: ...
    def make_reindexer(self) -> Callable[[Sequence[Expr]], Sequence[Expr]]: ...
    def make_indexer(self) -> Callable[[Sequence[Expr]], Expr]: ...
    def make_loader(self) -> Callable[[Sequence[Expr]], OpsValue]: ...
    @property
    def dtype(self) -> torch.dtype: ...
    def get_layout(self) -> Layout: ...
    def get_device(self) -> torch.device | None: ...
    def get_origin_node(self) -> torch.fx.Node | None: ...
    def get_name(self) -> str: ...
    def get_pointwise_size(self) -> Sequence[Expr]: ...
    def mark_reuse(self, users: int) -> None: ...
    def has_exceeded_max_reads(self) -> bool: ...
    def realize(self) -> str | None: ...
    def realize_hint(self) -> None: ...
    def get_storage_numel(self) -> _IntLike: ...
    def is_extern(self) -> bool: ...
    def is_module_buffer(self) -> bool: ...
    def get_read_names(self) -> OrderedSet[str]: ...
    def get_reads(self) -> OrderedSet[Dep]: ...
    def unwrap_view(self) -> IRNode: ...
    def constant_to_device(self, device: torch.device) -> IRNode: ...

@ir_dataclass
class ExpandView(BaseView):
    size: Sequence[Expr]
    @classmethod
    def create(cls, x: IRNode, new_size: Sequence[_IntLike]) -> BaseView: ...
    def get_size(self) -> Sequence[Expr]: ...
    def make_reindexer(self) -> Callable[[Sequence[Expr]], Sequence[Expr]]: ...

@ir_dataclass
class PermuteView(BaseView):
    dims: list[Expr]
    @classmethod
    def create(cls, x: IRNode, dims: Sequence[int]) -> BaseView: ...
    def get_size(self) -> Sequence[Expr]: ...
    def make_reindexer(self) -> Callable[[Sequence[Expr]], Sequence[Expr]]: ...

@ir_dataclass
class SqueezeView(BaseView):
    @classmethod
    def create(cls, x: IRNode, *, dim: int | None = ...) -> IRNode: ...
    @staticmethod
    def squeezer(size: Sequence[Expr]) -> tuple[list[int], Callable[[Sequence[Expr]], tuple[Expr]]]: ...
    def __init__(self, data: Any) -> None: ...

@ir_dataclass
class GenericView(BaseView):
    size: Sequence[Expr]
    reindex: Callable[[Sequence[Expr]], Sequence[Expr]]
    def make_reindexer(self) -> Callable[[Sequence[Expr]], Sequence[Expr]]: ...
    def reindex_str(self) -> str: ...

    __repr__ = ...
    @classmethod
    def create(
        cls, x: IRNode, new_size: Sequence[Expr], reindex: Callable[[Sequence[Expr]], Sequence[Expr]]
    ) -> BaseView: ...
    def get_size(self) -> Sequence[Expr]: ...

@ir_dataclass
class View(GenericView):
    @staticmethod
    def handle_negative_index(idx: Expr, size: Expr) -> Expr: ...
    @classmethod
    def create(cls, x: IRNode, new_size: Sequence[Expr]) -> IRNode: ...
    @staticmethod
    def resolve_negative_size(old_size: Sequence[Expr], new_size: Sequence[Expr]) -> tuple[list[Expr], list[Expr]]: ...
    @classmethod
    def dynamic_reshape_indexer(
        cls, old_size: Sequence[_IntLike], new_size: Sequence[_IntLike], dense_dim: int | None = ...
    ) -> Callable[[Sequence[_T]], Sequence[_V]]: ...

@ir_dataclass
class ReinterpretView(BaseView):
    layout: Layout
    def __post_init__(self) -> None: ...

    __repr__ = ...
    def get_name(self) -> str: ...
    def get_device(self) -> torch.device | None: ...
    def get_origin_node(self) -> torch.fx.Node | None: ...
    @property
    def dtype(self) -> torch.dtype: ...
    def get_size(self) -> Sequence[Expr]: ...
    def get_stride(self) -> Sequence[Expr]: ...
    def make_loader(self) -> Callable[[Sequence[Expr]], OpsValue]: ...
    def make_indexer(self) -> Callable[[Sequence[Expr]], Expr]: ...
    def get_layout(self) -> Layout: ...
    def freeze_layout(self) -> None: ...
    @cache_on_self_and_args("ReinterpretView")
    def get_free_symbol_uses(self, unbacked_only: bool = ...) -> OrderedSet[sympy.Symbol]: ...
    def codegen_reference(self, writer: IndentedBuffer | None = ...) -> str: ...
    def num_reads(self) -> int: ...

@ir_dataclass
class DtypeView(BaseView):
    target_dtype: torch.dtype
    @classmethod
    def create(cls, x: IRNode, new_dtype: torch.dtype) -> BaseView: ...

    __repr__ = ...
    @property
    def dtype(self) -> torch.dtype: ...
    def get_size(self) -> Sequence[Expr]: ...
    def make_loader(self) -> Callable[[Sequence[Expr]], OpsValue]: ...

class SliceView(View):
    @classmethod
    def normalize_start_end(cls, x: IRNode, dim: int, start: int, end: int) -> tuple[int, int]: ...
    @classmethod
    def create(cls, x: IRNode, dim: int, start: int, end: int, step: int = ..., clamp: bool = ...) -> IRNode: ...

@ir_dataclass
class BaseConstant(IRNode):
    dtype: torch.dtype
    device: torch.device
    def get_size(self) -> Sequence[Expr]: ...
    def get_device(self) -> torch.device | None: ...
    def get_origin_node(self) -> torch.fx.Node | None: ...
    def get_reads(self) -> OrderedSet[Dep]: ...

@ir_dataclass
class Constant(BaseConstant):
    value: Any
    dtype: torch.dtype
    device: torch.device
    def make_loader(self) -> Callable[[Sequence[Expr]], OpsValue]: ...
    def realize(self) -> str | None: ...
    def constant_to_device(self, device: torch.device) -> IRNode: ...

@ir_dataclass
class IndexingConstant(BaseConstant):
    index: Any
    dtype: torch.dtype
    device: torch.device
    def make_loader(self) -> Callable[[Sequence[Expr]], OpsValue]: ...
    def constant_to_device(self, device: torch.device) -> IRNode: ...

def is_contiguous_strides_for_shape(stride: Sequence[_IntLike], shape: Sequence[_IntLike]) -> bool: ...
def get_align_for_dtype(dtype: torch.dtype) -> int: ...

class OutputSpec:
    def get_device(self) -> torch.device | None: ...
    def storage_size(self) -> int: ...
    def get_free_symbol_uses(self, unbacked_only: bool = ...) -> OrderedSet[sympy.Symbol]: ...

@ir_dataclass
class Layout(OutputSpec):
    def __init__(
        self,
        device: torch.device,
        dtype: torch.dtype,
        size: Sequence[Expr],
        stride: Sequence[Expr] | None = ...,
        offset: Expr = ...,
        is_pinned: bool = ...,
    ) -> None: ...
    @property
    def size(self) -> Sequence[Expr]: ...
    @size.setter
    def size(self, value: Sequence[Expr]) -> None: ...
    @property
    def stride(self) -> Sequence[Expr]: ...
    @stride.setter
    def stride(self, value: Sequence[Expr]) -> None: ...
    @property
    def offset(self) -> Expr: ...
    @offset.setter
    def offset(self, value: Expr) -> None: ...

    __repr__ = ...
    def get_device(self) -> torch.device: ...
    def get_example(self) -> torch.Tensor: ...
    def is_contiguous(self) -> bool: ...
    @staticmethod
    def is_channels_last_contiguous(shape: Sequence[_IntLike], strides: Sequence[_IntLike]) -> bool: ...
    def is_transposed(self) -> bool: ...
    def is_stride_ordered(self, order: Sequence[int]) -> bool: ...
    def is_channels_last_stride_ordered(self) -> bool: ...
    def pad_strides(self) -> None: ...
    def should_pad_strides(self) -> bool: ...
    def as_fixed(self) -> FixedLayout: ...
    def make_indexer(self) -> Callable[[Sequence[Expr]], Expr]: ...
    def __eq__(self, other: object) -> bool: ...
    def storage_size(self) -> Expr: ...
    @cache_on_self_and_args("Layout")
    def get_free_symbol_uses(self, unbacked_only: bool = ...) -> OrderedSet[sympy.Symbol]: ...

class FixedLayout(Layout):
    def make_indexer(self) -> Callable[[Sequence[Expr]], Expr]: ...

class FlexibleLayout(Layout):
    allow_indexing = ...
    @staticmethod
    def contiguous_strides(sizes: Sequence[int]) -> list[Expr]: ...
    @staticmethod
    def fill_ordered(sizes: Sequence[int], order: Sequence[int]) -> list[Expr]: ...
    @staticmethod
    def stride_ordered(sizes: Sequence[int], order: Sequence[int]) -> Sequence[Expr]: ...
    @staticmethod
    def stride_ordered_for_memory_format(
        sizes: Sequence[int], memory_format: torch.memory_format
    ) -> Sequence[Expr]: ...
    @staticmethod
    def same_ordered(sizes: Sequence[int], stride: Sequence[_IntLike]) -> Sequence[Expr]: ...
    @property
    def size(self) -> Sequence[Expr]: ...
    @size.setter
    def size(self, value: Sequence[Expr]) -> None: ...
    @property
    def stride(self) -> Sequence[Expr]: ...
    @stride.setter
    def stride(self, value: Sequence[Expr]) -> None: ...
    @property
    def offset(self) -> Expr: ...
    @offset.setter
    def offset(self, value: Expr) -> None: ...
    def as_stride_order(self, order: Sequence[int], allow_padding: bool = ...) -> FixedLayout: ...
    def as_exact_strides(self, exact_strides: Sequence[_IntLike], allow_padding: bool = ...) -> FixedLayout: ...
    def as_fill_order(self, order: Sequence[int]) -> FixedLayout: ...
    def as_same_order(self, stride: Sequence[_IntLike]) -> FixedLayout: ...
    def get_initial_free_symbol_uses(self) -> dict[tuple[str, bool], sympy.Symbol]: ...
    def assert_free_symbol_uses_unchanged(self, name: str, value: IterateExprs) -> None: ...
    def __init__(
        self,
        device: torch.device,
        dtype: torch.dtype,
        size: Sequence[Expr],
        stride_order: Sequence[int | Integer] | None = ...,
        is_pinned: bool = ...,
    ) -> None: ...

class NonOwningLayout(Layout):
    def __init__(self, view: BaseView | TensorBox) -> None: ...
    def make_indexer(self) -> Callable[[Sequence[Expr]], Expr]: ...
    def maybe_guard_aligned(self) -> bool: ...
    @cache_on_self_and_args("NonOwningLayout")
    def get_free_symbol_uses(self, unbacked_only: bool = ...) -> OrderedSet[sympy.Symbol]: ...

class CommBufferType(Enum):
    SYMM_MEM = ...

class CommBufferLayout(FixedLayout):
    comm_buffer_type: CommBufferType
    group_name: str
    def __init__(self, layout: FlexibleLayout, comm_buffer_type: CommBufferType, group_name: str) -> None: ...

@ir_dataclass
class NoneLayout(OutputSpec):
    device: torch.device | None
    size: list[int] = ...
    stride: list[int] = ...
    def storage_size(self) -> int: ...
    def as_fixed(self) -> OutputSpec: ...
    def get_device(self) -> torch.device | None: ...

class MutationLayoutSHOULDREMOVE(Layout):
    def __init__(self, target: IRNode) -> None: ...
    @property
    def stride(self) -> Sequence[Expr]: ...
    @stride.setter
    def stride(self, value: Never) -> None: ...
    def storage_size(self) -> Expr: ...
    def get_buffer(self) -> Buffer: ...
    def real_layout(self) -> Layout: ...
    @classmethod
    def realize_into(cls, src: IRNode, dst: IRNode, unsafe_alias: bool = ...) -> IRNode: ...
    def as_fixed(self) -> Self: ...
    def make_indexer(self) -> Callable[[Sequence[Expr]], Expr]: ...

@ir_dataclass(frozen=False)
class Buffer(IRNode, CodegenSymbol):
    name: str | None
    layout: OutputSpec
    def __post_init__(self) -> None: ...
    def make_indexer(self) -> Callable[[Sequence[Expr]], Expr]: ...
    def get_name(self) -> str: ...
    def get_example(self) -> torch.Tensor | sympy.Symbol: ...
    def get_device(self) -> torch.device | None: ...
    def get_defining_op(self) -> Operation | None: ...
    @property
    def dtype(self) -> torch.dtype: ...
    def get_size(self) -> Sequence[Expr]: ...
    def get_stride(self) -> list[Expr]: ...
    def get_offset(self) -> Expr: ...
    def get_layout(self) -> Layout: ...
    def get_output_spec(self) -> OutputSpec: ...
    def get_storage_numel(self) -> int: ...
    def get_is_pinned(self) -> bool: ...
    def freeze_layout(self) -> None: ...
    def freeze_layout_with_stride_order(self, order: Sequence[int], allow_padding: bool = ...) -> None: ...
    def freeze_layout_with_fill_order(self, order: Sequence[int]) -> None: ...
    def freeze_layout_with_same_order(self, stride: Sequence[int]) -> None: ...
    def freeze_layout_with_exact_strides(self, exact_strides: Sequence[int], allow_padding: bool = ...) -> None: ...
    def is_zero_elements(self) -> bool: ...
    def make_loader(self) -> Callable[[Sequence[Expr]], OpsValue]: ...
    def codegen_reference(self, writer: IndentedBuffer | None = ...) -> str: ...
    def decide_layout(self) -> None: ...
    def get_inputs_that_alias_output(self) -> Sequence[str]: ...
    def get_mutation_names(self) -> Sequence[str]: ...
    def get_read_names(self) -> OrderedSet[str]: ...
    @cache_on_self_and_args("Buffer")
    def get_free_symbol_uses(self, unbacked_only: bool = ...) -> OrderedSet[sympy.Symbol]: ...
    def get_unbacked_symbol_defs(self) -> OrderedSet[sympy.Symbol]: ...
    def realize(self) -> str | None: ...
    def should_allocate(self) -> bool: ...

@ir_dataclass(frozen=False)
class OperationBuffer(Buffer, Operation):
    def get_outputs(self) -> list[Buffer]: ...
    def get_defining_op(self) -> Operation: ...

    get_operation_name = Operation.get_operation_name
    def __post_init__(self) -> None: ...

class InputBuffer(Buffer):
    def num_reads(self) -> int: ...

class DonatedBuffer(InputBuffer): ...

class ConstantBuffer(InputBuffer):
    override_device: torch.device | None = ...
    def make_loader(self) -> Callable[[Sequence[Expr]], OpsValue]: ...
    def constant_to_device(self, device: torch.device) -> IRNode: ...

@ir_dataclass
class NoneAsConstantBuffer(IRNode):
    def get_reads(self) -> OrderedSet[Dep]: ...
    @cache_on_self_and_args("NoneAsConstantBuffer")
    def get_free_symbol_uses(self, unbacked_only: bool = ...) -> OrderedSet[sympy.Symbol]: ...
    def codegen_reference(self, writer: IndentedBuffer | None = ...) -> str: ...
    def get_output_spec(self) -> OutputSpec: ...
    def has_tensor_output(self) -> bool: ...

@ir_dataclass
class ShapeAsConstantBuffer(IRNode):
    expr: Expr
    @cache_on_self_and_args("ShapeAsConstantBuffer")
    def get_free_symbol_uses(self, unbacked_only: bool = ...) -> OrderedSet[sympy.Symbol]: ...
    def codegen_reference(self, writer: IndentedBuffer | None = ...) -> str: ...
    def has_tensor_output(self) -> bool: ...

@ir_dataclass(frozen=False)
class ComputedBuffer(OperationBuffer):
    data: Loops
    _force_realize: ClassVar[bool] = ...
    @staticmethod
    @contextlib.contextmanager
    def force_realize() -> Iterator[None]: ...
    def get_computed_buffer_name(self) -> str | None: ...
    def num_reads(self) -> int: ...
    def get_reads(self) -> OrderedSet[Dep]: ...
    def get_read_names(self) -> OrderedSet[str]: ...
    def get_read_writes(self) -> dependencies.ReadWrites: ...
    @cache_on_self_and_args("ComputedBuffer")
    def get_free_symbol_uses(self, unbacked_only: bool = ...) -> OrderedSet[sympy.Symbol]: ...
    def make_loader(self) -> Callable[[Sequence[Expr]], OpsValue]: ...
    def has_store_function(self) -> bool: ...
    def get_store_function(self) -> Callable[..., None]: ...
    def get_fill_order(self) -> list[int] | None: ...
    def decide_layout(self) -> None: ...
    @cache_on_self
    def get_default_sizes_body(
        self,
    ) -> tuple[tuple[list[Expr], list[Expr]], LoopBody, tuple[list[Expr], list[Expr]]]: ...
    def simplify_and_reorder(
        self,
        extra_indexing_constraints: tuple[dict[Any, Any], list[Any]] | None = ...,
        recompute_sizes_body_func: Callable[..., Any] | None = ...,
    ) -> tuple[tuple[list[Expr], list[Expr]], LoopBody | None]: ...
    def get_reduction_size(self) -> Sequence[Expr]: ...
    def get_reduction_type(self) -> str | None: ...
    def is_no_op(self) -> bool: ...
    def should_allocate(self) -> bool: ...
    def constant_to_device(self, device: torch.device) -> IRNode: ...

class TemplateBuffer(OperationBuffer):
    def __init__(
        self, layout: OutputSpec, inputs: Sequence[IRNode], make_kernel_render: Callable[..., Any] | None
    ) -> None: ...
    def get_read_writes(self) -> dependencies.ReadWrites: ...
    def extract_read_writes(self, normalize: bool = ...) -> dependencies.ReadWrites: ...
    def get_reduction_size(self) -> Sequence[Expr]: ...
    def get_reduction_type(self) -> str | None: ...
    def should_allocate(self) -> bool: ...
    def simplify_and_reorder(
        self,
        extra_indexing_constraints: tuple[dict[Any, Any], list[Any]] | None = ...,
        recompute_sizes_body_func: Callable[..., Any] | None = ...,
    ) -> tuple[tuple[Sequence[Expr], list[Expr]], LoopBody | None]: ...

class TritonTemplateBuffer(TemplateBuffer):
    def __init__(
        self,
        layout: Layout,
        inputs: Sequence[IRNode],
        make_kernel_render: Callable[_P, _T] | None,
        mutated_inputs: Iterable[IRNode] | None = ...,
        allowed_prologue_inps: OrderedSet[str] | None = ...,
    ) -> None: ...
    @cache_on_self_and_args("TritonTemplateBuffer")
    def get_free_symbol_uses(self, unbacked_only: bool = ...) -> OrderedSet[sympy.Symbol]: ...
    def get_outputs(self) -> list[Buffer]: ...
    def get_allowed_prologue_inps(self) -> OrderedSet[str]: ...

type PrimitiveInfoType = int | float | bool | str | list[int | str | float | bool]

class ChoiceCaller:
    def __init__(self, name: str, input_nodes: list[Buffer], layout: Layout, description: str) -> None: ...
    def benchmark(self, *args: Any, out: torch.Tensor) -> float: ...
    def call_name(self) -> str: ...
    def to_callable(self) -> Callable[..., Any]: ...
    def kernel_hash_key(self) -> str: ...
    def hash_key(self) -> str: ...
    def output_node(self) -> TensorBox | ShapeAsConstantBuffer: ...
    def info_dict(self) -> dict[str, PrimitiveInfoType | list[PrimitiveInfoType]]: ...
    def autoheuristic_id(self) -> str: ...

class TritonTemplateCallerBase(ChoiceCaller):
    def get_make_kernel_render(self) -> Any: ...

class MultiTemplateBuffer(TritonTemplateBuffer):
    def __init__(
        self,
        layout: Layout,
        inputs: Sequence[IRNode],
        choice_timings_fn: Callable[[int | None], dict[ChoiceCaller, float]],
        unfiltered_choices: list[ChoiceCaller],
        allowed_prologue_inps: OrderedSet[str],
    ) -> None: ...
    @property
    def output_plannable(self) -> bool: ...
    def choice_timings(self, hint_override: int | None = ...) -> dict[ChoiceCaller, float]: ...
    @contextlib.contextmanager
    def swap_as_triton_caller(self, caller: TritonTemplateCallerBase) -> Iterator[None]: ...
    def finalize_as_triton_caller(self, caller: TritonTemplateCallerBase) -> None: ...
    def get_min_choice(self, hint_override: int | None = ...) -> tuple[ChoiceCaller, float]: ...
    def finalize_as_triton_callers(self, callers: dict[int | None, TritonTemplateCallerBase]) -> None: ...

class CUDATemplateBuffer(TemplateBuffer):
    def __init__(
        self,
        layout: Layout,
        inputs: Sequence[IRNode],
        make_kernel_render: Callable[_P, _T],
        workspace_size: int,
        template: CUDATemplate,
        supports_epilogue_fusion: bool,
    ) -> None: ...
    def get_workspace_size(self) -> int: ...
    def emulate_store_fn(self) -> None: ...

class CppTemplateBuffer(TemplateBuffer):
    def __init__(
        self,
        layout: Layout,
        inputs: Sequence[IRNode],
        make_kernel_render: Callable[_P, _T],
        template: CUDATemplate,
        choice: Any,
    ) -> None: ...
    def get_layout(self) -> Layout: ...

class CuteDSLTemplateBuffer(TemplateBuffer):
    def __init__(
        self,
        layout: Layout,
        inputs: Sequence[IRNode],
        make_kernel_render: Callable[_P, _T],
        template: Any,
        mutated_inputs: Iterable[IRNode] | None = ...,
    ) -> None: ...
    def get_outputs(self) -> list[Buffer]: ...

def is_node_sequence(nodes: Sequence[IRNode | Sequence[IRNode]]) -> TypeIs[Sequence[IRNode]]: ...

@ir_dataclass(frozen=False)
class InputsKernel(OperationBuffer):
    inputs: Sequence[IRNode | Sequence[IRNode]]
    def input_name(self, i: int) -> str: ...
    def get_read_writes(self) -> dependencies.ReadWrites: ...
    def get_reads(self) -> OrderedSet[Dep]: ...
    @classmethod
    def unwrap_storage_for_input(cls, x: IRNode) -> IRNode: ...
    @staticmethod
    def unwrap_storage(inputs: Sequence[IRNode | Sequence[IRNode]]) -> list[IRNode | Sequence[IRNode]]: ...
    def is_extern(self) -> bool: ...
    def num_reads(self) -> int: ...
    @cache_on_self_and_args("InputsKernel")
    def get_free_symbol_uses(self, unbacked_only: bool = ...) -> OrderedSet[sympy.Symbol]: ...

class NopKernel(InputsKernel):
    def is_no_op(self) -> bool: ...
    def get_reads(self) -> OrderedSet[Dep]: ...

class ConcatKernel(NopKernel):
    @classmethod
    def create(cls, inputs: Sequence[IRNode], dim: int) -> StorageBox: ...
    @classmethod
    def can_realize_into_without_copy(cls, src: IRNode, dst: IRNode | None = ...) -> bool: ...
    @cache_on_self_and_args("ConcatKernel")
    def get_free_symbol_uses(self, unbacked_only: bool = ...) -> OrderedSet[sympy.Symbol]: ...
    @classmethod
    def realize_into(cls, src: IRNode, dst: IRNode) -> IRNode: ...
    def should_allocate(self) -> bool: ...

@ir_dataclass(frozen=False)
class ExternKernel(InputsKernel):
    constant_args: Sequence[Any] = ...
    kwargs: dict[str, Any] = ...
    output_view: ReinterpretView | None = ...
    python_kernel_name: str | None = ...
    cpp_kernel_name: str | None = ...
    ordered_kwargs_for_cpp_kernel: Iterable[str] = ...
    op_overload: _OpOverloads | None = ...
    arg_properties: list[dict[str, Any]] | None = ...
    allarg_properties: dict[str, dict[str, Any]] = ...
    kwarg_properties: dict[str, dict[str, Any]] | None = ...
    unbacked_bindings: dict[sympy.Symbol, pytree.KeyPath] = ...
    mutation_outputs: list[MutationOutput] = ...
    def __init__(
        self,
        name: str | None,
        layout: OutputSpec,
        inputs: Sequence[IRNode | Sequence[IRNode]],
        constant_args: Sequence[Any] = ...,
        kwargs: dict[str, Any] | None = ...,
        output_view: ReinterpretView | None = ...,
        python_kernel_name: str | None = ...,
        cpp_kernel_name: str | None = ...,
        ordered_kwargs_for_cpp_kernel: Iterable[str] = ...,
        op_overload: _OpOverloads | None = ...,
    ) -> None: ...
    def get_outputs(self) -> list[Buffer]: ...
    def get_unbacked_symbol_defs(self) -> OrderedSet[sympy.Symbol]: ...
    def collect_arg_kwarg_properties(self) -> None: ...
    def decide_layout(self) -> None: ...
    def codegen_comment(self, wrapper: PythonWrapperCodegen) -> None: ...
    def codegen(self, wrapper: PythonWrapperCodegen) -> None: ...
    def set_cpp_kernel_name(self, cpp_kernel_name: str | None = ...) -> None: ...
    def set_python_kernel_name(self, python_kernel_name: str | None) -> None: ...
    def get_kernel_name(self) -> str: ...
    @staticmethod
    def copy_input(x: IRNode) -> TensorBox | ShapeAsConstantBuffer: ...
    @classmethod
    def process_kernel(
        cls, kernel: _OpOverloads, *args: Any, **kwargs: Any
    ) -> tuple[Any, list[Any], list[Any], Callable[[Any, Any], Any], dict[sympy.Symbol, pytree.KeyPath] | None]: ...
    @classmethod
    def convert_to_reinterpret_view(cls, x: IRNode) -> ReinterpretView: ...
    @classmethod
    def realize_input(cls, x: IRNode) -> IRNode: ...
    @classmethod
    def require_stride1(cls, x: IRNode) -> IRNode: ...
    @classmethod
    def require_strides(
        cls,
        x: IRNode,
        order: Sequence[int] | None = ...,
        exact_strides: Sequence[_IntLike] | None = ...,
        allow_padding: bool = ...,
    ) -> IRNode: ...
    @classmethod
    def require_exact_strides(
        cls, x: IRNode, exact_strides: Sequence[_IntLike], allow_padding: bool = ...
    ) -> IRNode: ...
    @classmethod
    def require_stride_order(cls, x: IRNode, order: Sequence[int], allow_padding: bool = ...) -> IRNode: ...
    @classmethod
    def require_channels_last(cls, x: IRNode) -> IRNode: ...
    @classmethod
    def require_channels_last_3d(cls, x: IRNode) -> IRNode: ...
    @classmethod
    def require_contiguous(cls, x: IRNode) -> IRNode: ...
    @classmethod
    def require_contiguous_strides(cls, x: IRNode) -> IRNode: ...
    def apply_constraint(self) -> None: ...
    def fill_non_provided_args(self, args: Sequence[Any], kwargs: dict[str, Any]) -> Sequence[Any]: ...
    def codegen_const_args(self, names: list[str] | None = ...) -> list[str]: ...
    def codegen_args(self) -> list[str]: ...
    def get_kwargs_value(self, arg_name: str, **kwargs: Any) -> Any: ...
    def codegen_kwargs(self, skip_out: bool = ...) -> list[str]: ...
    def get_op_name(self) -> str: ...
    def codegen_size_asserts(self, wrapper: PythonWrapperCodegen) -> None: ...
    def codegen_alignment_asserts(self, wrapper: PythonWrapperCodegen) -> None: ...
    def codegen_memory_tracking(self, wrapper: PythonWrapperCodegen) -> None: ...
    def get_group_stride(self) -> tuple[list[Sequence[Expr]], list[Expr]]: ...
    def canonicalize(self) -> tuple[Expr, Sequence[Expr]]: ...
    @cache_on_self_and_args("ExternKernel")
    def get_free_symbol_uses(self, unbacked_only: bool = ...) -> OrderedSet[sympy.Symbol]: ...

    __repr__ = ...

@ir_dataclass(frozen=False)
class ExternKernelOut(ExternKernel):
    def codegen(self, wrapper: PythonWrapperCodegen) -> None: ...
    def __init__(
        self,
        layout: Layout,
        inputs: Sequence[IRNode],
        constant_args: Sequence[Any] = ...,
        kwargs: dict[str, Any] | None = ...,
        output_view: ReinterpretView | None = ...,
        python_kernel_name: str | None = ...,
        cpp_kernel_name: str | None = ...,
        ordered_kwargs_for_cpp_kernel: Sequence[Any] = ...,
        op_overload: _OpOverloads | None = ...,
    ) -> None: ...
    def should_allocate(self) -> bool: ...

class RandomSeeds(ExternKernelOut):
    def __init__(self, count: int, device: torch.device) -> None: ...

class ExternKernelAlloc(ExternKernel):
    def codegen(self, wrapper: PythonWrapperCodegen) -> None: ...
    def __init__(
        self,
        layout: OutputSpec,
        inputs: Sequence[IRNode],
        constant_args: Sequence[Any] = ...,
        kwargs: dict[str, Any] | None = ...,
        python_kernel_name: str | None = ...,
        cpp_kernel_name: str | None = ...,
        ordered_kwargs_for_cpp_kernel: Sequence[Any] = ...,
        op_overload: _OpOverloads | None = ...,
    ) -> None: ...
    def should_allocate(self) -> bool: ...
    def apply_constraint(self) -> None: ...

class MutationOutput(Buffer):
    def __init__(self, layout: OutputSpec, mutated_node: IRNode, mutating_node: Operation) -> None: ...
    def get_defining_op(self) -> Operation: ...
    def get_mutation_names(self) -> Sequence[str]: ...
    def should_allocate(self) -> bool: ...
    def get_mutation_buffers(self) -> Sequence[IRNode]: ...

class TMADescriptor(ExternKernel):
    _CACHE: dict[Any, TMADescriptor] = ...
    @classmethod
    def create(cls, tensor: IRNode, tma_meta: tuple[str, tuple[Any, ...]]) -> TMADescriptor: ...
    def __init__(self, tensor: IRNode, inputs: Sequence[Any], constant_args: Sequence[Any]) -> None: ...
    def codegen(self, wrapper: PythonWrapperCodegen) -> None: ...
    def get_tensor(self) -> IRNode: ...

class TMADescriptorExperimental(TMADescriptor):
    def __init__(
        self,
        tensor: IRNode,
        dims: list[int | torch.SymInt],
        block_dims: list[int | torch.SymInt],
        element_size: int | None = ...,
    ) -> None: ...

class TMADescriptorStable(TMADescriptor):
    def __init__(self, tensor: IRNode, block_shape: list[int | torch.SymInt]) -> None: ...

class SubgraphBuffer(ExternKernel):
    def __init__(
        self,
        layout: Layout,
        input_nodes: list[Buffer],
        gm: torch.fx.GraphModule,
        example_inputs: list[Any],
        subgraph_name: str,
    ) -> None: ...
    def codegen(self, wrapper: PythonWrapperCodegen) -> None: ...

class UserDefinedTritonKernel(ExternKernel):
    def get_kernel_and_metadata(self) -> tuple[Kernel, Any, list[str], list[str]]: ...
    @override
    def codegen(self, wrapper: PythonWrapperCodegen) -> None: ...
    @cache_on_self_and_args("UserDefinedTritonKernel")
    def get_free_symbol_uses(self, unbacked_only: bool = ...) -> OrderedSet[sympy.Symbol]: ...
    def get_unbacked_symbol_defs(self) -> OrderedSet[sympy.Symbol]: ...
    def __init__(
        self, *, kernel_idx: int, grid: Any, tma_descriptor_metadata: dict[str, Any], kernel_args: dict[str, Any]
    ) -> None: ...
    def get_outputs(self) -> list[Buffer]: ...
    def get_device(self) -> torch.device | None: ...

class InplaceBernoulliFallback(ExternKernel):
    def codegen(self, wrapper: PythonWrapperCodegen) -> None: ...
    def should_allocate(self) -> bool: ...
    def get_mutation_names(self) -> Sequence[str]: ...
    def get_unbacked_symbol_defs(self) -> OrderedSet[sympy.Symbol]: ...
    def __init__(self, op_overload: _OpOverloads, x: IRNode, *constant_args: Any) -> None: ...

class InplaceCopyFallback(ExternKernel):
    def codegen(self, wrapper: PythonWrapperCodegen) -> None: ...
    def should_allocate(self) -> bool: ...
    def get_mutation_names(self) -> Sequence[str]: ...
    def get_unbacked_symbol_defs(self) -> OrderedSet[sympy.Symbol]: ...
    def __init__(self, layout: OutputSpec, inputs: Sequence[IRNode], constant_args: Sequence[Any]) -> None: ...
    @classmethod
    def create(cls, dst: IRNode, src: IRNode, non_blocking: bool = ...) -> InplaceCopyFallback: ...

class MutatingFirstArgExternKernel(ExternKernel):
    def codegen(self, wrapper: PythonWrapperCodegen) -> None: ...
    def should_allocate(self) -> bool: ...
    def get_mutation_names(self) -> Sequence[str]: ...
    def get_unbacked_symbol_defs(self) -> OrderedSet[sympy.Symbol]: ...
    def has_side_effects(self) -> bool: ...

class ResizeStorageBytes(MutatingFirstArgExternKernel):
    def __init__(self, variable: IRNode, new_size: int) -> None: ...

class SetSourceTensorKernel(ExternKernelAlloc):
    def __init__(self, self_tensor: IRNode, storage_tensor: IRNode) -> None: ...
    def get_inputs_that_alias_output(self) -> Sequence[str]: ...

class ScatterFallback(ExternKernel):
    def codegen(self, wrapper: PythonWrapperCodegen) -> None: ...
    def should_allocate(self) -> bool: ...
    def get_mutation_names(self) -> list[str]: ...
    def get_unbacked_symbol_defs(self) -> OrderedSet[sympy.Symbol]: ...
    def __init__(
        self,
        op_overload: _OpOverloads,
        x: IRNode,
        dim: int,
        index: IRNode,
        src: IRNode,
        *,
        reduce: str | None = ...,
        include_self: bool = ...,
    ) -> None: ...

class IndexPutFallback(ExternKernel):
    def codegen(self, wrapper: PythonWrapperCodegen) -> None: ...
    def should_allocate(self) -> bool: ...
    def get_mutation_names(self) -> Sequence[str]: ...
    def get_unbacked_symbol_defs(self) -> OrderedSet[sympy.Symbol]: ...
    def __init__(
        self, op_overload: torch._ops.OpOverload, x: IRNode, indices: list[Any], values: Sequence[Any], accumulate: Any
    ) -> None: ...

class DeviceCopy(ExternKernelOut):
    @classmethod
    def create(cls, x: IRNode, device: torch.device, non_blocking: bool) -> IRNode: ...
    def codegen(self, wrapper: PythonWrapperCodegen) -> None: ...

class DynamicSelectStorageOffset(ExternKernel):
    def get_reads(self) -> OrderedSet[Dep]: ...
    def should_allocate(self) -> bool: ...
    def __init__(
        self,
        unbacked_offset_symbol: sympy.Symbol,
        index: sympy.Symbol,
        base_offset: sympy.Symbol | int,
        base_dim_stride: sympy.Symbol | int,
        size: sympy.Symbol | int,
    ) -> None: ...
    def get_unbacked_symbol_defs(self) -> OrderedSet[sympy.Symbol]: ...
    @cache_on_self_and_args("DynamicSelectStorageOffset")
    def get_free_symbol_uses(self, unbacked_only: bool = ...) -> OrderedSet[sympy.Symbol]: ...
    def codegen(self, wrapper: PythonWrapperCodegen) -> None: ...

class DynamicScalar(ExternKernel):
    def get_reads(self) -> OrderedSet[Dep]: ...
    def should_allocate(self) -> bool: ...
    def __init__(self, sym: sympy.Symbol, keypath: pytree.KeyPath, data: IRNode) -> None: ...
    def get_unbacked_symbol_defs(self) -> OrderedSet[sympy.Symbol]: ...
    def codegen(self, wrapper: PythonWrapperCodegen) -> None: ...

class AssertScalar(ExternKernel):
    def get_reads(self) -> OrderedSet[Dep]: ...
    def should_allocate(self) -> bool: ...
    def __init__(self, scalar: SympyBoolean, msg: str) -> None: ...
    def has_side_effects(self) -> bool: ...
    @cache_on_self_and_args("AssertScalar")
    def get_free_symbol_uses(self, unbacked_only: bool = ...) -> OrderedSet[sympy.Symbol]: ...
    def codegen(self, wrapper: PythonWrapperCodegen) -> None: ...

@ir_dataclass(frozen=False)
class ExternKernelNode:
    name: str
    node: export_schema.Node

class FallbackKernel(ExternKernelAlloc):
    def __init__(
        self,
        layout: OutputSpec,
        kernel: _OpOverloads,
        tensor_args: Sequence[IRNode],
        nontensor_args: Sequence[Any],
        unflatten_args: Callable[..., Any],
        kwargs: dict[str, Any] | None = ...,
        *,
        unbacked_bindings: dict[sympy.Symbol, pytree.KeyPath] | None = ...,
    ) -> None: ...
    def get_read_writes(self) -> dependencies.ReadWrites: ...
    def codegen_unbacked_symbol_defs(self, wrapper: PythonWrapperCodegen) -> None: ...
    def get_unbacked_symbol_defs(self) -> Container[sympy.Symbol]: ...
    def codegen_args(self) -> list[str]: ...
    @staticmethod
    def find_device(tensor_args: Sequence[torch.Tensor] | None, example_output: Sequence[Any]) -> Any: ...
    def has_side_effects(self) -> bool: ...
    def get_inputs_that_alias_output(self) -> Sequence[str]: ...
    def get_mutation_names(self) -> Sequence[str]: ...
    def export_extern_kernel_node(self): ...
    @override
    def codegen(self, wrapper: PythonWrapperCodegen) -> None: ...
    @staticmethod
    def tensor_to_layout(output: torch.Tensor) -> FixedLayout: ...
    @classmethod
    def create(cls, kernel: _OpOverloads, *args: Any, **kwargs: Any) -> FallbackKernel: ...
    def apply_constraint(self) -> None: ...

@ir_dataclass(frozen=False)
class ComplexView(FallbackKernel):
    def should_allocate(self) -> bool: ...
    def get_inputs_that_alias_output(self) -> Sequence[str]: ...
    def __init__(
        self,
        layout: OutputSpec,
        kernel: _OpOverloads,
        tensor_args: Sequence[IRNode],
        nontensor_args: Sequence[Any],
        unflatten_args: Callable[..., Any],
        *,
        unbacked_bindings: dict[sympy.Symbol, pytree.KeyPath] | None = ...,
    ) -> None: ...

class MemoryCheckKernel(FallbackKernel):
    def codegen(self, wrapper: PythonWrapperCodegen) -> None: ...

@ir_dataclass
class MultiOutputLayout(OutputSpec):
    device: torch.device
    def get_device(self) -> torch.device | None: ...

class MultiOutput(ExternKernel):
    def codegen(self, wrapper: PythonWrapperCodegen) -> None: ...
    def __init__(
        self,
        layout: OutputSpec,
        input: IRNode,
        indices: list[tuple[Any, ...]],
        skip_size_stride_alignment_checks: bool = ...,
    ) -> None: ...
    @cache_on_self_and_args("MultiOutput")
    def get_free_symbol_uses(self, unbacked_only: bool = ...) -> OrderedSet[sympy.Symbol]: ...
    def should_allocate(self) -> bool: ...
    def get_inputs_that_alias_output(self) -> Sequence[str]: ...

@dataclasses.dataclass
class MutableBox(IRNode):
    data: IRNode
    def has_exceeded_max_reads(self) -> bool: ...
    def get_device(self) -> torch.device | None: ...
    def make_loader(self) -> Callable[[Sequence[Expr]], OpsValue]: ...
    def make_indexer(self) -> Callable[[Sequence[Expr]], Expr]: ...
    def get_stride(self) -> Sequence[_IntLike]: ...
    def get_name(self) -> str: ...
    def has_large_inner_fn(self, threshold: int | None = ...) -> bool: ...
    def mark_reuse(self, users: int) -> None: ...
    def realize_hint(self) -> None: ...
    def unwrap_view(self) -> IRNode: ...
    def is_input_buffer(self) -> bool: ...
    def freeze_layout(self) -> None: ...
    def freeze_layout_with_stride_order(self, order: Sequence[int], allow_padding: bool = ...) -> None: ...
    def freeze_layout_with_fill_order(self, order: Sequence[int]) -> None: ...
    def freeze_layout_with_same_order(self, stride: Sequence[_IntLike]) -> None: ...
    def freeze_layout_with_exact_strides(
        self, exact_strides: Sequence[_IntLike], allow_padding: bool = ...
    ) -> None: ...
    def get_read_writes(self) -> dependencies.ReadWrites: ...
    def get_reads(self) -> OrderedSet[Dep]: ...
    def num_reads(self) -> int: ...
    def get_storage_numel(self) -> _IntLike: ...
    def get_reduction_type(self) -> str | None: ...
    def get_reduction_size(self) -> Sequence[Expr]: ...
    def is_extern(self) -> bool: ...
    def is_no_op(self) -> bool: ...
    def constant_to_device(self, device: torch.device) -> IRNode: ...
    def get_mutation_names(self) -> Sequence[str]: ...
    def get_operation_name(self) -> str: ...
    def get_inputs_that_alias_output(self) -> Sequence[str]: ...
    def realize(self) -> str | None: ...
    @cache_on_self_and_args("MutableBox")
    def get_free_symbol_uses(self, unbacked_only: bool = ...) -> OrderedSet[sympy.Symbol]: ...
    def get_read_names(self) -> OrderedSet[str]: ...
    def get_defining_op(self) -> Operation | None: ...
    def codegen_reference(self, writer: IndentedBuffer | None = ...) -> str: ...
    @property
    def layout(self) -> OutputSpec: ...
    def get_layout(self) -> Layout: ...
    def get_output_spec(self) -> OutputSpec: ...
    def get_size(self) -> Sequence[Expr]: ...
    @property
    def dtype(self) -> torch.dtype: ...

    __repr__ = ...

class TensorBox(MutableBox):
    @staticmethod
    def create(data: IRNode) -> TensorBox | ShapeAsConstantBuffer: ...

class StorageBox(MutableBox):
    def is_input_buffer(self) -> bool: ...
    def is_module_buffer(self) -> bool: ...
    def realize(self) -> str | None: ...
    def realize_hint(self) -> None: ...
    def has_accumulated_enough_reads_by_size(self, threshold: int) -> bool: ...
    def has_exceeded_max_reads(self) -> bool: ...
    def should_realize_on_reuse(self, users: int) -> bool: ...
    def mark_reuse(self, users: int) -> None: ...
    def num_reads(self) -> int: ...

@ir_dataclass(frozen=False)
class Subgraph(IRNode):
    name: str
    graph_module: torch.fx.GraphModule
    graph: GraphLowering | None = ...

@ir_dataclass(frozen=False)
class InvokeSubgraph(ExternKernel):
    subgraph: Subgraph | None = ...
    operands: Sequence[IRNode] | None = ...
    outputs: Sequence[IRNode] | None = ...
    def __init__(self, subgraph: Subgraph, operands: Sequence[IRNode], layout: MultiOutputLayout) -> None: ...
    @classmethod
    def create(
        cls, subgraph: Subgraph, *operands: IRNode
    ) -> list[ShapeAsConstantBuffer | NoneAsConstantBuffer | MultiOutput]: ...
    def codegen(self, wrapper: PythonWrapperCodegen) -> None: ...

@ir_dataclass(frozen=False)
class Conditional(ExternKernel):
    predicate: IRNode | None = ...
    operands: Sequence[IRNode] | None = ...
    true_subgraph: Subgraph | None = ...
    false_subgraph: Subgraph | None = ...
    outputs: Sequence[MultiOutput] | None = ...
    def __init__(
        self,
        predicate: IRNode,
        operands: Sequence[IRNode],
        true_subgraph: Subgraph,
        false_subgraph: Subgraph,
        layout: MultiOutputLayout,
        unbacked_bindings: dict[sympy.Symbol, pytree.KeyPath] | None,
    ) -> None: ...
    @classmethod
    def create(
        cls,
        predicate: TensorBox,
        true_fn: Subgraph,
        false_fn: Subgraph,
        operands: list[TensorBox | ShapeAsConstantBuffer],
    ) -> Sequence[IRNode]: ...
    def codegen(self, wrapper: PythonWrapperCodegen) -> None: ...
    def get_unbacked_symbol_defs(self) -> OrderedSet[sympy.Symbol]: ...

@ir_dataclass(frozen=False)
class WhileLoop(ExternKernel):
    carried_inputs: Sequence[IRNode] | None = ...
    additional_inputs: Sequence[IRNode] | None = ...
    cond_subgraph: Subgraph | None = ...
    body_subgraph: Subgraph | None = ...
    outputs: Sequence[MultiOutput] | None = ...
    def __init__(
        self,
        carried_inputs: Sequence[IRNode],
        additional_inputs: Sequence[IRNode],
        cond_subgraph: Subgraph,
        body_subgraph: Subgraph,
        layout: MultiOutputLayout,
        unbacked_bindings: dict[sympy.Symbol, pytree.KeyPath] | None,
        stack_output: bool,
    ) -> None: ...
    @classmethod
    def create(
        cls,
        cond_fn: Subgraph,
        body_fn: Subgraph,
        carried_inputs: Sequence[IRNode],
        additional_inputs: Sequence[IRNode],
        stack_output: bool,
    ) -> IRNode | Sequence[IRNode]: ...
    def codegen(self, wrapper: PythonWrapperCodegen) -> None: ...
    def get_unbacked_symbol_defs(self) -> OrderedSet[sympy.Symbol]: ...

class EffectfulKernel(FallbackKernel):
    def __init__(
        self,
        layout: OutputSpec,
        kernel: _OpOverloads,
        tensor_args: Sequence[IRNode],
        nontensor_args: Sequence[Any],
        unflatten_args: Callable[..., Any],
        kwargs: dict[str, Any] | None = ...,
        *,
        unbacked_bindings: dict[sympy.Symbol, pytree.KeyPath] | None = ...,
    ) -> None: ...
    def get_read_writes(self) -> dependencies.ReadWrites: ...
    def has_side_effects(self) -> bool: ...

class NonTensorObj(IRNode):
    @cache_on_self_and_args("NonTensorObj")
    def get_free_symbol_uses(self, unbacked_only: bool = ...) -> OrderedSet[sympy.Symbol]: ...

@ir_dataclass
class TorchBindObject(NonTensorObj):
    name: str
    value: FakeScriptObject | torch.ScriptObject
    def get_name(self) -> str: ...
    def codegen_reference(self, writer: IndentedBuffer | None = ...) -> str: ...
    def get_value(self) -> FakeScriptObject | torch.ScriptObject: ...
    def get_real_obj(self) -> torch.ScriptObject: ...
    def get_buf_bytes(self) -> int: ...

@ir_dataclass
class GeneratorState(NonTensorObj):
    name: str
    device: torch.device
    def get_name(self) -> str: ...
    def codegen_reference(self, writer: IndentedBuffer | None = ...) -> str: ...

class _CollectiveKernel(FallbackKernel):
    def should_allocate(self) -> bool: ...
    def has_side_effects(self) -> bool: ...
    def set_cpp_kernel_name(self, cpp_kernel_name: str | None = ...) -> None: ...
    @classmethod
    def create_inplace(cls, kernel: _OpOverloads, inputs: IRNode | list[IRNode], *args: Any, **kwargs: Any) -> None: ...
    @classmethod
    def create_out_of_place(
        cls, kernel: _OpOverloads, inputs: TensorBox | list[TensorBox], *args: Any, **kwargs: Any
    ) -> list[MultiOutput] | _CollectiveKernel: ...

class _AllReduce_Kernel(_CollectiveKernel):
    def __init__(
        self,
        layout: OutputSpec,
        kernel: _OpOverloads,
        tensor_args: Sequence[IRNode],
        nontensor_args: Sequence[Any],
        unflatten_args: Callable[..., Any],
        kwargs: dict[str, Any] | None = ...,
        *,
        unbacked_bindings: dict[sympy.Symbol, pytree.KeyPath] | None = ...,
    ) -> None: ...
    def codegen(self, wrapper: PythonWrapperCodegen) -> None: ...

class _AllReduceKernel(_CollectiveKernel):
    def __init__(
        self,
        layout: OutputSpec,
        kernel: _OpOverloads,
        tensor_args: Sequence[IRNode],
        nontensor_args: Sequence[Any],
        unflatten_args: Callable[..., Any],
        kwargs: dict[str, Any] | None = ...,
        *,
        unbacked_bindings: dict[sympy.Symbol, pytree.KeyPath] | None = ...,
    ) -> None: ...
    def codegen(self, wrapper: PythonWrapperCodegen) -> None: ...

class _WaitKernel(_CollectiveKernel):
    def __init__(
        self,
        layout: OutputSpec,
        kernel: _OpOverloads,
        tensor_args: Sequence[IRNode],
        nontensor_args: Sequence[Any],
        unflatten_args: Callable[..., Any],
        kwargs: dict[str, Any] | None = ...,
        *,
        unbacked_bindings: dict[sympy.Symbol, pytree.KeyPath] | None = ...,
    ) -> None: ...
    def codegen(self, wrapper: PythonWrapperCodegen) -> None: ...
    def get_volatile_reads(self) -> Sequence[IRNode]: ...
    @classmethod
    def create_wait(cls, kernel: _OpOverloads, inp: TensorBox) -> None: ...
    def get_read_writes(self) -> dependencies.ReadWrites: ...

def maybe_free_unbacked_symbols(s: object) -> OrderedSet[Symbol]: ...
def maybe_free_symbols(s: object) -> OrderedSet[Symbol]: ...
