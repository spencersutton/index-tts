import torch
from collections.abc import Sequence
from typing import Any, Optional, Union, TypeAlias
from ...ir import ComputedBuffer, IRNode, ShapeAsConstantBuffer, Subgraph, TensorBox

"""Common utilities and functions for flex attention kernels"""
SubgraphResults: TypeAlias = Union[list[Optional[ComputedBuffer]], Optional[ComputedBuffer]]

def zeros_and_scatter_lowering(shape: list[int], indices, values):  # -> Any:

    ...
def get_fwd_subgraph_outputs(
    subgraph_buffer: SubgraphResults, mask_graph_buffer: SubgraphResults
) -> list[Optional[ComputedBuffer]]: ...
def build_subgraph_module_buffer(
    args: list[Union[TensorBox, ShapeAsConstantBuffer]], graph_module: torch.fx.GraphModule
) -> SubgraphResults: ...
def build_subgraph_buffer(
    args: list[Union[TensorBox, ShapeAsConstantBuffer]], subgraph: Subgraph
) -> SubgraphResults: ...
def maybe_realize(args: list[Optional[IRNode]]):  # -> PyTree:

    ...
def create_placeholder(
    name: str, dtype: torch.dtype, device: torch.device, size: Optional[list[int]] = ...
) -> Union[TensorBox, ShapeAsConstantBuffer]: ...
def construct_strides(sizes: Sequence[int], fill_order: Sequence[int]) -> Sequence[int]: ...
def infer_dense_strides(size: Sequence[int], orig_strides: Sequence[int]):  # -> Sequence[int]:

    ...
def create_indices_fake(x) -> torch.Tensor: ...
def create_num_blocks_fake_generator(sparse_indices):  # -> Callable[..., Tensor]:

    ...
def contiguous_last_dim(x):  # -> Any:

    ...
def set_head_dim_values(kernel_options: dict[str, Any], qk_head_dim, v_head_dim, graph_sizevars):  # -> None:

    ...
def is_power_of_2(n): ...
def next_power_of_two(n):  # -> Any | Literal[1]:
    ...

_TEMPLATE_DIR = ...

def load_template(name: str) -> str: ...
