import collections
from collections.abc import Callable
from enum import Enum
from typing import Any, NamedTuple, TypeVar

import sympy
import torch.fx
from torch.fx.proxy import TracerBase

from .ops_handler import DefaultHandler, OpsHandler, WrapperHandler
from .utils import cache_on_self

T = TypeVar("T")

class InterpreterShim(torch.fx.Interpreter):
    def __init__(self, graph, submodules) -> None: ...
    def run_node(self, n: torch.fx.Node) -> Any: ...
    def run(self, *args, **kwargs): ...

class LightTracer(TracerBase):
    def __init__(self) -> None: ...

class MemoryEntry(NamedTuple):
    """MemoryEntry(index_name, buffer_name, mode)"""

    index_name: str
    buffer_name: str | None
    mode: str | None

class MemoryUsageType(Enum):
    LOAD = ...
    LOAD_SEED = ...
    STORE = ...
    STORE_REDUCTION = ...
    INDEX_EXPR = ...
    CHECK_BOUNDS = ...
    BUCKETIZE = ...

class LoopBody:
    """
    Captures the body of a Loops subclass into an FX graph.  Persists any
    indexing simplifications and makes it easier to analyze loop bodies.
    """

    indexing_exprs: dict[str, sympy.Expr]
    indexing_exprs_name: dict[sympy.Expr, str]
    submodules: dict[str, Any]
    subblocks: dict[str, LoopBodyBlock]
    indirect_vars: list[sympy.Symbol]
    indirect_var_ranges: dict[sympy.Symbol, sympy.Expr]
    root_block: LoopBodyBlock
    memory_usage: dict[MemoryUsageType, list[MemoryEntry]]
    op_counts: collections.Counter[str]
    def __init__(self, fn, args, var_ranges, iter_vars, reduce_vars) -> None: ...
    def has_op(self, name: str): ...
    def merge_loops(self) -> LoopBody:
        """Merge both iteration and reduction loops and return a new LoopBody."""
    def expand_dimension_for_pointwise_node(self, dimension: int, new_range: int) -> LoopBody:
        """
        Expand node on `dimension` to `new_range` and rely on index modular to avoid
        out-of-boundary access.
        """
    def reorder_iter_loops(self, new_order) -> LoopBody:
        """Reorder iteration loops and return a new LoopBody."""
    @property
    def vars(self): ...
    @cache_on_self
    def get_nodes(self): ...
    @cache_on_self
    def bounds(self): ...
    def get_read_expr(self, buffer_name): ...
    def get_write_expr(self, buffer_name): ...
    def get_read_exprs(self): ...
    def get_all_read_expr(self, buffer_name): ...
    def get_write_exprs(self): ...
    def get_all_write_expr(self, buffer_name): ...
    def debug_str(self): ...
    def is_memory_copy(self) -> bool:
        """
        True of this contains only a single loads and store.
        Note, this could involve a layout change.
        """

    __repr__ = ...
    def add_index_expr(
        self, expr: sympy.Expr, mtype: MemoryUsageType, buffer_name: str | None = ..., mode: str | None = ...
    ): ...
    def add_submodule(self, block, prefix):
        """Not actually for nn.Modules, but subblocks in generated code are mapped to FX call_module opcodes"""
    def add_indirect(self, size): ...
    def replace_indirect(self, old, new):
        """Swap in a variable used in indirect indexing"""
    def get_index(self, name): ...
    def indexing_from_args(self, indices): ...
    def __call__(self, *indices): ...
    def bind_set_indirect_shim(self, var, size, check, wrap_neg): ...
    def bind_scan_shim(self, combine_fn): ...
    def bind_masked_shim(self, name): ...

class LoopBodyBlock:
    """
    Captures the body of a Loops subclass into an FX graph.
    In normal cases there will be a 1:1 mapping between LoopBody and
    LoopBodyBlock, however in the case of ops.masked() the masked out
    operations will manifest as an extra LoopBodyBlock.
    """
    def __init__(self, body: LoopBody, fn: Callable[..., Any], args: list[Any]) -> None: ...
    def __call__(self): ...
    def debug_str(self, name=...): ...
    def contains_only_ops(self, allowed_ops) -> bool: ...
    def clone(self, body: LoopBody):
        """Shallow copy with a new parent LoopBody"""

class CountOps(DefaultHandler):
    def __init__(self, inner: OpsHandler[Any], counts: collections.Counter[str]) -> None: ...

class CaptureIndexing(WrapperHandler):
    name = ...
    def __init__(self, inner: OpsHandler[Any], body: LoopBody, tracer: LightTracer) -> None: ...
    def load(self, name: str, index: sympy.Expr): ...
    def load_seed(self, name: str, index: int): ...
    def store(self, name, index, value, mode=...): ...
    def store_reduction(self, name, index, value): ...
    def reduction(self, dtype, src_dtype, reduction_type, value): ...
    def index_expr(self, index, dtype): ...
    def check_bounds(self, index, size, lower, upper): ...
    def bucketize(
        self,
        values: T,
        boundaries: tuple[str, sympy.Expr, sympy.Expr, sympy.Expr],
        boundary_indices: T,
        indexing_dtype: torch.dtype,
        right: bool,
        sorter: tuple[str, sympy.Expr] | None = ...,
        sorter_indices: T | None = ...,
    ) -> T:
        """See [Note: Inductor bucketize op]"""
    def masked(self, mask_proxy, masked_body: Callable[..., Any], other_proxy):
        """Recursively capture the masked out body in another LoopBodyBlock"""
    def scan(
        self, dtype_proxy, combine_fn: Callable[[tuple[Any, ...], tuple[Any, ...]], tuple[Any, ...]], value_proxy
    ): ...
    def sort(self, dtypes, values, stable, descending): ...
    def frexp(self, value_proxy): ...
    def indirect_indexing(self, index_proxy, size, check=..., wrap_neg=...):
        """
        Flow data from tensors into indexing formulas.
        Introduce a call_module to update the indexing.
        """
    def output(self, *result): ...
