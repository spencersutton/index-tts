import collections
import sympy
import torch.fx
from enum import Enum
from typing import Any, NamedTuple, Optional, TYPE_CHECKING, TypeVar
from collections.abc import Callable
from torch.fx.proxy import TracerBase
from .ops_handler import DefaultHandler, OpsHandler, WrapperHandler
from .utils import cache_on_self

if TYPE_CHECKING: ...
T = TypeVar("T")

class InterpreterShim(torch.fx.Interpreter):
    def __init__(self, graph, submodules) -> None: ...
    def run_node(self, n: torch.fx.Node) -> Any: ...
    def run(self, *args, **kwargs):  # -> Any:
        ...

class LightTracer(TracerBase):
    def __init__(self) -> None: ...

class MemoryEntry(NamedTuple):
    index_name: str
    buffer_name: str | None
    mode: str | None

class MemoryUsageType(Enum):
    LOAD = ...
    LOAD_SEED = ...
    STORE = ...
    STORE_REDUCTION = ...
    INDEX_EXPR = ...
    CHECK_BOUNDS = ...
    BUCKETIZE = ...

class LoopBody:
    indexing_exprs: dict[str, sympy.Expr]
    indexing_exprs_name: dict[sympy.Expr, str]
    submodules: dict[str, Any]
    subblocks: dict[str, LoopBodyBlock]
    indirect_vars: list[sympy.Symbol]
    indirect_var_ranges: dict[sympy.Symbol, sympy.Expr]
    root_block: LoopBodyBlock
    memory_usage: dict[MemoryUsageType, list[MemoryEntry]]
    op_counts: collections.Counter[str]
    def __init__(self, fn, args, var_ranges, iter_vars, reduce_vars) -> None: ...
    def has_op(self, name: str):  # -> bool:
        ...
    def merge_loops(self) -> LoopBody: ...
    def expand_dimension_for_pointwise_node(self, dimension: int, new_range: int) -> LoopBody: ...
    def reorder_iter_loops(self, new_order) -> LoopBody: ...
    @property
    def vars(self):  # -> tuple[Any, Any]:
        ...
    @cache_on_self
    def get_nodes(self):  # -> list[Any]:
        ...
    @cache_on_self
    def bounds(self):  # -> BoundVars:
        ...
    def get_read_expr(self, buffer_name): ...
    def get_write_expr(self, buffer_name): ...
    def get_read_exprs(self):  # -> list[Any]:
        ...
    def get_all_read_expr(self, buffer_name):  # -> list[Any]:
        ...
    def get_write_exprs(self):  # -> list[Any]:
        ...
    def get_all_write_expr(self, buffer_name):  # -> list[Any]:
        ...
    def debug_str(self):  # -> str:
        ...
    def is_memory_copy(self) -> bool: ...

    __repr__ = ...
    def add_index_expr(
        self, expr: sympy.Expr, mtype: MemoryUsageType, buffer_name: str | None = ..., mode: str | None = ...
    ):  # -> str:
        ...
    def add_submodule(self, block, prefix):  # -> str:

        ...
    def add_indirect(self, size): ...
    def replace_indirect(self, old, new):  # -> None:

        ...
    def get_index(self, name): ...
    def indexing_from_args(self, indices):  # -> dict[str, Any]:
        ...
    def __call__(self, *indices):  # -> Any:
        ...
    def bind_set_indirect_shim(self, var, size, check, wrap_neg):  # -> Callable[..., None]:
        ...
    def bind_scan_shim(self, combine_fn):  # -> Callable[..., tuple[Any, ...]]:
        ...
    def bind_masked_shim(self, name):  # -> Callable[..., Any]:
        ...

class LoopBodyBlock:
    def __init__(self, body: LoopBody, fn: Callable[..., Any], args: list[Any]) -> None: ...
    def __call__(self):  # -> Any:
        ...
    def debug_str(self, name=...):  # -> str:
        ...
    def contains_only_ops(self, allowed_ops) -> bool: ...
    def clone(self, body: LoopBody):  # -> LoopBodyBlock:

        ...

class CountOps(DefaultHandler):
    def __init__(self, inner: OpsHandler[Any], counts: collections.Counter[str]) -> None: ...

class CaptureIndexing(WrapperHandler):
    name = ...
    def __init__(self, inner: OpsHandler[Any], body: LoopBody, tracer: LightTracer) -> None: ...
    def load(self, name: str, index: sympy.Expr):  # -> Any:
        ...
    def load_seed(self, name: str, index: int):  # -> Any:
        ...
    def store(self, name, index, value, mode=...):  # -> None:
        ...
    def store_reduction(self, name, index, value):  # -> None:
        ...
    def reduction(self, dtype, src_dtype, reduction_type, value):  # -> tuple[Any, ...] | Any:
        ...
    def index_expr(self, index, dtype):  # -> Any:
        ...
    def check_bounds(self, index, size, lower, upper):  # -> None:
        ...
    def bucketize(
        self,
        values: T,
        boundaries: tuple[str, sympy.Expr, sympy.Expr, sympy.Expr],
        boundary_indices: T,
        indexing_dtype: torch.dtype,
        right: bool,
        sorter: tuple[str, sympy.Expr] | None = ...,
        sorter_indices: T | None = ...,
    ) -> T: ...
    def masked(self, mask_proxy, masked_body: Callable[..., Any], other_proxy):  # -> Proxy:

        ...
    def scan(
        self, dtype_proxy, combine_fn: Callable[[tuple[Any, ...], tuple[Any, ...]], tuple[Any, ...]], value_proxy
    ):  # -> tuple[Any, ...]:
        ...
    def sort(self, dtypes, values, stable, descending):  # -> tuple[Any, ...]:
        ...
    def frexp(self, value_proxy):  # -> tuple[Any, Any]:
        ...
    def indirect_indexing(self, index_proxy, size, check=..., wrap_neg=...): ...
    def output(self, *result):  # -> None:
        ...
