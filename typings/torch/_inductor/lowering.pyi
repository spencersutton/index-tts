import contextlib
from collections.abc import Callable, Iterable, Sequence
from typing import Any, ParamSpec, TypeVar

import sympy
import torch
import torch.fx
from torch._higher_order_ops.associative_scan import associative_scan_op
from torch._higher_order_ops.triton_kernel_wrap import triton_kernel_wrapper_mutation
from torch._prims_common import ELEMENTWISE_TYPE_PROMOTION_KIND

from . import inductor_prims, ir
from .ir import ShapeAsConstantBuffer, TensorBox
from .ops_handler import ReductionType

_T = TypeVar("_T")
_P = ParamSpec("_P")
FALLBACK_ALLOW_LIST = ...
log = ...
lowerings: dict[Callable[..., Any] | str, Callable[..., Any]] = ...
_maybe_layout_constraints: dict[torch._ops.OpOverload, Callable[..., Any] | None] = ...
fallbacks = ...
aten = ...
tr_c10d = ...
prims = ...
needs_realized_inputs = ...
foreach_ops = ...
inplace_foreach_ops = ...
inplaceable_foreach_ops: dict[torch._ops.OpOverload, torch._ops.OpOverload] = ...
quantized_decomposed = ...

def cur_node_has_non_foreach_users(): ...
def group_foreach_args(arg_pairs: Iterable[tuple[Any, Any] | Any]): ...
def maybe_layout_constraints(fn: Callable[..., Any]) -> Callable[..., Any] | None: ...
def tag_to_layout_constraint(tag): ...
def assert_nyi(cond, msg): ...
def add_needs_realized_inputs(fn): ...
def add_layout_constraint(fn, constraint): ...

DTYPE_ID_LOOKUP = ...

def decode_dtype(dtype: int): ...
def is_integer_type(x): ...
def is_boolean_type(x): ...
def get_promoted_dtype(*args, type_promotion_kind: ELEMENTWISE_TYPE_PROMOTION_KIND): ...
def get_overloads(aten_fn): ...
def in_namespace(op, namespace): ...
def maybe_copy_cpu_scalar(x: TensorBox, device: torch.device) -> TensorBox: ...
def transform_args(
    args: list[Any],
    kwargs: dict[str, Any],
    broadcast: bool,
    type_promotion_kind: ELEMENTWISE_TYPE_PROMOTION_KIND | None,
    convert_input_to_bool: bool,
) -> tuple[list[Any], dict[str, Any]]: ...
def register_lowering(
    aten_fn,
    broadcast=...,
    type_promotion_kind: ELEMENTWISE_TYPE_PROMOTION_KIND | None = ...,
    convert_input_to_bool=...,
    lowering_dict=...,
) -> Callable[[Callable[_P, _T]], Callable[_P, _T]]: ...
def broadcast_symbolic_shapes(a, b): ...
def promote_constants(inputs, override_return_dtype=..., type_promotion_kind=...): ...
def make_pointwise(
    fn,
    override_return_dtype=...,
    override_device=...,
    override_fn_when_input_bool=...,
    allow_alpha=...,
    triton_fallback=...,
): ...
def make_foreach_pointwise(pw_fn, allow_alpha=...): ...
def to_dtype(x: TensorBox | ShapeAsConstantBuffer, dtype: torch.dtype, copy: bool = ...): ...
def to_dtype_bitcast(x: TensorBox, dtype: torch.dtype, *, copy=...): ...
def to_device(x: TensorBox, device: torch.device, *, copy=..., non_blocking=...): ...
def register_pointwise(
    aten_fn,
    name=...,
    broadcast=...,
    type_promotion_kind=...,
    convert_input_to_bool=...,
    override_return_dtype=...,
    override_fn_when_input_bool=...,
    allow_alpha=...,
    triton_fallback=...,
): ...
def register_frexp(): ...
def register_foreach_pointwise(aten_fn, pointwise_lowering_fn, allow_alpha=...): ...
@register_lowering(aten.where, broadcast=False, type_promotion_kind=None)
def where(cond, a, b): ...
@register_lowering(aten.broadcast_tensors, broadcast=False, type_promotion_kind=None)
def broadcast_tensors(*inputs): ...
@register_lowering([aten.alias, aten.detach, aten.detach_, aten.lift, prims.view_of])
def nop(x): ...

if hasattr(aten, "lift_fresh"): ...

@register_lowering(aten.squeeze, type_promotion_kind=None)
def squeeze(x, dim=...): ...
@register_lowering(aten.squeeze_copy, type_promotion_kind=None)
def squeeze_copy(x, dim=...): ...
@register_lowering([aten.squeeze_])
def squeeze_(x, dim=...): ...
@register_lowering(aten.isinf)
def isinf(x): ...
@register_lowering(aten.isnan)
def isnan(x): ...
@register_lowering(aten.ceil)
def ceil(x): ...
@register_lowering(aten.floor)
def floor(x): ...
@register_lowering(aten.round.default)
def round(x): ...
@register_lowering(aten.trunc)
def trunc(x): ...
@register_lowering(aten.expand, type_promotion_kind=None)
def expand(x, sizes): ...
@register_lowering(prims.broadcast_in_dim, type_promotion_kind=None)
def broadcast_in_dim(a, shape, broadcast_dimensions): ...
@register_lowering(aten.expand_as, type_promotion_kind=None)
def expand_as(x, y): ...
@register_lowering(aten.repeat)
def repeat(x, repeats): ...
@register_lowering(aten._unsafe_view, type_promotion_kind=None)
@register_lowering(aten.view, type_promotion_kind=None)
@register_lowering(aten.reshape, type_promotion_kind=None)
def view(x: TensorBox, sizes: Sequence[sympy.Expr]) -> TensorBox: ...
@register_lowering(aten.permute, type_promotion_kind=None)
def permute(x, dims): ...
@register_lowering(aten.slice, type_promotion_kind=None)
def slice_(x, dim=..., start=..., end=..., step=..., clamp=...): ...
@register_lowering(aten.as_strided, type_promotion_kind=None)
def as_strided(x, size, stride, storage_offset=...): ...
@register_lowering(aten.as_strided_, type_promotion_kind=None)
def as_strided_(x, size, stride, storage_offset=...): ...
@register_lowering(aten.as_strided_copy, type_promotion_kind=None)
def as_strided_copy(x, size, stride, storage_offset=...): ...
def pointwise_cat(inputs, dim=...): ...
@register_lowering(quantized_decomposed.quantize_per_channel, type_promotion_kind=None)
def quantized_decomposed_quantize_per_channel(
    input: TensorBox,
    scales: TensorBox,
    zero_points: TensorBox,
    axis: int,
    quant_min: int,
    quant_max: int,
    dtype: torch.dtype,
) -> TensorBox | ShapeAsConstantBuffer: ...
@register_lowering(aten._assert_async.msg)
def lower_assert_async(cond, msg): ...
@register_lowering(aten._functional_assert_async.msg)
def lower_assert_functional_async(cond, msg): ...
@register_lowering(quantized_decomposed.dequantize_per_channel, type_promotion_kind=None)
def quantized_decomposed_dequantize_per_channel(
    input: TensorBox,
    scales: TensorBox,
    zero_points: TensorBox,
    axis: int,
    quant_min: int,
    quant_max: int,
    dtype: torch.dtype,
    *,
    out_dtype: torch.dtype | None = ...,
) -> TensorBox | ShapeAsConstantBuffer: ...
@register_lowering(quantized_decomposed.quantize_per_tensor.default, type_promotion_kind=None)
def quantized_decomposed_quantize_per_tensor_default(
    input: TensorBox, scale: float, zero_point: int, quant_min: int, quant_max: int, dtype: torch.dtype
) -> TensorBox | ShapeAsConstantBuffer: ...
@register_lowering(quantized_decomposed.dequantize_per_tensor.default, type_promotion_kind=None)
def quantized_decomposed_dequantize_per_tensor_default(
    input: TensorBox,
    scale: float,
    zero_point: int,
    quant_min: int,
    quant_max: int,
    dtype: torch.dtype,
    *,
    out_dtype: torch.dtype | None = ...,
) -> TensorBox | ShapeAsConstantBuffer: ...
@register_lowering(quantized_decomposed.quantize_per_tensor.tensor, type_promotion_kind=None)
def quantized_decomposed_quantize_per_tensor_tensor(
    input: TensorBox, scale: TensorBox, zero_point: TensorBox, quant_min: int, quant_max: int, dtype: torch.dtype
) -> TensorBox | ShapeAsConstantBuffer: ...
@register_lowering(quantized_decomposed.dequantize_per_tensor.tensor, type_promotion_kind=None)
def quantized_decomposed_dequantize_per_tensor_tensor(
    input: TensorBox,
    scale: TensorBox,
    zero_point: TensorBox,
    quant_min: int,
    quant_max: int,
    dtype: torch.dtype,
    *,
    out_dtype: torch.dtype | None = ...,
) -> TensorBox | ShapeAsConstantBuffer: ...
@register_lowering(aten.cat)
def cat(inputs, dim=...): ...
@register_lowering(aten.diagonal, type_promotion_kind=None)
def diagonal(input, offset: int = ..., dim1: int = ..., dim2: int = ...): ...
@register_lowering(aten.diagonal_copy, type_promotion_kind=None)
def diagonal_copy(input, offset: int = ..., dim1: int = ..., dim2: int = ...): ...
@register_lowering(aten.diagonal_scatter, type_promotion_kind=None)
def diagonal_scatter(input, src, offset: int = ..., dim1: int = ..., dim2: int = ...): ...
@register_lowering(aten.select, type_promotion_kind=None)
def select(x, dim, idx): ...
@register_lowering(aten.split, type_promotion_kind=None)
def split(x, sizes, dim=...): ...
@register_lowering(aten.split_with_sizes, type_promotion_kind=None)
def split_with_sizes(x, sizes, dim=...): ...
@register_lowering(aten.unbind, type_promotion_kind=None)
def unbind(x, dim=...): ...
@register_lowering(aten.unfold, type_promotion_kind=None)
def unfold(x, dimension, size, step): ...
@register_lowering(aten.unsqueeze, type_promotion_kind=None)
def unsqueeze(x, dim): ...
@register_lowering(aten.unsqueeze_, type_promotion_kind=None)
def unsqueeze_(x, dim): ...
@register_lowering(aten.glu)
def glu(x, dim=...): ...
def fallback_handler(kernel, add_to_fallback_set=...): ...
def unsupported_input_tensor(t: torch.Tensor, node=...): ...
def unsupported_output_tensor(t: torch.Tensor, node=...): ...
def fallback_node_due_to_unsupported_type(node: torch.fx.Node, allow_cpu_inputs=...): ...
def make_fallback(op, layout_constraint=..., warn=..., override_decomp=...): ...
def philox_rand_offset(shape): ...
@register_lowering(torch.ops.rngprims.philox_rand, type_promotion_kind=None)
def philox_rand(size, seed, offset, stride, device, dtype): ...
@register_lowering(aten.native_dropout, type_promotion_kind=None)
def native_dropout(x, p, train): ...
@register_lowering(aten.bernoulli_, type_promotion_kind=None)
def bernoulli_(x, *args): ...
@register_lowering(aten.bernoulli.p, type_promotion_kind=None)
def bernoulli_p(x, *args): ...
def warn_triton_random(): ...

fallback_rand_default = ...
fallback_rand_generator = ...
fallback_randn_default = ...
fallback_randn_generator = ...

@register_lowering(aten.rand)
def rand(*args, **kwargs): ...
@register_lowering(aten.randn)
def randn(*args, **kwargs): ...
@register_lowering(inductor_prims.force_stride_order, type_promotion_kind=None)
def inductor_force_stride_order(input_tensor, stride): ...
@register_lowering(inductor_prims.seed, type_promotion_kind=None)
def inductor_seed(device: torch.device): ...
@register_lowering(inductor_prims.seeds, type_promotion_kind=None)
def inductor_seeds(count, device): ...
@register_lowering(inductor_prims.lookup_seed, type_promotion_kind=None)
def inductor_lookup_seed(seeds, index): ...
@register_lowering(inductor_prims.random, type_promotion_kind=None)
def inductor_random(size: list[int], seed: TensorBox, mode: str, *, offset: int = ...): ...
@register_lowering(inductor_prims.randint, type_promotion_kind=None)
def inductor_randint(low: int, high: int, size: list[int], seed: TensorBox, *, offset: int = ...): ...
@register_lowering(aten.searchsorted.Tensor, type_promotion_kind=None)
def searchsorted(
    sorted_sequence: TensorBox,
    self: TensorBox,
    *,
    out_int32: bool = ...,
    right: bool = ...,
    side: str | None = ...,
    sorter: TensorBox | None = ...,
) -> TensorBox | ShapeAsConstantBuffer: ...
@register_lowering(aten.bucketize, type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.NO_OPMATH)
def bucketize(input: TensorBox, boundaries: TensorBox, *, out_int32: bool = ..., right: bool = ...): ...
def require_dense(_, *args, **kwargs): ...
def require_contiguous(_, *args, **kwargs): ...
def require_contiguous_strides(_, *args, **kwargs): ...
def require_channels_last(_, *args, **kwargs): ...
def constrain_to_fake_tensor(arg, fake_arg): ...
def constrain_to_fake_tensors(args, kwargs, fake_args, fake_kwargs): ...
def constrain_to_fx_strides(fx_node, *args, **kwargs): ...
def sdpa_constraint(fx_node, *args, **kwargs): ...

if torch.xpu.is_available(): ...

@register_lowering(aten.copy, type_promotion_kind=None)
def copy(self, src, non_blocking=...): ...
@register_lowering(aten.clone)
def clone(x, *, memory_format=...): ...
def clone_preserve_reinterpret_view(x): ...

if hasattr(aten, "lift_fresh_copy"): ...

@register_lowering(prims.iota)
def iota(length, *, start, step, dtype, device, requires_grad): ...
@register_lowering(aten.select_scatter, type_promotion_kind=None)
def select_scatter(x, src, dim: int, index: int): ...
@register_lowering(aten.slice_scatter, type_promotion_kind=None)
def slice_scatter(x, src, dim=..., start=..., end=..., step=...): ...
@register_lowering([torch.tensor, aten.scalar_tensor])
def tensor(data, *, dtype=..., device=..., layout=..., pin_memory=...): ...
@register_lowering(torch.as_tensor)
def as_tensor(data, dtype=..., device=...): ...
@register_lowering(torch.LongTensor)
def long_tensor(data): ...
def full_like(x, fill_value, **kwargs): ...
def tensor_constructor(fill_value): ...
@register_lowering([torch.empty, aten.empty])
def empty(*size, names=..., dtype=..., layout=..., device=..., pin_memory=..., memory_format=...): ...
def create_tensor_like(creation_fn): ...
def constant_like(fill_value): ...

empty_like = ...
ones_like = ...
zeros_like = ...

def new_constant(fill_value): ...
@register_lowering(aten.new_empty)
def new_empty(x, size, *, dtype=..., layout=..., device=..., pin_memory=...): ...
@register_lowering(aten.empty_strided)
def empty_strided(size, stride, *, dtype=..., layout=..., device=..., pin_memory=...): ...
@register_lowering(aten.new_empty_strided)
def new_empty_strided(x, size, stride, *, dtype=..., layout=..., device=..., pin_memory=...): ...
@register_lowering(prims.copy_strided.default)
def copy_strided(x, stride): ...
@register_lowering([torch.full, aten.full])
def full(size, fill_value, **kwargs): ...
@register_lowering(aten.gather, type_promotion_kind=None)
def gather(x, dim, index, sparse_grad=...): ...
@register_lowering(aten.embedding, type_promotion_kind=None)
def embedding(weight, indices, padding_idx=..., scale_grad_by_freq=..., sparse=...): ...
def check_and_broadcast_indices(indices, device): ...
def index_output_size_and_inner_fn(
    x_size, indices, tensor_indices, tensor_size, indices_loaders, indexed_size, x_loader, check, wrap_neg=...
): ...
def index_impl(x, indices, check): ...
def index_impl_helper(x, indices, check, wrap_neg=...): ...
@register_lowering(aten.index, type_promotion_kind=None)
def index(x, indices): ...
@register_lowering(aten.index_put, type_promotion_kind=None)
def index_put(x, indices, values, accumulate=...): ...
def index_put_as_masked_fill(self, indices, value, accumulate): ...
def index_put_fallback(self, indices, values, accumulate): ...
@register_lowering(aten.index_put_, type_promotion_kind=None)
def index_put_(self, indices, values, accumulate=...): ...
def index_put_impl_(self, indices, values, accumulate, check, may_realize=...): ...

fallback__unsafe_masked_index = ...
fallback__unsafe_masked_index_put_accumulate = ...

@make_pointwise
def clamp(a, min, max): ...
@register_lowering(aten.as_strided_scatter, type_promotion_kind=None)
def as_strided_scatter(self, src, size, stride, storage_offset=...): ...
@register_lowering(aten.scatter, type_promotion_kind=None)
def scatter(x, dim: int, index, src, **kwargs): ...
def scatter_fallback(
    op_overload: torch._ops.OpOverload,
    self,
    dim: int,
    index,
    src,
    *,
    reduce: str | None = ...,
    include_self: bool = ...,
): ...
@register_lowering(aten.scatter_, type_promotion_kind=None)
def scatter_(self, dim: int, index, src, *, reduce: str | None = ...): ...
@register_lowering(aten.scatter_add, type_promotion_kind=None)
def scatter_add(x, dim: int, index, src): ...
@register_lowering(aten.scatter_add_, type_promotion_kind=None)
def scatter_add_(x, dim: int, index, src): ...
@register_lowering(aten.scatter_reduce, type_promotion_kind=None)
def scatter_reduce(x, dim: int, index, src, reduction_type, **kwargs): ...
@register_lowering(aten.scatter_reduce_, type_promotion_kind=None)
def scatter_reduce_(self, dim: int, index, src, reduce, *, include_self: bool = ...): ...
def upsample_nearestnd(x, output_size, scales_x: tuple[float | None, ...], n: int = ..., exact: bool = ...): ...
@register_lowering(aten.upsample_nearest1d.default)
def upsample_nearest1d(x, output_size, scales: float | None = ...): ...
@register_lowering(aten.upsample_nearest2d.default)
def upsample_nearest2d(x, output_size, scales_h: float | None = ..., scales_w: float | None = ...): ...
@register_lowering(aten.upsample_nearest3d.default)
def upsample_nearest3d(
    x, output_size, scales_d: float | None = ..., scales_h: float | None = ..., scales_w: float | None = ...
): ...
@register_lowering(prims.rev.default)
def rev(x, dims): ...
def inplace_constant_pad_nd(x: TensorBox, padding: Sequence[int], fill_value: float) -> TensorBox | None: ...
@register_lowering(aten.constant_pad_nd, type_promotion_kind=None)
def constant_pad_nd(x, padding, fill_value=...): ...
def range_mask_low(i: sympy.Expr, low: sympy.Expr | int): ...
def range_mask_high(i: sympy.Expr, high: sympy.Expr): ...
def range_mask(i: sympy.Expr, high: sympy.Expr, low: sympy.Expr): ...
def constant_boundary_condition(x, fill_value, padding=..., pad_fill_value=..., dim=...): ...
def pooling_size(x, i, kernel_size, stride, padding, ceil_mode, *, dilation=...): ...
def should_fallback_max_pool_with_indices(kernel_size, *, n_dim): ...
def max_pool_checks(x, kernel_size, stride, padding, dilation, n_dim, *, assert_fallback=...): ...
@register_lowering(aten.max_pool2d_with_indices, type_promotion_kind=None)
def max_pool2d_with_indices(x, kernel_size, stride=..., padding=..., dilation=..., ceil_mode=...): ...
@register_lowering(aten.max_pool3d_with_indices, type_promotion_kind=None)
def max_pool3d_with_indices(x, kernel_size, stride=..., padding=..., dilation=..., ceil_mode=...): ...

fallback_max_pool2d_with_indices_backward = ...

@register_lowering(aten.max_pool2d_with_indices_backward, type_promotion_kind=None)
def max_pool2d_with_indices_backward(grad_output, x, kernel_size, stride, padding, dilation, ceil_mode, indices): ...
def pad_adaptive_loader(x, pad_val=...): ...
def compute_indices_adaptive_pooling(start_index, end_index, h_in, w_in, h_out, w_out): ...

fallback_adaptive_avg_pool2d = ...
fallback_adaptive_max_pool2d = ...

@register_lowering(aten.adaptive_max_pool2d)
def adaptive_max_pool2d(x, output_size): ...
@register_lowering(aten.fractional_max_pool2d)
def fractional_max_pool2d(x, kernel_size, output_size, random_samples): ...
@register_lowering(aten.fractional_max_pool3d)
def fractional_max_pool3d(x, kernel_size, output_size, random_samples): ...
@register_lowering(aten.upsample_nearest2d_backward.default)
def upsample_nearest2d_backward(x, output_size=..., input_size=..., scales_h=..., scales_w=...): ...

fallback_avg_pool2d = ...
fallback_avg_pool3d = ...

@register_lowering(aten.avg_pool2d, type_promotion_kind=None)
def avg_pool2d(x, kernel_size, stride=..., padding=..., ceil_mode=..., count_include_pad=..., divisor_override=...): ...
@register_lowering(aten.avg_pool3d, type_promotion_kind=None)
def avg_pool3d(x, kernel_size, stride=..., padding=..., ceil_mode=..., count_include_pad=..., divisor_override=...): ...

fallback_avg_pool2d_backward = ...

@register_lowering(aten.avg_pool2d_backward, type_promotion_kind=None)
def avg_pool2d_backward(
    grad_output, x, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override=...
): ...

fallback_avg_pool3d_backward = ...

@register_lowering(aten.avg_pool3d_backward, type_promotion_kind=None)
def avg_pool3d_backward(
    grad_output, x, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override=...
): ...
def make_reduction(reduction_type: ReductionType, override_return_dtype=...): ...
@register_lowering(aten.mean)
def mean(x, axis=..., keepdim=..., *, dtype=...): ...
def var_mean_sum_(x, axis, correction, keepdim, return_mean): ...
def use_two_step_variance(x, axis, keepdim): ...
def var_mean_welford_(x, axis, *, correction, keepdim, return_mean): ...
def var_mean_helper_(x, *, axis, correction, keepdim, return_mean): ...
@register_lowering([aten.var, prims.var])
def var_(x, axis=..., *, correction=..., keepdim=...): ...
@register_lowering(aten.var_mean)
def var_mean(x, axis=..., *, correction=..., keepdim=...): ...
def pow_recursive(x, y, dtype): ...
@make_pointwise
def pow_native(a, b): ...

fallback_pow_tensor_tensor = ...
fallback_pow_scalar = ...
fallback_pow_tensor_scalar = ...

@register_lowering(aten.pow, broadcast=True)
def pow(a, b): ...
def mutate_to(changed, val, unsafe_alias=...): ...
@register_lowering(aten.fill_)
def fill_(x, fill_value): ...
@register_lowering(aten.copy_, type_promotion_kind=None)
def copy_(dst, src, non_blocking=...): ...
@make_pointwise
def floordiv(a, b): ...
@make_pointwise
def truncdiv(a, b): ...
@register_lowering(aten.div, broadcast=True)
def div_mode(a, b, rounding_mode=...): ...
@register_lowering([aten.mul], broadcast=True)
def mul(a, b): ...
def get_constant_value(x: ir.IRNode) -> ir.Constant | None: ...
@register_lowering([prims.div], broadcast=True)
def div_prim(a, b): ...
@register_lowering(
    [aten.true_divide, aten.div.Tensor],
    broadcast=True,
    type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT,
)
def div(a, b): ...
@register_lowering([aten.fmod, prims.fmod], broadcast=True)
def fmod(a, b): ...
@register_lowering([aten.sum, prims.sum])
def sum_(x, axis=..., keepdims=..., *, dtype=...): ...

fallback_cumsum = ...
fallback_cumprod = ...
fallback_logcumsumexp = ...
fallback_cummax = ...
fallback_cummin = ...

@register_lowering(aten.cumsum)
def cumsum(x, axis=..., dtype=...): ...
@register_lowering(aten.cumprod)
def cumprod(x, axis=..., dtype=...): ...
@register_lowering(aten.logcumsumexp)
def logcumsumexp(x, dim): ...
@register_lowering(aten.cummax, type_promotion_kind=None)
def cummax(x, axis=...): ...
@register_lowering(aten.cummin, type_promotion_kind=None)
def cummin(x, axis=...): ...
@register_lowering(aten.prod)
def prod(x, axis=..., keepdims=..., *, dtype=...): ...
@register_lowering(aten.any)
def reduce_any(x, dim=..., keepdim=...): ...
@register_lowering(aten.max, type_promotion_kind=None)
def reduce_max(x, dim=..., keepdim=...): ...
@register_lowering(aten.min, type_promotion_kind=None)
def reduce_min(x, dim=..., keepdim=...): ...

reduce_amax = ...
reduce_amin = ...
reduce_argmax = ...
reduce_argmin = ...
add = ...
sort_fallback = ...

@register_lowering(aten.sort.stable, type_promotion_kind=None)
def sort_stable(x, *, stable=..., dim=..., descending=...): ...
@register_lowering(aten.sort.default, type_promotion_kind=None)
def sort(x, dim=..., descending=...): ...
def register_pointwise_numeric(op, name=..., triton_fallback=...): ...
def register_pointwise_numeric_ldf64(op: torch._ops.OpOverloadPacket): ...

rsqrt = ...
exp = ...
exp2 = ...
expm1 = ...
relu = ...
sigmoid = ...
sqrt = ...
square = ...
sub = ...
abs = ...
bitwise_and = ...
bitwise_left_shift = ...
bitwise_not = ...
bitwise_or = ...
bitwise_right_shift = ...
bitwise_xor = ...
erf = ...
logical_and = ...
logical_not = ...
logical_or = ...
logical_xor = ...
maximum = ...
minimum = ...
neg = ...
abs = ...
reciprocal = ...
sign = ...
gt = ...
foreach_add_list = ...
foreach_add_scalar = ...
foreach_mul_list = ...
foreach_mul_scalar = ...
foreach_div_list = ...
foreach_div_scalar = ...
foreach_copy = ...

def register_foreach_inplace(aten_op, outplace_aten_op, outplace_op): ...
def register_inplace(aten_op, outplace_op): ...
@register_lowering(aten.sym_constrain_range)
def sym_constrain_range(a, min=..., max=...): ...
@register_lowering(aten.sym_size.int)
def sym_size(a, dim): ...
@register_lowering(aten.sym_stride.int)
def sym_stride(a, dim): ...
@register_lowering(aten.sym_numel)
def sym_numel(a): ...
@register_lowering(torch.sym_sum)
def sym_sum(args): ...
@register_lowering(aten._foobar)
def foobar(self, *args, **kwargs): ...
@register_lowering(torch.ops.inductor.resize_storage_bytes_)
def resize_storage_bytes_(variable, new_size): ...
@register_lowering(torch.ops.aten.set_.source_Tensor)
def set__source_tensor(self, source_tensor): ...

if hasattr(torch.ops.fsdp, "copy_"):
    @register_lowering(torch.ops.fsdp.copy_.default)
    def fsdp_copy_(dst, src): ...

@register_lowering(torch.ops.aten.resize)
def resize(x, size, *, memory_format=...): ...
@register_lowering(triton_kernel_wrapper_mutation)
def triton_kernel_wrap_(*, kernel_idx, constant_args_idx, grid, tma_descriptor_metadata, kwargs): ...
@register_lowering(torch.ops.higher_order.cond, type_promotion_kind=None)
def cond(pred, true_fn, false_fn, operands): ...
@register_lowering(torch.ops.higher_order.while_loop, type_promotion_kind=None)
def while_loop(cond_fn, body_fn, carried_inputs, additional_inputs, stack_output=...): ...
@register_lowering(torch.ops.higher_order.invoke_subgraph, type_promotion_kind=None)
def invoke_subgraph(subgraph_fn: ir.Subgraph, identifier: str, *operands): ...
@register_lowering(torch._higher_order_ops.invoke_quant, type_promotion_kind=None)
def invoke_quant_tracer(subgraph_fn: ir.Subgraph, *operands, scheme=...): ...
@register_lowering(associative_scan_op, type_promotion_kind=None)
def associative_scan(combine_fn: ir.Subgraph, xs, additional_inputs: tuple[torch.Tensor]): ...
@register_lowering(torch.ops.higher_order.with_effects, type_promotion_kind=None)
def with_effects(token, op, *args, **kwargs): ...
@register_lowering(inductor_prims.prepare_softmax_online, type_promotion_kind=None)
def prepare_softmax_online(x, dim): ...
@contextlib.contextmanager
def force_fallback(op: torch._ops.OpOverload): ...
