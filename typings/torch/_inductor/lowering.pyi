import contextlib
import sympy
import torch
import torch.fx
from collections.abc import Iterable, Sequence
from typing import Any, Optional, TYPE_CHECKING, TypeVar, Union
from collections.abc import Callable
from typing import ParamSpec
from torch._higher_order_ops.associative_scan import associative_scan_op
from torch._higher_order_ops.triton_kernel_wrap import triton_kernel_wrapper_mutation
from torch._prims_common import ELEMENTWISE_TYPE_PROMOTION_KIND
from . import inductor_prims, ir
from .ir import IRNode, ShapeAsConstantBuffer, TensorBox
from .virtualized import ops
from .ops_handler import ReductionType

if TYPE_CHECKING: ...
_T = TypeVar("_T")
_P = ParamSpec("_P")
FALLBACK_ALLOW_LIST = ...
log = ...
lowerings: dict[Callable[..., Any] | str, Callable[..., Any]] = ...
_maybe_layout_constraints: dict[torch._ops.OpOverload, Callable[..., Any] | None] = ...
fallbacks = ...
aten = ...
tr_c10d = ...
prims = ...
needs_realized_inputs = ...
foreach_ops = ...
inplace_foreach_ops = ...
inplaceable_foreach_ops: dict[torch._ops.OpOverload, torch._ops.OpOverload] = ...
quantized_decomposed = ...

def cur_node_has_non_foreach_users():  # -> bool:
    ...
def group_foreach_args(arg_pairs: Iterable[tuple[Any, Any] | Any]):  # -> defaultdict[Any, list[Any]]:
    ...
def maybe_layout_constraints(fn: Callable[..., Any]) -> Callable[..., Any] | None: ...
def tag_to_layout_constraint(
    tag,
):  # -> Callable[..., tuple[tuple[Any | dict[Any, Any | dict[Any, Any | dict[Any, Any] | list[Any] | tuple[Any, ...]] | list[Any] | tuple[Any, ...]] | list[Any] | tuple[Any, ...], ...], dict[Any, Any | dict[Any, Any | dict[Any, Any | dict[Any, Any] | list[Any] | tuple[Any, ...]] | list[Any] | tuple[Any, ...]] | list[Any] | tuple[Any, ...]]]] | Callable[..., tuple[PyTree, PyTree]] | Callable[..., tuple[tuple[Any | dict[Any, Any | dict[Any, Any | dict[Any, Any]]], ...], dict[str, Any | dict[Any, Any | dict[Any, Any | dict[Any, Any]]]]]] | None:
    ...
def assert_nyi(cond, msg):  # -> None:
    ...
def add_needs_realized_inputs(fn):  # -> list[list[Any] | None] | None:
    ...
def add_layout_constraint(fn, constraint):  # -> None:
    ...

DTYPE_ID_LOOKUP = ...

def decode_dtype(dtype: int):  # -> int:
    ...
def is_integer_type(x):  # -> bool:
    ...
def is_boolean_type(x):  # -> bool:
    ...
def get_promoted_dtype(*args, type_promotion_kind: ELEMENTWISE_TYPE_PROMOTION_KIND):  # -> dtype:
    ...
def get_overloads(aten_fn):  # -> list[Any]:
    ...
def in_namespace(op, namespace):  # -> bool:
    ...
def maybe_copy_cpu_scalar(x: TensorBox, device: torch.device) -> TensorBox: ...
def transform_args(
    args: list[Any],
    kwargs: dict[str, Any],
    broadcast: bool,
    type_promotion_kind: ELEMENTWISE_TYPE_PROMOTION_KIND | None,
    convert_input_to_bool: bool,
) -> tuple[list[Any], dict[str, Any]]: ...
def register_lowering(
    aten_fn,
    broadcast=...,
    type_promotion_kind: ELEMENTWISE_TYPE_PROMOTION_KIND | None = ...,
    convert_input_to_bool=...,
    lowering_dict=...,
) -> Callable[[Callable[_P, _T]], Callable[_P, _T]]: ...
def broadcast_symbolic_shapes(a, b):  # -> tuple[Any, ...]:

    ...
def promote_constants(
    inputs, override_return_dtype=..., type_promotion_kind=...
):  # -> list[IndexingConstant | Constant] | list[Any]:
    ...
def make_pointwise(
    fn,
    override_return_dtype=...,
    override_device=...,
    override_fn_when_input_bool=...,
    allow_alpha=...,
    triton_fallback=...,
):  # -> Callable[..., Any | TensorBox | ShapeAsConstantBuffer]:
    ...
def make_foreach_pointwise(pw_fn, allow_alpha=...):  # -> Callable[..., list[None]]:
    ...
def to_dtype(
    x: TensorBox | ShapeAsConstantBuffer, dtype: torch.dtype, copy: bool = ...
):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
def to_dtype_bitcast(x: TensorBox, dtype: torch.dtype, *, copy=...):  # -> TensorBox | ShapeAsConstantBuffer | PyTree:
    ...
def to_device(
    x: TensorBox, device: torch.device, *, copy=..., non_blocking=...
):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
def register_pointwise(
    aten_fn,
    name=...,
    broadcast=...,
    type_promotion_kind=...,
    convert_input_to_bool=...,
    override_return_dtype=...,
    override_fn_when_input_bool=...,
    allow_alpha=...,
    triton_fallback=...,
):  # -> Callable[..., Any]:

    ...
def register_frexp():  # -> Callable[..., tuple[Any | TensorBox | ShapeAsConstantBuffer, Any | TensorBox | ShapeAsConstantBuffer]]:

    ...
def register_foreach_pointwise(aten_fn, pointwise_lowering_fn, allow_alpha=...):  # -> _Wrapped[..., Any, ..., Any]:
    ...
@register_lowering(aten.where, broadcast=False, type_promotion_kind=None)
def where(cond, a, b):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
@register_lowering(aten.broadcast_tensors, broadcast=False, type_promotion_kind=None)
def broadcast_tensors(*inputs):  # -> list[Any]:
    ...
@register_lowering([aten.alias, aten.detach, aten.detach_, aten.lift, prims.view_of])
def nop(x): ...

if hasattr(aten, "lift_fresh"): ...

@register_lowering(aten.squeeze, type_promotion_kind=None)
def squeeze(x, dim=...):  # -> TensorBox:
    ...
@register_lowering(aten.squeeze_copy, type_promotion_kind=None)
def squeeze_copy(x, dim=...):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
@register_lowering([aten.squeeze_])
def squeeze_(x, dim=...):  # -> TensorBox:
    ...
@register_lowering(aten.isinf)
def isinf(x):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
@register_lowering(aten.isnan)
def isnan(x):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
@register_lowering(aten.ceil)
def ceil(x):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
@register_lowering(aten.floor)
def floor(x):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
@register_lowering(aten.round.default)
def round(x):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
@register_lowering(aten.trunc)
def trunc(x):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
@register_lowering(aten.expand, type_promotion_kind=None)
def expand(x, sizes):  # -> BaseView | TensorBox:
    ...
@register_lowering(prims.broadcast_in_dim, type_promotion_kind=None)
def broadcast_in_dim(a, shape, broadcast_dimensions):  # -> BaseView | TensorBox:
    ...
@register_lowering(aten.expand_as, type_promotion_kind=None)
def expand_as(x, y):  # -> BaseView | TensorBox:
    ...
@register_lowering(aten.repeat)
def repeat(x, repeats):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
@register_lowering(aten._unsafe_view, type_promotion_kind=None)
@register_lowering(aten.view, type_promotion_kind=None)
@register_lowering(aten.reshape, type_promotion_kind=None)
def view(x: TensorBox, sizes: Sequence[sympy.Expr]) -> TensorBox: ...
@register_lowering(aten.permute, type_promotion_kind=None)
def permute(x, dims):  # -> TensorBox:
    ...
@register_lowering(aten.slice, type_promotion_kind=None)
def slice_(x, dim=..., start=..., end=..., step=..., clamp=...):  # -> TensorBox:
    ...
@register_lowering(aten.as_strided, type_promotion_kind=None)
def as_strided(x, size, stride, storage_offset=...):  # -> TensorBox:
    ...
@register_lowering(aten.as_strided_, type_promotion_kind=None)
def as_strided_(x, size, stride, storage_offset=...):  # -> TensorBox:
    ...
@register_lowering(aten.as_strided_copy, type_promotion_kind=None)
def as_strided_copy(x, size, stride, storage_offset=...):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
def pointwise_cat(inputs, dim=...):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
@register_lowering(quantized_decomposed.quantize_per_channel, type_promotion_kind=None)
def quantized_decomposed_quantize_per_channel(
    input: TensorBox,
    scales: TensorBox,
    zero_points: TensorBox,
    axis: int,
    quant_min: int,
    quant_max: int,
    dtype: torch.dtype,
) -> TensorBox | ShapeAsConstantBuffer: ...
@register_lowering(aten._assert_async.msg)
def lower_assert_async(cond, msg):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
@register_lowering(aten._functional_assert_async.msg)
def lower_assert_functional_async(cond, msg):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
@register_lowering(quantized_decomposed.dequantize_per_channel, type_promotion_kind=None)
def quantized_decomposed_dequantize_per_channel(
    input: TensorBox,
    scales: TensorBox,
    zero_points: TensorBox,
    axis: int,
    quant_min: int,
    quant_max: int,
    dtype: torch.dtype,
    *,
    out_dtype: torch.dtype | None = ...,
) -> TensorBox | ShapeAsConstantBuffer: ...
@register_lowering(quantized_decomposed.quantize_per_tensor.default, type_promotion_kind=None)
def quantized_decomposed_quantize_per_tensor_default(
    input: TensorBox, scale: float, zero_point: int, quant_min: int, quant_max: int, dtype: torch.dtype
) -> TensorBox | ShapeAsConstantBuffer: ...
@register_lowering(quantized_decomposed.dequantize_per_tensor.default, type_promotion_kind=None)
def quantized_decomposed_dequantize_per_tensor_default(
    input: TensorBox,
    scale: float,
    zero_point: int,
    quant_min: int,
    quant_max: int,
    dtype: torch.dtype,
    *,
    out_dtype: torch.dtype | None = ...,
) -> TensorBox | ShapeAsConstantBuffer: ...
@register_lowering(quantized_decomposed.quantize_per_tensor.tensor, type_promotion_kind=None)
def quantized_decomposed_quantize_per_tensor_tensor(
    input: TensorBox, scale: TensorBox, zero_point: TensorBox, quant_min: int, quant_max: int, dtype: torch.dtype
) -> TensorBox | ShapeAsConstantBuffer: ...
@register_lowering(quantized_decomposed.dequantize_per_tensor.tensor, type_promotion_kind=None)
def quantized_decomposed_dequantize_per_tensor_tensor(
    input: TensorBox,
    scale: TensorBox,
    zero_point: TensorBox,
    quant_min: int,
    quant_max: int,
    dtype: torch.dtype,
    *,
    out_dtype: torch.dtype | None = ...,
) -> TensorBox | ShapeAsConstantBuffer: ...
@register_lowering(aten.cat)
def cat(inputs, dim=...):  # -> PyTree | TensorBox | ShapeAsConstantBuffer:
    ...
@register_lowering(aten.diagonal, type_promotion_kind=None)
def diagonal(input, offset: int = ..., dim1: int = ..., dim2: int = ...):  # -> TensorBox:
    ...
@register_lowering(aten.diagonal_copy, type_promotion_kind=None)
def diagonal_copy(input, offset: int = ..., dim1: int = ..., dim2: int = ...):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
@register_lowering(aten.diagonal_scatter, type_promotion_kind=None)
def diagonal_scatter(
    input, src, offset: int = ..., dim1: int = ..., dim2: int = ...
):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
@register_lowering(aten.select, type_promotion_kind=None)
def select(x, dim, idx):  # -> TensorBox:
    ...
@register_lowering(aten.split, type_promotion_kind=None)
def split(x, sizes, dim=...):  # -> list[Any]:
    ...
@register_lowering(aten.split_with_sizes, type_promotion_kind=None)
def split_with_sizes(x, sizes, dim=...):  # -> list[Any]:
    ...
@register_lowering(aten.unbind, type_promotion_kind=None)
def unbind(x, dim=...):  # -> list[TensorBox]:
    ...
@register_lowering(aten.unfold, type_promotion_kind=None)
def unfold(x, dimension, size, step):  # -> TensorBox:
    ...
@register_lowering(aten.unsqueeze, type_promotion_kind=None)
def unsqueeze(x, dim):  # -> TensorBox:
    ...
@register_lowering(aten.unsqueeze_, type_promotion_kind=None)
def unsqueeze_(x, dim):  # -> TensorBox:
    ...
@register_lowering(aten.glu)
def glu(x, dim=...): ...
def fallback_handler(kernel, add_to_fallback_set=...):  # -> Callable[..., PyTree]:
    ...
def unsupported_input_tensor(t: torch.Tensor, node=...):  # -> bool:

    ...
def unsupported_output_tensor(t: torch.Tensor, node=...):  # -> bool:

    ...
def fallback_node_due_to_unsupported_type(node: torch.fx.Node, allow_cpu_inputs=...):  # -> bool:
    ...
def make_fallback(op, layout_constraint=..., warn=..., override_decomp=...):  # -> None:
    ...
def philox_rand_offset(shape):  # -> TensorBox | ShapeAsConstantBuffer:

    ...
@register_lowering(torch.ops.rngprims.philox_rand, type_promotion_kind=None)
def philox_rand(
    size, seed, offset, stride, device, dtype
):  # -> tuple[TensorBox | ShapeAsConstantBuffer, TensorBox | ShapeAsConstantBuffer]:
    ...
@register_lowering(aten.native_dropout, type_promotion_kind=None)
def native_dropout(x, p, train):  # -> PyTree:
    ...
@register_lowering(aten.bernoulli_, type_promotion_kind=None)
def bernoulli_(x, *args): ...
@register_lowering(aten.bernoulli.p, type_promotion_kind=None)
def bernoulli_p(x, *args): ...
def warn_triton_random():  # -> None:
    ...

fallback_rand_default = ...
fallback_rand_generator = ...
fallback_randn_default = ...
fallback_randn_generator = ...

@register_lowering(aten.rand)
def rand(*args, **kwargs):  # -> PyTree:
    ...
@register_lowering(aten.randn)
def randn(*args, **kwargs):  # -> PyTree:
    ...
@register_lowering(inductor_prims.force_stride_order, type_promotion_kind=None)
def inductor_force_stride_order(input_tensor, stride):  # -> Any:
    ...
@register_lowering(inductor_prims.seed, type_promotion_kind=None)
def inductor_seed(device: torch.device): ...
@register_lowering(inductor_prims.seeds, type_promotion_kind=None)
def inductor_seeds(count, device):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
@register_lowering(inductor_prims.lookup_seed, type_promotion_kind=None)
def inductor_lookup_seed(seeds, index):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
@register_lowering(inductor_prims.random, type_promotion_kind=None)
def inductor_random(
    size: list[int], seed: TensorBox, mode: str, *, offset: int = ...
):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
@register_lowering(inductor_prims.randint, type_promotion_kind=None)
def inductor_randint(
    low: int, high: int, size: list[int], seed: TensorBox, *, offset: int = ...
):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
@register_lowering(aten.searchsorted.Tensor, type_promotion_kind=None)
def searchsorted(
    sorted_sequence: TensorBox,
    self: TensorBox,
    *,
    out_int32: bool = ...,
    right: bool = ...,
    side: str | None = ...,
    sorter: TensorBox | None = ...,
) -> TensorBox | ShapeAsConstantBuffer: ...
@register_lowering(aten.bucketize, type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.NO_OPMATH)
def bucketize(
    input: TensorBox, boundaries: TensorBox, *, out_int32: bool = ..., right: bool = ...
):  # -> PyTree | TensorBox | ShapeAsConstantBuffer:
    ...
def require_dense(_, *args, **kwargs):  # -> tuple[PyTree, PyTree]:
    ...
def require_contiguous(_, *args, **kwargs):  # -> tuple[PyTree, PyTree]:
    ...
def require_contiguous_strides(_, *args, **kwargs):  # -> tuple[PyTree, PyTree]:
    ...
def require_channels_last(_, *args, **kwargs):  # -> tuple[PyTree, PyTree]:
    ...
def constrain_to_fake_tensor(
    arg, fake_arg
):  # -> Any | dict[Any, Any | dict[Any, Any | dict[Any, Any] | list[Any] | tuple[Any, ...]] | list[Any] | tuple[Any, ...]] | list[Any] | tuple[Any, ...]:
    ...
def constrain_to_fake_tensors(
    args, kwargs, fake_args, fake_kwargs
):  # -> tuple[tuple[Any | dict[Any, Any | dict[Any, Any | dict[Any, Any] | list[Any] | tuple[Any, ...]] | list[Any] | tuple[Any, ...]] | list[Any] | tuple[Any, ...], ...], dict[Any, Any | dict[Any, Any | dict[Any, Any | dict[Any, Any] | list[Any] | tuple[Any, ...]] | list[Any] | tuple[Any, ...]] | list[Any] | tuple[Any, ...]]]:
    ...
def constrain_to_fx_strides(
    fx_node, *args, **kwargs
):  # -> tuple[tuple[Any | dict[Any, Any | dict[Any, Any | dict[Any, Any]]], ...], dict[str, Any | dict[Any, Any | dict[Any, Any | dict[Any, Any]]]]]:
    ...
def sdpa_constraint(fx_node, *args, **kwargs):  # -> tuple[tuple[Any, ...], dict[str, Any]]:
    ...

if torch.xpu.is_available(): ...

@register_lowering(aten.copy, type_promotion_kind=None)
def copy(self, src, non_blocking=...):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
@register_lowering(aten.clone)
def clone(x, *, memory_format=...):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
def clone_preserve_reinterpret_view(x):  # -> TensorBox | ShapeAsConstantBuffer:
    ...

if hasattr(aten, "lift_fresh_copy"): ...

@register_lowering(prims.iota)
def iota(length, *, start, step, dtype, device, requires_grad):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
@register_lowering(aten.select_scatter, type_promotion_kind=None)
def select_scatter(x, src, dim: int, index: int):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
@register_lowering(aten.slice_scatter, type_promotion_kind=None)
def slice_scatter(x, src, dim=..., start=..., end=..., step=...):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
@register_lowering([torch.tensor, aten.scalar_tensor])
def tensor(data, *, dtype=..., device=..., layout=..., pin_memory=...):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
@register_lowering(torch.as_tensor)
def as_tensor(data, dtype=..., device=...):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
@register_lowering(torch.LongTensor)
def long_tensor(data):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
def full_like(x, fill_value, **kwargs): ...
def tensor_constructor(fill_value):  # -> Callable[..., TensorBox | ShapeAsConstantBuffer]:
    ...
@register_lowering([torch.empty, aten.empty])
def empty(
    *size, names=..., dtype=..., layout=..., device=..., pin_memory=..., memory_format=...
):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
def create_tensor_like(creation_fn):  # -> Callable[..., Any]:

    ...
def constant_like(fill_value):  # -> Callable[..., Any]:
    ...

empty_like = ...
ones_like = ...
zeros_like = ...

def new_constant(fill_value):  # -> Callable[..., TensorBox | ShapeAsConstantBuffer]:
    ...
@register_lowering(aten.new_empty)
def new_empty(x, size, *, dtype=..., layout=..., device=..., pin_memory=...):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
@register_lowering(aten.empty_strided)
def empty_strided(
    size, stride, *, dtype=..., layout=..., device=..., pin_memory=...
):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
@register_lowering(aten.new_empty_strided)
def new_empty_strided(
    x, size, stride, *, dtype=..., layout=..., device=..., pin_memory=...
):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
@register_lowering(prims.copy_strided.default)
def copy_strided(x, stride):  # -> Any:
    ...
@register_lowering([torch.full, aten.full])
def full(size, fill_value, **kwargs):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
@register_lowering(aten.gather, type_promotion_kind=None)
def gather(x, dim, index, sparse_grad=...):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
@register_lowering(aten.embedding, type_promotion_kind=None)
def embedding(
    weight, indices, padding_idx=..., scale_grad_by_freq=..., sparse=...
):  # -> PyTree | TensorBox | ShapeAsConstantBuffer:
    ...
def check_and_broadcast_indices(indices, device):  # -> tuple[list[None], list[int]]:
    ...
def index_output_size_and_inner_fn(
    x_size, indices, tensor_indices, tensor_size, indices_loaders, indexed_size, x_loader, check, wrap_neg=...
):  # -> tuple[Any, Callable[..., list[Any] | Any]]:
    ...
def index_impl(x, indices, check):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
def index_impl_helper(
    x, indices, check, wrap_neg=...
):  # -> tuple[Any, Callable[..., Any], Callable[..., list[Any] | Any]]:
    ...
@register_lowering(aten.index, type_promotion_kind=None)
def index(x, indices):  # -> TensorBox | ShapeAsConstantBuffer | PyTree:
    ...
@register_lowering(aten.index_put, type_promotion_kind=None)
def index_put(x, indices, values, accumulate=...):  # -> TensorBox:
    ...
def index_put_as_masked_fill(self, indices, value, accumulate):  # -> TensorBox:
    ...
def index_put_fallback(self, indices, values, accumulate): ...
@register_lowering(aten.index_put_, type_promotion_kind=None)
def index_put_(self, indices, values, accumulate=...):  # -> TensorBox:
    ...
def index_put_impl_(self, indices, values, accumulate, check, may_realize=...):  # -> TensorBox:
    ...

fallback__unsafe_masked_index = ...
fallback__unsafe_masked_index_put_accumulate = ...

@make_pointwise
def clamp(a, min, max):  # -> Any:
    ...
@register_lowering(aten.as_strided_scatter, type_promotion_kind=None)
def as_strided_scatter(self, src, size, stride, storage_offset=...):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
@register_lowering(aten.scatter, type_promotion_kind=None)
def scatter(x, dim: int, index, src, **kwargs):  # -> TensorBox:
    ...
def scatter_fallback(
    op_overload: torch._ops.OpOverload,
    self,
    dim: int,
    index,
    src,
    *,
    reduce: str | None = ...,
    include_self: bool = ...,
):  # -> None:
    ...
@register_lowering(aten.scatter_, type_promotion_kind=None)
def scatter_(self, dim: int, index, src, *, reduce: str | None = ...):  # -> TensorBox:
    ...
@register_lowering(aten.scatter_add, type_promotion_kind=None)
def scatter_add(x, dim: int, index, src):  # -> TensorBox:
    ...
@register_lowering(aten.scatter_add_, type_promotion_kind=None)
def scatter_add_(x, dim: int, index, src):  # -> TensorBox:
    ...
@register_lowering(aten.scatter_reduce, type_promotion_kind=None)
def scatter_reduce(x, dim: int, index, src, reduction_type, **kwargs):  # -> TensorBox:
    ...
@register_lowering(aten.scatter_reduce_, type_promotion_kind=None)
def scatter_reduce_(self, dim: int, index, src, reduce, *, include_self: bool = ...):  # -> TensorBox:
    ...
def upsample_nearestnd(
    x, output_size, scales_x: tuple[float | None, ...], n: int = ..., exact: bool = ...
):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
@register_lowering(aten.upsample_nearest1d.default)
def upsample_nearest1d(x, output_size, scales: float | None = ...):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
@register_lowering(aten.upsample_nearest2d.default)
def upsample_nearest2d(
    x, output_size, scales_h: float | None = ..., scales_w: float | None = ...
):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
@register_lowering(aten.upsample_nearest3d.default)
def upsample_nearest3d(
    x, output_size, scales_d: float | None = ..., scales_h: float | None = ..., scales_w: float | None = ...
):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
@register_lowering(prims.rev.default)
def rev(x, dims):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
def inplace_constant_pad_nd(x: TensorBox, padding: Sequence[int], fill_value: float) -> TensorBox | None: ...
@register_lowering(aten.constant_pad_nd, type_promotion_kind=None)
def constant_pad_nd(x, padding, fill_value=...):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
def range_mask_low(i: sympy.Expr, low: sympy.Expr | int):  # -> Any:
    ...
def range_mask_high(i: sympy.Expr, high: sympy.Expr):  # -> Any:
    ...
def range_mask(i: sympy.Expr, high: sympy.Expr, low: sympy.Expr):  # -> Any:
    ...
def constant_boundary_condition(x, fill_value, padding=..., pad_fill_value=..., dim=...):  # -> Callable[..., Any]:
    ...
def pooling_size(
    x, i, kernel_size, stride, padding, ceil_mode, *, dilation=...
):  # -> tuple[FloorDiv | Any, Any | Literal[False]]:
    ...
def should_fallback_max_pool_with_indices(kernel_size, *, n_dim):  # -> Any:
    ...
def max_pool_checks(
    x, kernel_size, stride, padding, dilation, n_dim, *, assert_fallback=...
):  # -> tuple[Sequence[int], Sequence[int], Sequence[int], Sequence[int], Any]:
    ...
@register_lowering(aten.max_pool2d_with_indices, type_promotion_kind=None)
def max_pool2d_with_indices(
    x, kernel_size, stride=..., padding=..., dilation=..., ceil_mode=...
):  # -> tuple[TensorBox | ShapeAsConstantBuffer, TensorBox | ShapeAsConstantBuffer]:
    ...
@register_lowering(aten.max_pool3d_with_indices, type_promotion_kind=None)
def max_pool3d_with_indices(
    x, kernel_size, stride=..., padding=..., dilation=..., ceil_mode=...
):  # -> tuple[TensorBox | ShapeAsConstantBuffer, TensorBox | ShapeAsConstantBuffer]:
    ...

fallback_max_pool2d_with_indices_backward = ...

@register_lowering(aten.max_pool2d_with_indices_backward, type_promotion_kind=None)
def max_pool2d_with_indices_backward(
    grad_output, x, kernel_size, stride, padding, dilation, ceil_mode, indices
):  # -> PyTree | TensorBox | ShapeAsConstantBuffer:
    ...
def pad_adaptive_loader(x, pad_val=...):  # -> Callable[..., Any]:
    ...
def compute_indices_adaptive_pooling(
    start_index, end_index, h_in, w_in, h_out, w_out
):  # -> tuple[partial[Any], partial[Any], partial[Any], partial[Any]]:
    ...

fallback_adaptive_avg_pool2d = ...
fallback_adaptive_max_pool2d = ...

@register_lowering(aten.adaptive_max_pool2d)
def adaptive_max_pool2d(
    x, output_size
):  # -> tuple[TensorBox | ShapeAsConstantBuffer, TensorBox | ShapeAsConstantBuffer] | PyTree:
    ...
@register_lowering(aten.fractional_max_pool2d)
def fractional_max_pool2d(
    x, kernel_size, output_size, random_samples
):  # -> tuple[TensorBox, TensorBox | ShapeAsConstantBuffer]:
    ...
@register_lowering(aten.fractional_max_pool3d)
def fractional_max_pool3d(
    x, kernel_size, output_size, random_samples
):  # -> tuple[TensorBox, TensorBox | ShapeAsConstantBuffer]:
    ...
@register_lowering(aten.upsample_nearest2d_backward.default)
def upsample_nearest2d_backward(
    x, output_size=..., input_size=..., scales_h=..., scales_w=...
):  # -> PyTree | TensorBox | ShapeAsConstantBuffer:
    ...

fallback_avg_pool2d = ...
fallback_avg_pool3d = ...

@register_lowering(aten.avg_pool2d, type_promotion_kind=None)
def avg_pool2d(
    x, kernel_size, stride=..., padding=..., ceil_mode=..., count_include_pad=..., divisor_override=...
):  # -> PyTree | TensorBox | ShapeAsConstantBuffer:
    ...
@register_lowering(aten.avg_pool3d, type_promotion_kind=None)
def avg_pool3d(
    x, kernel_size, stride=..., padding=..., ceil_mode=..., count_include_pad=..., divisor_override=...
):  # -> PyTree | TensorBox | ShapeAsConstantBuffer:
    ...

fallback_avg_pool2d_backward = ...

@register_lowering(aten.avg_pool2d_backward, type_promotion_kind=None)
def avg_pool2d_backward(
    grad_output, x, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override=...
):  # -> PyTree | TensorBox | ShapeAsConstantBuffer:
    ...

fallback_avg_pool3d_backward = ...

@register_lowering(aten.avg_pool3d_backward, type_promotion_kind=None)
def avg_pool3d_backward(
    grad_output, x, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override=...
):  # -> PyTree | TensorBox | ShapeAsConstantBuffer:
    ...
def make_reduction(
    reduction_type: ReductionType, override_return_dtype=...
):  # -> Callable[..., TensorBox | ShapeAsConstantBuffer]:
    ...
@register_lowering(aten.mean)
def mean(x, axis=..., keepdim=..., *, dtype=...):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
def var_mean_sum_(
    x, axis, correction, keepdim, return_mean
):  # -> tuple[Any | TensorBox | ShapeAsConstantBuffer] | tuple[Any | TensorBox | ShapeAsConstantBuffer, TensorBox | ShapeAsConstantBuffer | Any]:
    ...
def use_two_step_variance(x, axis, keepdim):  # -> Literal[False]:
    ...
def var_mean_welford_(
    x, axis, *, correction, keepdim, return_mean
):  # -> tuple[Any | TensorBox | ShapeAsConstantBuffer, TensorBox | ShapeAsConstantBuffer] | tuple[Any | TensorBox | ShapeAsConstantBuffer]:
    ...
def var_mean_helper_(
    x, *, axis, correction, keepdim, return_mean
):  # -> TensorBox | ShapeAsConstantBuffer | tuple[TensorBox | ShapeAsConstantBuffer | Any, ...]:
    ...
@register_lowering([aten.var, prims.var])
def var_(
    x, axis=..., *, correction=..., keepdim=...
):  # -> TensorBox | ShapeAsConstantBuffer | tuple[TensorBox | ShapeAsConstantBuffer | Any, ...]:
    ...
@register_lowering(aten.var_mean)
def var_mean(
    x, axis=..., *, correction=..., keepdim=...
):  # -> TensorBox | ShapeAsConstantBuffer | tuple[TensorBox | ShapeAsConstantBuffer | Any, ...]:
    ...
def pow_recursive(x, y, dtype):  # -> Any:
    ...
@make_pointwise
def pow_native(a, b):  # -> Any:
    ...

fallback_pow_tensor_tensor = ...
fallback_pow_scalar = ...
fallback_pow_tensor_scalar = ...

@register_lowering(aten.pow, broadcast=True)
def pow(a, b):  # -> TensorBox | ShapeAsConstantBuffer | PyTree:
    ...
def mutate_to(changed, val, unsafe_alias=...):  # -> TensorBox:
    ...
@register_lowering(aten.fill_)
def fill_(x, fill_value):  # -> TensorBox:
    ...
@register_lowering(aten.copy_, type_promotion_kind=None)
def copy_(dst, src, non_blocking=...):  # -> TensorBox:
    ...
@make_pointwise
def floordiv(a, b):  # -> Any:
    ...
@make_pointwise
def truncdiv(a, b):  # -> Any:
    ...
@register_lowering(aten.div, broadcast=True)
def div_mode(a, b, rounding_mode=...):  # -> Any | TensorBox | ShapeAsConstantBuffer:
    ...
@register_lowering([aten.mul], broadcast=True)
def mul(a, b): ...
def get_constant_value(x: ir.IRNode) -> ir.Constant | None: ...
@register_lowering([prims.div], broadcast=True)
def div_prim(a, b):  # -> Any | TensorBox | ShapeAsConstantBuffer:
    ...
@register_lowering(
    [aten.true_divide, aten.div.Tensor],
    broadcast=True,
    type_promotion_kind=ELEMENTWISE_TYPE_PROMOTION_KIND.INT_TO_FLOAT,
)
def div(a, b):  # -> Any | TensorBox | ShapeAsConstantBuffer:
    ...
@register_lowering([aten.fmod, prims.fmod], broadcast=True)
def fmod(a, b):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
@register_lowering([aten.sum, prims.sum])
def sum_(x, axis=..., keepdims=..., *, dtype=...):  # -> TensorBox | ShapeAsConstantBuffer:
    ...

fallback_cumsum = ...
fallback_cumprod = ...
fallback_logcumsumexp = ...
fallback_cummax = ...
fallback_cummin = ...

@register_lowering(aten.cumsum)
def cumsum(x, axis=..., dtype=...):  # -> TensorBox | ShapeAsConstantBuffer | PyTree:
    ...
@register_lowering(aten.cumprod)
def cumprod(x, axis=..., dtype=...):  # -> TensorBox | ShapeAsConstantBuffer | PyTree:
    ...
@register_lowering(aten.logcumsumexp)
def logcumsumexp(x, dim):  # -> TensorBox | ShapeAsConstantBuffer | PyTree:
    ...
@register_lowering(aten.cummax, type_promotion_kind=None)
def cummax(
    x, axis=...
):  # -> tuple[TensorBox | ShapeAsConstantBuffer, Any] | PyTree | tuple[TensorBox | ShapeAsConstantBuffer, TensorBox | ShapeAsConstantBuffer | None]:
    ...
@register_lowering(aten.cummin, type_promotion_kind=None)
def cummin(
    x, axis=...
):  # -> tuple[TensorBox | ShapeAsConstantBuffer, Any] | PyTree | tuple[TensorBox | ShapeAsConstantBuffer, TensorBox | ShapeAsConstantBuffer | None]:
    ...
@register_lowering(aten.prod)
def prod(x, axis=..., keepdims=..., *, dtype=...):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
@register_lowering(aten.any)
def reduce_any(x, dim=..., keepdim=...):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
@register_lowering(aten.max, type_promotion_kind=None)
def reduce_max(
    x, dim=..., keepdim=...
):  # -> tuple[TensorBox | ShapeAsConstantBuffer, TensorBox | ShapeAsConstantBuffer] | TensorBox | ShapeAsConstantBuffer:
    ...
@register_lowering(aten.min, type_promotion_kind=None)
def reduce_min(
    x, dim=..., keepdim=...
):  # -> tuple[TensorBox | ShapeAsConstantBuffer, TensorBox | ShapeAsConstantBuffer] | TensorBox | ShapeAsConstantBuffer:
    ...

reduce_amax = ...
reduce_amin = ...
reduce_argmax = ...
reduce_argmin = ...
add = ...
sort_fallback = ...

@register_lowering(aten.sort.stable, type_promotion_kind=None)
def sort_stable(
    x, *, stable=..., dim=..., descending=...
):  # -> tuple[TensorBox | ShapeAsConstantBuffer, TensorBox | ShapeAsConstantBuffer] | PyTree | tuple[TensorBox | ShapeAsConstantBuffer, TensorBox | ShapeAsConstantBuffer | Any]:
    ...
@register_lowering(aten.sort.default, type_promotion_kind=None)
def sort(
    x, dim=..., descending=...
):  # -> tuple[TensorBox | ShapeAsConstantBuffer, TensorBox | ShapeAsConstantBuffer] | PyTree | tuple[TensorBox | ShapeAsConstantBuffer, TensorBox | ShapeAsConstantBuffer | Any]:
    ...
def register_pointwise_numeric(op, name=..., triton_fallback=...):  # -> Callable[..., Any]:
    ...
def register_pointwise_numeric_ldf64(op: torch._ops.OpOverloadPacket):  # -> Callable[..., Any]:
    ...

rsqrt = ...
exp = ...
exp2 = ...
expm1 = ...
relu = ...
sigmoid = ...
sqrt = ...
square = ...
sub = ...
abs = ...
bitwise_and = ...
bitwise_left_shift = ...
bitwise_not = ...
bitwise_or = ...
bitwise_right_shift = ...
bitwise_xor = ...
erf = ...
logical_and = ...
logical_not = ...
logical_or = ...
logical_xor = ...
maximum = ...
minimum = ...
neg = ...
abs = ...
reciprocal = ...
sign = ...
gt = ...
foreach_add_list = ...
foreach_add_scalar = ...
foreach_mul_list = ...
foreach_mul_scalar = ...
foreach_div_list = ...
foreach_div_scalar = ...
foreach_copy = ...

def register_foreach_inplace(aten_op, outplace_aten_op, outplace_op):  # -> None:
    ...
def register_inplace(aten_op, outplace_op):  # -> Callable[..., TensorBox | Any]:
    ...
@register_lowering(aten.sym_constrain_range)
def sym_constrain_range(a, min=..., max=...):  # -> None:
    ...
@register_lowering(aten.sym_size.int)
def sym_size(a, dim): ...
@register_lowering(aten.sym_stride.int)
def sym_stride(a, dim): ...
@register_lowering(aten.sym_numel)
def sym_numel(a): ...
@register_lowering(torch.sym_sum)
def sym_sum(args): ...
@register_lowering(aten._foobar)
def foobar(self, *args, **kwargs): ...
@register_lowering(torch.ops.inductor.resize_storage_bytes_)
def resize_storage_bytes_(variable, new_size): ...
@register_lowering(torch.ops.aten.set_.source_Tensor)
def set__source_tensor(self, source_tensor):  # -> TensorBox | ShapeAsConstantBuffer:
    ...

if hasattr(torch.ops.fsdp, "copy_"):
    @register_lowering(torch.ops.fsdp.copy_.default)
    def fsdp_copy_(dst, src):  # -> TensorBox:
        ...

@register_lowering(torch.ops.aten.resize)
def resize(x, size, *, memory_format=...):  # -> TensorBox | ShapeAsConstantBuffer:
    ...
@register_lowering(triton_kernel_wrapper_mutation)
def triton_kernel_wrap_(
    *, kernel_idx, constant_args_idx, grid, tma_descriptor_metadata, kwargs
):  # -> dict[Any, TensorBox]:
    ...
@register_lowering(torch.ops.higher_order.cond, type_promotion_kind=None)
def cond(pred, true_fn, false_fn, operands):  # -> list[TensorBox | ShapeAsConstantBuffer]:
    ...
@register_lowering(torch.ops.higher_order.while_loop, type_promotion_kind=None)
def while_loop(
    cond_fn, body_fn, carried_inputs, additional_inputs, stack_output=...
):  # -> list[TensorBox | ShapeAsConstantBuffer]:
    ...
@register_lowering(torch.ops.higher_order.invoke_subgraph, type_promotion_kind=None)
def invoke_subgraph(
    subgraph_fn: ir.Subgraph, identifier: str, *operands
):  # -> list[TensorBox | ShapeAsConstantBuffer]:
    ...
@register_lowering(torch._higher_order_ops.invoke_quant, type_promotion_kind=None)
def invoke_quant_tracer(subgraph_fn: ir.Subgraph, *operands, scheme=...):  # -> Any | None:
    ...
@register_lowering(associative_scan_op, type_promotion_kind=None)
def associative_scan(combine_fn: ir.Subgraph, xs, additional_inputs: tuple[torch.Tensor]): ...
@register_lowering(torch.ops.higher_order.with_effects, type_promotion_kind=None)
def with_effects(token, op, *args, **kwargs):  # -> tuple[Any] | tuple[Any, PyTree] | tuple[Any, *tuple[Any, ...]]:
    ...
@register_lowering(inductor_prims.prepare_softmax_online, type_promotion_kind=None)
def prepare_softmax_online(x, dim):  # -> tuple[TensorBox | ShapeAsConstantBuffer, TensorBox | ShapeAsConstantBuffer]:

    ...
@contextlib.contextmanager
def force_fallback(op: torch._ops.OpOverload):  # -> Generator[None, Any, None]:

    ...
