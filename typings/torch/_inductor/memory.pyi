import dataclasses
from collections.abc import Callable
from typing import TypedDict

from torch.utils._ordered_set import OrderedSet

from .scheduler import BaseSchedulerNode, SchedulerBuffer

torch_log = ...

@dataclasses.dataclass
class PeakMemoryResult:
    """PeakMemoryResult(order: 'list[BaseSchedulerNode]', peak_memory: 'int', method: 'str')"""

    order: list[BaseSchedulerNode]
    peak_memory: int
    method: str

@dataclasses.dataclass
class MemoryPlanningInfoForBuffer:
    """MemoryPlanningInfoForBuffer(size_alloc: 'int' = 0, size_free: 'int' = 0, succ_nodes: 'OrderedSet[BaseSchedulerNode]' = <factory>)"""

    size_alloc: int = ...
    size_free: int = ...
    succ_nodes: OrderedSet[BaseSchedulerNode] = ...

@dataclasses.dataclass
class MemoryPlanningInfoForNode:
    """MemoryPlanningInfoForNode(index: 'int' = 0, size: 'int' = 0, pred_buffers: 'OrderedSet[Union[SchedulerBuffer, FreeableInputBuffer]]' = <factory>, pred_nodes: 'OrderedSet[BaseSchedulerNode]' = <factory>, succ_nodes: 'OrderedSet[BaseSchedulerNode]' = <factory>)"""

    index: int = ...
    size: int = ...
    pred_buffers: OrderedSet[SchedulerBuffer | FreeableInputBuffer] = ...
    pred_nodes: OrderedSet[BaseSchedulerNode] = ...
    succ_nodes: OrderedSet[BaseSchedulerNode] = ...

@dataclasses.dataclass
class FreeableInputBuffer:
    """FreeableInputBuffer(name: 'str', mpi_buffer: 'MemoryPlanningInfoForBuffer' = <factory>)"""

    name: str
    mpi_buffer: MemoryPlanningInfoForBuffer = ...
    def get_name(self) -> str: ...
    def __hash__(self) -> int: ...

def get_freeable_input_buf(
    nodes: list[BaseSchedulerNode], graph_inputs: OrderedSet[str]
) -> dict[str, FreeableInputBuffer]:
    """
    Create and keep track of all input buffers that can be freed during the program

    Returns:
        A dictionary containing all freeable input buffers, keyed by their names.
    """

def compute_size_for_scheduler_buffer(name_to_buf: dict[str, SchedulerBuffer]) -> dict[str, tuple[int, int]]:
    """
    Compute the size of each scheduler buffer, including (1) memory allocated when
    it is created and (2) memory deallocated when it is freed.

    We specially handle the case of MultiOutputLayout.
    Consider the following case:
        buf0 = some_ops_with_multi_outputs(...)
        buf1 = buf0[0] # assume 10 bytes
        buf2 = buf0[1] # assume 20 bytes
    In such cases,
        buf0: at creation, 30 bytes allocated, when deleted, 0 bytes freed
        buf1: at creation, 0 bytes allocated, when deleted, 10 bytes freed
        buf2: at creation, 0 bytes allocated, when deleted, 20 bytes freed

    When an operation mutates a buffer in-place, the scheduler creates a new buffer name
    to track the "before" and "after" states, even though they share the same memory.

    The mutated buffer represents a rename with zero allocation and deallocation cost.
    During dependency tracking, we transfer dependencies from the mutated name back to
    the original buffer, ensuring the original memory is only freed when all aliases
    are done.

    This handles cases where a buffer has multiple non-overlapping aliases - rather than
    trying to assign free costs to individual aliases, we forward all alias dependencies
    to the original buffer.

    Consider:
        buf0 = op0()
        buf1 = mutation_op_(buf0)
        del buf0
        ...
        op(buf1)
        del buf1

    The only memory events are the creation prior to op0, and the deletion following buf1.

    Returns:
        A dictionary mapping a scheduler buffer to a tuple of (size_alloc, size_free).
    """

def assign_memory_planning_info_for_scheduler_buffers(
    nodes: list[BaseSchedulerNode], name_to_buf: dict[str, SchedulerBuffer]
) -> None:
    """
    For each SchedulerBuffer, assign its size info and successor nodes.
    A buffer's successor nodes determines when a buffer can be freed.
    """

def assign_memory_planning_info_for_scheduler_nodes(
    nodes: list[BaseSchedulerNode],
    name_to_fused_node: dict[str, BaseSchedulerNode],
    name_to_buf: dict[str, SchedulerBuffer],
    name_to_freeable_input_buf: dict[str, FreeableInputBuffer],
) -> None:
    """Assign to each scheduler node its predecessor and successor nodes."""

@dataclasses.dataclass
class BufferInfo:
    """BufferInfo(buffer: 'Union[SchedulerBuffer, FreeableInputBuffer]', size_alloc: 'int', size_free: 'int', start_step: 'int', end_step: 'int')"""

    buffer: SchedulerBuffer | FreeableInputBuffer
    size_alloc: int
    size_free: int
    start_step: int
    end_step: int

def compute_memory_timeline(
    nodes: list[BaseSchedulerNode],
    name_to_freeable_input_buf: dict[str, FreeableInputBuffer],
    graph_outputs: OrderedSet[str],
) -> tuple[
    list[BufferInfo], dict[BaseSchedulerNode, int], dict[FreeableInputBuffer | SchedulerBuffer, BaseSchedulerNode]
]:
    """
    Compute buffer allocation and deallocation sizes and map their
    lifetime to the node schedule
    """

def estimate_peak_memory(
    nodes: list[BaseSchedulerNode],
    name_to_freeable_input_buf: dict[str, FreeableInputBuffer],
    graph_outputs: OrderedSet[str],
) -> tuple[int, list[int]]:
    """
    Given a list of nodes in their execution order, estimate the peak memory, by
    keeping track of the liveliness of SchedulerBuffers and FreeableInputBuffers.

    Returns:
        int: peak memory
        List[int]: memory usage at each node (or each step).
    """

@dataclasses.dataclass
class SNodeMemory:
    """SNodeMemory(size_alloc: 'int', size_free: 'int')"""

    size_alloc: int
    size_free: int

def estimate_peak_memory_allocfree(
    nodes: list[BaseSchedulerNode],
    name_to_freeable_input_buf: dict[str, FreeableInputBuffer],
    graph_outputs: OrderedSet[str],
) -> tuple[
    int,
    list[tuple[int, int]],
    dict[BaseSchedulerNode, SNodeMemory],
    dict[FreeableInputBuffer | SchedulerBuffer, BaseSchedulerNode],
]:
    """
    Alternative version of estimate_peak_memory, that respects the fact,
    that every SchedulerNode has multiple phases:
    1. alloc ( outputs )
    2. run_kernel
    3. dealloc last_use buffers
    estimate_peak_memory collapses memory into one value: size_alloc - size_free
    While peak memory happens after alloc.

    Duplicating the code to not migrate all callsites at once,
    In future usages of estimate_peak_memory will migrate to this version.
    """

def topological_sort_lpmf(
    nodes: list[BaseSchedulerNode],
    name_to_freeable_input_buf: dict[str, FreeableInputBuffer],
    name_to_buf: dict[str, SchedulerBuffer],
    graph_outputs: OrderedSet[str],
) -> list[BaseSchedulerNode]:
    """
    A bfs-based greedy topological order. LPMF stands for "Least Peak Memory First".

    The idea is from this paper:
    Buffer memory optimization for video codec application modeled in Simulink
    https://www.cs.york.ac.uk/rts/docs/DAC-1964-2006/PAPERS/2006/DAC06/PDFFILES/P0689.PDF

    The algorithm maintains the max memory so far.
    At every iteration, for each scheduleable node, it computes:
        - how much memory needs to be allocated for the output buffers of this node;
        - how much memory can be freed as a result of executing this node.
    This gives us two values for each node:
        (1) mem1: memory during the execution of the node;
        (2) mem2: memory after executing the node, after some input buffers are freed.
    The greedy approach select as follows:
        (i) if there are nodes whose mem1 values are below the max memory so far,
            then pick the node with the lowest mem2 value;
        (ii) otherwise, pick the one with the lowest mem1 value.
    """
    class NodeInfo(TypedDict): ...

    class BufferInfo(TypedDict):
        """BufferInfo(buffer: 'Union[SchedulerBuffer, FreeableInputBuffer]', size_alloc: 'int', size_free: 'int', start_step: 'int', end_step: 'int')"""

def topological_sort_bfs(nodes: list[BaseSchedulerNode]) -> list[BaseSchedulerNode]:
    """
    A BFS topological sort that selects nodes whose dependencies are executed the
    earliest. This follows a FIFO idea. Specifically, at every iteration, for each node
    that is schedulable, we gather the order in which its predecessor nodes are executed,
    and this sorted list of execution orders of predecessor nodes defines the priority.
    We select the node whose predecessors nodes are executed the earliest. The FIFO
    idea aims to reduce the liveness duration of buffers created.
    """
    class NodeInfo(TypedDict): ...

    @dataclasses.dataclass
    class NodeWithPriority: ...

def topological_sort_dfs(nodes: list[BaseSchedulerNode]) -> list[BaseSchedulerNode]:
    """
    This is a DFS topological sort. The setup is similar to `topological_sort_schedule`
    in scheduler.py. The difference is the order nodes are visited in the outer loop.
    In `topological_sort_schedule`, nodes are visited in their original order.
    In this function, nodes are visited based on their priority -- for each node, we
    compute the total memory of all buffers it reads from or writes to, and we visit
    the nodes in ascending order of this priority.
    """

def validate_graph_acyclic(nodes: list[BaseSchedulerNode]) -> None:
    """
    Validate that the graph is acyclic by checking predecessor relationships.

    Raises:
        RuntimeError: If a cycle is detected in the graph
    """

def validate_unique_buffer_names(
    nodes: list[BaseSchedulerNode],
    name_to_buf: dict[str, SchedulerBuffer],
    name_to_freeable_input_buf: dict[str, FreeableInputBuffer],
) -> None:
    """
    Validate that for each node's output buffer, the name_to_buf mapping is correct.
    For each output buffer buf, we should have name_to_buf[buf.get_name()] == buf.
    Also validate that no buffer names overlap with freeable input buffer names.

    Raises:
        RuntimeError: If buffer name mapping is incorrect or names overlap
    """

def prepare_planning_info(
    nodes: list[BaseSchedulerNode],
    name_to_buf: dict[str, SchedulerBuffer],
    name_to_fused_node: dict[str, BaseSchedulerNode],
    graph_inputs: OrderedSet[str],
    graph_outputs: OrderedSet[str],
) -> tuple[int, dict[str, FreeableInputBuffer]]:
    """
    Prepare planning info. As nodes are scheduled one at a time, these help
    keep track of when a buffer can be freed, and when a node can be scheduled

    Returns:
        int: peak memory estimation
        dict[str, FreeableInputBuffer]: name to freeable input buffer
    """

def reorder_for_peak_memory(
    nodes: list[BaseSchedulerNode],
    name_to_buf: dict[str, SchedulerBuffer],
    name_to_fused_node: dict[str, BaseSchedulerNode],
    graph_inputs: OrderedSet[str],
    graph_outputs: OrderedSet[str],
    methods: list[Callable[..., list[BaseSchedulerNode]]] = ...,
) -> list[BaseSchedulerNode]:
    """
    Try a few heuristics based topological sort algorithms, and pick the one whose
    resulting topological order has the lowest peak memory estimation.
    """
