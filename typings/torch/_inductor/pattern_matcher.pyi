"""
# Inductor Pattern Matcher

The pattern matcher enables search/replace within an FX graph.

The main entrypoint to the pattern matcher is register_replacement(). Given a
search function and a replacement function this will register a replacement with
a pass (such as torch._inductor.fx_passes.joint_graph.patterns).

Internally the pattern matcher represents patterns as a graph (a DAG). Creating
new patterns manually as a graph is cumbersome and error-prone so the standard
way to create patterns (using register_replacement()) is to provide a search
function and a replacement function which is traced and converted into a graph.

Because the search functions are built somewhat generic (they tend to ignore
tensor sizes, for example) register_replacement() allows you to specify an
`extra_check` function which performs additional checks to verify that the
matched pattern fully matches before returning it.

## Precompiled Patterns

New patterns are added using register_replacement(). Patterns added in this way
can have a compile-time overhead because they need to be traced before
use. Patterns can be precompiled and added using gen_register_replacement()
instead. To do this you call gen_register_replacement() instead of
register_replacement(). The arguments are the same except for an additional
unique name which is used as a lookup key.

## Internals

The match DAG is represented by a graph of `PatternExpr` nodes. Each PatternExpr
implements a `_match` method which returns either a `Match` object for a
successful match or a `FailedMatch` object for a failure to match.
"""

import dataclasses
import functools
from abc import ABC, abstractmethod
from collections.abc import Callable, Generator, Iterable, Mapping, Sequence
from typing import Any, Protocol, TypeIs, TypeVar

import torch
import torch.fx
import torch.utils._pytree as pytree
from torch.utils._ordered_set import OrderedSet

from .._functorch import config as functorch_config

log = ...
aten = ...
prims = ...
type Constant = Any
type NodeOrConstant = Constant | torch.fx.Node
backend = ...

class SearchFn(Protocol):
    __name__: str
    def __call__(self, *args: Any, **kwargs: Any) -> Any: ...

class ReplaceFn(Protocol):
    def __call__(self, *args: Any, **kwargs: Any) -> Any: ...

class TraceFn(Protocol):
    def __call__(self, fn: SearchFn | ReplaceFn, *args: Any, **kwargs: Any) -> torch.fx.GraphModule: ...

T = TypeVar("T")
type FnsType = torch.fx.node.Target | str

class Multiple:
    def __init__(self) -> None: ...

MULTIPLE = ...

class Match:
    """
    Represents a successfully matched pattern.

    The `Match` object is returned to represent a successfully matched
    pattern. Included in the Match are the pattern that was matched, the graph
    nodes matched, and any args that were used during the matching.

    The args and kwargs are specific to the type of pattern that was matched and
    provide hints about what was matched.
    """

    pattern: PatternExpr
    args: list[Any]
    kwargs: dict[str, Any]
    nodes: list[torch.fx.Node]
    targets: dict[_TargetExpr, torch.fx.node.Target]
    ctx: MatchContext
    replacement_graph: torch.fx.GraphModule | None
    def __init__(
        self,
        ctx: MatchContext,
        pattern: PatternExpr,
        args: Sequence[Any] | None = ...,
        kwargs: dict[str, Any] | None = ...,
    ) -> None: ...
    @property
    def graph(self) -> torch.fx.Graph: ...
    def extend(self, other: Match) -> None: ...
    def bundle(self) -> Match: ...
    def erase_nodes(self) -> None: ...
    def output_nodes(self) -> list[torch.fx.Node | None]: ...
    def output_node(self) -> torch.fx.Node: ...
    def replace_with_graph(self, replacement_graph: torch.fx.Graph, args: Sequence[Any]) -> None: ...
    def replace_by_example(
        self,
        replacement_fn: ReplaceFn,
        args: Sequence[Any],
        trace_fn: TraceFn | None = ...,
        run_functional_passes: bool = ...,
    ) -> None:
        """
        Replace with a graph generated by tracing the replacement_fn.

        Args:
            run_functional_passes (bool). If we should run passes that
                assume functional IR (like DCE, remove_noop_ops), on the
                replacement graph.
        """

class FailedMatch(RuntimeError):
    """
    Represents a unsuccessful match.

    The `FailedMatch` object is returned to represent a failure to match a
    pattern.
    """

    format_string: str
    def __init__(self, format_string: str, *args: Any, **kwargs: Any) -> None: ...
    def __bool__(self) -> bool: ...

type MatchResult = Match | FailedMatch

def is_match(m: MatchResult) -> TypeIs[Match]:
    """
    TypeIs cannot act on `self`. Thus this function exists to let mypy
    recognize FailedMatch.__bool__ as a TypeIs.
    """

class MatchContext:
    """Internal state needed while running PatternExpr._match()."""

    outputs: list[PatternExpr | None]
    pattern_to_node: dict[PatternExpr, torch.fx.Node | None]
    graph: torch.fx.Graph
    exclusive_node_set: list[NodeOrConstant]
    def __init__(
        self,
        outputs: list[PatternExpr | None],
        pattern_to_node: dict[PatternExpr, torch.fx.Node] | None = ...,
        *,
        graph: torch.fx.Graph,
    ) -> None: ...
    def match(self, pattern: PatternExpr, node: NodeOrConstant) -> MatchResult:
        """wrapper to check reused nodes in patterns"""
    def filter_multi_user_patterns(self) -> dict[PatternExpr, torch.fx.Node]: ...

class PatternExpr(ABC):
    """Base class for types of patterns."""
    def match(self, node: torch.fx.Node) -> MatchResult: ...
    def has_multiple_users(self) -> bool: ...
    def find_anchor_nodes(
        self, ctx: MatchContext, searched: OrderedSet[torch.fx.Node]
    ) -> Generator[torch.fx.Node | None]: ...
    def pattern_eq(self, other: Any) -> bool:
        """
        Compare two `PatternExpr`s and return true if they are the
        same. Note this is NOT matching a pattern - it is comparing the pattern
        structures (for debugging).
        """

class Arg(PatternExpr):
    """
    Capture an arg which will become an input to the handler.  Args are
    passed in depth first order.
    """

class Ignored(PatternExpr):
    """Match an arg, but don't pass it to handler"""
    def pretty_print(self, pp: PatternPrettyPrinter) -> str: ...

class KeywordArg(PatternExpr):
    """Capture a kwarg which will become an input to the handler."""
    def __init__(self, name: str) -> None: ...
    def pattern_eq(self, other: Any) -> bool: ...

class ExclusiveKeywordArg(PatternExpr):
    """Capture a kwarg which will become an input to the handler."""

    name: str
    def __init__(self, name: str) -> None: ...
    def pattern_eq(self, other: Any) -> bool: ...

class _TargetExpr(PatternExpr):
    """Base class for filtering match by node.target"""

    fns: list[FnsType]
    fns_set: OrderedSet[FnsType]
    def __init__(self, fns: FnsType | Sequence[FnsType], users: Multiple | int = ...) -> None: ...
    @property
    @abstractmethod
    def op(self) -> str: ...
    def fns_repr(self) -> str: ...
    def has_multiple_users(self) -> bool: ...
    def find_anchor_nodes(
        self, ctx: MatchContext, searched: OrderedSet[torch.fx.Node]
    ) -> Generator[torch.fx.Node | None]: ...
    def pattern_eq(self, other: Any) -> bool: ...

type _SimpleSpec = tuple[Any, ...]

class _TargetArgsExpr(_TargetExpr):
    """Base class for filtering match by node.{target,args,kwargs}"""
    def __init__(
        self, fns: torch.fx.node.Target | str | Sequence[Any], *args: Any, _users: int | Multiple = ..., **kwargs: Any
    ) -> None: ...
    @staticmethod
    def simple_flatten(
        args: Sequence[Any], kwargs: Mapping[Any, Any]
    ) -> tuple[Sequence[Any], _SimpleSpec | pytree.TreeSpec]: ...
    @staticmethod
    def pytree_flatten(
        args: Sequence[Any], kwargs: Mapping[Any, Any]
    ) -> tuple[Sequence[Any], _SimpleSpec | pytree.TreeSpec]: ...
    def pretty_print(self, pp: PatternPrettyPrinter) -> str: ...
    def find_anchor_nodes(
        self, ctx: MatchContext, searched: OrderedSet[torch.fx.Node]
    ) -> Generator[torch.fx.Node | None]:
        """
        This is used when we are matching a pattern with multiple outputs.
        There is a partial match (stored in ctx) and we want to walk
        this pattern to find a connection to an already-matched node.

        Yields candidate nodes that `self._match` might like.
        """
    def pattern_eq(self, other: Any) -> bool: ...

class CallFunction(_TargetArgsExpr):
    """Matches a call_function node in the FX graphs: `fns[i](*args, **kwargs)`"""

    op = ...

class CallMethod(_TargetArgsExpr):
    """Matches a call_method node in the FX graphs: `fns[i].method(*args, **kwargs)`"""

    op = ...

class CallModule(_TargetArgsExpr):
    """Matches a call_module node in the FX graphs: `module(*args, **kwargs)`"""

    op = ...

class _TargetExprVarArgs(_TargetExpr):
    """Matches a call_function node with any arguments which are passed into the pattern"""

class CallFunctionVarArgs(_TargetExprVarArgs):
    op = ...

class CallMethodVarArgs(_TargetExprVarArgs):
    op = ...

class CallModuleVarArgs(_TargetExprVarArgs):
    op = ...

class ListOf(PatternExpr):
    """Matches a repeated pattern"""
    def __init__(self, pattern: PatternExpr, partial: bool = ...) -> None: ...
    def pattern_eq(self, other: Any) -> bool: ...

class MultiOutputPattern(PatternExpr):
    outputs: list[PatternExpr | None]
    def __init__(self, outputs: Sequence[PatternExpr | None]) -> None: ...
    @property
    def fns(self) -> Callable[..., Any] | str | Sequence[Any]: ...
    def pretty_print(self, pp: PatternPrettyPrinter) -> str: ...
    def match(self, node: torch.fx.Node) -> MatchResult: ...
    def pattern_eq(self, other: Any) -> bool: ...

class RepeatedExpr(PatternExpr):
    """Checks for a repeated pattern. Useful for repeated operations after a node such as `split` or `unbind`"""
    def __init__(self, inner_pattern: _TargetExpr) -> None: ...
    @property
    def fns(self) -> Sequence[FnsType]: ...
    def pattern_eq(self, other: Any) -> bool: ...

class PatternPrettyPrinter:
    """
    Serializes Patterns to executable python.
    XXX: currently only used and tested for fuse attention patterns. May not cover
    all patterns.
    """
    def __init__(self) -> None: ...
    @staticmethod
    @functools.cache
    def run(obj: PatternExpr, output_name: str = ...) -> str:
        """Serializes obj to python code with obj written out to `output_name`"""
    def pretty_print(self, obj: Any) -> str: ...
    def memoize(self, obj: _TargetArgsExpr) -> str: ...

class _PassDictsType(Protocol):
    def __getitem__(self, k: tuple[str, torch.fx.node.Target]) -> list[PatternEntry]: ...

@dataclasses.dataclass
class PatternEntry:
    """PatternEntry(pattern: 'PatternExpr', extra_check: 'Callable[[Match], bool]')"""

    pattern: PatternExpr
    extra_check: Callable[[Match], bool]
    def apply(self, match: Match, graph: torch.fx.Graph, node: torch.fx.Node) -> None: ...
    def register(
        self,
        pass_dicts: _PassDictsType | Sequence[_PassDictsType],
        target: torch.fx.node.Target | None = ...,
        prepend: bool = ...,
    ) -> None: ...

@dataclasses.dataclass
class LoweringPatternEntry(PatternEntry):
    """LoweringPatternEntry(pattern: 'PatternExpr', extra_check: 'Callable[[Match], bool]', handler: 'Callable[..., Any]')"""

    handler: Callable[..., Any]
    def apply(self, match: Match, graph: torch.fx.Graph, node: torch.fx.Node) -> None: ...

@dataclasses.dataclass
class GraphPatternEntry(PatternEntry):
    """A pattern that runs a function on the FX graph"""

    handler: Callable[..., Any]
    def apply(self, match: Match, graph: torch.fx.Graph, node: torch.fx.Node) -> None: ...

@dataclasses.dataclass
class ReplacementPatternEntry(PatternEntry):
    """ReplacementPatternEntry(pattern: 'PatternExpr', extra_check: 'Callable[[Match], bool]', normalize_args: 'Callable[..., list[Any]]')"""

    normalize_args: Callable[..., list[Any]]
    @staticmethod
    def replace_with_graph(
        match: Match,
        graph: torch.fx.Graph,
        replacement_graph: torch.fx.Graph | torch.fx.GraphModule,
        args: Sequence[torch.fx.Node],
    ) -> None: ...
    def apply(self, match: Match, graph: torch.fx.Graph, node: torch.fx.Node) -> None: ...

def log_trace_failure(search_fn: Callable[..., Any], e: RuntimeError) -> None: ...
def check_and_add_duplicate_pattern(
    pattern: PatternExpr,
    graph: torch.fx.Graph | None,
    seen_patterns: dict[str, list[str | None]],
    skip_duplicates: bool = ...,
) -> bool:
    """
    Check if a pattern is a duplicate. Because we ignore certain types in searching, but not
    in matching, use the graph to distinguish equivalent search patterns.

    Returns True if a duplicate is found and `skip_duplicates=True` is passed in. Errors if
    `skip_duplicates` is False and a duplicate is found.
    """

def register_replacement(
    search_fn: SearchFn,
    replace_fn: ReplaceFn,
    example_inputs: Iterable[Any],
    trace_fn: TraceFn,
    pass_dicts: _PassDictsType | Sequence[_PassDictsType],
    extra_check: Callable[[Match], bool] = ...,
    scalar_workaround: dict[str, float | int] | None = ...,
    exclusive_arg_names: Sequence[str] = ...,
    search_fn_pattern: PatternExpr | None = ...,
    skip_duplicates: bool = ...,
) -> bool:
    """
    Create a replacement rule based on example functions that get traced
    to create patterns.  This supports both training and inference when
    run on a joint forward+backward graph.

    Args:
        search_fn: traced to give original pattern
        replace_fn: traced to give replacement graph
        example_inputs: example inputs for initial trace
        trace_fn: fwd_only or joint_fwd_bwd
        pass_dict: dict of passes to register to
        extra_check: additional check to run on match(using real shapes)
    """

_serialized_patterns: OrderedSet[str] = ...
SERIALIZED_PATTERN_PATH = ...
_known_precompiled_patterns: list[
    tuple[Any, Iterable[Any], Callable[[Callable[..., Any], Iterable[Any]], torch.fx.GraphModule], Any, PatternExpr]
] = ...

def gen_register_replacement(
    unique_name: str,
    search_fn: SearchFn,
    replace_fn: ReplaceFn,
    example_inputs: Iterable[Any],
    trace_fn: TraceFn,
    pass_dicts: _PassDictsType | Sequence[_PassDictsType],
    extra_check: Callable[[Match], bool] = ...,
    scalar_workaround: dict[str, float | int] | None = ...,
    exclusive_arg_names: Sequence[str] = ...,
    skip_duplicates: bool = ...,
) -> None: ...
@functorch_config.patch(functionalize_rng_ops=False)
def gen_pattern_and_search_gm(
    search_fn: SearchFn,
    example_inputs: Sequence[Any],
    trace_fn: TraceFn,
    scalar_workaround: dict[str, float | int] | None = ...,
    exclusive_arg_names: Sequence[str] = ...,
) -> tuple[PatternExpr, torch.fx.GraphModule]: ...
def gen_pattern(
    search_fn: SearchFn,
    example_inputs: Sequence[Any],
    trace_fn: TraceFn,
    scalar_workaround: dict[str, float | int] | None = ...,
    exclusive_arg_names: Sequence[str] = ...,
) -> PatternExpr: ...
def register_lowering_pattern(
    pattern: PatternExpr, extra_check: Callable[[Match], bool] = ..., *, pass_dict: _PassDictsType, prepend: bool = ...
) -> Callable[[Callable[..., Any]], Callable[..., Any]]:
    """
    Register an aten to inductor IR replacement pattern.  The decorated
    function is saved and then called a lowering time allowing direct
    pattern to inductor IR conversion.
    """

def register_graph_pattern(
    pattern: PatternExpr, extra_check: Callable[[Match], bool] = ..., *, pass_dict: _PassDictsType, prepend: bool = ...
) -> Callable[[Callable[..., Any]], Callable[..., Any]]:
    """
    Register a pattern that runs a function on the FX graph, allowing
    custom transformation code.
    """

def is_start_of_fx_graph(graph: torch.fx.Graph, node: torch.fx.Node) -> bool: ...

_mutation_op_re = ...

def fixme_incorrect_inductor_schema_op(op: torch._ops.OpOverload) -> bool: ...
def is_mutation_op(node: torch.fx.Node) -> bool: ...
def same_mutation_regions(a: torch.fx.Node, b: torch.fx.Node) -> bool: ...
def get_mutation_region_id(graph: torch.fx.Graph, node: torch.fx.Node) -> int: ...
def should_compute_mutation_region_ids(graph: torch.fx.Graph) -> bool: ...
def compute_mutation_region_ids(graph: torch.fx.Graph) -> None: ...

class PatternMatcherPass:
    def __init__(self, pass_name: str | None = ...) -> None: ...
    def __getitem__(self, item: tuple[str, torch.fx.node.Target]) -> list[PatternEntry]: ...
    def apply(self, gm: torch.fx.GraphModule | torch.fx.Graph) -> int: ...
    def clear(self) -> None: ...

def fx_to_pattern(
    gm: torch.fx.GraphModule | torch.fx.Graph,
    ignore_types: Sequence[type[Any]] = ...,
    argnames: Sequence[str] = ...,
    scalar_workaround: dict[str, float | int] | None = ...,
    exclusive_arg_names: Sequence[str] = ...,
) -> PatternExpr:
    """
    Convert an FX graph into a PatternExpr.  This is useful for simple
    patterns that can only match single functions and fixed-length lists.
    """

@torch.no_grad()
def fwd_only(
    fn: Callable[..., Any],
    args: Sequence[Any],
    *,
    run_functional_passes: bool = ...,
    get_decomp_fn: Callable[..., Any] | None = ...,
) -> torch.fx.GraphModule:
    """Build a normalized inference graph, for use with fx_to_pattern"""

@torch.enable_grad()
def joint_fwd_bwd(fn: Callable[..., Any], args: Sequence[Any]) -> torch.fx.GraphModule:
    """Build a normalized training graph, for use with fx_to_pattern"""

def stable_topological_sort(graph: torch.fx.Graph) -> None: ...
def init_once_fakemode(fn: Callable[..., Any]) -> Callable[[], Any]:
    """Wrapper around lazy init functions in fx_passes/"""

def config_flag(name: str) -> Callable[[Match], Any]:
    """Function for extra_check to put pass behind a flag"""

def clone_graph(input_graph: torch.fx.GraphModule) -> torch.fx.GraphModule: ...

_seen_patterns: OrderedSet[str] = ...

def get_arg_value(node: torch.fx.Node, arg_number: int, kwarg_name: str | None = ...) -> Any: ...
def filter_nodes(nodes: Iterable[torch.fx.Node], fn: Any) -> list[torch.fx.Node]: ...
def extract_target(node: torch.fx.Node) -> torch.fx.node.Target:
    """
    For call_function and call_method, we directly use the target function;
    For call_module, the target is string, and we treat the module class
     as a function.
    """
