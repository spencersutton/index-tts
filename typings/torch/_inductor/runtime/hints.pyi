import functools
import typing
from enum import Enum
from typing import Optional, Union
from torch.utils._triton import has_triton_package

TRITON_MAX_BLOCK = ...
TRITON_MAX_RSPLIT = ...

class ReductionHint(Enum):
    INNER = ...
    OUTER = ...
    OUTER_TINY = ...
    DEFAULT = ...

class TileHint(Enum):
    SQUARE = ...
    DEFAULT = ...

if has_triton_package(): ...
else:
    AttrsDescriptorWrapper = ...
_NUM_THREADS_PER_WARP = ...

class HeuristicType(Enum):
    PERSISTENT_REDUCTION = ...
    POINTWISE = ...
    REDUCTION = ...
    SPLIT_SCAN = ...
    TEMPLATE = ...
    USER_AUTOTUNE = ...
    FIXED = ...

class AutotuneHint(Enum):
    ONE_ELEMENT_PER_THREAD = ...
    __repr__ = ...

class DeviceProperties(typing.NamedTuple):
    type: str
    index: int
    multi_processor_count: int
    cc: int
    major: Optional[int] = ...
    regs_per_multiprocessor: Optional[int] = ...
    max_threads_per_multi_processor: Optional[int] = ...
    warp_size: Optional[int] = ...
    @classmethod
    @functools.cache
    def create(cls, device) -> DeviceProperties: ...

class HalideInputSpec(typing.NamedTuple):
    ctype: str
    name: str
    shape: Optional[list[str]] = ...
    stride: Optional[list[str]] = ...
    offset: Optional[str] = ...
    alias_of: Optional[str] = ...
    def bindings_type(self) -> str: ...
    def halide_type(self) -> str: ...
    def is_scalar(self) -> bool: ...
    def is_buffer(self) -> bool: ...

class HalideMeta(typing.NamedTuple):
    argtypes: list[HalideInputSpec]
    target: str
    scheduler: Optional[str] = ...
    scheduler_flags: Optional[dict[str, Union[int, str]]] = ...
    cuda_device: Optional[int] = ...
    def args(self) -> list[str]: ...
    def is_cuda(self) -> bool: ...
