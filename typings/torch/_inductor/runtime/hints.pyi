import functools
import typing
from enum import Enum
from typing import Optional, Union

from torch.utils._triton import has_triton_package

TRITON_MAX_BLOCK = ...
TRITON_MAX_RSPLIT = ...

class ReductionHint(Enum):
    INNER = ...
    OUTER = ...
    OUTER_TINY = ...
    DEFAULT = ...

class TileHint(Enum):
    SQUARE = ...
    DEFAULT = ...

if has_triton_package(): ...
else:
    AttrsDescriptorWrapper = ...
_NUM_THREADS_PER_WARP = ...

class HeuristicType(Enum):
    PERSISTENT_REDUCTION = ...
    POINTWISE = ...
    REDUCTION = ...
    SPLIT_SCAN = ...
    TEMPLATE = ...
    USER_AUTOTUNE = ...
    FIXED = ...

class AutotuneHint(Enum):
    ONE_ELEMENT_PER_THREAD = ...
    __repr__ = ...

class DeviceProperties(typing.NamedTuple):
    type: str
    index: int
    multi_processor_count: int
    cc: int
    major: int | None = ...
    regs_per_multiprocessor: int | None = ...
    max_threads_per_multi_processor: int | None = ...
    warp_size: int | None = ...
    @classmethod
    @functools.cache
    def create(cls, device) -> DeviceProperties: ...

class HalideInputSpec(typing.NamedTuple):
    ctype: str
    name: str
    shape: list[str] | None = ...
    stride: list[str] | None = ...
    offset: str | None = ...
    alias_of: str | None = ...
    def bindings_type(self) -> str: ...
    def halide_type(self) -> str: ...
    def is_scalar(self) -> bool: ...
    def is_buffer(self) -> bool: ...

class HalideMeta(typing.NamedTuple):
    argtypes: list[HalideInputSpec]
    target: str
    scheduler: str | None = ...
    scheduler_flags: dict[str, int | str] | None = ...
    cuda_device: int | None = ...
    def args(self) -> list[str]: ...
    def is_cuda(self) -> bool: ...
