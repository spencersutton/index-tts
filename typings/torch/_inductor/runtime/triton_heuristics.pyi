import dataclasses
from collections.abc import Callable, Container
from typing import Any, Literal, TypeVar

from torch._guards import CompileId
from torch.utils._ordered_set import OrderedSet

from .autotune_cache import AutotuneCache
from .hints import AutotuneHint, DeviceProperties, HeuristicType
from .static_cuda_launcher import StaticallyLaunchedCudaKernel
from .triton_compat import CompiledKernel, Config, KernelInterface

class InductorConfig(Config):
    def __init__(self, *args, dynamic_scale_rblock=..., **kwargs) -> None: ...

class InductorConfig(Config):
    def __init__(self, *args, dynamic_scale_rblock=..., **kwargs) -> None: ...

class NoTritonConfigsError(RuntimeError): ...

type LauncherType = Any
type _KernelType = CompiledKernel | StaticallyLaunchedCudaKernel
_T = TypeVar("_T", bound=_KernelType)
log = ...
triton_name_sub = ...

def generate_lookup_hash_from_source_code(size_hints_str: str, source_code: str) -> str: ...
def lookup_autotune_config(size_hints, fn) -> Config | None: ...
def get_total_reduction_numel(numels: dict[str, int]) -> int: ...
def autotune_hints_to_configs(
    hints: OrderedSet[AutotuneHint], size_hints, block_size: int, device_props: DeviceProperties
) -> list[Config]: ...
def disable_pointwise_autotuning(inductor_meta): ...
def check_autotune_cache(
    configs: list[Config], filename: str | None, inductor_meta: dict[str, Any]
) -> tuple[list[Config], AutotuneCache | None, dict[str, Any]]: ...

class CachingAutotuner(KernelInterface):
    def __init__(
        self,
        fn,
        triton_meta,
        configs,
        save_cache_hook,
        mutated_arg_names: list[str],
        optimize_mem,
        heuristic_type,
        size_hints=...,
        inductor_meta=...,
        custom_kernel=...,
        filename: str | None = ...,
        reset_to_zero_arg_names: list[str] | None = ...,
        autotune_cache_info: dict[str, Any] | None = ...,
    ) -> None: ...
    def is_statically_launchable(self): ...
    def recheck_autotune_cache(self, reload_kernel_from_src: Callable[[], CachingAutotuner]) -> None: ...
    def set_compile_info(self, compile_id: CompileId | None, is_backward: bool) -> None: ...
    def precompile(
        self,
        warm_cache_only=...,
        reload_kernel: Callable[[], CachingAutotuner] | None = ...,
        static_triton_bundle_key: str | None = ...,
    ): ...
    def prepare_for_pickle(self) -> tuple[Any, Any, Any, Any, Any, Any]: ...
    def restore_after_unpickle(self, old_values: tuple[Any, Any, Any, Any, Any, Any] | None) -> None: ...
    def prepare_for_caching(self) -> None: ...
    def __getstate__(self) -> dict[str, Any]: ...
    def __setstate__(self, state: dict[str, Any]) -> None: ...
    def get_device_interface(self): ...
    def bench(self, launcher, *args, with_profiler=..., **kwargs): ...
    def copy_args_to_cpu_if_needed(self, *args, **kwargs): ...
    def restore_args_from_cpu(self, cpu_copies): ...
    def reset_to_zero_args(self, *args, **kwargs): ...
    def maybe_clone_args(self, exclude: Container[str], *args, **kwargs) -> tuple[list[Any], dict[str, Any]]: ...
    def clone_args(self, *args, **kwargs) -> tuple[list[Any], dict[str, Any]]: ...
    def benchmark_all_configs(self, *args, **kwargs): ...
    def autotune_to_one_config(self, *args, **kwargs): ...
    def save_gpu_kernel(self, stream, launcher): ...
    def coordinate_descent_tuning(self, launcher, *args, **kwargs): ...
    def get_profiler_kwargs(self, stream, launcher): ...
    def run(self, *args, stream, benchmark_run=..., **kwargs): ...

class _ConstRepr:
    def __init__(self, value: str) -> None: ...
    def __call__(self, _=...) -> str: ...

class CompileResult[T: _KernelType]:
    def __init__(
        self, kernel: _T, config: Config, compile_meta: dict[str, Any], inductor_meta: dict[str, Any]
    ) -> None: ...
    def make_launcher(self) -> LauncherType: ...

class CannotStaticallyLaunchKernel(Exception): ...

class StaticTritonCompileResult(CompileResult[StaticallyLaunchedCudaKernel]):
    @staticmethod
    def can_statically_launch(
        kernel: CompiledKernel,
        inductor_meta: dict[str, Any],
        triton_meta: dict[str, Any],
        heuristic_type: HeuristicType,
    ) -> StaticallyLaunchedCudaKernel | None: ...
    def reload_cubin_path(self): ...
    def make_launcher(self) -> LauncherType: ...

class TritonCompileResult(CompileResult[CompiledKernel]):
    def __getstate__(self) -> dict[str, Any]: ...
    def __setstate__(self, state: dict[str, Any]) -> None: ...
    def make_launcher(self) -> LauncherType: ...

collected_calls: list[Any] = ...

def start_graph(): ...
def end_graph(output_file): ...

class DebugAutotuner(CachingAutotuner):
    def __init__(self, *args, regex_filter=..., with_profiler=..., with_bandwidth_info=..., **kwargs) -> None: ...
    def run(self, *args, stream, **kwargs): ...

def hash_configs(configs: list[Config]): ...
def cached_autotune(
    size_hints: list[int] | None,
    configs: list[Config],
    triton_meta,
    heuristic_type,
    filename=...,
    inductor_meta=...,
    custom_kernel=...,
): ...
def unique_configs(configs: list[Config]): ...
def check_config(cfg, *, xnumel=..., ynumel=..., znumel=...): ...
def check_max_block(cfg: dict[str, int]): ...
def triton_config(
    size_hints, x, y=..., z=..., num_stages=..., num_elements_per_warp=..., min_elem_per_thread=...
) -> Config: ...
def triton_config_reduction(
    size_hints, x: int, r: int, num_stages=..., num_warps=..., register_intensive=..., dynamic_scale_rblock=...
) -> Config: ...
def triton_config_tiled_reduction(size_hints, x, y, r, num_stages=..., register_intensive=...): ...
def pointwise(size_hints, triton_meta, tile_hint=..., filename=..., min_elem_per_thread=..., inductor_meta=...): ...
def match_target_block_product(size_hints, tiling_scores, target_block_product, min_block_size=...): ...
def adapt_config_for_tiling(
    size_hints,
    tiling_scores,
    original_x,
    original_r,
    num_warps=...,
    num_stages=...,
    register_intensive=...,
    persistent_reduction=...,
) -> Config: ...
def reduction(size_hints, reduction_hint=..., triton_meta=..., filename=..., inductor_meta=...): ...
def cooperative_reduction(size_hints, reduction_hint, triton_meta, filename, inductor_meta): ...
def persistent_reduction(size_hints, reduction_hint=..., triton_meta=..., filename=..., inductor_meta=...): ...
def split_scan(size_hints, reduction_hint=..., triton_meta=..., filename=..., inductor_meta=...): ...
def template(
    num_stages,
    num_warps,
    triton_meta,
    num_consumer_groups=...,
    num_buffers_warp_spec=...,
    filename=...,
    inductor_meta=...,
): ...
def config_to_dict(config: Config) -> dict[str, Any]: ...
def config_from_dict(config: dict[str, Any]) -> Config: ...
def fixed_config(config, filename, triton_meta, inductor_meta): ...
def user_autotune(configs, triton_meta, filename=..., inductor_meta=..., custom_kernel=...): ...
def foreach(triton_meta, num_warps, filename=..., inductor_meta=...): ...

@dataclasses.dataclass
class GridExpr:
    inductor_meta: dict[str, Any]
    mode: Literal["python", "cpp", "python_slow"] = ...
    prefix: list[str] = ...
    x_grid: str | int = ...
    y_grid: str | int = ...
    z_grid: str | int = ...
    def __post_init__(self) -> None: ...
    def generate(self, meta: dict[str, int]) -> None: ...
    def ceildiv(self, numel: str | int, block: None | int | str) -> str | int: ...
    def maximum(self, seq: list[int | str]) -> int | str: ...
    def summation(self, seq: list[int | str]) -> int | str: ...
    def assign_tmp(self, name: str, expr: str | int) -> str: ...
    @staticmethod
    def from_meta(
        inductor_meta: dict[str, Any], cfg: Config | dict[str, int], mode: Literal["python", "cpp", "python_slow"] = ...
    ) -> GridExpr: ...
    def eval_slow(self, meta: dict[str, int]) -> tuple[int, int, int]: ...

class Grid1D(GridExpr):
    def generate(self, meta: dict[str, int]) -> None: ...

class Grid2D(GridExpr):
    def generate(self, meta: dict[str, int]) -> None: ...

class Grid3D(GridExpr):
    def generate(self, meta: dict[str, int]) -> None: ...

class Grid2DWithYZOverflow(GridExpr):
    def generate(self, meta: dict[str, int]) -> None: ...

class CooperativeReductionGrid(GridExpr):
    def generate(self, meta: dict[str, int]) -> None: ...

class SplitScanGrid(GridExpr):
    def generate(self, meta: dict[str, int]) -> None: ...

class FixedGrid(GridExpr):
    @staticmethod
    def setup_grid_as_args() -> dict[str, Any]: ...
    def generate(self, meta: dict[str, int]) -> None: ...

class PrecomputedGrid(GridExpr):
    def generate(self, meta: dict[str, int]) -> None: ...

class ComboKernelGrid(GridExpr):
    def generate(self, meta: dict[str, int]): ...
    def combo_x_grid(self, xnumels: list[int | str], no_x_dims: list[bool], meta: dict[str, int]) -> str | int: ...

class SequentialComboKernelGrid(ComboKernelGrid):
    def combo_x_grid(self, xnumels: list[int | str], no_x_dims: list[bool], meta: dict[str, int]) -> str | int: ...

class RoundRobinComboKernelGrid(ComboKernelGrid):
    def combo_x_grid(self, xnumels: list[int | str], no_x_dims: list[bool], meta: dict[str, int]) -> str: ...
