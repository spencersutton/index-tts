import dataclasses
from typing import Any, Callable, Generic, Literal, Optional, TYPE_CHECKING, TypeVar, Union, TypeAlias
from torch.utils._ordered_set import OrderedSet
from .autotune_cache import AutotuneCache
from .hints import AutotuneHint, DeviceProperties, HeuristicType
from .static_cuda_launcher import StaticallyLaunchedCudaKernel
from .triton_compat import CompiledKernel, Config, KernelInterface
from collections.abc import Container
from torch._guards import CompileId

class InductorConfig(Config):
    def __init__(self, *args, dynamic_scale_rblock=..., **kwargs) -> None: ...

class NoTritonConfigsError(RuntimeError): ...

if TYPE_CHECKING:
    LauncherType: TypeAlias = Any
_KernelType: TypeAlias = Union[CompiledKernel, StaticallyLaunchedCudaKernel]
_T = TypeVar("_T", bound=_KernelType)
log = ...
triton_name_sub = ...

def generate_lookup_hash_from_source_code(size_hints_str: str, source_code: str) -> str: ...
def lookup_autotune_config(size_hints, fn) -> Optional[Config]: ...
def get_total_reduction_numel(numels: dict[str, int]) -> int: ...
def autotune_hints_to_configs(
    hints: OrderedSet[AutotuneHint], size_hints, block_size: int, device_props: DeviceProperties
) -> list[Config]: ...
def disable_pointwise_autotuning(inductor_meta):  # -> bool:
    ...
def check_autotune_cache(
    configs: list[Config], filename: Optional[str], inductor_meta: dict[str, Any]
) -> tuple[list[Config], Optional[AutotuneCache], dict[str, Any]]: ...

class CachingAutotuner(KernelInterface):
    def __init__(
        self,
        fn,
        triton_meta,
        configs,
        save_cache_hook,
        mutated_arg_names: list[str],
        optimize_mem,
        heuristic_type,
        size_hints=...,
        inductor_meta=...,
        custom_kernel=...,
        filename: Optional[str] = ...,
        reset_to_zero_arg_names: Optional[list[str]] = ...,
        autotune_cache_info: Optional[dict[str, Any]] = ...,
    ) -> None: ...
    def is_statically_launchable(self):  # -> bool:

        ...
    def recheck_autotune_cache(self, reload_kernel_from_src: Callable[[], CachingAutotuner]) -> None: ...
    def set_compile_info(self, compile_id: Optional[CompileId], is_backward: bool) -> None: ...
    def precompile(
        self,
        warm_cache_only=...,
        reload_kernel: Optional[Callable[[], CachingAutotuner]] = ...,
        static_triton_bundle_key: Optional[str] = ...,
    ):  # -> None:
        ...
    def prepare_for_pickle(self) -> tuple[Any, Any, Any, Any, Any, Any]: ...
    def restore_after_unpickle(self, old_values: Optional[tuple[Any, Any, Any, Any, Any, Any]]) -> None: ...
    def prepare_for_caching(self) -> None: ...
    def __getstate__(self) -> dict[str, Any]: ...
    def __setstate__(self, state: dict[str, Any]) -> None: ...
    def get_device_interface(self):  # -> type[DeviceInterface]:
        ...
    def bench(self, launcher, *args, with_profiler=..., **kwargs):  # -> float | list[float]:

        ...
    def copy_args_to_cpu_if_needed(self, *args, **kwargs):  # -> dict[Any, Any]:

        ...
    def restore_args_from_cpu(self, cpu_copies):  # -> None:
        ...
    def reset_to_zero_args(self, *args, **kwargs):  # -> None:
        ...
    def maybe_clone_args(self, exclude: Container[str], *args, **kwargs) -> tuple[list[Any], dict[str, Any]]: ...
    def clone_args(self, *args, **kwargs) -> tuple[list[Any], dict[str, Any]]: ...
    def benchmark_all_configs(self, *args, **kwargs):  # -> dict[LauncherType, float | list[float]]:
        ...
    def autotune_to_one_config(self, *args, **kwargs):  # -> None:

        ...
    def save_gpu_kernel(self, stream, launcher):  # -> None:
        ...
    def coordinate_descent_tuning(self, launcher, *args, **kwargs): ...
    def get_profiler_kwargs(self, stream, launcher):  # -> dict[str, str | Any]:
        ...
    def run(self, *args, stream, benchmark_run=..., **kwargs):  # -> LauncherType:
        ...

class _ConstRepr:
    def __init__(self, value: str) -> None: ...
    def __call__(self, _=...) -> str: ...

class CompileResult(Generic[_T]):
    def __init__(
        self, kernel: _T, config: Config, compile_meta: dict[str, Any], inductor_meta: dict[str, Any]
    ) -> None: ...
    def make_launcher(self) -> LauncherType: ...

class CannotStaticallyLaunchKernel(Exception): ...

class StaticTritonCompileResult(CompileResult[StaticallyLaunchedCudaKernel]):
    @staticmethod
    def can_statically_launch(
        kernel: CompiledKernel,
        inductor_meta: dict[str, Any],
        triton_meta: dict[str, Any],
        heuristic_type: HeuristicType,
    ) -> Optional[StaticallyLaunchedCudaKernel]: ...
    def reload_cubin_path(self):  # -> None:

        ...
    def make_launcher(self) -> LauncherType: ...

class TritonCompileResult(CompileResult[CompiledKernel]):
    def __getstate__(self) -> dict[str, Any]: ...
    def __setstate__(self, state: dict[str, Any]) -> None: ...
    def make_launcher(self) -> LauncherType: ...

collected_calls: list[Any] = ...

def start_graph():  # -> None:
    ...
def end_graph(output_file):  # -> None:
    ...

class DebugAutotuner(CachingAutotuner):
    def __init__(self, *args, regex_filter=..., with_profiler=..., with_bandwidth_info=..., **kwargs) -> None: ...
    def run(self, *args, stream, **kwargs):  # -> None:
        ...

def hash_configs(configs: list[Config]):  # -> str:

    ...
def cached_autotune(
    size_hints: Optional[list[int]],
    configs: list[Config],
    triton_meta,
    heuristic_type,
    filename=...,
    inductor_meta=...,
    custom_kernel=...,
):  # -> Callable[..., DebugAutotuner | CachingAutotuner]:

    ...
def unique_configs(configs: list[Config]):  # -> list[Any]:

    ...
def check_config(cfg, *, xnumel=..., ynumel=..., znumel=...):  # -> None:
    ...
def check_max_block(cfg: dict[str, int]):  # -> None:

    ...
def triton_config(
    size_hints, x, y=..., z=..., num_stages=..., num_elements_per_warp=..., min_elem_per_thread=...
) -> Config: ...
def triton_config_reduction(
    size_hints, x: int, r: int, num_stages=..., num_warps=..., register_intensive=..., dynamic_scale_rblock=...
) -> Config: ...
def triton_config_tiled_reduction(size_hints, x, y, r, num_stages=..., register_intensive=...):  # -> object:

    ...
def pointwise(
    size_hints, triton_meta, tile_hint=..., filename=..., min_elem_per_thread=..., inductor_meta=...
):  # -> Callable[..., DebugAutotuner | CachingAutotuner]:

    ...
def match_target_block_product(
    size_hints, tiling_scores, target_block_product, min_block_size=...
):  # -> dict[Any, Any]:

    ...
def adapt_config_for_tiling(
    size_hints,
    tiling_scores,
    original_x,
    original_r,
    num_warps=...,
    num_stages=...,
    register_intensive=...,
    persistent_reduction=...,
) -> Config: ...
def reduction(
    size_hints, reduction_hint=..., triton_meta=..., filename=..., inductor_meta=...
):  # -> Callable[..., DebugAutotuner | CachingAutotuner]:

    ...
def cooperative_reduction(
    size_hints, reduction_hint, triton_meta, filename, inductor_meta
):  # -> Callable[..., DebugAutotuner | CachingAutotuner]:
    ...
def persistent_reduction(
    size_hints, reduction_hint=..., triton_meta=..., filename=..., inductor_meta=...
):  # -> Callable[..., DebugAutotuner | CachingAutotuner]:
    ...
def split_scan(
    size_hints, reduction_hint=..., triton_meta=..., filename=..., inductor_meta=...
):  # -> Callable[..., DebugAutotuner | CachingAutotuner]:

    ...
def template(
    num_stages,
    num_warps,
    triton_meta,
    num_consumer_groups=...,
    num_buffers_warp_spec=...,
    filename=...,
    inductor_meta=...,
):  # -> Callable[..., DebugAutotuner | CachingAutotuner]:

    ...
def config_to_dict(config: Config) -> dict[str, Any]: ...
def config_from_dict(config: dict[str, Any]) -> Config: ...
def fixed_config(config, filename, triton_meta, inductor_meta):  # -> Callable[..., DebugAutotuner | CachingAutotuner]:

    ...
def user_autotune(
    configs, triton_meta, filename=..., inductor_meta=..., custom_kernel=...
):  # -> Callable[..., DebugAutotuner | CachingAutotuner]:

    ...
def foreach(
    triton_meta, num_warps, filename=..., inductor_meta=...
):  # -> Callable[..., DebugAutotuner | CachingAutotuner]:

    ...

@dataclasses.dataclass
class GridExpr:
    inductor_meta: dict[str, Any]
    mode: Literal["python", "cpp", "python_slow"] = ...
    prefix: list[str] = ...
    x_grid: Union[str, int] = ...
    y_grid: Union[str, int] = ...
    z_grid: Union[str, int] = ...
    def __post_init__(self) -> None: ...
    def generate(self, meta: dict[str, int]) -> None: ...
    def ceildiv(self, numel: Union[str, int], block: Union[None, int, str]) -> Union[str, int]: ...
    def maximum(self, seq: list[Union[int, str]]) -> Union[int, str]: ...
    def summation(self, seq: list[Union[int, str]]) -> Union[int, str]: ...
    def assign_tmp(self, name: str, expr: Union[str, int]) -> str: ...
    @staticmethod
    def from_meta(
        inductor_meta: dict[str, Any],
        cfg: Union[Config, dict[str, int]],
        mode: Literal["python", "cpp", "python_slow"] = ...,
    ) -> GridExpr: ...
    def eval_slow(self, meta: dict[str, int]) -> tuple[int, int, int]: ...

class Grid1D(GridExpr):
    def generate(self, meta: dict[str, int]) -> None: ...

class Grid2D(GridExpr):
    def generate(self, meta: dict[str, int]) -> None: ...

class Grid3D(GridExpr):
    def generate(self, meta: dict[str, int]) -> None: ...

class Grid2DWithYZOverflow(GridExpr):
    def generate(self, meta: dict[str, int]) -> None: ...

class CooperativeReductionGrid(GridExpr):
    def generate(self, meta: dict[str, int]) -> None: ...

class SplitScanGrid(GridExpr):
    def generate(self, meta: dict[str, int]) -> None: ...

class FixedGrid(GridExpr):
    @staticmethod
    def setup_grid_as_args() -> dict[str, Any]: ...
    def generate(self, meta: dict[str, int]) -> None: ...

class PrecomputedGrid(GridExpr):
    def generate(self, meta: dict[str, int]) -> None: ...

class ComboKernelGrid(GridExpr):
    def generate(self, meta: dict[str, int]):  # -> None:
        ...
    def combo_x_grid(
        self, xnumels: list[Union[int, str]], no_x_dims: list[bool], meta: dict[str, int]
    ) -> Union[str, int]: ...

class SequentialComboKernelGrid(ComboKernelGrid):
    def combo_x_grid(
        self, xnumels: list[Union[int, str]], no_x_dims: list[bool], meta: dict[str, int]
    ) -> Union[str, int]: ...

class RoundRobinComboKernelGrid(ComboKernelGrid):
    def combo_x_grid(self, xnumels: list[Union[int, str]], no_x_dims: list[bool], meta: dict[str, int]) -> str: ...
