import contextlib
import dataclasses
import functools
from collections.abc import Callable, Sequence
from types import ModuleType
from typing import TYPE_CHECKING, Any, Generic, Optional, ParamSpec, TypeAlias, TypeVar, Union

import sympy
import torch
from torch.utils._ordered_set import OrderedSet

from . import dependencies, ir
from .codegen.common import BackendFeature
from .dependencies import Dep, MemoryDep, StarDep, WeakDep
from .ir import GraphPartitionSignature
from .loop_body import LoopBody
from .memory import MemoryPlanningInfoForBuffer, MemoryPlanningInfoForNode
from .utils import IndentedBuffer, cache_on_self

if TYPE_CHECKING: ...
log = ...
fusion_log = ...
loop_ordering_log = ...
compute_dependencies_log = ...
type PartitionType = list[BaseSchedulerNode]

@dataclasses.dataclass
class SchedulerBuffer:
    scheduler: Scheduler
    node: ir.Buffer
    defining_op: BaseSchedulerNode | None
    users: list[NodeUser] = ...
    mpi_buffer: MemoryPlanningInfoForBuffer = ...
    def defining_op_name(self) -> str: ...
    def __hash__(self) -> int: ...
    def debug_str(self) -> str: ...
    def get_name(self) -> str: ...
    def allocate(self) -> None: ...
    def can_free(self) -> bool: ...
    def set_users(self, users: list[NodeUser]) -> None: ...
    def get_aliases(self) -> Sequence[str]: ...
    def get_mutations(self) -> Sequence[str]: ...
    def get_device(self) -> torch.device | None: ...

@dataclasses.dataclass
class SchedulerDonatedBuffer(SchedulerBuffer):
    defining_op: BaseSchedulerNode | None = ...

class BaseSchedulerNode:
    group: tuple[torch.device, tuple[tuple[sympy.Expr, ...], ...]]
    read_writes: dependencies.ReadWrites
    unmet_dependencies: OrderedSet[Dep]
    min_order: int
    max_order: int
    mpi_node: MemoryPlanningInfoForNode
    override_estimated_runtime: float | None = ...
    def __init__(self, scheduler: Scheduler) -> None: ...
    def debug_str(self) -> str: ...
    def debug_str_extra(self) -> str: ...
    def debug_str_short(self) -> str: ...
    def log_details(self) -> None: ...
    def reorder_loops_by_dep_pair(self, self_dep: MemoryDep, other_dep: MemoryDep) -> bool: ...
    def update_mutated_names(self, renames: dict[str, str]) -> None: ...
    def add_fake_dep(self, dep: Dep) -> None: ...
    def has_aliasing_or_mutation(self) -> bool: ...
    def set_read_writes(self, rw: dependencies.ReadWrites) -> None: ...
    def set_last_usage(self, future_used_buffers: OrderedSet[str], mutation_real_name: dict[str, str]) -> None: ...
    def mark_run(self) -> None: ...
    def used_buffer_names(self) -> OrderedSet[str]: ...
    def used_or_aliased_buffer_names(self) -> OrderedSet[str]: ...
    def prune_deps(self) -> None: ...
    def prune_weak_deps(self) -> None: ...
    def prune_redundant_deps(self, name_to_fused_node: dict[str, BaseSchedulerNode]) -> None: ...
    def get_name(self) -> str: ...
    def get_first_name(self) -> str: ...
    @cache_on_self
    def get_operation_names(self) -> OrderedSet[str]: ...
    @cache_on_self
    def get_buffer_names(self) -> OrderedSet[str]: ...
    @cache_on_self
    def can_codegen_in_low_precision(self) -> bool: ...
    @cache_on_self
    def can_codegen_without_upcasts(self) -> bool: ...
    def get_nodes(self) -> Sequence[BaseSchedulerNode]: ...
    def get_outputs(self) -> Sequence[SchedulerBuffer]: ...
    def get_output(self, buf_name: str) -> SchedulerBuffer: ...
    def get_device(self) -> torch.device | None: ...
    def is_cpu(self) -> bool: ...
    def is_gpu(self) -> bool: ...
    def is_reduction(self) -> bool: ...
    def is_split_scan(self) -> bool: ...
    def is_template(self) -> bool: ...
    def is_extern(self) -> bool: ...
    def is_foreach(self) -> bool: ...
    def can_inplace(self, read_dep: dependencies.Dep) -> bool: ...
    def has_side_effects(self) -> bool: ...
    def decide_inplace_update(self) -> None: ...
    def codegen_originating_info(self, buffer: IndentedBuffer, only_once: bool = ...) -> None: ...
    @cache_on_self
    def get_read_write_buffers_sizes(self) -> int: ...
    @cache_on_self
    def get_read_buffer_sizes(self) -> int: ...
    @cache_on_self
    def get_write_buffer_sizes(self) -> int: ...
    def get_read_write_buffers_sizes_impl(self, include_reads: bool, include_writes: bool) -> int: ...
    def get_read_write_buffer_accesses(self, include_reads: bool, include_writes: bool) -> dict[str, int]: ...
    @cache_on_self
    def estimate_flops(self) -> int | None: ...
    def get_estimated_runtime(self) -> float: ...
    def get_template_node(self) -> ir.TemplateBuffer | None: ...
    def get_template_node_or_throw(self) -> ir.TemplateBuffer: ...
    @staticmethod
    def get_prologue_template_epilogue(
        nodes: list[BaseSchedulerNode],
    ) -> tuple[list[BaseSchedulerNode], BaseSchedulerNode, list[BaseSchedulerNode]]: ...

@functools.cache
def get_estimate_runtime_cache() -> torch._inductor.codecache.LocalCache: ...
def get_estimate_runtime_cache_key_from_snode(snode: BaseSchedulerNode) -> str: ...
def maybe_estimate_runtime_benchmark(snode: BaseSchedulerNode) -> float | None: ...

class WhyNoFuse:
    __slots__ = ...
    reason: str
    args: tuple[Any, ...]
    def __init__(self, node1: BaseSchedulerNode, node2: BaseSchedulerNode) -> None: ...
    def __call__(self, reason: str, *args: Any) -> None: ...

def pformat(obj: Any) -> str: ...

class OutputNode:
    def __init__(self, dep: StarDep) -> None: ...
    def is_reduction(self) -> bool: ...
    def get_inputs_that_alias_output(self) -> Sequence[str]: ...
    def get_name(self) -> str: ...

    __repr__ = ...

class ExternKernelSchedulerNode(BaseSchedulerNode):
    def __init__(self, scheduler: Scheduler, node: ir.Operation) -> None: ...
    def debug_str_extra(self) -> str: ...
    def is_extern(self) -> bool: ...
    def has_side_effects(self) -> bool: ...

class NopKernelSchedulerNode(BaseSchedulerNode):
    def __init__(self, scheduler: Scheduler, node: ir.Operation) -> None: ...

class SchedulerNode(BaseSchedulerNode):
    _sizes: tuple[Sequence[sympy.Expr], ...]
    _body: LoopBody
    def __init__(self, scheduler: Scheduler, node: ir.ComputedBuffer | ir.TemplateBuffer) -> None: ...
    def recompute_size_and_body(
        self,
        extra_indexing_constraints: tuple[dict[Any, Any], list[Any]] | None = ...,
        recompute_sizes_body_func: Callable[..., Any] | None = ...,
    ) -> None: ...
    def refresh_dependencies(self, normalize: bool, need_clear_tiling_cache: bool) -> None: ...
    def apply_new_loop_order(self, new_order: Sequence[int]) -> None: ...
    def expand_dimension_for_pointwise_node(self, dimension: int, new_range: int) -> None: ...
    def merge_loops(self) -> None: ...
    def reorder_loops_by_dep_pair(self, self_dep: MemoryDep, other_dep: MemoryDep) -> bool: ...
    def debug_str_extra(self) -> str: ...
    def get_ranges(self) -> Sequence[Sequence[sympy.Expr]]: ...
    def is_reduction(self) -> bool: ...
    def is_split_scan(self) -> bool: ...
    def is_template(self) -> bool: ...
    def get_template_node(self) -> ir.TemplateBuffer | None: ...
    def run(self, *index_vars: Sequence[sympy.Expr]) -> None: ...
    def ranges_from_index_vars(self, index_vars: Sequence[Sequence[sympy.Expr]]) -> dict[sympy.Expr, sympy.Expr]: ...
    def codegen(self, index_vars: Sequence[Sequence[sympy.Expr]]) -> None: ...
    def pointwise_or_reduction_read_writes(self, pointwise: bool = ...) -> dependencies.ReadWrites: ...
    @cache_on_self
    def pointwise_read_writes(self) -> dependencies.ReadWrites: ...
    @cache_on_self
    def reduction_read_writes(self) -> dependencies.ReadWrites: ...
    def can_inplace(self, read_dep: dependencies.Dep) -> bool: ...
    @cache_on_self
    def has_side_effects(self) -> bool: ...

def refresh_group_node_dependencies(group_snode: FusedSchedulerNode | GroupedSchedulerNode) -> None: ...
def init_group_node(
    group_snode: FusedSchedulerNode | GroupedSchedulerNode, scheduler: Scheduler, snodes: list[BaseSchedulerNode]
) -> None: ...

class FusedSchedulerNode(BaseSchedulerNode):
    snodes: list[BaseSchedulerNode]
    @classmethod
    def fuse(cls, node1: BaseSchedulerNode, node2: BaseSchedulerNode) -> FusedSchedulerNode: ...
    @cache_on_self
    def estimate_flops(self) -> int | None: ...
    def reorder_loops_by_dep_pair(self, self_dep: MemoryDep, other_dep: MemoryDep) -> bool: ...
    def __init__(self, scheduler: Scheduler, snodes: list[BaseSchedulerNode]) -> None: ...
    @cache_on_self
    def get_name(self) -> str: ...
    def get_first_name(self) -> str: ...
    @cache_on_self
    def get_buffer_names(self) -> OrderedSet[str]: ...
    def get_outputs(self) -> list[SchedulerBuffer]: ...
    def debug_str_extra(self) -> str: ...
    def debug_str_short(self) -> str: ...
    def set_last_usage(self, future_used_buffers: OrderedSet[str], mutation_real_name: dict[str, str]) -> None: ...
    @cache_on_self
    def used_buffer_names(self) -> OrderedSet[str]: ...
    @cache_on_self
    def used_or_aliased_buffer_names(self) -> OrderedSet[str]: ...
    def get_nodes(self) -> Sequence[BaseSchedulerNode]: ...
    @cache_on_self
    def is_reduction(self) -> bool: ...
    @cache_on_self
    def is_split_scan(self) -> bool: ...
    @cache_on_self
    def is_template(self) -> bool: ...
    @cache_on_self
    def get_template_node(self) -> ir.TemplateBuffer | None: ...
    def get_device(self) -> torch.device: ...
    @cache_on_self
    def has_aliasing_or_mutation(self) -> bool: ...
    def update_mutated_names(self, renames: dict[str, str]) -> None: ...
    def add_fake_dep(self, name: Dep) -> None: ...
    def can_inplace(self, read_dep: dependencies.Dep) -> bool: ...
    def debug_str(self) -> str: ...
    @cache_on_self
    def has_side_effects(self) -> bool: ...

class ForeachKernelSchedulerNode(FusedSchedulerNode):
    def get_consumer_subnode_for(self, producer: BaseSchedulerNode) -> BaseSchedulerNode | None: ...
    def get_producer_subnode_for(self, consumer: BaseSchedulerNode) -> BaseSchedulerNode | None: ...
    @classmethod
    def can_fuse(cls, producer: BaseSchedulerNode, consumer: BaseSchedulerNode) -> bool: ...
    @classmethod
    def fuse(cls, producer: BaseSchedulerNode, consumer: BaseSchedulerNode) -> ForeachKernelSchedulerNode: ...
    def __init__(
        self,
        scheduler: Scheduler,
        snodes: list[BaseSchedulerNode],
        use_custom_partition_algo: bool,
        prev_node_1: BaseSchedulerNode | None = ...,
        prev_node_2: BaseSchedulerNode | None = ...,
        enable_autotune: bool = ...,
    ) -> None: ...
    @classmethod
    def combinable_nodes(cls, nodes: list[BaseSchedulerNode]) -> list[BaseSchedulerNode]: ...

    group_algorithm_for_combo_kernels: Callable[[Scheduler], list[list[BaseSchedulerNode]]] = ...
    @staticmethod
    def set_group_algorithm_for_combo_kernels(
        custom_group_algorithm: Callable[[Scheduler], list[list[BaseSchedulerNode]]],
    ) -> None: ...
    @staticmethod
    def group_nodes_for_combo_kernels(scheduler: Scheduler) -> list[list[BaseSchedulerNode]]: ...
    def mark_run(self) -> None: ...
    def codegen(self) -> None: ...
    def is_foreach(self) -> bool: ...
    def get_subkernel_nodes(self) -> list[BaseSchedulerNode]: ...
    def get_nodes(self) -> Sequence[BaseSchedulerNode]: ...
    def get_first_name(self) -> str: ...
    def prune_redundant_deps(self, name_to_fused_node: dict[str, BaseSchedulerNode]) -> None: ...

class GroupedSchedulerNode(BaseSchedulerNode):
    snodes: list[BaseSchedulerNode]
    @classmethod
    def create(cls, snodes: list[BaseSchedulerNode]) -> GroupedSchedulerNode: ...
    def __init__(self, scheduler: Scheduler, snodes: list[BaseSchedulerNode], temp_grouping: bool = ...) -> None: ...
    def unpack(self) -> list[BaseSchedulerNode]: ...
    def add_fake_dep(self, fake_dep: Dep) -> None: ...
    @cache_on_self
    def get_name(self) -> str: ...
    def get_first_name(self) -> str: ...
    @cache_on_self
    def get_buffer_names(self) -> OrderedSet[str]: ...
    def get_outputs(self) -> list[SchedulerBuffer]: ...
    @cache_on_self
    def estimate_flops(self) -> int | None: ...
    def get_nodes(self) -> Sequence[BaseSchedulerNode]: ...
    @classmethod
    def can_fuse(cls, producer: BaseSchedulerNode, consumer: BaseSchedulerNode) -> bool: ...

def pick_loop_order(
    stride_lengths: list[list[int]], sizes: Sequence[sympy.Expr], priority_idx: Sequence[int] = ...
) -> list[int]: ...

@dataclasses.dataclass
class NodeUser:
    node: BaseSchedulerNode | OutputNode
    can_inplace: bool = ...
    is_weak: bool = ...
    def __hash__(self) -> int: ...
    def __eq__(self, other: object) -> bool: ...
    def get_name(self) -> str: ...
    def merge(self, other: NodeUser) -> NodeUser: ...

_post_grad_graph_counter = ...

def used_non_deterministic_runtime_estimations() -> bool: ...

class Scheduler:
    def __init__(self, nodes: list[ir.Operation]) -> None: ...
    def get_donated_buffers(self) -> dict[str, SchedulerDonatedBuffer]: ...
    @property
    def current_device(self) -> torch.device | None: ...
    @current_device.setter
    def current_device(self, device: torch.device | None) -> None: ...
    def debug_draw_graph(self) -> None: ...
    def debug_print_nodes(self, label: str) -> None: ...
    def create_scheduler_node(self, node: ir.Operation) -> BaseSchedulerNode: ...
    def create_foreach_nodes(self) -> None: ...
    def compute_dependencies(self) -> None: ...
    def insert_memory_check_nodes(self) -> None: ...
    def dead_node_elimination(self) -> None: ...
    def topological_sort_schedule(self, nodes: list[BaseSchedulerNode]) -> list[BaseSchedulerNode]: ...
    def compute_ancestors(self) -> None: ...
    def merge_loops(self) -> None: ...
    def fuse_nodes(self, nodes: list[BaseSchedulerNode]) -> list[BaseSchedulerNode]: ...
    def process_grouped_nodes(self) -> None: ...
    def benchmark_fused_nodes(self, nodes: Sequence[BaseSchedulerNode]) -> tuple[float, str]: ...
    def generate_kernel_code_from_nodes(
        self, nodes: Sequence[BaseSchedulerNode], benchmark_kernel: bool, hint_override: int | None = ...
    ) -> str: ...
    def benchmark_codegened_module(self, module: ModuleType, device: torch.device) -> tuple[float, str]: ...
    def finalize_multi_template_buffers(self) -> None: ...
    def speedup_by_fusion(self, node1: BaseSchedulerNode, node2: BaseSchedulerNode) -> bool | Callable[[], bool]: ...
    def get_fused_node(self, node: BaseSchedulerNode) -> BaseSchedulerNode: ...
    def fuse_nodes_once(self, nodes: list[BaseSchedulerNode]) -> list[BaseSchedulerNode]: ...
    def create_combo_kernel_nodes(self, num_ck_nodes: int | None = ...) -> None: ...
    def prune_redundant_deps(self, nodes: list[BaseSchedulerNode]) -> None: ...
    def get_possible_fusions(
        self, nodes: list[BaseSchedulerNode]
    ) -> list[tuple[BaseSchedulerNode, BaseSchedulerNode]]: ...
    def will_fusion_create_cycle(self, node1: BaseSchedulerNode, node2: BaseSchedulerNode) -> bool: ...
    def can_fusion_increase_peak_memory(self, node1: BaseSchedulerNode, node2: BaseSchedulerNode) -> bool: ...
    def fusion_accumulate_large_reads(
        self, node1: BaseSchedulerNode, node2: BaseSchedulerNode, threshold: int
    ) -> bool: ...
    def are_long_distant_nodes(self, node1: BaseSchedulerNode, node2: BaseSchedulerNode) -> bool: ...
    def decide_fusion_fail_reason(
        self, node1: BaseSchedulerNode, node2: BaseSchedulerNode, common_buf_names: tuple[str] | OrderedSet[str]
    ) -> str: ...
    def shared_data_after_reordering_loop(self, node1: BaseSchedulerNode, node2: BaseSchedulerNode) -> int: ...
    def unfusable_node(self, node: BaseSchedulerNode) -> bool: ...
    def check_prologue_fusion_heuristics_fusable(
        self, prologue_node: BaseSchedulerNode, template_node: BaseSchedulerNode, why: WhyNoFuse
    ) -> bool: ...
    def get_expand_dim_for_pointwise_nodes(
        self, node1: BaseSchedulerNode, node2: BaseSchedulerNode
    ) -> tuple[int, SchedulerNode, sympy.Expr] | None: ...
    def can_fuse(self, node1: BaseSchedulerNode, node2: BaseSchedulerNode) -> bool: ...
    def can_fuse_vertical(self, node1: BaseSchedulerNode, node2: BaseSchedulerNode) -> bool: ...
    def fusable_weak_dep(self, weak_dep: WeakDep, node1: BaseSchedulerNode, node2: BaseSchedulerNode) -> bool: ...
    def fusable_read_and_write(self, read: Dep, write: MemoryDep) -> bool: ...
    def dep_size_hint(self, dep: Dep) -> int: ...
    def score_fusion_memory(self, node1: BaseSchedulerNode, node2: BaseSchedulerNode) -> int: ...
    def get_possible_fusions_with_highest_priority(
        self, possible_fusions: list[tuple[BaseSchedulerNode, BaseSchedulerNode]]
    ) -> list[tuple[BaseSchedulerNode, BaseSchedulerNode]]: ...
    def score_fusion_key(self, nodes: tuple[BaseSchedulerNode, BaseSchedulerNode]) -> Any: ...
    def compute_last_usage(self) -> None: ...
    def free_buffers(self) -> None: ...
    def flush(self) -> None: ...
    def codegen_extern_call(self, scheduler_node: ExternKernelSchedulerNode) -> None: ...
    def create_backend(self, device: torch.device) -> BaseScheduling: ...
    def get_backend(self, device: torch.device | None) -> BaseScheduling: ...
    def enter_context(self, node: BaseSchedulerNode) -> None: ...
    def can_buffer_be_removed_through_fusion(self, name: str, fused_node_names: OrderedSet[str]) -> bool: ...
    def should_partition(self, node: BaseSchedulerNode, should_log: bool = ...) -> bool: ...
    def get_name_to_nodes(self) -> dict[str, ir.IRNode | ir.TorchBindObject | sympy.Expr]: ...
    def compute_graph_partition_maps(self, signatures: list[GraphPartitionSignature]) -> None: ...
    def get_graph_partition_symbol_inputs(
        self, partition: PartitionType, input_nodes: dict[str, ir.IRNode | ir.TorchBindObject | sympy.Expr]
    ) -> OrderedSet[sympy.Symbol]: ...
    def get_graph_partition_signature(
        self, partitions: list[PartitionType], skip_cudagraphs: list[bool]
    ) -> list[GraphPartitionSignature]: ...
    def clean_removed_buffer_from_partition_signatures(
        self, signature: GraphPartitionSignature
    ) -> GraphPartitionSignature: ...
    def reorder_for_minimizing_partition(self, nodes: list[BaseSchedulerNode]) -> list[BaseSchedulerNode]: ...
    def maybe_reorder_for_minimizing_partition(self, nodes: list[BaseSchedulerNode]) -> list[BaseSchedulerNode]: ...
    def reorder_for_partition_with_simple_dependency(
        self, nodes: list[BaseSchedulerNode]
    ) -> list[BaseSchedulerNode]: ...
    def graph_partition(self) -> tuple[list[PartitionType], list[GraphPartitionSignature]]: ...
    def codegen(self) -> None: ...
    def use_default_device_context(
        self, partitions: list[PartitionType], signatures: list[GraphPartitionSignature]
    ) -> contextlib.AbstractContextManager[None]: ...
    def update_graph_partition_default_device(
        self, partitions: list[PartitionType], signatures: list[GraphPartitionSignature]
    ) -> None: ...
    def benchmark_combo_kernel(
        self, node_list: Sequence[BaseSchedulerNode]
    ) -> tuple[float, float, list[str | None]]: ...
    def speedup_by_combo_kernel(self, nodes: list[BaseSchedulerNode]) -> bool: ...
    def get_buffer_layout(self, buf_name: str) -> ir.Layout: ...
    def update_zero_dim_cpu_tensor(self) -> None: ...

class BaseScheduling:
    def __init__(self, scheduler: Scheduler | None) -> None: ...
    def free_buffers_in_scheduler(self) -> None: ...
    def get_backend_features(self, device: torch.device) -> OrderedSet[BackendFeature]: ...
    def can_fuse_vertical(self, node1: BaseSchedulerNode, node2: BaseSchedulerNode) -> bool: ...
    def can_fuse_horizontal(self, node1: BaseSchedulerNode, node2: BaseSchedulerNode) -> bool: ...
    def can_fuse_multi_outputs_template(self, node1: BaseSchedulerNode, node2: BaseSchedulerNode) -> bool: ...
    def fuse(self, node1: BaseSchedulerNode, node2: BaseSchedulerNode) -> FusedSchedulerNode: ...
    def group_fn(self, sizes: Sequence[Sequence[sympy.Expr]]) -> tuple[tuple[sympy.Expr, ...], ...]: ...
    def codegen_template(
        self,
        template_node: BaseSchedulerNode,
        epilogue_nodes: Sequence[BaseSchedulerNode],
        prologue_nodes: Sequence[BaseSchedulerNode],
    ) -> str | None: ...
    def generate_kernel_code_from_nodes(
        self, nodes: Sequence[BaseSchedulerNode], benchmark_kernel: bool, hint_override: int | None = ...
    ) -> str: ...
    def codegen_node(self, node: FusedSchedulerNode | SchedulerNode) -> None: ...
    def codegen_sync(self) -> None: ...
    def ready_to_flush(self) -> bool: ...
    def flush(self) -> None: ...
    def benchmark_fused_nodes(self, nodes: Sequence[BaseSchedulerNode]) -> tuple[float, str]: ...
    def benchmark_codegened_module(self, module: ModuleType) -> tuple[float, str]: ...
    def get_fusion_pair_priority(self, node1: BaseSchedulerNode, node2: BaseSchedulerNode) -> int: ...
    def benchmark_combo_kernel(
        self, node_list: Sequence[BaseSchedulerNode]
    ) -> tuple[float, float, list[str | None]]: ...
