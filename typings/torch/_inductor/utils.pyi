import contextlib
import dataclasses
import enum
import functools
from collections.abc import (
    Callable,
    Collection,
    Generator,
    Iterable,
    Iterator,
    Mapping,
    MutableMapping,
    MutableSet,
    Sequence,
    ValuesView,
)
from typing import (
    Any,
    Concatenate,
    Literal,
    NamedTuple,
    ParamSpec,
    Protocol,
    Self,
    TypeGuard,
    TypeVar,
    dataclass_transform,
)

import sympy
import torch
from torch._prims_common import ELEMENTWISE_TYPE_PROMOTION_KIND
from torch.fx import GraphModule
from torch.fx.experimental.symbolic_shapes import IterateExprs, ShapeEnv
from torch.fx.node import Node
from torch.utils._ordered_set import OrderedSet
from torch.utils._sympy.symbol import SymT
from torch.utils._sympy.value_ranges import ValueRanges

from .codegen.common import WorkspaceArg
from .codegen.wrapper import PythonWrapperCodegen
from .graph import GraphLowering
from .ir import Buffer, ExternKernel, IRNode, Layout, Operation, ReinterpretView
from .output_code import CompiledFxGraph
from .scheduler import BaseSchedulerNode, SchedulerBuffer

OPTIMUS_EXCLUDE_POST_GRAD = ...

GPU_TYPES = ...
T = TypeVar("T")

@functools.cache
def get_gpu_type() -> str: ...

_IS_WINDOWS = ...
log = ...
perf_hint_log = ...
_T = TypeVar("_T")
type VarRanges = dict[sympy.Expr, sympy.Expr]
type InputType = torch.Tensor | int | torch.SymInt | None
GPU_KERNEL_BIN_EXTS = ...
GPU_ALIGN_BYTES = ...
ALIGNMENT = ...
TMA_ALIGNMENT = ...
TMA_DESCRIPTOR_SIZE = ...
ALIGN_BYTES = ...

class align(sympy.Function):
    nargs = ...
    is_integer = ...
    @classmethod
    def eval(cls, value: sympy.Expr) -> sympy.Expr | None: ...

@dataclasses.dataclass(frozen=True)
class GraphPartitionMap:
    id: int
    input_index_mapping: list[int | None]
    output_index_mapping: list[int | None]
    constant_names: list[str]

def fp8_bench(fn: Callable[[], Any], warmup: int = ..., rep: int = ...) -> float: ...
def do_bench_using_profiling(fn: Callable[[], Any], warmup: int = ..., rep: int = ...) -> float: ...
@functools.cache
def has_torchvision_roi_align() -> bool: ...
def decode_device(device: torch.device | None | str) -> torch.device: ...
def sympy_product(it: Iterable[sympy.Expr]) -> sympy.Expr: ...
def sympy_dot(seq1: Sequence[sympy.Expr], seq2: Sequence[sympy.Expr]) -> sympy.Expr: ...
def unique[_T](it: Iterable[_T]) -> ValuesView[_T]: ...
def ceildiv(number: int | sympy.Expr, denom: int | sympy.Expr) -> int | sympy.Expr: ...
def convert_shape_to_inductor(lst: Iterable[int | torch.SymInt]) -> list[sympy.Expr]: ...
def convert_to_symint(i: int | sympy.Expr) -> int | torch.SymInt: ...
def convert_shape_to_symint(lst: Iterable[int | sympy.Expr]) -> list[int | torch.SymInt]: ...
def is_view(op: torch._ops.OpOverload) -> bool: ...
def is_pointwise_use(use: Node, is_pointwise_fn: Callable[[torch._ops.OpOverload], bool] = ...) -> bool: ...
def gen_gm_and_inputs(
    target: Any, args: list[Any], kwargs: dict[str, Any]
) -> tuple[GraphModule, list[torch.Tensor]]: ...
def synchronize(device: str = ...) -> None: ...
def timed(model: Callable[..., Any], example_inputs: Sequence[Any], times: int = ..., device: str = ...) -> float: ...
def print_performance(
    model: Callable[..., Any],
    example_inputs: Sequence[Any] = ...,
    times: int = ...,
    repeat: int = ...,
    baseline: float = ...,
    device: str = ...,
) -> float: ...
def precompute_method(obj: Any, method: str) -> None: ...
def precompute_methods(obj: Any, methods: list[str]) -> None: ...
def cmp(a: int, b: int) -> int: ...
def pad_listlike(x: int | Sequence[int], size: int) -> Sequence[int]: ...
def tuple_sorted[_T](x: tuple[_T, ...]) -> list[_T]: ...

P = ParamSpec("P")
RV = TypeVar("RV", covariant=True)
type FN_TYPE[**P, RV] = Callable[Concatenate[Any, P], RV]

class CachedMethod[**P, RV](Protocol):
    @staticmethod
    def clear_cache(cache: Any) -> None: ...
    def __call__(self, *args: P.args, **kwargs: P.kwargs) -> RV: ...

def cache_on_self[**P, RV](fn: Callable[Concatenate[Any, P], RV]) -> CachedMethod[P, RV]: ...
def cache_property_on_self[**P, RV](fn: Callable[P, RV]) -> CachedMethod[P, RV]: ...
def cache_on_self_and_args(class_name: str) -> Callable[[FN_TYPE[P, RV]], FN_TYPE[P, RV]]: ...
def aggregate_origins(node_schedule: Sequence[BaseSchedulerNode] | ExternKernel) -> OrderedSet[Node]: ...
def get_fused_kernel_name(
    node_schedule: Sequence[BaseSchedulerNode],
    descriptive_names: Literal[True, "torch", "original_aten", "inductor_node"],
) -> str: ...
def get_kernel_metadata(
    node_schedule: Sequence[BaseSchedulerNode] | ExternKernel, wrapper: PythonWrapperCodegen
) -> tuple[str, str]: ...
def dominated_nodes(
    initial_queue: Iterable[torch.fx.Node], skip_filter: Callable[[Any], bool] | None = ...
) -> OrderedSet[torch.fx.Node]: ...
def gather_origins(args: Sequence[IRNode], kwargs: dict[str, IRNode]) -> OrderedSet[torch.fx.Node]: ...
def sympy_str(expr: sympy.Expr) -> str: ...
def get_bounds_index_expr(index: sympy.Expr) -> ValueRanges[Any]: ...
def prefix_is_reduction(prefix: str) -> bool: ...
def sympy_index_symbol_with_prefix(prefix: SymT, idx: int) -> sympy.Symbol: ...
def generate_assert(check: bool) -> bool: ...
def sympy_index_symbol(name: str) -> sympy.Symbol: ...
def sympy_subs(expr: sympy.Expr, replacements: dict[sympy.Expr, Any]) -> sympy.Expr: ...
def is_symbolic(a: Any) -> TypeGuard[torch.SymInt | torch.Tensor]: ...
def any_is_symbolic(*args: Any) -> bool: ...
def get_first_incompatible_cudagraph_node(gm: torch.fx.GraphModule) -> torch.fx.Node | None: ...
def output_node(gm: torch.fx.GraphModule) -> Node: ...
def get_all_devices(gm: torch.fx.GraphModule) -> OrderedSet[torch.device]: ...
def unload_xpu_triton_pyds() -> None: ...

_registered_caches: list[Any] = ...

def clear_on_fresh_cache(obj: Any) -> Any: ...
def clear_caches() -> None: ...
@contextlib.contextmanager
def fresh_cache(
    cache_entries: dict[str, Any] | None = ..., dir: str | None = ..., delete: bool = ...
) -> Iterator[None]: ...

clear_on_fresh_inductor_cache = ...
clear_inductor_caches = ...
fresh_inductor_cache = ...

def argsort(seq: Sequence[Any]) -> list[int]: ...
def argsort_sym(shape_env: ShapeEnv, seq: Sequence[int | torch.SymInt | sympy.Expr]) -> list[int]: ...
@functools.lru_cache(8)
def get_dtype_size(dtype: torch.dtype) -> int: ...

class LineContext(NamedTuple):
    context: Any

@dataclasses.dataclass
class ValueWithLineMap:
    value: str
    line_map: list[tuple[int, LineContext]]

class IndentedBuffer:
    tabwidth = ...
    def __init__(self, initial_indent: int = ...) -> None: ...
    @contextlib.contextmanager
    def set_tabwidth(self, tabwidth: int) -> Iterator[None]: ...
    def getvaluewithlinemap(self) -> ValueWithLineMap: ...
    def getvalue(self) -> str: ...
    def getrawvalue(self) -> str: ...
    def clear(self) -> None: ...
    def __bool__(self) -> bool: ...
    def prefix(self) -> str: ...
    def newline(self) -> None: ...
    def writeline(self, line: LineContext | DeferredLineBase | str) -> None: ...
    def writelines(self, lines: Sequence[LineContext | DeferredLineBase | str]) -> None: ...
    def indent(self, offset: int = ...) -> contextlib.AbstractContextManager[None]: ...
    def do_indent(self, offset: int = ...) -> None: ...
    def do_unindent(self, offset: int = ...) -> None: ...
    def splice(self, other_code: IndentedBuffer | str, strip: bool = ...) -> None: ...
    def map(self, func: Callable[[Any], Any]) -> IndentedBuffer: ...
    def __add__(self, other: Self) -> IndentedBuffer: ...
    def contains(self, new_line: DeferredLineBase | LineContext | str) -> bool: ...

class FakeIndentedBuffer(IndentedBuffer):
    def __init__(self) -> None: ...
    def __getattribute__(self, name: str) -> Any: ...

@contextlib.contextmanager
def restore_stdout_stderr() -> Iterator[None]: ...

class DeferredLineBase:
    def __init__(self, line: str) -> None: ...
    def __call__(self) -> str | None: ...
    def with_prefix(self, prefix: str) -> Self: ...
    def lstrip(self) -> Self: ...
    def __getitem__(self, index: int | slice) -> Self: ...
    def __bool__(self) -> bool: ...
    def __len__(self) -> int: ...

class DelayReplaceLine(DeferredLineBase):
    def __init__(self, key: str, value_fn: Callable[[], str], line: str) -> None: ...
    def __call__(self) -> str: ...

@functools.cache
def is_big_gpu(index_or_device: int | torch.device = ...) -> bool: ...
@functools.lru_cache
def get_max_num_sms() -> int: ...
@functools.lru_cache
def using_b200() -> bool: ...
def get_num_sms() -> int: ...
def get_tma_workspace_arg(
    num_tma_descriptors: int, device: torch.device, num_programs: int | None = ...
) -> WorkspaceArg: ...
def use_triton_template(
    layout: Layout, *, enable_int32: bool = ..., enable_float8: bool = ..., check_max_autotune: bool = ...
) -> bool: ...
def can_use_tma(*matrices: IRNode, add_guards: bool = ...) -> bool: ...
def use_triton_tma_template(*matrices: IRNode, add_guards: bool = ...) -> bool: ...
def use_cutlass_template(layout: Layout, m: int, n: int, k: int) -> bool: ...

type _IntLike = int | sympy.Expr

@functools.cache
def use_decompose_k_choice(m: _IntLike, n: _IntLike, k: _IntLike) -> bool: ...
@functools.cache
def use_contiguous(m: _IntLike, n: _IntLike, k: _IntLike) -> bool: ...
@functools.cache
def get_k_splits(m: _IntLike, n: _IntLike, k: _IntLike) -> list[int]: ...
@functools.cache
def try_import_ck_lib() -> tuple[str | None, Callable[[], list[Any]], Callable[[], list[Any]], type[Any]]: ...
def use_ck_template(layout: Layout) -> bool: ...
def use_ck_gemm_template(layout: Layout, m: int, n: int, k: int) -> bool: ...
def use_ck_tile_gemm_template(layout: Layout, m: int, n: int, k: int) -> bool: ...
def use_ck_conv_template(layout: Layout) -> bool: ...
def use_cpp_bmm_template(layout: Layout, mat1: ReinterpretView | Buffer, mat2: IRNode) -> bool: ...
def use_cpp_gemm_template(
    layout: Layout,
    mat1: IRNode,
    mat2: IRNode,
    mat2_transposed: bool = ...,
    require_constant_mat2: bool = ...,
    is_woq_int4: bool = ...,
    q_group_size: int | None = ...,
) -> bool: ...
def use_aten_gemm_kernels() -> bool: ...

class DebugDirManager:
    counter = ...
    prev_debug_name: str
    def __init__(self) -> None: ...
    def __enter__(self) -> None: ...
    def __exit__(self, *args: object) -> None: ...

def run_and_get_code(fn: Callable[P, _T], *args: P.args, **kwargs: P.kwargs) -> tuple[_T, list[str]]: ...
def run_and_get_kernels(fn: Callable[P, _T], *args: P.args, **kwargs: P.kwargs) -> tuple[_T, list[str]]: ...
def run_fw_bw_and_get_code(fn: Callable[..., Any]) -> tuple[Any, list[str]]: ...
def get_code(fn: Callable[P, _T], *args: P.args, **kwargs: P.kwargs) -> list[str]: ...
def get_triton_code(fn: Callable[P, _T], *args: P.args, **kwargs: P.kwargs) -> str: ...
def run_and_get_triton_code(fn: Callable[P, _T], *args: P.args, **kwargs: P.kwargs) -> str: ...
def run_and_get_graph_lowering(
    fn: Callable[P, _T], *args: P.args, **kwargs: P.kwargs
) -> tuple[Any, list[GraphLowering]]: ...
@contextlib.contextmanager
def override_lowering(aten_op: Callable[..., Any], override_fn: Callable[..., Any]) -> Iterator[None]: ...
def add_scheduler_init_hook(pre_fn: Callable[..., Any], post_fn: Callable[..., Any] | None = ...) -> Any: ...
def developer_warning(msg: str) -> None: ...
def get_benchmark_name() -> str | None: ...
def is_ones(items: Sequence[Any]) -> bool: ...
def is_zeros(items: Sequence[Any]) -> bool: ...
def is_cpu_device(inputs: Sequence[torch.Tensor]) -> bool: ...
def get_sympy_Expr_dtype(val: sympy.Expr) -> torch.dtype: ...
@contextlib.contextmanager
def maybe_profile(should_profile: bool, *args: Any, **kwargs: Any) -> Iterator[Any]: ...
def parallel_num_threads() -> int: ...
@functools.cache
def get_backend_num_stages() -> int: ...
@functools.cache
def get_device_tflops(dtype: torch.dtype) -> float: ...
@functools.cache
def get_gpu_dram_gbps() -> int: ...
def get_gpu_shared_memory() -> int: ...
def is_welford_reduction(reduction_type: str) -> bool: ...
def reduction_num_outputs(reduction_type: str) -> int: ...
def is_linux() -> bool: ...
def is_windows() -> bool: ...
def has_free_symbols(itr: Iterable[Any]) -> bool: ...
def is_dynamic(*args: Any) -> bool: ...

class Placeholder(enum.Enum):
    KERNEL_NAME = ...
    DESCRIPTIVE_NAME = ...

def pass_execution_and_save(func: Callable[..., Any], gm: GraphModule, inp: Sequence[Any], msg: str) -> None: ...
def is_multi_outputs_template(input_buf: Buffer | Operation | None) -> bool: ...
def is_output_of_multi_outputs_template(input_buf: Buffer | Operation | None) -> bool: ...
def is_collective(node: Node | Operation | None, op: torch._ops.OperatorBase | None = ...) -> bool: ...
def is_wait(node: IRNode | Operation | None) -> bool: ...
def contains_collective(snode: BaseSchedulerNode) -> bool: ...
def contains_wait(snode: BaseSchedulerNode) -> bool: ...
def is_fallback_op(node: Operation | None, op: torch._ops.OpOverload | Collection[torch._ops.OpOverload]) -> bool: ...
def buf_name_to_fused_snode(buf_name: str, name_to_buf: dict[str, Any], name_to_fused_node: dict[str, Any]) -> Any: ...
def find_recursive_deps_of_node(
    snode: BaseSchedulerNode,
    collected_node_set: MutableSet[BaseSchedulerNode],
    name_to_buf: dict[str, SchedulerBuffer],
    name_to_fused_node: dict[str, BaseSchedulerNode],
    criteria_cb: Callable[[Any], bool] = ...,
) -> None: ...
def find_recursive_users_of_node(
    snode: BaseSchedulerNode,
    collected_node_set: MutableSet[BaseSchedulerNode],
    name_to_buf: dict[str, SchedulerBuffer],
    name_to_fused_node: dict[str, BaseSchedulerNode],
    criteria_cb: Callable[[Any], bool] = ...,
) -> None: ...
def num_fw_fixed_arguments(dynamo_gm_num_inputs: int, aot_fw_gm_num_inputs: int) -> int: ...
def count_tangents(fx_g: torch.fx.GraphModule) -> int: ...

@dataclasses.dataclass
class BoxedBool:
    value: bool
    def __bool__(self) -> bool: ...
    @staticmethod
    def disable(obj: Any) -> BoxedBool | bool: ...

@contextlib.contextmanager
def collect_defined_kernels(kernel_list: list[str]) -> Iterator[None]: ...
def get_cloned_parameter_buffer_name(name: str) -> str: ...
def is_gpu(device: str | None) -> bool: ...
def device_need_guard(device: str) -> bool: ...
def needs_fallback_due_to_atomic_add_limitations(dtype: torch.dtype) -> bool: ...
def use_scatter_fallback(
    op_overload: torch._ops.OpOverload,
    reduction_type: str | None,
    self_dtype: torch.dtype,
    src_dtype: torch.dtype,
    src_device_type: str,
    src_is_tensor: bool,
) -> bool: ...
def dump_node_schedule(node_schedule: Sequence[BaseSchedulerNode]) -> None: ...
def tensor_is_aligned(tensor: torch.Tensor) -> bool: ...
def should_assume_input_aligned(example_input: torch.Tensor) -> bool: ...
def maybe_get_suppress_shape_guards_ctx() -> contextlib.AbstractContextManager[None]: ...
def run_and_get_cpp_code(fn: Callable[P, _T], *args: P.args, **kwargs: P.kwargs) -> tuple[_T, str]: ...
def shape_env_from_inputs(inputs: Sequence[InputType]) -> ShapeEnv | None: ...
def align_inputs_from_check_idxs[_T](
    model: Callable[[list[InputType]], _T], inputs_to_check: Sequence[int], mutated_input_idxs: OrderedSet[int]
) -> Callable[[list[InputType]], _T]: ...
def clone_preserve_strides(x: torch.Tensor) -> torch.Tensor: ...
def copy_misaligned_inputs(
    new_inputs: list[InputType], check_inputs_idxs: Sequence[int], return_pair_idxs: OrderedSet[int] | None = ...
) -> tuple[list[torch.Tensor], list[torch.Tensor]]: ...
def remove_unaligned_input_idxs(inputs: Sequence[InputType], static_input_idxs: Sequence[int]) -> Sequence[int]: ...
def expr_fits_within_32bit(e: sympy.Expr) -> bool: ...
def set_tracing_context_output_strides(example_inputs: Sequence[Any], compiled_graph: CompiledFxGraph) -> None: ...
def should_use_remote_fx_graph_cache() -> bool: ...
def normalize_name(name: str) -> str: ...

_triton_type_mapping = ...
_torch_triton_mapping = ...
_triton_type_re = ...

def triton_type(dtype: torch.dtype) -> str: ...
def triton_type_to_torch(dtype: str) -> torch.dtype: ...
def is_same_tensor(data: torch.Tensor, value: torch.Tensor) -> bool: ...
def is_same_mkldnn_tensor(data: torch.Tensor, value: torch.Tensor) -> bool: ...
@functools.cache
def boolean_ops() -> tuple[str, ...]: ...

@dataclasses.dataclass
class OpDtypeRule:
    type_promotion_kind: ELEMENTWISE_TYPE_PROMOTION_KIND
    override_return_dtype: torch.dtype | None

op_dtype_propagation_rules: dict[str, OpDtypeRule] = ...

def register_op_dtype_propagation_rules(
    name: str, type_promotion_kind: ELEMENTWISE_TYPE_PROMOTION_KIND, override_return_dtype: torch.dtype | None
) -> None: ...

op_requires_libdevice_fp64: OrderedSet[str] = ...

def register_op_requires_libdevice_fp64(name: str) -> None: ...
def get_current_backend() -> str: ...
def upcast_compute_type(dtype: torch.dtype) -> torch.dtype: ...

KeyType = TypeVar("KeyType")
ValType = TypeVar("ValType")

class ScopedDict(MutableMapping[KeyType, ValType]):
    def __init__(self, original_dict: Mapping[KeyType, ValType]) -> None: ...
    def __getitem__(self, key: KeyType) -> ValType: ...
    def __setitem__(self, key: KeyType, value: ValType) -> None: ...
    def __contains__(self, key: object) -> bool: ...
    def get(self, key: KeyType, default: ValType | None = ...) -> ValType | None: ...
    def __len__(self) -> int: ...
    def __iter__(self) -> Iterator[KeyType]: ...
    def __bool__(self) -> bool: ...
    def __delitem__(self, key: KeyType) -> None: ...

@dataclass_transform(frozen_default=True)
def ir_dataclass(cls: type[Any] | None = ..., /, *, frozen: bool = ...) -> Any: ...
def get_donated_idxs() -> list[int] | None: ...

class TritonAttrsDescriptorVersion(enum.Enum):
    V0_NO_TRITON = ...
    V1_COMPILER = ...
    V2_BACKENDS = ...
    V3_BACKENDS_TUPLE = ...
    V4_DICT = ...

@functools.cache
def get_triton_attrs_descriptor_version() -> TritonAttrsDescriptorVersion: ...
def triton_version_uses_attrs_dict() -> bool: ...
def is_cudagraph_unsafe_op(node: Operation) -> bool: ...
def get_ld_library_path() -> str: ...
def is_codegen_graph_partition_subgraph(wrapper: PythonWrapperCodegen) -> bool: ...
def is_using_cudagraph_partition() -> bool: ...
def dtype_from_size(size: int) -> torch.dtype: ...

SUPPORTED_MKLDNN_DEVICES = ...

def is_mkldnn_bf16_supported(device_type: str) -> bool: ...
def is_mkldnn_fp16_supported(device_type: str) -> bool: ...
def tabulate_2d(elements: Sequence[Sequence[T]], headers: Sequence[T]) -> str: ...
def zip_dicts(
    dict1: Mapping[KeyType, ValType],
    dict2: Mapping[KeyType, ValType],
    d1_default: ValType | None = ...,
    d2_default: ValType | None = ...,
) -> Generator[tuple[KeyType, ValType | None, ValType | None]]: ...
def maybe_aoti_standalone_config(config_patches: dict[str, Any]) -> dict[str, Any]: ...
def is_valid_aoti_model_name() -> bool: ...
def get_free_symbols(x: IterateExprs, unbacked_only: bool) -> OrderedSet[sympy.Symbol]: ...
def maybe_log_cudagraph_partition(msg: str, prefix: str | None = ..., node: BaseSchedulerNode | None = ...) -> None: ...
def python_subprocess_env() -> dict[str, str]: ...

@dataclasses.dataclass(frozen=True)
class CUDAGraphWrapperMetadata:
    num_partitions: int
    partition_index: int

type PartitionFnType = Callable[..., Any]
type CUDAGraphWrapperType = Callable[[PartitionFnType, CUDAGraphWrapperMetadata], PartitionFnType]

class CUDAGraphWrapper:
    wrapper: CUDAGraphWrapperType | None = ...

_unstable_customized_partition_wrapper = ...

def set_customized_partition_wrappers(wrapper: CUDAGraphWrapperType) -> None: ...
def snode_args_kwargs(snode: BaseSchedulerNode) -> tuple[list[Any], dict[str, Any]]: ...
