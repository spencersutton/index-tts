import contextlib
import pickle
import sys
import weakref
import torch
import torch.distributed.rpc
from typing import Any, Final, TypeVar, Union
from collections.abc import Callable
from typing import ParamSpec

"""
The weak_script annotation needs to be here instead of inside torch/jit/ so it
can be used in other places in torch/ (namely torch.nn) without running into
circular dependency problems
"""
_P = ParamSpec("_P")
_R = TypeVar("_R")
IS_PY310_PLUS: Final[bool] = ...
BuiltinUnionType: type | tuple[type, ...]
if sys.version_info >= (3, 10):
    BuiltinUnionType = ...
else: ...
LockType: type
LockType = ...
boolean_dispatched: weakref.WeakKeyDictionary[Callable, dict[str, Callable]] = ...
FAKE_FILENAME_PREFIX = ...

def is_final(ann) -> bool: ...

class BroadcastingListCls:
    def __getitem__(self, types):  # -> None:
        ...

BroadcastingList1 = ...

def is_scripting() -> bool: ...

class SourceLoader:
    def __init__(self) -> None: ...
    def cache(self, fn, source):  # -> None:
        ...
    def get_source(self, fn):  # -> None:
        ...

loader = ...

def createResolutionCallbackFromEnv(lookup_base):  # -> Callable[..., Any | None]:

    ...
def createResolutionCallbackFromFrame(frames_up: int = ...):  # -> Callable[..., Any | None]:

    ...
def get_closure(fn):  # -> dict[Any, Any]:

    ...
def createResolutionCallbackFromClosure(fn):  # -> Callable[..., Any | None]:

    ...
def can_compile_class(cls) -> bool: ...
def get_callable_argument_names(fn) -> list[str]: ...
def get_annotation_str(annotation):  # -> str | LiteralString | None:

    ...
def get_type_hint_captures(fn):  # -> dict[Any, Any]:

    ...
def createResolutionCallbackForClassMethods(cls):  # -> Callable[..., Any | None]:

    ...
def boolean_dispatch(arg_name, arg_index, default, if_true, if_false, module_name, func_name):  # -> Callable[..., Any]:

    ...

class FunctionModifiers:
    UNUSED = ...
    IGNORE = ...
    EXPORT = ...
    DEFAULT = ...
    COPY_TO_SCRIPT_WRAPPER = ...
    _DROP = ...

def export(fn: Callable[_P, _R]) -> Callable[_P, _R]: ...
def unused(fn: Callable[_P, _R]) -> Callable[_P, _R]: ...

class _IgnoreContextManager(contextlib.AbstractContextManager):
    def __init__(self, **kwargs) -> None: ...
    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None: ...

def ignore(drop=..., **kwargs):  # -> <callable subtype of bool> | Callable[..., Any]:

    ...
def module_has_exports(mod):  # -> bool:
    ...
def should_drop(fn) -> bool: ...
def is_ignored_fn(fn) -> bool: ...
def is_static_fn(cls, fn) -> bool: ...
def get_static_fn(cls, fn):  # -> Any:
    ...
def get_torchscript_modifier(fn):  # -> Any | str | None:
    ...
def copy_torchscript_modifier(orig, new) -> None: ...

_overloaded_fns: dict[str, list[Callable]] = ...
_OVERLOAD_EXAMPLE = ...

def get_overload_no_implementation_error_message(kind, obj):  # -> str:
    ...
def get_class_name_lineno(method) -> tuple[str, int]: ...

_overloaded_methods: dict[str, dict[str, list[Callable]]] = ...
_overloaded_method_class_fileno: dict[tuple[str, str], int] = ...

def is_tuple(ann) -> bool: ...
def is_list(ann) -> bool: ...
def is_dict(ann) -> bool: ...
def is_union(ann):  # -> bool:
    ...
def is_optional(ann):  # -> bool:
    ...
def is_future(ann) -> bool: ...
def is_await(ann) -> bool: ...

if torch.distributed.rpc.is_available():
    def is_rref(ann) -> bool: ...
    def is_rref_instance(obj) -> bool: ...

else:
    def is_rref_instance(obj) -> bool: ...

def raise_error_container_parameter_missing(target_type) -> None: ...

_RAW_TYPE_NAME_MAPPING = ...

def check_args_exist(target_type) -> None: ...
def check_empty_containers(obj) -> None: ...
def container_checker(obj, target_type) -> bool: ...

class _TensorExtractor(pickle.Pickler):
    def __init__(self, *args, tensors: list[torch.Tensor], **kwargs) -> None: ...
    def persistent_id(self, obj):  # -> Literal[''] | None:
        ...

if sys.version_info >= (3, 11): ...
