import abc
import contextlib
import types
import torch
from collections.abc import Iterator
from typing import Any, Callable, ClassVar, Generic, TYPE_CHECKING, Union, final
from typing_extensions import Concatenate, ParamSpec, TypeVar
from torch._C import DispatchKey
from torch._functorch.pyfunctorch import TransformType
from torch.utils._python_dispatch import TorchDispatchMode
from torch._subclasses.functional_tensor import BaseFunctionalizeAPI

if TYPE_CHECKING: ...
_T = TypeVar("_T", default=Any)
_P = ParamSpec("_P", default=...)
_SET_GLOBAL_FLAGS = ...

@contextlib.contextmanager
def dl_open_guard():  # -> Generator[None, Any, None]:

    ...

class OperatorBase:
    def __init__(self) -> None: ...
    def __call__(self, *args, **kwargs): ...
    def has_kernel_for_dispatch_key(self, k):  # -> bool:
        ...
    def has_kernel_for_any_dispatch_key(self, ks):  # -> bool:
        ...
    def py_impl(
        self, k: Union[type[Union[TorchDispatchMode, torch.Tensor]], TransformType, DispatchKey]
    ) -> Callable[[Callable[_P, _T]], Callable[_P, _T]]: ...
    def py_functionalize_impl(
        self, fn: Callable[Concatenate[BaseFunctionalizeAPI, _P], _T]
    ) -> Callable[Concatenate[BaseFunctionalizeAPI, _P], _T]: ...
    def name(self): ...

def resolve_key(op: OperatorBase, k: DispatchKey):  # -> DispatchKey:
    ...

_higher_order_ops: dict[str, HigherOrderOperator] = ...
_HIGHER_ORDER_OP_DEFAULT_FALLTHROUGH_DISPATCH_KEYS = ...

class HigherOrderOperator(OperatorBase, abc.ABC):
    def __init__(self, name, *, cacheable=...) -> None: ...
    def py_impl(
        self, k: Union[type[Union[TorchDispatchMode, torch.Tensor]], TransformType, DispatchKey]
    ) -> Callable[[Callable[_P, _T]], Callable[_P, _T]]: ...
    def py_autograd_impl(self, fn: Callable[_P, _T]) -> Callable[_P, _T]: ...
    @property
    def namespace(self):  # -> str:
        ...
    @final
    def cacheable(self) -> bool: ...
    def fallthrough(self, dispatch_key):  # -> None:
        ...
    def dispatch(self, /, dispatch_key, *args, **kwargs):  # -> Any | None:
        ...
    @abc.abstractmethod
    def __call__(self, /, *args, **kwargs):  # -> Any | None:
        ...
    def gen_schema(self, *args, **kwargs): ...
    def name(self):  # -> Any:
        ...

def key_extractor(tensors, key_mask): ...

class _ModeStackStateForPreDispatch:
    def __init__(self) -> None: ...
    def set(self, index, mode):  # -> None:
        ...
    def get(self, index): ...
    def count(self):  # -> int:
        ...

_mode_stack_state_for_pre_dispatch = ...

def unset_mode_pre_dispatch(mode_key, schema_check=...):  # -> None:
    ...
def mode_stack_state_for_pre_dispatch():  # -> _ModeStackStateForPreDispatch:
    ...

cached_ops: set[OpOverload] = ...

def add_cached_op(op_overload):  # -> None:
    ...
def reset_cached_ops():  # -> None:
    ...
def get_cached_ops():  # -> set[OpOverload[..., Any]]:
    ...

class OpOverload(OperatorBase, Generic[_P, _T]):
    def __init__(
        self,
        overloadpacket: OpOverloadPacket,
        op: Callable[_P, _T],
        op_dk: Callable[Concatenate[DispatchKey, _P], _T],
        schema: torch._C.FunctionSchema,
        tags: list[Any],
    ) -> None: ...
    def __deepcopy__(self, memo=...):  # -> Self:
        ...
    def __repr__(self):  # -> str:
        ...
    def __call__(self, /, *args: _P.args, **kwargs: _P.kwargs) -> _T: ...
    def redispatch(self, /, keyset: torch._C.DispatchKeySet, *args: _P.args, **kwargs: _P.kwargs) -> _T: ...
    def __hash__(self) -> int: ...
    def has_kernel_for_dispatch_key(self, k: DispatchKey) -> bool: ...
    def has_kernel_for_any_dispatch_key(self, ks: torch._C.DispatchKeySet) -> bool: ...
    @property
    def namespace(self) -> str: ...
    def decompose(self, *args: _P.args, **kwargs: _P.kwargs) -> _T: ...
    def name(self):  # -> str:
        ...
    @property
    def overloadpacket(self):  # -> OpOverloadPacket[..., Any]:
        ...
    @property
    def op(self):  # -> Callable[_P, _T]:
        ...
    @property
    def tags(self):  # -> list[Any]:
        ...

class TorchBindOpOverload(OpOverload[_P, _T]):
    def __call__(self, /, *args: _P.args, **kwargs: _P.kwargs) -> _T: ...

class OpOverloadPacket(Generic[_P, _T]):
    __file__: ClassVar[str] = ...
    def __init__(
        self, qualified_op_name: str, op_name: str, op: Callable[_P, _T], overload_names: list[str]
    ) -> None: ...
    def __deepcopy__(self, memo=...):  # -> Self:
        ...
    def __repr__(self):  # -> str:
        ...
    def __hash__(self) -> int: ...
    @property
    def op(self):  # -> Callable[_P, _T]:
        ...
    def __getattr__(self, key: str) -> OpOverload[_P, _T]: ...
    def __iter__(self) -> Iterator[str]: ...
    def __call__(self, /, *args: _P.args, **kwargs: _P.kwargs) -> _T: ...
    def overloads(self):  # -> list[str]:
        ...

class _OpNamespace(types.ModuleType):
    __file__ = ...
    def __init__(self, name: str) -> None: ...
    def __iter__(self) -> Iterator[str]: ...
    def __getattr__(self, op_name: str) -> OpOverloadPacket: ...

class _HigherOrderNamespace(types.ModuleType):
    __file__ = ...
    def __init__(self) -> None: ...
    def __iter__(self) -> Iterator[str]: ...
    def __getattr__(self, name: str) -> HigherOrderOperator: ...

class _Ops(types.ModuleType):
    __file__ = ...
    def __init__(self) -> None: ...
    def __getattr__(self, name: str) -> _OpNamespace: ...
    def __iter__(self) -> Iterator[str]: ...
    def import_module(self, module):  # -> None:

        ...
    def load_library(self, path):  # -> None:

        ...

ops: _Ops = ...
