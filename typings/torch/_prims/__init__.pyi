import operator
from collections.abc import Callable, Sequence
from enum import Enum
from functools import partial, reduce
from typing import Optional, Union

import torch
import torch._prims_common as utils
import torch.library
from torch import Tensor, sym_float
from torch._C import _get_default_device
from torch._higher_order_ops.effects import new_token_tensor
from torch._library.utils import is_functional_schema
from torch._prims.debug_prims import register_debug_prims
from torch._prims.rng_prims import register_rng_prims
from torch._prims_common import (
    RETURN_TYPE,
    Dim,
    DimsSequenceType,
    DimsType,
    IntLike,
    Number,
    NumberType,
    ShapeType,
    StrideType,
    TensorLike,
    TensorLikeType,
    type_to_dtype,
)
from torch._prims_common.wrappers import backwards_not_supported
from torch._subclasses.fake_tensor import FakeTensor, FakeTensorMode
from torch.overrides import handle_torch_function, has_torch_function
from torch.utils._pytree import tree_flatten, tree_map, tree_unflatten

prim = ...
prim_impl = ...
prim_backend_select_impl = ...
prim_autograd_impl = ...
prim_meta_impl = ...
__all__ = [
    "RETURN_TYPE",
    "abs",
    "acos",
    "acosh",
    "asin",
    "asinh",
    "atan",
    "atanh",
    "cos",
    "cosh",
    "bessel_i0",
    "bessel_i0e",
    "bessel_i1",
    "bessel_i1e",
    "bessel_j0",
    "bessel_j1",
    "bitwise_not",
    "cbrt",
    "ceil",
    "conj_physical",
    "digamma",
    "erf",
    "erf_inv",
    "erfc",
    "erfcx",
    "exp",
    "expm1",
    "exp2",
    "fill",
    "floor",
    "imag",
    "isfinite",
    "lgamma",
    "log",
    "log1p",
    "log2",
    "log10",
    "ndtri",
    "neg",
    "real",
    "reciprocal",
    "round",
    "sign",
    "signbit",
    "sin",
    "sinh",
    "spherical_bessel_j0",
    "sqrt",
    "tan",
    "tanh",
    "trunc",
    "add",
    "atan2",
    "bitwise_and",
    "bitwise_or",
    "bitwise_xor",
    "div",
    "eq",
    "fmax",
    "fmin",
    "fmod",
    "frexp",
    "gcd",
    "ge",
    "gt",
    "hypot",
    "igamma",
    "igammac",
    "le",
    "lt",
    "maximum",
    "minimum",
    "mul",
    "ne",
    "nextafter",
    "pow",
    "remainder",
    "rsqrt",
    "shift_left",
    "shift_right_arithmetic",
    "shift_right_logical",
    "sub",
    "zeta",
    "as_strided",
    "broadcast_in_dim",
    "collapse_view",
    "conj",
    "expand_dims",
    "slice",
    "slice_in_dim",
    "split_dim",
    "squeeze",
    "transpose",
    "view_of",
    "view_element_type",
    "as_strided_scatter",
    "collapse",
    "cat",
    "reshape",
    "rev",
    "where",
    "clone",
    "convert_element_type",
    "device_put",
    "item",
    "maximum_value",
    "minimum_value",
    "copy_strided",
    "copy_to",
    "resize",
    "amax",
    "amin",
    "prod",
    "sum",
    "xor_sum",
    "var",
    "empty_strided",
    "empty_permuted",
    "scalar_tensor",
    "iota",
    "svd",
    "normal",
    "_uniform_helper",
    "fft_r2c",
    "fft_c2c",
    "fft_c2r",
    "_make_token",
    "_sink_tokens",
]

def TensorMeta(
    tensorlike: NumberType | torch.Tensor | None = ...,
    *,
    shape: ShapeType | None = ...,
    strides: StrideType | None = ...,
    dtype: torch.dtype | None = ...,
    device: torch.device | str | None = ...,
):  # -> Tensor:
    ...

class ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND(Enum):
    DEFAULT = ...
    INT_TO_FLOAT = ...
    ALWAYS_BOOL = ...
    COMPLEX_TO_FLOAT = ...

abs = ...
acos = ...
acosh = ...
asin = ...
asinh = ...
atan = ...
atanh = ...
cos = ...
cosh = ...
bessel_j0 = ...
bessel_j1 = ...
bessel_i0 = ...
bessel_i0e = ...
bessel_i1 = ...
bessel_i1e = ...
bitwise_not = ...
cbrt = ...
ceil = ...
conj_physical = ...
clone = ...
digamma = ...
erf = ...
erf_inv = ...
erfc = ...
erfcx = ...
exp = ...
expm1 = ...
exp2 = ...
fill = ...
floor = ...
imag = ...
isfinite = ...
lgamma = ...
log = ...
log1p = ...
log2 = ...
log10 = ...
real = ...
reciprocal = ...
ndtri = ...
neg = ...
round = ...
rsqrt = ...
sign = ...
signbit = ...
sin = ...
sinh = ...
spherical_bessel_j0 = ...
sqrt = ...
tan = ...
tanh = ...
trunc = ...
add = ...
atan2 = ...
bitwise_and = ...
bitwise_or = ...
bitwise_xor = ...
div = ...
eq = ...
fmax = ...
fmin = ...
fmod = ...
gcd = ...
ge = ...
gt = ...
hypot = ...
igamma = ...
igammac = ...
le = ...
lt = ...
maximum = ...
minimum = ...
mul = ...
ne = ...
nextafter = ...
pow = ...
remainder = ...
shift_left = ...
shift_right_arithmetic = ...
shift_right_logical = ...
sub = ...
zeta = ...
_as_strided_doc = ...
as_strided = ...
_broadcast_in_dim_doc = ...
broadcast_in_dim = ...
_collapse_view_doc = ...
collapse_view = ...
_conj_doc = ...
conj = ...

def expand_dims(a: TensorLikeType, dimensions: DimsSequenceType, ndim=...) -> TensorLikeType: ...

_split_dim_doc = ...
split_dim = ...
_squeeze_doc = ...
squeeze = ...
_transpose_doc = ...
transpose = ...
_view_of_doc = ...
view_of = ...
_view_element_type_doc = ...
view_element_type = ...
_as_strided_scatter_doc = ...
as_strided_scatter = ...
_collapse_doc = ...
collapse = ...
_cat_doc = ...
cat = ...
_reshape_doc = ...
reshape = ...
_rev_doc = ...
rev = ...
_where_doc = ...
where = ...
_convert_element_type_doc = ...
convert_element_type = ...
_device_put_doc = ...
device_put = ...
_item_doc = ...
item = ...
_maximum_value_doc = ...
maximum_value = ...
_minimum_value_doc = ...
minimum_value = ...
_copy_to_doc = ...
copy_to = ...
_copy_strided_doc = ...
copy_strided = ...
_resize_doc = ...
resize = ...
_sum_doc = ...
_xor_sum_doc = ...
_prod_doc = ...
_amax_doc = ...
_amin_doc = ...
_var_doc = ...
sum = ...
xor_sum = ...
prod = ...

def torch_var(input, dim=..., correction=..., **kwargs):  # -> Tensor:
    ...

var = ...
amax = ...
amin = ...
_iota_doc = ...
iota = ...
_empty_doc = ...
empty = ...
_empty_strided_doc = ...
empty_strided = ...
_empty_permuted_doc = ...
empty_permuted = ...
_full_doc = ...
full = ...
_full_like_doc = ...
full_like = ...
_scalar_tensor_doc = ...
scalar_tensor = ...
_svd_doc = ...
svd = ...
_normal_doc = ...
normal = ...
_uniform_doc = ...
_uniform_helper = ...
_fft_r2c_doc = ...
fft_r2c = ...
_fft_c2c_doc = ...
fft_c2c = ...
_fft_c2r_doc = ...
fft_c2r = ...
frexp = ...
_make_token = ...
_sink_tokens = ...
