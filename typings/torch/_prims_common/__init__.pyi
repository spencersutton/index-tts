import operator
import typing
import warnings
import torch
import sympy
from collections.abc import Sequence
from contextlib import AbstractContextManager, nullcontext
from enum import Enum
from functools import reduce
from typing import Any, Callable, NamedTuple, Optional, TYPE_CHECKING, TypeVar, Union, cast, overload
from typing_extensions import TypeAlias, deprecated
from torch import sym_float, sym_int, sym_max

if TYPE_CHECKING:
    class _WorksWithInt(typing.Protocol):
        def __add__(self, other: Any) -> typing.Self: ...
        def __radd__(self, other: Any) -> typing.Self: ...
        def __mul__(self, other: Any) -> typing.Self: ...
        def __rmul__(self, other: Any) -> typing.Self: ...

    _IntLikeT = TypeVar("_IntLikeT", bound=_WorksWithInt)
ShapeType: TypeAlias = Union[torch.Size, list[int], tuple[int, ...]]
StrideType: TypeAlias = Union[list[int], tuple[int, ...]]
DimsType: TypeAlias = Union[int, list[int], tuple[int, ...]]
DimsSequenceType: TypeAlias = Union[list[int], tuple[int, ...]]
NumberTypeType: TypeAlias = Union[type[bool], type[int], type[float], type[complex]]
NumberType: TypeAlias = Union[bool, int, float, complex]
RealNumberType: TypeAlias = Union[bool, int, float]
Number = ...
Dim = int
IntLike = ...
FloatLike = ...
BoolLike = ...
IntWithoutSymInt = int
FloatWithoutSymFloat = float
DeviceLikeType: TypeAlias = Union[str, torch.device, int]
Tensor = torch.Tensor
torch_function_passthrough = ...
TensorLikeType = torch.Tensor
TensorLike = torch.Tensor
TensorSequenceType: TypeAlias = Union[list[TensorLikeType], tuple[TensorLikeType, ...]]
TensorOrNumberLikeType: TypeAlias = Union[TensorLikeType, NumberType]
CustomOutParamAnnotation = ...

def same_shape(a: ShapeType, b: ShapeType, *, allow_rhs_unbacked=...) -> bool: ...
def compare_tensor_meta(
    a: TensorLikeType, b: TensorLikeType, check_sizes=..., check_strides=..., *, allow_rhs_unbacked=..., check_conj=...
):  # -> None:

    ...
def check_significant_strides(
    a: TensorLikeType, b: TensorLikeType, *, only_cuda=..., allow_rhs_unbacked=...
) -> tuple[bool, Optional[int]]: ...
def check_all_strides(a: TensorLikeType, b: TensorLikeType, *, only_cuda=...) -> tuple[bool, Optional[int]]: ...
def check_contiguous_sizes_strides(sizes, strides, false_if_dde=...):  # -> bool:

    ...
def is_contiguous(a: TensorLikeType, false_if_dde=...) -> bool: ...
def is_channels_last_contiguous_2d(a: Tensor, false_if_dde=...) -> bool: ...
def is_channels_last_contiguous_3d(a: Tensor, false_if_dde=...) -> bool: ...

_memory_formats = ...

def validate_memory_format(memory_format: torch.memory_format):  # -> None:
    ...
def is_contiguous_for_memory_format(a: Tensor, *, memory_format: torch.memory_format, false_if_dde=...) -> bool: ...
def is_contiguous_or_false(a: TensorLikeType) -> bool: ...
def is_channels_last_contiguous_or_false_2d(a: Tensor) -> bool: ...
def is_channels_last_contiguous_or_false_3d(a: Tensor) -> bool: ...
def is_contiguous_for_memory_format_or_false(a: Tensor, *, memory_format: torch.memory_format) -> bool: ...
def is_channels_last_contiguous(a: Tensor) -> bool: ...
def is_channels_last_contiguous_or_false(a: Tensor) -> bool: ...
def is_non_overlapping_and_dense(a: Tensor) -> bool: ...
def compute_elementwise_output_logical_to_physical_perm(*tensors, _skip_checks=...) -> list[int]: ...
def compute_elementwise_output_strides(*tensors) -> tuple[int, ...]: ...
def apply_perm(inp, perm):  # -> list[int]:
    ...
def invert_perm(perm):  # -> list[int]:
    ...
def validate_dim_length(length: int):  # -> None:

    ...
def validate_shape(shape: ShapeType):  # -> None:

    ...
def validate_strides(strides: StrideType):  # -> None:

    ...
def validate_idx(rank: int, idx: int):  # -> None:

    ...
def validate_dimension_indices(rank: int, indices: DimsSequenceType):  # -> None:
    ...
def validate_exclusive_idx(rank: int, ex_idx: int):  # -> None:

    ...
def canonicalize_dim(rank: int, idx: int, wrap_scalar: bool = ...) -> int: ...
@overload
def canonicalize_dims(rank: int, indices: Sequence[int], wrap_scalar: bool = ...) -> tuple[int, ...]: ...
@overload
def canonicalize_dims(rank: int, indices: int, wrap_scalar: bool = ...) -> int: ...
def canonicalize_dims(rank, indices, wrap_scalar=...):  # -> int | tuple[int, ...]:
    ...
def is_valid_permutation(rank: int, perm: DimsSequenceType) -> bool: ...
def is_same_shape(a: Sequence, b: Sequence) -> bool: ...
def is_cpu_scalar_tensor(a: Any) -> bool: ...
def check_same_device(*args, allow_cpu_scalar_tensors):  # -> None:

    ...
def canonicalize_device(device: DeviceLikeType) -> torch.device: ...
def check_same_shape(*args, allow_cpu_scalar_tensors: bool):  # -> None:

    ...
def extract_shape(*args, allow_cpu_scalar_tensors: bool) -> Optional[ShapeType]: ...
def extract_dims_from_varargs(dims: Union[DimsSequenceType, tuple[DimsSequenceType, ...]]) -> DimsSequenceType: ...
def extract_shape_from_varargs(shape: Union[ShapeType, tuple[ShapeType]], validate=...) -> tuple[int, ...]: ...
def infer_size_shapes(a: ShapeType, b: ShapeType) -> tuple[int, ...]: ...
def infer_size(shape: ShapeType, numel: int) -> tuple[int, ...]: ...

_integer_dtypes = ...
_low_precision_dtypes = ...
_complex_dtypes = ...

def is_boolean_dtype(dtype: torch.dtype) -> bool: ...
def is_integer_dtype(dtype: torch.dtype) -> bool: ...
def is_low_precision_dtype(dtype: torch.dtype) -> bool: ...
def is_float_dtype(dtype: torch.dtype) -> bool: ...
def is_complex_dtype(dtype: torch.dtype) -> bool: ...
def is_grad_dtype(dtype: torch.dtype) -> bool: ...

_complex_to_real_dtype_map = ...
_real_to_complex_dtype_map = ...

def corresponding_real_dtype(dtype: torch.dtype) -> torch.dtype: ...
def corresponding_complex_dtype(dtype: torch.dtype) -> torch.dtype: ...
def dtype_to_type(dtype: torch.dtype) -> type: ...
def dtype_to_type_ctor(dtype: torch.dtype) -> Callable[[NumberType], NumberType]: ...
def type_to_dtype(typ: type) -> torch.dtype: ...
def get_dtype(x: Union[torch.Tensor, NumberType]):  # -> dtype:
    ...

_ordered_types = ...

def check_fp_or_complex(dtype: torch.dtype, fn_name: str, allow_low_precision_dtypes: bool = ...):  # -> None:

    ...
def check_is_matrix(A: TensorLikeType, f_name: str, arg_name: str = ...):  # -> None:
    ...
def get_higher_type(a: type, b: type) -> type: ...
def get_higher_dtype(
    a: Optional[Union[torch.dtype, TensorLikeType, NumberType]],
    b: Optional[Union[torch.dtype, TensorLikeType, NumberType]],
) -> Optional[torch.dtype]: ...
def check_pin_memory(pin_memory: bool):  # -> None:
    ...
def check_layout(layout: torch.layout):  # -> None:
    ...
def is_weakly_lesser_type(a: type, b: type) -> bool: ...
def can_safe_cast_to(*, cast_to: torch.dtype, cast_from: torch.dtype) -> bool: ...
def check_same_dtype(*args):  # -> None:

    ...

_computation_dtype_map = ...

def get_computation_dtype(dtype: torch.dtype) -> torch.dtype: ...

_cpu_acc_type_map = ...

def get_acc_type(dtype: torch.dtype, device: torch.device) -> torch.dtype: ...

class ELEMENTWISE_TYPE_PROMOTION_KIND(Enum):
    DEFAULT = ...
    NO_OPMATH = ...
    INT_TO_FLOAT = ...
    ALWAYS_BOOL = ...
    COMPLEX_TO_FLOAT = ...
    BOOL_TO_LONG = ...

class REDUCTION_OUTPUT_TYPE_KIND(Enum):
    SAME = ...
    COMPLEX_TO_FLOAT = ...
    KEEP_PROMOTED_TYPE = ...
    ALWAYS_BOOL = ...

class RETURN_TYPE(Enum):
    NEW = ...
    VIEW = ...
    INPLACE = ...
    NONE = ...

def number_type(x: Union[NumberType, torch.SymInt, torch.SymFloat, torch.SymBool]) -> type: ...
def expr_type(x: sympy.Basic) -> type: ...
def elementwise_dtypes(
    *_args, type_promotion_kind: ELEMENTWISE_TYPE_PROMOTION_KIND
) -> tuple[torch.dtype, torch.dtype]: ...
def reduction_dtypes(
    arg, output_dtype_kind: REDUCTION_OUTPUT_TYPE_KIND, dtype: Optional[torch.dtype] = ...
) -> tuple[torch.dtype, Optional[torch.dtype]]: ...
def make_contiguous_strides_for(shape: ShapeType, row_major: bool = ...) -> tuple[Union[_IntLikeT, int], ...]: ...
def make_channels_last_1d_strides_for(shape: Sequence[_IntLikeT]) -> tuple[Union[_IntLikeT, int], ...]: ...
def make_channels_last_2d_strides_for(shape: Sequence[_IntLikeT]) -> tuple[Union[_IntLikeT, int], ...]: ...
def make_channels_last_3d_strides_for(shape: Sequence[_IntLikeT]) -> tuple[Union[_IntLikeT, int], ...]: ...
def make_channels_last_strides_for(shape: Sequence[_IntLikeT]) -> tuple[Union[_IntLikeT, int], ...]: ...
def compute_reduction_output_shape(shape: ShapeType, dimensions: Sequence) -> tuple[int, ...]: ...
def validate_no_repeating_dims(dims: Sequence):  # -> None:
    ...
def reduction_dims(shape: ShapeType, dims: Optional[Sequence]) -> tuple[int, ...]: ...
def set_correction(unbiased: Optional[bool] = ..., correction: Optional[NumberType] = ...) -> float: ...
def compute_required_storage_length(shape: ShapeType, strides: StrideType, storage_offset: int) -> int: ...
def check_in_bounds_for_storage(
    a: torch.TypedStorage, shape: ShapeType, strides: StrideType, storage_offset: int
):  # -> None:

    ...
@deprecated(
    "`torch._prims_common.check` is deprecated and will be removed in the future. "
    "Please use `torch._check*` functions instead.",
    category=FutureWarning,
)
def check(b: bool, s: Callable[[], str], exc_type: type[Exception] = ...) -> None: ...
def are_strides_like_channels_last(shape: Sequence[int], strides: Sequence[int]) -> bool: ...
def suggest_memory_format(x: TensorLikeType) -> torch.memory_format: ...
def prod(xs: Sequence[NumberType]) -> NumberType: ...
def is_expandable_to(shape: ShapeType, desired: ShapeType) -> bool: ...
def mask_tensor(mask: TensorLikeType, t: TensorLikeType):  # -> Tensor:

    ...
def get_aten_op(fn: Callable, name: str):  # -> Any:

    ...
def dtype_or_default(dtype: Optional[torch.dtype]) -> torch.dtype: ...
def device_or_default(device: Optional[DeviceLikeType]) -> DeviceLikeType: ...
def layout_or_default(layout: Optional[torch.layout]) -> torch.layout: ...
def clone_preserve_strides(x):  # -> Tensor:
    ...
def alert_not_deterministic(caller: str):  # -> None:
    ...

class CUDARngStateHelper:
    @staticmethod
    def get_torch_state_as_tuple(fake_mode: AbstractContextManager[Any] = ...):  # -> tuple[Tensor, Tensor]:
        ...
    @staticmethod
    def set_torch_state_tensor(seed, offset):  # -> None:
        ...
    @staticmethod
    def set_new_offset(relative_offset):  # -> None:
        ...
