import operator
import typing
import warnings
from collections.abc import Callable, Sequence
from contextlib import AbstractContextManager, nullcontext
from enum import Enum
from functools import reduce
from typing import TYPE_CHECKING, Any, NamedTuple, Optional, TypeAlias, TypeVar, Union, cast, overload
from warnings import deprecated

import sympy
import torch
from torch import sym_float, sym_int, sym_max

if TYPE_CHECKING:
    class _WorksWithInt(typing.Protocol):
        def __add__(self, other: Any) -> typing.Self: ...
        def __radd__(self, other: Any) -> typing.Self: ...
        def __mul__(self, other: Any) -> typing.Self: ...
        def __rmul__(self, other: Any) -> typing.Self: ...

    _IntLikeT = TypeVar("_IntLikeT", bound=_WorksWithInt)
type ShapeType = torch.Size | list[int] | tuple[int, ...]
type StrideType = list[int] | tuple[int, ...]
type DimsType = int | list[int] | tuple[int, ...]
type DimsSequenceType = list[int] | tuple[int, ...]
type NumberTypeType = type[bool] | type[int] | type[float] | type[complex]
type NumberType = bool | int | float | complex
type RealNumberType = bool | int | float
Number = ...
Dim = int
IntLike = ...
FloatLike = ...
BoolLike = ...
IntWithoutSymInt = int
FloatWithoutSymFloat = float
type DeviceLikeType = str | torch.device | int
Tensor = torch.Tensor
torch_function_passthrough = ...
TensorLikeType = torch.Tensor
TensorLike = torch.Tensor
type TensorSequenceType = list[TensorLikeType] | tuple[TensorLikeType, ...]
type TensorOrNumberLikeType = TensorLikeType | NumberType
CustomOutParamAnnotation = ...

def same_shape(a: ShapeType, b: ShapeType, *, allow_rhs_unbacked=...) -> bool: ...
def compare_tensor_meta(
    a: TensorLikeType, b: TensorLikeType, check_sizes=..., check_strides=..., *, allow_rhs_unbacked=..., check_conj=...
):  # -> None:

    ...
def check_significant_strides(
    a: TensorLikeType, b: TensorLikeType, *, only_cuda=..., allow_rhs_unbacked=...
) -> tuple[bool, int | None]: ...
def check_all_strides(a: TensorLikeType, b: TensorLikeType, *, only_cuda=...) -> tuple[bool, int | None]: ...
def check_contiguous_sizes_strides(sizes, strides, false_if_dde=...):  # -> bool:

    ...
def is_contiguous(a: TensorLikeType, false_if_dde=...) -> bool: ...
def is_channels_last_contiguous_2d(a: Tensor, false_if_dde=...) -> bool: ...
def is_channels_last_contiguous_3d(a: Tensor, false_if_dde=...) -> bool: ...

_memory_formats = ...

def validate_memory_format(memory_format: torch.memory_format):  # -> None:
    ...
def is_contiguous_for_memory_format(a: Tensor, *, memory_format: torch.memory_format, false_if_dde=...) -> bool: ...
def is_contiguous_or_false(a: TensorLikeType) -> bool: ...
def is_channels_last_contiguous_or_false_2d(a: Tensor) -> bool: ...
def is_channels_last_contiguous_or_false_3d(a: Tensor) -> bool: ...
def is_contiguous_for_memory_format_or_false(a: Tensor, *, memory_format: torch.memory_format) -> bool: ...
def is_channels_last_contiguous(a: Tensor) -> bool: ...
def is_channels_last_contiguous_or_false(a: Tensor) -> bool: ...
def is_non_overlapping_and_dense(a: Tensor) -> bool: ...
def compute_elementwise_output_logical_to_physical_perm(*tensors, _skip_checks=...) -> list[int]: ...
def compute_elementwise_output_strides(*tensors) -> tuple[int, ...]: ...
def apply_perm(inp, perm):  # -> list[int]:
    ...
def invert_perm(perm):  # -> list[int]:
    ...
def validate_dim_length(length: int):  # -> None:

    ...
def validate_shape(shape: ShapeType):  # -> None:

    ...
def validate_strides(strides: StrideType):  # -> None:

    ...
def validate_idx(rank: int, idx: int):  # -> None:

    ...
def validate_dimension_indices(rank: int, indices: DimsSequenceType):  # -> None:
    ...
def validate_exclusive_idx(rank: int, ex_idx: int):  # -> None:

    ...
def canonicalize_dim(rank: int, idx: int, wrap_scalar: bool = ...) -> int: ...
@overload
def canonicalize_dims(rank: int, indices: Sequence[int], wrap_scalar: bool = ...) -> tuple[int, ...]: ...
@overload
def canonicalize_dims(rank: int, indices: int, wrap_scalar: bool = ...) -> int: ...
def canonicalize_dims(rank, indices, wrap_scalar=...):  # -> int | tuple[int, ...]:
    ...
def is_valid_permutation(rank: int, perm: DimsSequenceType) -> bool: ...
def is_same_shape(a: Sequence, b: Sequence) -> bool: ...
def is_cpu_scalar_tensor(a: Any) -> bool: ...
def check_same_device(*args, allow_cpu_scalar_tensors):  # -> None:

    ...
def canonicalize_device(device: DeviceLikeType) -> torch.device: ...
def check_same_shape(*args, allow_cpu_scalar_tensors: bool):  # -> None:

    ...
def extract_shape(*args, allow_cpu_scalar_tensors: bool) -> ShapeType | None: ...
def extract_dims_from_varargs(dims: DimsSequenceType | tuple[DimsSequenceType, ...]) -> DimsSequenceType: ...
def extract_shape_from_varargs(shape: ShapeType | tuple[ShapeType], validate=...) -> tuple[int, ...]: ...
def infer_size_shapes(a: ShapeType, b: ShapeType) -> tuple[int, ...]: ...
def infer_size(shape: ShapeType, numel: int) -> tuple[int, ...]: ...

_integer_dtypes = ...
_low_precision_dtypes = ...
_complex_dtypes = ...

def is_boolean_dtype(dtype: torch.dtype) -> bool: ...
def is_integer_dtype(dtype: torch.dtype) -> bool: ...
def is_low_precision_dtype(dtype: torch.dtype) -> bool: ...
def is_float_dtype(dtype: torch.dtype) -> bool: ...
def is_complex_dtype(dtype: torch.dtype) -> bool: ...
def is_grad_dtype(dtype: torch.dtype) -> bool: ...

_complex_to_real_dtype_map = ...
_real_to_complex_dtype_map = ...

def corresponding_real_dtype(dtype: torch.dtype) -> torch.dtype: ...
def corresponding_complex_dtype(dtype: torch.dtype) -> torch.dtype: ...
def dtype_to_type(dtype: torch.dtype) -> type: ...
def dtype_to_type_ctor(dtype: torch.dtype) -> Callable[[NumberType], NumberType]: ...
def type_to_dtype(typ: type) -> torch.dtype: ...
def get_dtype(x: torch.Tensor | NumberType):  # -> dtype:
    ...

_ordered_types = ...

def check_fp_or_complex(dtype: torch.dtype, fn_name: str, allow_low_precision_dtypes: bool = ...):  # -> None:

    ...
def check_is_matrix(A: TensorLikeType, f_name: str, arg_name: str = ...):  # -> None:
    ...
def get_higher_type(a: type, b: type) -> type: ...
def get_higher_dtype(
    a: torch.dtype | TensorLikeType | NumberType | None,
    b: torch.dtype | TensorLikeType | NumberType | None,
) -> torch.dtype | None: ...
def check_pin_memory(pin_memory: bool):  # -> None:
    ...
def check_layout(layout: torch.layout):  # -> None:
    ...
def is_weakly_lesser_type(a: type, b: type) -> bool: ...
def can_safe_cast_to(*, cast_to: torch.dtype, cast_from: torch.dtype) -> bool: ...
def check_same_dtype(*args):  # -> None:

    ...

_computation_dtype_map = ...

def get_computation_dtype(dtype: torch.dtype) -> torch.dtype: ...

_cpu_acc_type_map = ...

def get_acc_type(dtype: torch.dtype, device: torch.device) -> torch.dtype: ...

class ELEMENTWISE_TYPE_PROMOTION_KIND(Enum):
    DEFAULT = ...
    NO_OPMATH = ...
    INT_TO_FLOAT = ...
    ALWAYS_BOOL = ...
    COMPLEX_TO_FLOAT = ...
    BOOL_TO_LONG = ...

class REDUCTION_OUTPUT_TYPE_KIND(Enum):
    SAME = ...
    COMPLEX_TO_FLOAT = ...
    KEEP_PROMOTED_TYPE = ...
    ALWAYS_BOOL = ...

class RETURN_TYPE(Enum):
    NEW = ...
    VIEW = ...
    INPLACE = ...
    NONE = ...

def number_type(x: NumberType | torch.SymInt | torch.SymFloat | torch.SymBool) -> type: ...
def expr_type(x: sympy.Basic) -> type: ...
def elementwise_dtypes(
    *_args, type_promotion_kind: ELEMENTWISE_TYPE_PROMOTION_KIND
) -> tuple[torch.dtype, torch.dtype]: ...
def reduction_dtypes(
    arg, output_dtype_kind: REDUCTION_OUTPUT_TYPE_KIND, dtype: torch.dtype | None = ...
) -> tuple[torch.dtype, torch.dtype | None]: ...
def make_contiguous_strides_for(shape: ShapeType, row_major: bool = ...) -> tuple[_IntLikeT | int, ...]: ...
def make_channels_last_1d_strides_for[IntLikeT: _WorksWithInt](
    shape: Sequence[_IntLikeT],
) -> tuple[_IntLikeT | int, ...]: ...
def make_channels_last_2d_strides_for[IntLikeT: _WorksWithInt](
    shape: Sequence[_IntLikeT],
) -> tuple[_IntLikeT | int, ...]: ...
def make_channels_last_3d_strides_for[IntLikeT: _WorksWithInt](
    shape: Sequence[_IntLikeT],
) -> tuple[_IntLikeT | int, ...]: ...
def make_channels_last_strides_for[IntLikeT: _WorksWithInt](
    shape: Sequence[_IntLikeT],
) -> tuple[_IntLikeT | int, ...]: ...
def compute_reduction_output_shape(shape: ShapeType, dimensions: Sequence) -> tuple[int, ...]: ...
def validate_no_repeating_dims(dims: Sequence):  # -> None:
    ...
def reduction_dims(shape: ShapeType, dims: Sequence | None) -> tuple[int, ...]: ...
def set_correction(unbiased: bool | None = ..., correction: NumberType | None = ...) -> float: ...
def compute_required_storage_length(shape: ShapeType, strides: StrideType, storage_offset: int) -> int: ...
def check_in_bounds_for_storage(
    a: torch.TypedStorage, shape: ShapeType, strides: StrideType, storage_offset: int
):  # -> None:

    ...
@deprecated(
    "`torch._prims_common.check` is deprecated and will be removed in the future. "
    "Please use `torch._check*` functions instead.",
    category=FutureWarning,
)
def check(b: bool, s: Callable[[], str], exc_type: type[Exception] = ...) -> None: ...
def are_strides_like_channels_last(shape: Sequence[int], strides: Sequence[int]) -> bool: ...
def suggest_memory_format(x: TensorLikeType) -> torch.memory_format: ...
def prod(xs: Sequence[NumberType]) -> NumberType: ...
def is_expandable_to(shape: ShapeType, desired: ShapeType) -> bool: ...
def mask_tensor(mask: TensorLikeType, t: TensorLikeType):  # -> Tensor:

    ...
def get_aten_op(fn: Callable, name: str):  # -> Any:

    ...
def dtype_or_default(dtype: torch.dtype | None) -> torch.dtype: ...
def device_or_default(device: DeviceLikeType | None) -> DeviceLikeType: ...
def layout_or_default(layout: torch.layout | None) -> torch.layout: ...
def clone_preserve_strides(x):  # -> Tensor:
    ...
def alert_not_deterministic(caller: str):  # -> None:
    ...

class CUDARngStateHelper:
    @staticmethod
    def get_torch_state_as_tuple(fake_mode: AbstractContextManager[Any] = ...):  # -> tuple[Tensor, Tensor]:
        ...
    @staticmethod
    def set_torch_state_tensor(seed, offset):  # -> None:
        ...
    @staticmethod
    def set_new_offset(relative_offset):  # -> None:
        ...
