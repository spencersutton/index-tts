import functools
from collections.abc import Callable
from typing import Union

import torch
from torch._ops import OpOverload

pytree = ...
__all__ = ["op_implementations_checks", "get_fast_op_impls", "stride_incorrect_op", "has_meta"]
op_implementations_dict = ...
op_implementations_checks = ...
aten = ...

def ordered_set(*items):  # -> dict[Any, bool]:
    ...
def is_noncontiguous_supported(device): ...

_like_tensor_constructors = ...
_device_not_kwarg_ops = ...
_non_kwarg_device_constructors = ...

def contains_tensor_types(type):  # -> bool:
    ...
def register_op_impl(run_impl_check: Callable[[OpOverload], bool] | OpOverload):  # -> Callable[..., Any]:
    ...
@register_op_impl(op_implementations_dict.__contains__)
def dispatch_to_op_implementations_dict(fake_mode, func, *args, **kwargs): ...
@register_op_impl(_is_tensor_constructor)
@register_op_impl([*_like_tensor_constructors])
def constructors(fake_mode, func, *args, **kwargs):  # -> FakeTensor:
    ...
@register_op_impl(aten.is_pinned.default)
def non_kwarg_is_pinned(fake_mode, func, *args, **kwargs): ...
@register_op_impl(aten.to.prim_Device)
@register_op_impl(aten.to.device)
def non_kwarg_to(fake_mode, func, *args, **kwargs): ...
def stride_incorrect_op(op):  # -> Literal[False]:
    ...
@register_op_impl(stride_incorrect_op)
def wordaround_stride_incorrect_op(fake_mode, func, *args, **kwargs):  # -> FakeTensor:
    ...
@register_op_impl(aten.resize_as_.default)
def resize_as_(fake_mode, func, *args, **kwargs): ...
@register_op_impl(
    lambda func: torch.Tag.dynamic_output_shape in func.tags
    and func not in [aten.index.Tensor, aten.nonzero.default, aten.repeat_interleave.Tensor]
)
def dyn_shape(fake_mode, func, *args, **kwargs): ...
@register_op_impl(aten._unique2.default)
def unique2(fake_mode, func, arg, sorted=..., return_inverse=..., return_counts=...):  # -> tuple[Any, ...]:
    ...
@register_op_impl(aten.select.int)
def meta_select(fake_mode, func, self, dim, index):  # -> _NotImplementedType:
    ...
@register_op_impl(aten.unique_dim.default)
def unique_dim(fake_mode, func, arg, dim, sorted=..., return_inverse=..., return_counts=...):  # -> tuple[Any, ...]:
    ...
@register_op_impl(aten.unique_consecutive.default)
def _(fake_mode, func, arg, return_inverse=..., return_counts=..., dim=...):  # -> tuple[Any, ...]:
    ...
@register_op_impl(aten.repeat_interleave.Tensor)
def repeat_interleave_tensor(fake_mode, func, repeats, output_size=...): ...
@register_op_impl(torch.ops.aten.item.default)
@register_op_impl(torch.ops.aten._local_scalar_dense.default)
def local_scalar_dense(fake_mode, func, arg): ...
@register_op_impl(torch.ops.aten.nonzero_numpy.default)
def nonzero_numpy(fake_mode, func, arg):  # -> Any:
    ...
@register_op_impl(torch.ops.aten.nonzero.default)
def nonzero(fake_mode, func, arg): ...
@register_op_impl(torch.ops.aten.masked_select.default)
def masked_select(fake_mode, func, self, mask): ...
@register_op_impl(torch.ops.aten._assert_tensor_metadata.default)
def assert_tensor_metadata(
    fake_mode, func, t, sizes=..., strides=..., dtype=..., *, device=..., layout=...
) -> None: ...
@register_op_impl(lambda func: torch.Tag.data_dependent_output in func.tags)
def data_dep(fake_mode, func, *args, **kwargs): ...
def check_no_bool_index_tensors(func, self, indices):  # -> None:
    ...
def run_and_return_new_tensor_of_input_device(fake_mode, func, args, kwargs):  # -> FakeTensor:
    ...

_is_builtin_namespaces = ...

def is_builtin(op):  # -> bool:
    ...
def has_meta(func): ...
@register_op_impl(lambda func: is_builtin(func) and func.name().startswith("aten::_foreach_") and has_meta(func))
def foreach_run_and_map_input_device(fake_mode, func, *args, **kwargs):  # -> _NotImplementedType | list[Any]:
    ...
@register_op_impl(aten.index.Tensor)
def index_tensor(fake_mode, func, *args, **kwargs): ...
@register_op_impl(aten._embedding_bag.default)
def embedding_bag(fake_mode, func, *args, **kwargs):  # -> tuple[Any, Any, Any, Any]:
    ...
@register_op_impl(aten._unsafe_index_put.default)
@register_op_impl(aten.copy.default)
@register_op_impl(aten.copy_.default)
@register_op_impl(aten.slice_scatter.default)
def multi_device_op_default(fake_mode, func, *args, **kwargs):  # -> FakeTensor:
    ...
@register_op_impl(aten.copy.out)
@register_op_impl(aten.slice_scatter.out)
def multi_device_op_out(fake_mode, func, *args, **kwargs):  # -> Any:
    ...
@register_op_impl(aten.index_put.default)
@register_op_impl(aten.index_put_.default)
def index_put_impl(fake_mode, func, *args, **kwargs):  # -> Any | FakeTensor:
    ...
@register_op_impl(aten._nested_tensor_from_tensor_list.default)
@register_op_impl(aten._nested_tensor_from_tensor_list.out)
@register_op_impl(aten._nested_view_from_buffer.default)
@register_op_impl(aten._nested_view_from_buffer_copy.default)
def nested_tensors_unsupported(fake_mode, func, *args, **kwargs): ...
@register_op_impl([
    x
    for x in _device_not_kwarg_ops
    if x
    not in (
        aten.is_pinned.default,
        aten.to.device,
        aten.to.prim_Device,
        aten._nested_tensor_from_tensor_list.default,
        aten._nested_tensor_from_tensor_list.out,
    )
])
def nyi(fake_mode, func, *args, **kwargs):  # -> None:
    ...
@register_op_impl([aten.convolution.default, aten.convolution_backward.default])
def conv(
    fake_mode, func, *args, **kwargs
):  # -> FakeTensor | tuple[Any | FakeTensor, Any | FakeTensor, Any | FakeTensor]:
    ...
@register_op_impl(torch.ops.aten.bincount.default)
def bincount(fake_mode, func, inputs, weights=..., minlength=...): ...

FAST_OP_IMPLEMENTATIONS = ...

def register_fast_op_impl(func: OpOverload):  # -> Callable[..., Any]:
    ...
def infer_size(a, b):  # -> tuple[int, ...]:
    ...
def make_fast_binary_impl(slow_ref, type_promotion_kind=...):  # -> Callable[..., Any | FakeTensor]:
    ...
def fast_detach(fake_mode, x, include_real=...):  # -> FakeTensor:
    ...
@functools.cache
def get_fast_op_impls():  # -> dict[Any, Any]:
    ...
