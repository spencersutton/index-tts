import contextlib
from abc import ABC, abstractmethod
from collections.abc import Callable
from contextlib import AbstractContextManager
from typing import Any, Optional, Union

import torch
from torch.utils._python_dispatch import TorchDispatchMode

not_implemented_log = ...

class FunctionalTensor(torch.Tensor):
    elem: torch.Tensor
    _mode_key = ...
    _extra_dispatch_keys = ...
    metadata_fns = ...
    _inference_mode_base: FunctionalTensor | None = ...
    def __new__(cls, elem, mode):  # -> Self:
        ...
    def __torch_dispatch__(self, func, types, args=..., kwargs=...):  # -> _NotImplementedType:
        ...
    @staticmethod
    def to_functional(x):  # -> FunctionalTensor:
        ...
    def from_functional(self):  # -> Tensor:
        ...
    def is_base_tensor(self) -> bool: ...
    def replace_(self, output) -> None: ...
    def commit_update(self) -> None: ...
    def sync(self) -> None: ...
    def mark_mutation_hidden_from_autograd(self) -> None: ...
    def tolist(self) -> Any: ...
    def to(self, *args, **kwargs):  # -> Tensor:
        ...
    def cuda(self, device=..., *args, **kwargs):  # -> Tensor:
        ...

    char = ...
    cpu = ...
    bfloat16 = ...
    byte = ...
    double = ...
    float = ...
    bool = ...
    half = ...
    int = ...
    long = ...
    def to_dense(self):  # -> Tensor:
        ...
    @property
    def layout(self): ...
    def __bool__(self):  # -> bool:
        ...

class FunctionalTensorMode(TorchDispatchMode):
    def __init__(self, pre_dispatch=..., export=..., _allow_token_discovery=...) -> None: ...
    def __enter__(self):  # -> Self:
        ...
    def __exit__(self, a, b, c):  # -> None:
        ...
    def __torch_dispatch__(self, func, types, args=..., kwargs=...):  # -> _NotImplementedType | Any | tuple[Any, ...]:
        ...
    @classmethod
    def is_infra_mode(cls) -> bool: ...

@contextlib.contextmanager
def disable_functional_mode():  # -> Generator[Any | None, Any, None]:
    ...
def dispatch_functionalize(func, mode: FunctionalTensorMode = ...):  # -> Callable[..., PyTree]:
    ...

class BaseFunctionalizeAPI(ABC):
    @abstractmethod
    def wrap_tensors(self, args: tuple[Any]) -> tuple[Any]: ...
    @abstractmethod
    def unwrap_tensors(self, args: torch.Tensor | tuple[torch.Tensor, ...]) -> Any: ...
    @abstractmethod
    def functionalize(self, inner_f: Callable) -> Callable: ...
    @abstractmethod
    def redispatch_to_next(self) -> AbstractContextManager: ...
    @abstractmethod
    def replace(self, input_tensor, output_tensor) -> None: ...
    @abstractmethod
    def commit_update(self, tensor) -> None: ...
    @abstractmethod
    def sync(self, tensor) -> None: ...
    @abstractmethod
    def mark_mutation_hidden_from_autograd(self, tensor) -> None: ...

class PythonFunctionalizeAPI(BaseFunctionalizeAPI):
    def __init__(self, mode: FunctionalTensorMode | None = ..., pre_dispatch: bool = ...) -> None: ...
    def wrap_tensors(self, args: tuple[Any]) -> tuple[Any]: ...
    def unwrap_tensors(self, args: torch.Tensor | tuple[torch.Tensor, ...] | list[torch.Tensor]) -> Any: ...
    def functionalize(self, inner_f: Callable) -> Callable: ...
    def redispatch_to_next(self) -> AbstractContextManager: ...
    def replace(self, input_tensor, output_tensor) -> None: ...
    def commit_update(self, tensor) -> None: ...
    def sync(self, tensor) -> None: ...
    def mark_mutation_hidden_from_autograd(self, tensor) -> None: ...

class CppFunctionalizeAPI(BaseFunctionalizeAPI):
    def wrap_tensors(self, args: tuple[Any]) -> tuple[Any]: ...
    def unwrap_tensors(
        self, args: torch.Tensor | tuple[torch.Tensor, ...]
    ) -> torch.Tensor | tuple[torch.Tensor, ...]: ...
    def functionalize(self, inner_f: Callable) -> Callable: ...
    def redispatch_to_next(self) -> AbstractContextManager: ...
    def replace(self, input_tensor, output_tensor) -> None: ...
    def commit_update(self, tensor) -> None: ...
    def sync(self, tensor) -> None: ...
    def mark_mutation_hidden_from_autograd(self, tensor) -> None: ...

class FunctorchFunctionalizeAPI(BaseFunctionalizeAPI):
    def __init__(self, interpreter) -> None: ...
    def wrap_tensors(self, args: tuple[Any]) -> tuple[Any]: ...
    def unwrap_tensors(
        self, args: torch.Tensor | tuple[torch.Tensor, ...]
    ) -> torch.Tensor | tuple[torch.Tensor, ...]: ...
    def functionalize(self, inner_f: Callable) -> Callable: ...
    def redispatch_to_next(self) -> AbstractContextManager: ...
    def replace(self, input_tensor, output_tensor) -> None: ...
    def commit_update(self, tensor) -> None: ...
    def sync(self, tensor) -> None: ...
    def mark_mutation_hidden_from_autograd(self, tensor) -> None: ...

def mb_unwrap_functional_tensor(tensor: torch.Tensor):  # -> Tensor:
    ...
