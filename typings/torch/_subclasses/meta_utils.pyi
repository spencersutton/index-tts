from abc import abstractmethod
from collections.abc import Callable, Generator
from contextlib import contextmanager
from dataclasses import dataclass
from typing import (
    TYPE_CHECKING,
    ClassVar,
    Generic,
    NewType,
    Optional,
    Protocol,
    TypeGuard,
    TypeVar,
    Union,
    Unpack,
    override,
)

import torch
from torch._C._autograd import CreationMeta
from torch._C._functorch import CInterpreter, is_batchedtensor, is_gradtrackingtensor, is_legacy_batchedtensor
from torch._guards import Source
from torch._subclasses.fake_tensor import FakeTensor, FakeTensorMode
from torch.fx.experimental.symbolic_shapes import ShapeEnv, SymbolicContext
from torch.utils._python_dispatch import is_traceable_wrapper_subclass
from typing_extensions import TypedDict

if TYPE_CHECKING: ...
DimList = list
_TensorLikeT = TypeVar("_TensorLikeT", MetaTensorDesc, torch.Tensor)
_T = TypeVar("_T")
_TensorT = TypeVar("_TensorT", bound=torch.Tensor)
_TensorT_cov = TypeVar("_TensorT_cov", bound=torch.Tensor, covariant=True)

def safe_is_leaf(t: MetaTensorDesc | torch.Tensor) -> bool: ...
def safe_grad[TensorLikeT: (MetaTensorDesc, torch.Tensor)](t: _TensorLikeT) -> _TensorLikeT | None: ...
def assert_eq(a: _T, b: _T) -> None: ...

tls = ...

@contextmanager
def disable_inference_mode_for_fake_prop() -> Generator[None]: ...
def assert_metadata_eq(
    assert_eq: Callable[[object, object], None],
    m1: MetaTensorDesc | torch.Tensor,
    m2: torch.Tensor,
    *,
    skip_symbolic: bool = ...,
    skip_leaf: bool = ...,
) -> None: ...
def is_sparse_coo(t: object) -> TypeGuard[torch.Tensor]: ...
def is_sparse_compressed_layout(layout: torch.layout) -> bool: ...
def is_sparse_compressed(t: object) -> TypeGuard[torch.Tensor]: ...
def is_sparse_any(t: object) -> TypeGuard[torch.Tensor]: ...

MetaStorageId = NewType("MetaStorageId", int)
MetaTensorId = NewType("MetaTensorId", int)
_DescriberId = NewType("_DescriberId", int)
DESCRIBER_NEXT_ID = ...

class MetaTensorDescriber:
    def __init__(self, *, copy_data: bool = ...) -> None: ...
    def get_tensor_id(self, t: torch.Tensor) -> MetaTensorId: ...
    def get_storage_id(self, s: torch.UntypedStorage) -> MetaStorageId: ...
    def describe_storage(self, s: torch.UntypedStorage, *, trace: bool = ...) -> MetaStorageDesc: ...
    def describe_tensor(self, t: torch.Tensor, *, recurse: bool = ..., trace: bool = ...) -> MetaTensorDesc: ...

@dataclass(frozen=True)
class MetaStorageDesc:
    id: MetaStorageId
    size: int
    data: torch.UntypedStorage | None
    def as_json(self, describer_id: _DescriberId) -> dict[str, object]: ...

@dataclass(frozen=True)
class ViewFunc[TensorT: torch.Tensor]:
    @abstractmethod
    def apply(
        self,
        t: _TensorT,
        new_base: _TensorT,
        symint_visitor_fn: Callable[[int], int] | None = ...,
        tensor_visitor_fn: Callable[[torch.Tensor], _TensorT] | None = ...,
    ) -> _TensorT: ...
    @staticmethod
    def from_tensor(t: torch.Tensor) -> ViewFunc: ...

@dataclass(frozen=True)
class _FakeTensorViewFunc(ViewFunc["FakeTensor"]):
    @override
    def apply(
        self,
        t: torch.Tensor,
        new_base: torch.Tensor,
        symint_visitor_fn: Callable[[int], int] | None = ...,
        tensor_visitor_fn: Callable[[torch.Tensor], FakeTensor] | None = ...,
    ) -> FakeTensor: ...

@dataclass(frozen=True)
class _CustomViewFunc[TensorT: torch.Tensor](ViewFunc[TensorT]):
    func: Callable[
        [torch.Tensor, Callable[[int], int] | None, Callable[[torch.Tensor], _TensorT] | None],
        _TensorT,
    ]
    @override
    def apply(
        self,
        t: torch.Tensor,
        new_base: torch.Tensor,
        symint_visitor_fn: Callable[[int], int] | None = ...,
        tensor_visitor_fn: Callable[[torch.Tensor], _TensorT] | None = ...,
    ) -> _TensorT: ...

class _MetaTensorCallback[TensorT_cov: torch.Tensor](Protocol):
    def __call__(self, arg: Callable[[], torch.Tensor], /, *, device: torch.device | str) -> _TensorT_cov: ...

class _MetaTensorCallbackKwargs(TypedDict, total=False):
    device: torch.device | str

class _MetaTensorCallbackOptDevice[TensorT_cov: torch.Tensor](Protocol):
    def __call__(
        self, arg: Callable[[], torch.Tensor], /, **kwargs: Unpack[_MetaTensorCallbackKwargs]
    ) -> _TensorT_cov: ...

@dataclass(frozen=True)
class MetaTensorDesc[TensorT: torch.Tensor]:
    id: MetaTensorId
    ndim: int
    dtype: torch.dtype
    device: torch.device
    size: tuple[int, ...]
    dynamo_dynamic_indices: list[int]
    dynamo_hint_overrides: dict[int, int]
    layout: torch.layout = ...
    is_inference: bool = ...
    is_leaf: bool = ...
    requires_grad: bool = ...
    is_sparse: bool = ...
    is_mkldnn: bool = ...
    is_functorch_wrapped: bool = ...
    is_batchedtensor: bool = ...
    is_legacy_batchedtensor: bool = ...
    is_gradtrackingtensor: bool = ...
    is_view: bool = ...
    is_nested: bool = ...
    nested_int: int | None = ...
    is_traceable_wrapper_subclass: bool = ...
    is_functional: bool = ...
    is_conj: bool = ...
    is_neg: bool = ...
    is_parameter: bool = ...
    stride: tuple[int, ...] | None = ...
    storage_offset: int = ...
    storage: MetaStorageDesc | None = ...
    sparse_dim: int | None = ...
    dense_dim: int | None = ...
    is_coalesced: bool | None = ...
    crow_indices: MetaTensorDesc | None = ...
    col_indices: MetaTensorDesc | None = ...
    ccol_indices: MetaTensorDesc | None = ...
    row_indices: MetaTensorDesc | None = ...
    values: MetaTensorDesc | None = ...
    unwrapped: MetaTensorDesc | None = ...
    bdim: int | None = ...
    base: MetaTensorDesc | None = ...
    attrs: dict[str, MetaTensorDesc] | None = ...
    creation_meta: CreationMeta | None = ...
    grad: MetaTensorDesc | None = ...
    _UNSERIALIZABLE: ClassVar[set[str]] = ...
    ctx: object | None = ...
    type: type | None = ...
    fake_mode: FakeTensorMode | None = ...
    view_func: ViewFunc | None = ...
    level: int | None = ...
    current_level: int | None = ...
    functorch_stack: list[CInterpreter] | None = ...
    autograd_meta_from: torch.Tensor | None = ...
    data: torch.Tensor | None = ...
    def as_json(self, describer_id: _DescriberId) -> dict[str, object]: ...
    @property
    def shape(self) -> tuple[int, ...]: ...

class MetaConverter[TensorT: torch.Tensor]:
    def __init__(self, *, copy_data: bool = ...) -> None: ...
    def successful(self) -> bool: ...
    def get_tensor_memo(self, t: MetaTensorDesc) -> torch.Tensor | None: ...
    def set_tensor_memo(self, t: MetaTensorDesc, v: _TensorT) -> None: ...
    def get_storage_memo(self, s: MetaStorageDesc) -> torch.UntypedStorage | None: ...
    def set_storage_memo(self, s: MetaStorageDesc, v: torch.UntypedStorage) -> None: ...
    def meta_storage(
        self, s: MetaStorageDesc, callback: Callable[[Callable[[], torch.Tensor]], _TensorT]
    ) -> torch.UntypedStorage: ...
    def meta_tensor(
        self,
        t: MetaTensorDesc,
        shape_env: ShapeEnv | None,
        callback_: _MetaTensorCallback[_TensorT],
        source: Source | None,
        symbolic_context: SymbolicContext | None,
    ) -> _TensorT: ...
    def __call__(
        self,
        t: torch.Tensor,
        shape_env: ShapeEnv | None = ...,
        *,
        callback: _MetaTensorCallback[_TensorT] | None = ...,
        source: Source | None = ...,
        symbolic_context: SymbolicContext | None = ...,
        trace: bool = ...,
    ) -> _TensorT: ...
