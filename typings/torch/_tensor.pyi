from collections.abc import Iterator
import enum
import torch
import torch._C as _C
from typing import Any, Optional, TypeVar, Union
from typing import ParamSpec

class Tensor(torch._C.TensorBase):
    _is_param: bool
    def __deepcopy__(self, memo):  # -> Any | Tensor:
        ...
    def __reduce_ex__(self, proto):  # -> Any | tuple[Callable[..., Any], tuple[Any, type[Tensor], Any, Any]]:
        ...
    def storage(self):  # -> Any | TypedStorage:

        ...
    def __setstate__(self, state):  # -> Any | None:
        ...
    def __repr__(self, *, tensor_contents=...):  # -> Any:
        ...
    def backward(self, gradient=..., retain_graph=..., create_graph=..., inputs=...):  # -> Any | None:

        ...
    def register_hook(self, hook):  # -> Any | RemovableHandle:

        ...
    def register_post_accumulate_grad_hook(self, hook):  # -> Any | RemovableHandle:

        ...
    def reinforce(self, reward): ...

    detach = ...
    detach_ = ...
    def is_shared(self):  # -> Any:

        ...
    def share_memory_(self):  # -> Any | Self:

        ...
    def module_load(self, other, assign=...):  # -> Any | Tensor:

        ...
    def __reversed__(self):  # -> Any | Self | Tensor:

        ...
    def norm(self, p: float | str | None = ..., dim=..., keepdim=..., dtype=...):  # -> Any | Tensor:

        ...
    def solve(self, other):  # -> tuple[Tensor, Tensor]:
        ...
    def lstsq(self, other):  # -> tuple[Tensor, Tensor]:
        ...
    def eig(self, eigenvectors=...):  # -> tuple[Tensor, Tensor]:
        ...
    def symeig(self, eigenvectors=...):  # -> tuple[Tensor, Tensor]:
        ...
    def lu(self, pivot=..., get_infos=...):  # -> Any | tuple[Tensor, Tensor, Tensor] | tuple[Tensor, Tensor]:

        ...
    def stft(
        self,
        n_fft: int,
        hop_length: int | None = ...,
        win_length: int | None = ...,
        window: Tensor | None = ...,
        center: bool = ...,
        pad_mode: str = ...,
        normalized: bool = ...,
        onesided: bool | None = ...,
        return_complex: bool | None = ...,
        align_to_window: bool | None = ...,
    ):  # -> Any | Tensor:

        ...
    def istft(
        self,
        n_fft: int,
        hop_length: int | None = ...,
        win_length: int | None = ...,
        window: Tensor | None = ...,
        center: bool = ...,
        normalized: bool = ...,
        onesided: bool | None = ...,
        length: int | None = ...,
        return_complex: bool = ...,
    ):  # -> Any:

        ...
    def resize(self, *sizes):  # -> Any | None:
        ...
    def resize_as(self, tensor):  # -> Any | None:
        ...
    def split(self, split_size, dim=...):  # -> Any | tuple[Tensor, ...]:

        ...
    def unique(self, sorted=..., return_inverse=..., return_counts=..., dim=...):  # -> Any:

        ...
    def unique_consecutive(self, return_inverse=..., return_counts=..., dim=...):  # -> Any:

        ...
    @_handle_torch_function_and_wrap_type_error_to_not_implemented
    def __rsub__(self, other: Tensor | bool | complex) -> Tensor: ...
    @_handle_torch_function_and_wrap_type_error_to_not_implemented
    def __rdiv__(self, other: Tensor | bool | complex) -> Tensor: ...

    __rtruediv__ = ...
    __itruediv__ = ...
    __pow__ = ...
    __ipow__ = ...
    @_handle_torch_function_and_wrap_type_error_to_not_implemented
    def __rmod__(self, other: Tensor | bool | complex) -> Tensor: ...
    def __format__(self, format_spec):  # -> Any | str:
        ...
    @_handle_torch_function_and_wrap_type_error_to_not_implemented
    def __rpow__(self, other: Tensor | bool | complex) -> Tensor: ...
    @_handle_torch_function_and_wrap_type_error_to_not_implemented
    def __floordiv__(self, other: Tensor | float | bool) -> Tensor: ...
    @_handle_torch_function_and_wrap_type_error_to_not_implemented
    def __rfloordiv__(self, other: Tensor | float | bool) -> Tensor: ...
    @_handle_torch_function_and_wrap_type_error_to_not_implemented
    def __rlshift__(self, other: Tensor | bool | complex) -> Tensor: ...
    @_handle_torch_function_and_wrap_type_error_to_not_implemented
    def __rrshift__(self, other: Tensor | bool | complex) -> Tensor: ...
    @_handle_torch_function_and_wrap_type_error_to_not_implemented
    def __rmatmul__(self, other: Tensor) -> Tensor: ...

    __pos__ = ...
    __neg__ = ...
    __abs__ = ...
    def __len__(self):  # -> Any:
        ...
    def __iter__(self) -> Iterator[Tensor]: ...
    def __hash__(self) -> int: ...
    def __dir__(self):  # -> Any | list[str]:
        ...

    __array_priority__ = ...
    def __array__(self, dtype=...):  # -> Any | ndarray[_AnyShape, dtype[Any]]:
        ...
    def __array_wrap__(self, array):  # -> Any | Tensor:
        ...
    def __contains__(self, element: Any, /) -> bool: ...
    @property
    def __cuda_array_interface__(self):  # -> Any | dict[str, str | tuple[Any, ...] | int | None]:

        ...
    def storage_type(self):  # -> Any:

        ...
    def refine_names(self, *names):  # -> Any | Tensor:

        ...
    def align_to(self, *names):  # -> Any | Tensor:

        ...
    def unflatten(self, dim, sizes):  # -> Any | Tensor:

        ...
    def rename_(self, *names, **rename_map):  # -> Any:

        ...
    def rename(self, *names, **rename_map):  # -> Any:

        ...
    def to_sparse_coo(self):  # -> Tensor:

        ...
    def dim_order(self, *, ambiguity_check: bool | list[torch.memory_format] = ...):  # -> Any | tuple[int, ...]:

        ...
    @classmethod
    def __torch_function__(
        cls, func, types, args=..., kwargs=...
    ):  # -> _NotImplementedType | list[Any] | tuple[Any, ...] | Tensor:

        ...

    __torch_dispatch__ = ...
    def __dlpack__(
        self,
        *,
        stream: Any | None = ...,
        max_version: tuple[int, int] | None = ...,
        dl_device: tuple[enum.IntEnum, int] | None = ...,
        copy: bool | None = ...,
    ):  # -> Any:

        ...
    def __dlpack_device__(self) -> tuple[enum.IntEnum, int]: ...

    __module__ = ...
