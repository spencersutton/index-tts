import enum
from collections.abc import Iterator
from typing import Any

import torch

class Tensor(torch._C.TensorBase):
    _is_param: bool
    def __deepcopy__(self, memo): ...
    def __reduce_ex__(self, proto): ...
    def storage(self): ...
    def __setstate__(self, state): ...
    def __repr__(self, *, tensor_contents=...) -> str: ...
    def backward(self, gradient=..., retain_graph=..., create_graph=..., inputs=...): ...
    def register_hook(self, hook): ...
    def register_post_accumulate_grad_hook(self, hook): ...
    def reinforce(self, reward): ...

    detach = ...
    detach_ = ...
    def is_shared(self): ...
    def share_memory_(self): ...
    def module_load(self, other, assign=...): ...
    def __reversed__(self): ...
    def norm(self, p: float | str | None = ..., dim=..., keepdim=..., dtype=...): ...
    def solve(self, other): ...
    def lstsq(self, other): ...
    def eig(self, eigenvectors=...): ...
    def symeig(self, eigenvectors=...): ...
    def lu(self, pivot=..., get_infos=...): ...
    def stft(
        self,
        n_fft: int,
        hop_length: int | None = ...,
        win_length: int | None = ...,
        window: Tensor | None = ...,
        center: bool = ...,
        pad_mode: str = ...,
        normalized: bool = ...,
        onesided: bool | None = ...,
        return_complex: bool | None = ...,
        align_to_window: bool | None = ...,
    ): ...
    def istft(
        self,
        n_fft: int,
        hop_length: int | None = ...,
        win_length: int | None = ...,
        window: Tensor | None = ...,
        center: bool = ...,
        normalized: bool = ...,
        onesided: bool | None = ...,
        length: int | None = ...,
        return_complex: bool = ...,
    ): ...
    def resize(self, *sizes): ...
    def resize_as(self, tensor): ...
    def split(self, split_size: int | list[Any], dim: int = ...) -> tuple[Tensor, ...]: ...
    def unique(self, sorted=..., return_inverse=..., return_counts=..., dim=...): ...
    def unique_consecutive(self, return_inverse=..., return_counts=..., dim=...): ...
    @_handle_torch_function_and_wrap_type_error_to_not_implemented
    def __rsub__(self, other: Tensor | bool | complex) -> Tensor: ...
    @_handle_torch_function_and_wrap_type_error_to_not_implemented
    def __rdiv__(self, other: Tensor | bool | complex) -> Tensor: ...

    __rtruediv__ = ...
    __itruediv__ = ...
    __pow__ = ...
    __ipow__ = ...
    @_handle_torch_function_and_wrap_type_error_to_not_implemented
    def __rmod__(self, other: Tensor | bool | complex) -> Tensor: ...
    def __format__(self, format_spec) -> str: ...
    @_handle_torch_function_and_wrap_type_error_to_not_implemented
    def __rpow__(self, other: Tensor | bool | complex) -> Tensor: ...
    @_handle_torch_function_and_wrap_type_error_to_not_implemented
    def __floordiv__(self, other: Tensor | float | bool) -> Tensor: ...
    @_handle_torch_function_and_wrap_type_error_to_not_implemented
    def __rfloordiv__(self, other: Tensor | float | bool) -> Tensor: ...
    @_handle_torch_function_and_wrap_type_error_to_not_implemented
    def __rlshift__(self, other: Tensor | bool | complex) -> Tensor: ...
    @_handle_torch_function_and_wrap_type_error_to_not_implemented
    def __rrshift__(self, other: Tensor | bool | complex) -> Tensor: ...
    @_handle_torch_function_and_wrap_type_error_to_not_implemented
    def __rmatmul__(self, other: Tensor) -> Tensor: ...

    __pos__ = ...
    __neg__ = ...
    __abs__ = ...
    def __len__(self) -> int: ...
    def __iter__(self) -> Iterator[Tensor]: ...
    def __hash__(self) -> int: ...
    def __dir__(self): ...

    __array_priority__ = ...
    def __array__(self, dtype=...): ...
    def __array_wrap__(self, array): ...
    def __contains__(self, element: Any, /) -> bool: ...
    @property
    def __cuda_array_interface__(self): ...
    def storage_type(self): ...
    def refine_names(self, *names): ...
    def align_to(self, *names): ...
    def unflatten(self, dim, sizes): ...
    def rename_(self, *names, **rename_map): ...
    def rename(self, *names, **rename_map): ...
    def to_sparse_coo(self): ...
    def dim_order(self, *, ambiguity_check: bool | list[torch.memory_format] = ...): ...
    @classmethod
    def __torch_function__(cls, func, types, args=..., kwargs=...): ...

    __torch_dispatch__ = ...
    def __dlpack__(
        self,
        *,
        stream: Any | None = ...,
        max_version: tuple[int, int] | None = ...,
        dl_device: tuple[enum.IntEnum, int] | None = ...,
        copy: bool | None = ...,
    ): ...
    def __dlpack_device__(self) -> tuple[enum.IntEnum, int]: ...

    __module__ = ...
