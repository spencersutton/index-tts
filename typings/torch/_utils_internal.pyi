import functools
import os
from typing import Any, Optional, TypeVar
from collections.abc import Callable
from typing import ParamSpec

_T = TypeVar("_T")
_P = ParamSpec("_P")
log = ...
if os.environ.get("TORCH_COMPILE_STROBELIGHT", False): ...
if os.path.basename(os.path.dirname(__file__)) == "shared":
    torch_parent = ...
else:
    torch_parent = ...

def get_file_path(*path_components: str) -> str: ...
def get_file_path_2(*path_components: str) -> str: ...
def get_writable_path(path: str) -> str: ...
def prepare_multiprocessing_environment(path: str) -> None: ...
def resolve_library_path(path: str) -> str: ...
def throw_abstract_impl_not_imported_error(opname, module, context): ...
def compile_time_strobelight_meta(phase_name: str) -> Callable[[Callable[_P, _T]], Callable[_P, _T]]: ...
def signpost_event(category: str, name: str, parameters: dict[str, Any]):  # -> None:
    ...
def add_mlhub_insight(category: str, insight: str, insight_description: str):  # -> None:
    ...
def log_compilation_event(metrics):  # -> None:
    ...
def upload_graph(graph):  # -> None:
    ...
def set_pytorch_distributed_envs_from_justknobs():  # -> None:
    ...
def log_export_usage(**kwargs):  # -> None:
    ...
def log_draft_export_usage(**kwargs):  # -> None:
    ...
def log_trace_structured_event(*args, **kwargs) -> None: ...
def log_cache_bypass(*args, **kwargs) -> None: ...
def log_torchscript_usage(api: str, **kwargs):  # -> None:
    ...
def check_if_torch_exportable():  # -> Literal[False]:
    ...
def export_training_ir_rollout_check() -> bool: ...
def full_aoti_runtime_assert() -> bool: ...
def log_torch_jit_trace_exportability(api: str, type_of_export: str, export_outcome: str, result: str):  # -> None:
    ...
def justknobs_check(name: str, default: bool = ...) -> bool: ...
def justknobs_getval_int(name: str) -> int: ...
def is_fb_unit_test() -> bool: ...
@functools.cache
def max_clock_rate():  # -> Literal[1700, 1502, 1967, 1144, 1100]:

    ...
def get_mast_job_name_version() -> tuple[str, int] | None: ...

TEST_MASTER_ADDR = ...
TEST_MASTER_PORT = ...
USE_GLOBAL_DEPS = ...
USE_RTLD_GLOBAL_WITH_LIBTORCH = ...
REQUIRES_SET_PYTHON_MODULE = ...

def maybe_upload_prof_stats_to_manifold(profile_path: str) -> str | None: ...
def log_chromium_event_internal(
    event: dict[str, Any], stack: list[str], logger_uuid: str, start_time_ns: int
):  # -> None:
    ...
def record_chromium_event_internal(event: dict[str, Any]):  # -> None:
    ...
def profiler_allow_cudagraph_cupti_lazy_reinit_cuda12():  # -> Literal[True]:
    ...
def deprecated():  # -> Callable[..., Callable[_P, _T]]:

    ...
def get_default_numa_options():  # -> None:

    ...
def log_triton_builds(fail: str | None):  # -> None:
    ...
def find_compile_subproc_binary() -> str | None: ...
