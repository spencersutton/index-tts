from warnings import deprecated

import torch
from torch import Tensor, nn
from torch._jit_internal import Optional
from torch.nn.utils.rnn import PackedSequence

__all__ = [
    "GRU",
    "LSTM",
    "GRUCell",
    "LSTMCell",
    "PackedParameter",
    "RNNBase",
    "RNNCell",
    "RNNCellBase",
    "apply_permutation",
    "pack_weight_bias",
]

@deprecated(
    "`apply_permutation` is deprecated, please use `tensor.index_select(dim, permutation)` instead",
    category=FutureWarning,
)
def apply_permutation(tensor: Tensor, permutation: Tensor, dim: int = ...) -> Tensor: ...
def pack_weight_bias(qweight, bias, dtype) -> Any: ...

class PackedParameter(torch.nn.Module):
    def __init__(self, param) -> None: ...

class RNNBase(torch.nn.Module):
    _FLOAT_MODULE = nn.RNNBase
    _version = ...
    def __init__(
        self,
        mode,
        input_size,
        hidden_size,
        num_layers=...,
        bias=...,
        batch_first=...,
        dropout=...,
        bidirectional=...,
        dtype=...,
    ) -> None: ...
    def extra_repr(self) -> str: ...
    def check_input(self, input: Tensor, batch_sizes: Optional[Tensor]) -> None: ...
    def get_expected_hidden_size(self, input: Tensor, batch_sizes: Optional[Tensor]) -> tuple[int, int, int]: ...
    def check_hidden_size(self, hx: Tensor, expected_hidden_size: tuple[int, int, int], msg: str = ...) -> None: ...
    def check_forward_args(self, input: Tensor, hidden: Tensor, batch_sizes: Optional[Tensor]) -> None: ...
    def permute_hidden(self, hx: Tensor, permutation: Optional[Tensor]) -> Tensor: ...
    def set_weight_bias(self, weight_bias_dict) -> None: ...
    @classmethod
    def from_float(cls, mod, use_precomputed_fake_quant=...) -> LSTM | GRU: ...
    def get_weight(self) -> Dict[Any, Any]: ...
    def get_bias(self) -> Dict[Any, Any]: ...

class LSTM(RNNBase):
    _FLOAT_MODULE = nn.LSTM
    __overloads__ = ...
    def __init__(self, *args, **kwargs) -> None: ...
    def forward_impl(
        self,
        input: Tensor,
        hx: Optional[tuple[Tensor, Tensor]],
        batch_sizes: Optional[Tensor],
        max_batch_size: int,
        sorted_indices: Optional[Tensor],
    ) -> tuple[Tensor, tuple[Tensor, Tensor]]: ...
    @torch.jit.export
    def forward_tensor(
        self, input: Tensor, hx: Optional[tuple[Tensor, Tensor]] = ...
    ) -> tuple[Tensor, tuple[Tensor, Tensor]]: ...
    @torch.jit.export
    def forward_packed(
        self, input: PackedSequence, hx: Optional[tuple[Tensor, Tensor]] = ...
    ) -> tuple[PackedSequence, tuple[Tensor, Tensor]]: ...
    def permute_hidden(self, hx: tuple[Tensor, Tensor], permutation: Optional[Tensor]) -> tuple[Tensor, Tensor]: ...
    def check_forward_args(
        self, input: Tensor, hidden: tuple[Tensor, Tensor], batch_sizes: Optional[Tensor]
    ) -> None: ...
    @torch.jit.ignore
    def forward(
        self, input, hx=...
    ) -> tuple[PackedSequence, tuple[Tensor, Tensor]] | tuple[Tensor, tuple[Tensor, Tensor]]: ...
    @classmethod
    def from_float(cls, mod, use_precomputed_fake_quant=...) -> LSTM | GRU: ...
    @classmethod
    def from_reference(cls, ref_mod) -> Self: ...

class GRU(RNNBase):
    _FLOAT_MODULE = nn.GRU
    __overloads__ = ...
    def __init__(self, *args, **kwargs) -> None: ...
    def check_forward_args(self, input: Tensor, hidden: Tensor, batch_sizes: Optional[Tensor]) -> None: ...
    def forward_impl(
        self,
        input: Tensor,
        hx: Optional[Tensor],
        batch_sizes: Optional[Tensor],
        max_batch_size: int,
        sorted_indices: Optional[Tensor],
    ) -> tuple[Tensor, Tensor]: ...
    @torch.jit.export
    def forward_tensor(self, input: Tensor, hx: Optional[Tensor] = ...) -> tuple[Tensor, Tensor]: ...
    @torch.jit.export
    def forward_packed(self, input: PackedSequence, hx: Optional[Tensor] = ...) -> tuple[PackedSequence, Tensor]: ...
    def permute_hidden(self, hx: Tensor, permutation: Optional[Tensor]) -> Tensor: ...
    @torch.jit.ignore
    def forward(self, input, hx=...) -> tuple[PackedSequence, Tensor] | tuple[Tensor, Tensor]: ...
    @classmethod
    def from_float(cls, mod, use_precomputed_fake_quant=...) -> LSTM | GRU: ...
    @classmethod
    def from_reference(cls, ref_mod) -> Self: ...

class RNNCellBase(torch.nn.Module):
    __constants__ = ...
    def __init__(self, input_size, hidden_size, bias=..., num_chunks=..., dtype=...) -> None: ...
    def extra_repr(self) -> str: ...
    def check_forward_input(self, input) -> None: ...
    def check_forward_hidden(self, input: Tensor, hx: Tensor, hidden_label: str = ...) -> None: ...
    @classmethod
    def from_float(cls, mod, use_precomputed_fake_quant=...) -> LSTMCell | GRUCell | RNNCell: ...
    @classmethod
    def from_reference(cls, ref_mod) -> Self: ...
    def get_weight(self) -> Dict[Any, Any]: ...
    def get_bias(self) -> Dict[Any, Any]: ...
    def set_weight_bias(self, weight_bias_dict) -> None: ...

class RNNCell(RNNCellBase):
    __constants__ = ...
    def __init__(self, input_size, hidden_size, bias=..., nonlinearity=..., dtype=...) -> None: ...
    def forward(self, input: Tensor, hx: Optional[Tensor] = ...) -> Tensor: ...
    @classmethod
    def from_float(cls, mod, use_precomputed_fake_quant=...) -> LSTMCell | GRUCell | RNNCell: ...

class LSTMCell(RNNCellBase):
    def __init__(self, *args, **kwargs) -> None: ...
    def forward(self, input: Tensor, hx: Optional[tuple[Tensor, Tensor]] = ...) -> tuple[Tensor, Tensor]: ...
    @classmethod
    def from_float(cls, mod, use_precomputed_fake_quant=...) -> LSTMCell | GRUCell | RNNCell: ...

class GRUCell(RNNCellBase):
    def __init__(self, input_size, hidden_size, bias=..., dtype=...) -> None: ...
    def forward(self, input: Tensor, hx: Optional[Tensor] = ...) -> Tensor: ...
    @classmethod
    def from_float(cls, mod, use_precomputed_fake_quant=...) -> LSTMCell | GRUCell | RNNCell: ...
