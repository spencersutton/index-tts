import torch

from .activation import ELU, Hardswish, LeakyReLU, MultiheadAttention, PReLU, ReLU6, Sigmoid, Softmax
from .batchnorm import BatchNorm2d, BatchNorm3d
from .conv import Conv1d, Conv2d, Conv3d, ConvTranspose1d, ConvTranspose2d, ConvTranspose3d
from .dropout import Dropout
from .embedding_ops import Embedding, EmbeddingBag
from .functional_modules import FloatFunctional, FXFloatFunctional, QFunctional
from .linear import Linear
from .normalization import GroupNorm, InstanceNorm1d, InstanceNorm2d, InstanceNorm3d, LayerNorm
from .rnn import LSTM

__all__ = [
    "ELU",
    "LSTM",
    "BatchNorm2d",
    "BatchNorm3d",
    "Conv1d",
    "Conv2d",
    "Conv3d",
    "ConvTranspose1d",
    "ConvTranspose2d",
    "ConvTranspose3d",
    "DeQuantize",
    "Dropout",
    "Embedding",
    "EmbeddingBag",
    "FXFloatFunctional",
    "FloatFunctional",
    "GroupNorm",
    "Hardswish",
    "InstanceNorm1d",
    "InstanceNorm2d",
    "InstanceNorm3d",
    "LayerNorm",
    "LeakyReLU",
    "Linear",
    "MultiheadAttention",
    "PReLU",
    "QFunctional",
    "Quantize",
    "ReLU6",
    "Sigmoid",
    "Softmax",
]

class Quantize(torch.nn.Module):
    """
    Quantizes an incoming tensor

    Args:
     `scale`: scale of the output Quantized Tensor
     `zero_point`: zero_point of output Quantized Tensor
     `dtype`: data type of output Quantized Tensor
     `factory_kwargs`: Dictionary of kwargs used for configuring initialization
         of internal buffers. Currently, `device` and `dtype` are supported.
         Example: `factory_kwargs={'device': 'cuda', 'dtype': torch.float64}`
         will initialize internal buffers as type `torch.float64` on the current CUDA device.
         Note that `dtype` only applies to floating-point buffers.

    Examples::
        >>> t = torch.tensor([[1., -1.], [1., -1.]])
        >>> scale, zero_point, dtype = 1.0, 2, torch.qint8
        >>> qm = Quantize(scale, zero_point, dtype)
        >>> # xdoctest: +SKIP
        >>> qt = qm(t)
        >>> print(qt)
        tensor([[ 1., -1.],
                [ 1., -1.]], size=(2, 2), dtype=torch.qint8, scale=1.0, zero_point=2)
    """

    scale: torch.Tensor
    zero_point: torch.Tensor
    def __init__(self, scale, zero_point, dtype, factory_kwargs=...) -> None: ...
    def forward(self, X) -> Tensor: ...
    @staticmethod
    def from_float(mod, use_precomputed_fake_quant=...) -> Quantize: ...
    def extra_repr(self) -> str: ...

class DeQuantize(torch.nn.Module):
    """
    Dequantizes an incoming tensor

    Examples::
        >>> input = torch.tensor([[1., -1.], [1., -1.]])
        >>> scale, zero_point, dtype = 1.0, 2, torch.qint8
        >>> qm = Quantize(scale, zero_point, dtype)
        >>> # xdoctest: +SKIP
        >>> quantized_input = qm(input)
        >>> dqm = DeQuantize()
        >>> dequantized = dqm(quantized_input)
        >>> print(dequantized)
        tensor([[ 1., -1.],
                [ 1., -1.]], dtype=torch.float32)
    """
    def forward(self, Xq): ...
    @staticmethod
    def from_float(mod, use_precomputed_fake_quant=...) -> DeQuantize: ...
