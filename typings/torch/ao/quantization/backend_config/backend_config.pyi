from collections.abc import Callable
from dataclasses import dataclass
from enum import Enum
from typing import Any

import torch
from torch.ao.quantization.utils import Pattern

__all__ = ["BackendConfig", "BackendPatternConfig", "DTypeConfig", "DTypeWithConstraints", "ObservationType"]
INPUT_DTYPE_DICT_KEY = ...
OUTPUT_DTYPE_DICT_KEY = ...
WEIGHT_DTYPE_DICT_KEY = ...
BIAS_DTYPE_DICT_KEY = ...
IS_DYNAMIC_DICT_KEY = ...
NAME_DICT_KEY = ...
CONFIGS_DICT_KEY = ...
PATTERN_DICT_KEY = ...
PATTERN_COMPLEX_FORMAT_DICT_KEY = ...
OBSERVATION_TYPE_DICT_KEY = ...
DTYPE_CONFIGS_DICT_KEY = ...
ROOT_MODULE_DICT_KEY = ...
QAT_MODULE_DICT_KEY = ...
REFERENCE_QUANTIZED_MODULE_DICT_KEY = ...
FUSED_MODULE_DICT_KEY = ...
FUSER_METHOD_DICT_KEY = ...
ROOT_NODE_GETTER_DICT_KEY = ...
EXTRA_INPUTS_GETTER_DICT_KEY = ...
NUM_TENSOR_ARGS_TO_OBSERVATION_TYPE_DICT_KEY = ...
INPUT_TYPE_TO_INDEX_DICT_KEY = ...

class ObservationType(Enum):
    OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT = ...
    OUTPUT_SHARE_OBSERVER_WITH_INPUT = ...
    INPUT_OUTPUT_NOT_OBSERVED = ...

@dataclass
class DTypeWithConstraints:
    dtype: torch.dtype | None = ...
    quant_min_lower_bound: int | float | None = ...
    quant_max_upper_bound: int | float | None = ...
    scale_min_lower_bound: int | float | None = ...
    scale_max_upper_bound: int | float | None = ...
    scale_exact_match: float | None = ...
    zero_point_exact_match: int | None = ...

@dataclass
class DTypeConfig:
    input_dtype_with_constraints: DTypeWithConstraints
    output_dtype_with_constraints: DTypeWithConstraints
    weight_dtype_with_constraints: DTypeWithConstraints
    bias_dtype: torch.dtype | None
    is_dynamic: bool | None
    def __init__(
        self,
        input_dtype: torch.dtype | DTypeWithConstraints | None = ...,
        output_dtype: torch.dtype | DTypeWithConstraints | None = ...,
        weight_dtype: torch.dtype | DTypeWithConstraints | None = ...,
        bias_dtype: torch.dtype | None = ...,
        is_dynamic: bool | None = ...,
    ) -> None: ...
    @property
    def input_dtype(self) -> torch.dtype | None: ...
    @property
    def output_dtype(self) -> torch.dtype | None: ...
    @property
    def weight_dtype(self) -> torch.dtype | None: ...
    @classmethod
    def from_dict(cls, dtype_config_dict: dict[str, Any]) -> DTypeConfig: ...
    def to_dict(self) -> dict[str, Any]: ...

class BackendConfig:
    def __init__(self, name: str = ...) -> None: ...
    def set_name(self, name: str) -> BackendConfig: ...
    def set_backend_pattern_config(self, config: BackendPatternConfig) -> BackendConfig: ...
    def set_backend_pattern_configs(self, configs: list[BackendPatternConfig]) -> BackendConfig: ...
    @property
    def configs(self) -> list[BackendPatternConfig]: ...
    @classmethod
    def from_dict(cls, backend_config_dict: dict[str, Any]) -> BackendConfig: ...
    def to_dict(self) -> dict[str, Any]: ...

class BackendPatternConfig:
    def __init__(self, pattern: Pattern | None = ...) -> None: ...
    def set_pattern(self, pattern: Pattern) -> BackendPatternConfig: ...
    def set_observation_type(self, observation_type: ObservationType) -> BackendPatternConfig: ...
    def add_dtype_config(self, dtype_config: DTypeConfig) -> BackendPatternConfig: ...
    def set_dtype_configs(self, dtype_configs: list[DTypeConfig]) -> BackendPatternConfig: ...
    def set_root_module(self, root_module: type[torch.nn.Module]) -> BackendPatternConfig: ...
    def set_qat_module(self, qat_module: type[torch.nn.Module]) -> BackendPatternConfig: ...
    def set_reference_quantized_module(
        self, reference_quantized_module: type[torch.nn.Module]
    ) -> BackendPatternConfig: ...
    def set_fused_module(self, fused_module: type[torch.nn.Module]) -> BackendPatternConfig: ...
    def set_fuser_method(self, fuser_method: Callable) -> BackendPatternConfig: ...
    @classmethod
    def from_dict(cls, backend_pattern_config_dict: dict[str, Any]) -> BackendPatternConfig: ...
    def to_dict(self) -> dict[str, Any]: ...
