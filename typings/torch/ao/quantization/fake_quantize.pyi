from abc import ABC, abstractmethod
from typing import Any

import torch
from torch.nn import Module

"""Implements modules  used to perform fake quantization."""
__all__ = [
    "FakeQuantize",
    "FakeQuantizeBase",
    "FixedQParamsFakeQuantize",
    "FusedMovingAvgObsFakeQuantize",
    "default_affine_fixed_qparams_fake_quant",
    "default_dynamic_fake_quant",
    "default_embedding_fake_quant",
    "default_embedding_fake_quant_4bit",
    "default_fake_quant",
    "default_fixed_qparams_range_0to1_fake_quant",
    "default_fixed_qparams_range_neg1to1_fake_quant",
    "default_fused_act_fake_quant",
    "default_fused_per_channel_wt_fake_quant",
    "default_fused_wt_fake_quant",
    "default_histogram_fake_quant",
    "default_per_channel_weight_fake_quant",
    "default_symmetric_fixed_qparams_fake_quant",
    "default_weight_fake_quant",
    "disable_fake_quant",
    "disable_observer",
    "enable_fake_quant",
    "enable_observer",
    ...,
    "fused_wt_fake_quant_range_neg_127_to_127",
]

class FakeQuantizeBase(ABC, Module):
    fake_quant_enabled: torch.Tensor
    observer_enabled: torch.Tensor
    def __init__(self) -> None: ...
    @abstractmethod
    def forward(self, x) -> None: ...
    @abstractmethod
    def calculate_qparams(self, **kwargs) -> None: ...
    @torch.jit.export
    def enable_fake_quant(self, enabled: bool = ...) -> None: ...
    @torch.jit.export
    def disable_fake_quant(self) -> None: ...
    @torch.jit.export
    def enable_observer(self, enabled: bool = ...) -> None: ...
    @torch.jit.export
    def disable_observer(self) -> None: ...
    @classmethod
    def with_args(cls, **kwargs) -> _PartialWrapper: ...

class FakeQuantize(FakeQuantizeBase):
    scale: torch.Tensor
    zero_point: torch.Tensor
    def __init__(
        self,
        observer=...,
        quant_min=...,
        quant_max=...,
        is_dynamic=...,
        **observer_kwargs,
    ) -> None: ...
    @torch.jit.export
    def calculate_qparams(self) -> tuple[Tensor, Tensor]: ...
    def forward(self, X) -> Tensor: ...
    @torch.jit.export
    def extra_repr(self) -> str: ...

class FixedQParamsFakeQuantize(FakeQuantize):
    def __init__(self, observer) -> None: ...
    @torch.jit.export
    def calculate_qparams(self) -> tuple[Tensor, Tensor]: ...
    @torch.jit.export
    def extra_repr(self) -> str: ...

class FusedMovingAvgObsFakeQuantize(FakeQuantize):
    def __init__(
        self,
        observer: Any = ...,
        quant_min: int = ...,
        quant_max: int = ...,
        **observer_kwargs: Any,
    ) -> None: ...
    @torch.jit.export
    def calculate_qparams(self) -> tuple[torch.Tensor, torch.Tensor]: ...
    @torch.jit.export
    def extra_repr(self) -> str: ...
    def forward(self, X: torch.Tensor) -> torch.Tensor: ...

default_fake_quant = ...
default_weight_fake_quant = ...
default_dynamic_fake_quant = ...
default_fixed_qparams_range_neg1to1_fake_quant = ...
default_fixed_qparams_range_0to1_fake_quant = ...
default_symmetric_fixed_qparams_fake_quant = ...
default_affine_fixed_qparams_fake_quant = ...
default_per_channel_weight_fake_quant = ...
default_embedding_fake_quant = ...
default_embedding_fake_quant_4bit = ...
default_histogram_fake_quant = ...
default_fused_act_fake_quant = ...
default_fused_wt_fake_quant = ...
default_fused_per_channel_wt_fake_quant = ...
fused_wt_fake_quant_range_neg_127_to_127 = ...
fused_per_channel_wt_fake_quant_range_neg_127_to_127 = ...

def disable_fake_quant(mod) -> None: ...
def enable_fake_quant(mod) -> None: ...
def disable_observer(mod) -> None: ...
def enable_observer(mod) -> None: ...
