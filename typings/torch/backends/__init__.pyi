import types
from contextlib import contextmanager

from torch.backends import cpu as cpu
from torch.backends import cuda as cuda
from torch.backends import cudnn as cudnn
from torch.backends import cusparselt as cusparselt
from torch.backends import kleidiai as kleidiai
from torch.backends import mha as mha
from torch.backends import miopen as miopen
from torch.backends import mkl as mkl
from torch.backends import mkldnn as mkldnn
from torch.backends import mps as mps
from torch.backends import nnpack as nnpack
from torch.backends import openmp as openmp
from torch.backends import opt_einsum as opt_einsum
from torch.backends import quantized as quantized

__allow_nonbracketed_mutation_flag = ...

def disable_global_flags() -> None: ...
def flags_frozen() -> bool: ...

class ContextProp:
    def __init__(self, getter, setter) -> None: ...
    def __get__(self, obj, objtype): ...
    def __set__(self, obj, val) -> None:  # -> None:
        ...

class PropModule(types.ModuleType):
    def __init__(self, m, name) -> None: ...
    def __getattr__(self, attr): ...

class _FP32Precision:
    def __init__(self, backend, op) -> None: ...
    def __setattr__(self, name, value) -> None:  # -> None:
        ...
    def __getattr__(self, name) -> str: ...

def set_flags(_fp32_precision=...) -> tuple[str]: ...
@contextmanager
def flags(fp32_precision=...) -> Generator[None, Any, None]: ...

class GenericModule(PropModule):
    def __init__(self, m, name) -> None: ...

    fp32_precision = ...
