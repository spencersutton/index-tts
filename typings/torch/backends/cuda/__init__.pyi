import contextlib
from warnings import deprecated

import torch
from torch._C import _SDPAParams as SDPAParams

__all__ = [
    "SDPAParams",
    "allow_fp16_bf16_reduction_math_sdp",
    "can_use_cudnn_attention",
    "can_use_efficient_attention",
    "can_use_flash_attention",
    "cuBLASModule",
    "cuFFTPlanCache",
    "cuFFTPlanCacheAttrContextProp",
    "cuFFTPlanCacheManager",
    "cudnn_sdp_enabled",
    "cufft_plan_cache",
    "enable_cudnn_sdp",
    "enable_flash_sdp",
    "enable_math_sdp",
    "enable_mem_efficient_sdp",
    "flash_sdp_enabled",
    "fp16_bf16_reduction_math_sdp_allowed",
    "is_built",
    "is_flash_attention_available",
    "math_sdp_enabled",
    "matmul",
    "mem_efficient_sdp_enabled",
    "preferred_blas_library",
    "preferred_linalg_library",
    "preferred_rocm_fa_library",
    "sdp_kernel",
]

def is_built() -> bool: ...

class cuFFTPlanCacheAttrContextProp:
    def __init__(self, getter, setter) -> None: ...
    def __get__(self, obj, objtype): ...
    def __set__(self, obj, val) -> None:  # -> None:
        ...

class cuFFTPlanCache:
    def __init__(self, device_index) -> None: ...

    size = ...
    max_size = ...
    def clear(self) -> None: ...

class cuFFTPlanCacheManager:
    __initialized = ...
    def __init__(self) -> None: ...
    def __getitem__(self, device): ...
    def __getattr__(self, name) -> Any: ...
    def __setattr__(self, name, value) -> None:  # -> None:
        ...

class cuBLASModule:
    def __getattr__(self, name) -> bool | str: ...
    def __setattr__(self, name, value) -> None:  # -> str | None:
        ...

_LinalgBackends = ...
_LinalgBackends_str = ...

def preferred_linalg_library(
    backend: None | str | torch._C._LinalgBackend = ...,
) -> torch._C._LinalgBackend: ...

_BlasBackends = ...
_BlasBackends_str = ...

def preferred_blas_library(
    backend: None | str | torch._C._BlasBackend = ...,
) -> torch._C._BlasBackend: ...

_ROCmFABackends = ...
_ROCmFABackends_str = ...

def preferred_rocm_fa_library(
    backend: None | str | torch._C._ROCmFABackend = ...,
) -> torch._C._ROCmFABackend: ...
def flash_sdp_enabled() -> bool: ...
def enable_flash_sdp(enabled: bool) -> None: ...
def mem_efficient_sdp_enabled() -> bool: ...
def enable_mem_efficient_sdp(enabled: bool) -> None: ...
def math_sdp_enabled() -> bool: ...
def enable_math_sdp(enabled: bool) -> None: ...
def allow_fp16_bf16_reduction_math_sdp(enabled: bool) -> None: ...
def fp16_bf16_reduction_math_sdp_allowed() -> bool: ...
def is_flash_attention_available() -> bool: ...
def can_use_flash_attention(params: SDPAParams, debug: bool = ...) -> bool: ...
def can_use_efficient_attention(params: SDPAParams, debug: bool = ...) -> bool: ...
def can_use_cudnn_attention(params: SDPAParams, debug: bool = ...) -> bool: ...
def cudnn_sdp_enabled() -> bool: ...
def enable_cudnn_sdp(enabled: bool) -> None: ...
@contextlib.contextmanager
@deprecated(
    "`torch.backends.cuda.sdp_kernel()` is deprecated. "
    "In the future, this context manager will be removed. "
    "Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, "
    "with updated signature.",
    category=FutureWarning,
)
def sdp_kernel(
    enable_flash: bool = ...,
    enable_math: bool = ...,
    enable_mem_efficient: bool = ...,
    enable_cudnn: bool = ...,
) -> Generator[dict[Any, Any], Any, None]: ...

cufft_plan_cache = ...
matmul = ...
