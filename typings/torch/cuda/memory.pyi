import contextlib
from typing import Any
from warnings import deprecated

import torch
from torch._C import _cuda_CUDAAllocator, _MemPool
from torch.types import Device

__all__ = [
    "CUDAPluggableAllocator",
    "MemPool",
    "caching_allocator_alloc",
    "caching_allocator_delete",
    "caching_allocator_enable",
    "change_current_allocator",
    "empty_cache",
    "get_allocator_backend",
    "get_per_process_memory_fraction",
    "host_memory_stats",
    "host_memory_stats_as_nested_dict",
    "list_gpu_processes",
    "max_memory_allocated",
    "max_memory_cached",
    "max_memory_reserved",
    "mem_get_info",
    "memory_allocated",
    "memory_cached",
    "memory_reserved",
    "memory_snapshot",
    "memory_stats",
    "memory_stats_as_nested_dict",
    "memory_summary",
    "reset_accumulated_host_memory_stats",
    "reset_accumulated_memory_stats",
    "reset_max_memory_allocated",
    "reset_max_memory_cached",
    "reset_peak_host_memory_stats",
    "reset_peak_memory_stats",
    "set_per_process_memory_fraction",
    "use_mem_pool",
]
if not hasattr(torch._C, "_cuda_CUDAAllocator"): ...
if not hasattr(torch._C, "_MemPool"): ...

def caching_allocator_alloc(size, device: Device = ..., stream=...) -> int: ...
def caching_allocator_delete(mem_ptr) -> None: ...
def caching_allocator_enable(value: bool = ...) -> None: ...
def set_per_process_memory_fraction(fraction, device: Device = ...) -> None: ...
def get_per_process_memory_fraction(device: Device = ...) -> float: ...
def empty_cache() -> None: ...
def memory_stats(device: Device = ...) -> dict[str, Any]: ...
def memory_stats_as_nested_dict(device: Device = ...) -> dict[str, Any]: ...
def reset_accumulated_memory_stats(device: Device = ...) -> None: ...
def reset_peak_memory_stats(device: Device = ...) -> None: ...
def host_memory_stats() -> dict[str, Any]: ...
def host_memory_stats_as_nested_dict() -> dict[str, Any]: ...
def reset_accumulated_host_memory_stats() -> None: ...
def reset_peak_host_memory_stats() -> None: ...
def reset_max_memory_allocated(device: Device = ...) -> None: ...
def reset_max_memory_cached(device: Device = ...) -> None: ...
def memory_allocated(device: Device = ...) -> int: ...
def max_memory_allocated(device: Device = ...) -> int: ...
def memory_reserved(device: Device = ...) -> int: ...
def max_memory_reserved(device: Device = ...) -> int: ...
@deprecated("`torch.cuda.memory_cached` has been renamed to `torch.cuda.memory_reserved`", category=FutureWarning)
def memory_cached(device: Device = ...) -> int: ...
@deprecated(
    "`torch.cuda.max_memory_cached` has been renamed to `torch.cuda.max_memory_reserved`", category=FutureWarning
)
def max_memory_cached(device: Device = ...) -> int: ...
def memory_snapshot(mempool_id=...) -> Any: ...
def memory_summary(device: Device = ..., abbreviated: bool = ...) -> str: ...
def list_gpu_processes(device: Device = ...) -> str: ...
def mem_get_info(device: Device = ...) -> tuple[int, int]: ...
def get_allocator_backend() -> str: ...

class _CUDAAllocator:
    def __init__(self, allocator: torch._C._cuda_CUDAAllocator) -> None: ...
    def allocator(self) -> _cuda_CUDAAllocator: ...

class CUDAPluggableAllocator(_CUDAAllocator):
    def __init__(self, path_to_so_file: str, alloc_fn_name: str, free_fn_name: str) -> None: ...

def change_current_allocator(allocator: _CUDAAllocator) -> None: ...

class MemPool(_MemPool):
    def __init__(self, allocator: _cuda_CUDAAllocator | None = ..., use_on_oom: bool = ...) -> None: ...
    @property
    def id(self) -> tuple[int, int]: ...
    @property
    def allocator(self) -> _cuda_CUDAAllocator | None: ...
    def use_count(self) -> int: ...
    def snapshot(self) -> Any: ...

@contextlib.contextmanager
def use_mem_pool(pool: MemPool, device: Device = ...) -> Generator[None, Any, None]: ...
