import contextlib
from typing import Any, Optional, TypeAlias, Union

import torch
import torch.distributed as dist
from torch.distributed.device_mesh import DeviceMesh

type RANK_TYPES = (
    list[int] | list[list[int]] | dist.ProcessGroup | DeviceMesh | tuple[dist.tensor.DeviceMesh, int] | str
)

def wait_tensor(tensor):  # -> Any:

    ...
def broadcast(self: torch.Tensor, src: int, group: RANK_TYPES, tag: str = ...):  # -> Tensor:

    ...
def all_reduce(self: torch.Tensor, reduceOp: str, group: RANK_TYPES, tag: str = ...):  # -> Tensor:

    ...
def all_gather_tensor(self: torch.Tensor, gather_dim: int, group: RANK_TYPES, tag: str = ...) -> torch.Tensor: ...
def all_gather_tensor_autograd(
    self: torch.Tensor, gather_dim: int, group: RANK_TYPES, tag: str = ...
):  # -> Tensor | Any | None:

    ...
def reduce_scatter_tensor(
    self: torch.Tensor, reduceOp: str, scatter_dim: int, group: RANK_TYPES, tag: str = ...
):  # -> Tensor:

    ...
def reduce_scatter_tensor_autograd(
    self: torch.Tensor, reduceOp: str, scatter_dim: int, group: RANK_TYPES, tag: str = ...
):  # -> Any | None:

    ...
def all_reduce_coalesced(
    self: list[torch.Tensor], reduceOp: str, group: RANK_TYPES, tag: str = ...
) -> list[torch.Tensor]: ...
def all_gather_into_tensor_coalesced(
    self: list[torch.Tensor], group: RANK_TYPES, tag: str = ...
) -> list[torch.Tensor]: ...
def reduce_scatter_tensor_coalesced(
    inputs: list[torch.Tensor], reduceOp: str, scatter_dim: list[int], group: RANK_TYPES, tag: str = ...
) -> list[torch.Tensor]: ...
def all_to_all_single(
    self: torch.Tensor,
    output_split_sizes: list[int] | None,
    input_split_sizes: list[int] | None,
    group: RANK_TYPES,
    tag: str = ...,
) -> torch.Tensor: ...
def all_to_all_single_autograd(
    self: torch.Tensor,
    output_split_sizes: list[int] | None,
    input_split_sizes: list[int] | None,
    group: RANK_TYPES,
    tag: str = ...,
) -> torch.Tensor: ...
def permute_tensor(self: torch.Tensor, src_dst: list[int], group: RANK_TYPES, tag: str = ...) -> torch.Tensor: ...

class AsyncCollectiveTensor(torch.Tensor):
    elem: torch.Tensor
    completed: bool
    __slots__ = ...
    @staticmethod
    def __new__(cls, elem: torch.Tensor): ...
    def __tensor_flatten__(self):  # -> tuple[list[str], None]:
        ...
    def tolist(self):  # -> list[Any] | Any:
        ...
    @staticmethod
    def __tensor_unflatten__(inner_tensors, meta, outer_size, outer_stride):  # -> AsyncCollectiveTensor:
        ...
    def __coerce_same_metadata_as_tangent__(
        self, expected_metadata: Any, expected_type: type | None = ...
    ):  # -> Any | Tensor | None:
        ...
    def trigger_wait(self):  # -> Any | Tensor:
        ...
    def wait(self) -> torch.Tensor: ...
    @classmethod
    def __torch_dispatch__(cls, func, types, args=..., kwargs=...):  # -> AsyncCollectiveTensor | PyTree:
        ...
    def numpy(self):  # -> ndarray[_AnyShape, dtype[Any]]:
        ...

class _FromTorchTensor(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input: torch.Tensor) -> torch.Tensor: ...
    @staticmethod
    def backward(ctx, grad_output: torch.Tensor) -> torch.Tensor: ...

@contextlib.contextmanager
def allow_inflight_collective_as_graph_input_ctx(value: bool = ...):  # -> Generator[None, Any, None]:

    ...

lib_impl = ...
legacy_lib = ...
legacy_lib_impl = ...
ops_defs = ...
my_module = ...

def all_gather_tensor_inplace(
    output_tensor: torch.Tensor,
    input_tensor: torch.Tensor,
    group=...,
    async_op: bool = ...,
    tag: str = ...,
    gather_dim: int = ...,
):  # -> Tensor:
    ...
def reduce_scatter_tensor_inplace(
    output: torch.Tensor,
    input: torch.Tensor,
    op: str = ...,
    group=...,
    async_op: bool = ...,
    scatter_dim: int = ...,
    tag: str = ...,
):  # -> Tensor:
    ...

REDUCE_OP_TO_STR = ...

def all_reduce_inplace(
    tensor: torch.Tensor, op: str = ..., group=..., async_op: bool = ..., tag: str = ...
):  # -> Tensor:
    ...
def all_to_all_inplace(
    output: torch.Tensor,
    input: torch.Tensor,
    output_split_sizes=...,
    input_split_sizes=...,
    group=...,
    async_op=...,
    tag: str = ...,
):  # -> Tensor:
    ...
def all_gather_inplace(
    tensor_list: list[torch.Tensor], tensor: torch.Tensor, group=..., async_op=..., tag: str = ...
):  # -> list[Tensor]:
    ...

traceable_collective_remaps = ...
