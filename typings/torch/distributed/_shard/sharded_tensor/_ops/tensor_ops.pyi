import torch
from torch.distributed._shard.sharded_tensor import _sharded_op_impl

@_sharded_op_impl(torch.Tensor.device.__get__)
def tensor_device(types, args=..., kwargs=..., pg=...):  # -> device:
    ...
@_sharded_op_impl(torch.Tensor.is_meta.__get__)
def st_is_meta(types, args=..., kwargs=..., pg=...): ...
def sharded_type_as_check(*args, **kwargs):  # -> None:

    ...
def same_dtype(*args, **kwargs): ...
def sharded_type_as(args, kwargs, pg):  # -> tuple[list[Shard], Any]:

    ...
def sharded_deepcopy(args, kwargs, pg):  # -> tuple[Any, Any]:
    ...
@_sharded_op_impl(torch.Tensor.copy_)
def sharded_inplace_copy(types, args, kwargs, pg): ...
def sharded_clone(args, kwargs, pg):  # -> tuple[list[Shard], Any]:
    ...
def sharded_detach(args, kwargs, pg):  # -> tuple[list[Shard], Any]:
    ...
@_sharded_op_impl(torch.Tensor.requires_grad_)
def tensor_requires_grad_set(types, args=..., kwargs=..., pg=...):  # -> ShardedTensor:
    ...
