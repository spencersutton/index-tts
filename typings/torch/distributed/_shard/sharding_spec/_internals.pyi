from torch.distributed._shard.metadata import ShardMetadata

def validate_non_overlapping_shards_metadata(shards: list[ShardMetadata]):  # -> None:

    ...
def check_tensor(shards_metadata, tensor_dims) -> None: ...
def get_split_size(dim_size, chunks): ...
def get_chunked_dim_size(dim_size, split_size, idx):  # -> int:

    ...
def get_chunk_sharding_params(sharding_dim_size, world_size, spec, rank):  # -> tuple[int, int | Any]:

    ...
