import math
import os
import socket
import uuid
import torch
import torch.distributed._functional_collectives as funcol
import torch.distributed.distributed_c10d as c10d
from collections.abc import Generator, Sequence
from contextlib import contextmanager
from datetime import timedelta
from enum import Enum
from functools import partial
from typing import Any, Literal, TYPE_CHECKING, Union, overload
from collections.abc import Callable
from torch._C._autograd import DeviceType
from torch._C._distributed_c10d import ProcessGroup, Work as _Work, _SymmetricMemory
from torch.types import _device, _dtype, _int

_group_name_to_store: dict[str, c10d.Store] = ...

def enable_symm_mem_for_group(group_name: str) -> None: ...

_is_test_mode: bool = ...
_mocked_group_names: set[str] | None = ...

def is_symm_mem_enabled_for_group(group_name: str) -> bool: ...

_group_name_to_workspace_tensor: dict[str, torch.Tensor | None] = ...

def get_symm_mem_workspace(group_name: str, min_size: int) -> _SymmetricMemory: ...

_backend_streams: dict[int, torch.cuda.Stream] = ...
lib = ...

class _ScaleMode(Enum):
    UNSCALED = ...
    TENSOR_WISE = ...
    ROW_WISE_SHARDED = ...
    ROW_WISE_REPLICATED = ...

def make_contiguous_for_perm(t: torch.Tensor, perm: list[int]) -> torch.Tensor: ...
def restride_A_shard_for_fused_all_gather_matmul(t: torch.Tensor, gather_dim: int) -> torch.Tensor: ...
def restride_A_for_fused_matmul_reduce_scatter(t: torch.Tensor, scatter_dim: int) -> torch.Tensor: ...

class Work(_Work):
    def __init__(self) -> None: ...
    def wait(self, timeout: timedelta = ...) -> bool: ...

if TYPE_CHECKING: ...

@overload
def empty(*size: _int, dtype: _dtype | None = ..., device: _device | None = ...) -> torch.Tensor: ...
@overload
def empty(size: Sequence[_int], *, dtype: _dtype | None = ..., device: _device | None = ...) -> torch.Tensor: ...
def empty(*size: Any, dtype: _dtype | None = ..., device: _device | None = ...) -> torch.Tensor: ...
def rendezvous(tensor: torch.Tensor, group: str | ProcessGroup) -> _SymmetricMemory: ...
def is_nvshmem_available() -> bool: ...
def set_backend(name: Literal["NVSHMEM", "CUDA", "NCCL"]) -> None: ...
def get_backend(device: _device) -> str | None: ...
def get_mempool_allocator(device: _device):  # -> Any:

    ...

__all__ = ["empty", "rendezvous", "is_nvshmem_available", "set_backend", "get_backend"]
