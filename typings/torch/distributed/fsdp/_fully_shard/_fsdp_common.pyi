from dataclasses import dataclass
from enum import Enum
from typing import Optional
from torch.distributed.tensor import DeviceMesh

_compiled_autograd_enabled: bool = ...

def detect_compiled_autograd():  # -> None:
    ...
def compiled_autograd_enabled():  # -> bool:
    ...

@dataclass
class DataParallelMeshInfo:
    mesh: DeviceMesh
    shard_mesh_dim: int | None = ...
    replicate_mesh_dim: int | None = ...
    def __post_init__(self):  # -> None:
        ...

@dataclass
class FSDPMeshInfo(DataParallelMeshInfo):
    def __post_init__(self):  # -> None:
        ...

@dataclass
class DDPMeshInfo(DataParallelMeshInfo):
    def __post_init__(self):  # -> None:
        ...

@dataclass
class HSDPMeshInfo(FSDPMeshInfo, DDPMeshInfo):
    def __post_init__(self):  # -> None:
        ...

class TrainingState(Enum):
    FORWARD = ...
    PRE_BACKWARD = ...
    POST_BACKWARD = ...
    IDLE = ...
