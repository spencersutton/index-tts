from dataclasses import dataclass
from enum import Enum

from torch.distributed.tensor import DeviceMesh

_compiled_autograd_enabled: bool = ...

def detect_compiled_autograd(): ...
def compiled_autograd_enabled(): ...

@dataclass
class DataParallelMeshInfo:
    """DataParallelMeshInfo(mesh: torch.distributed.device_mesh.DeviceMesh, shard_mesh_dim: Optional[int] = None, replicate_mesh_dim: Optional[int] = None)"""

    mesh: DeviceMesh
    shard_mesh_dim: int | None = ...
    replicate_mesh_dim: int | None = ...
    def __post_init__(self): ...

@dataclass
class FSDPMeshInfo(DataParallelMeshInfo):
    """FSDPMeshInfo(mesh: torch.distributed.device_mesh.DeviceMesh, shard_mesh_dim: Optional[int] = None, replicate_mesh_dim: Optional[int] = None)"""
    def __post_init__(self): ...

@dataclass
class DDPMeshInfo(DataParallelMeshInfo):
    """DDPMeshInfo(mesh: torch.distributed.device_mesh.DeviceMesh, shard_mesh_dim: Optional[int] = None, replicate_mesh_dim: Optional[int] = None)"""
    def __post_init__(self): ...

@dataclass
class HSDPMeshInfo(FSDPMeshInfo, DDPMeshInfo):
    """HSDPMeshInfo(mesh: torch.distributed.device_mesh.DeviceMesh, shard_mesh_dim: Optional[int] = None, replicate_mesh_dim: Optional[int] = None)"""
    def __post_init__(self): ...

class TrainingState(Enum):
    """Describes the training state of one FSDP state / parameter group."""

    FORWARD = ...
    PRE_BACKWARD = ...
    POST_BACKWARD = ...
    IDLE = ...
