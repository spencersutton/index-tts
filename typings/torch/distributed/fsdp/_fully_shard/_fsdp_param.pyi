from collections.abc import Callable, Sequence
from dataclasses import dataclass
from enum import Enum
from typing import Any

import torch
from torch import nn
from torch.distributed.tensor import DTensor, Shard
from torch.distributed.tensor._dtensor_spec import DTensorSpec

from ._fsdp_api import MixedPrecisionPolicy, OffloadPolicy
from ._fsdp_common import FSDPMeshInfo

lib = ...

@torch.library.impl(lib, "copy_", "Meta")
@torch.library.impl(lib, "copy_", "CUDA")
@torch.library.impl(lib, "copy_", "XPU")
@torch.library.impl(lib, "copy_", "HPU")
@torch.library.impl(lib, "copy_", "CPU")
@torch.library.impl(lib, "copy_", "MTIA")
def copy_(tensor, data): ...
@torch.library.impl(lib, "copy_", "Functionalize")
def copy__functionalize(tensor, data): ...

class ShardedState(Enum):
    """
    - ``SHARDED``: The sharded parameter is registered to the module. It is the
      only contributor to parameter memory.
    - ``SHARDED_POST_FORWARD``: The unsharded parameter is resharded to a
      smaller world size. Since this data should not be used for computation,
      we do not register it to the module. Users should reshard the module
      before any in-place modifications. Both it and the sharded parameter
      contribute to parameter memory.
    - ``UNSHARDED``: The unsharded parameter is registered to the module. Both
      it and the sharded parameter contribute to parameter memory.
    """

    SHARDED = ...
    SHARDED_POST_FORWARD = ...
    UNSHARDED = ...

@dataclass
class ParamModuleInfo:
    """
    For a parameter, this stores the module and the parameter name to be able
    to do a parameter swap via ``setattr(module, param_name, ...)`` or to get
    the parameter via ``getattr(module, param_name)``. We additionally save
    shared modules and shared parameter names to update them accordingly.
    """

    module: nn.Module
    param_name: str
    shared_modules: list[nn.Module] = ...
    shared_param_names: list[str] = ...

@dataclass
class ExtensionsData:
    """ExtensionsData(all_gather_metadata: Optional[Any] = None, all_gather_input_sizes: collections.abc.Sequence[torch.Size] = ())"""

    all_gather_metadata: Any | None = ...
    all_gather_input_sizes: Sequence[torch.Size] = ...
    def clear(self): ...

class FSDPParam:
    """
    This class manages a parameter with FSDP or FSDP variants applied,
    implementing dim-0 per-parameter sharding.
    """

    orig_dtype: torch.dtype
    param_dtype: torch.dtype | None
    reduce_dtype: torch.dtype | None
    _orig_size: torch.Size
    sharded_size: torch.Size
    contiguous_sharded_stride: tuple[int, ...]
    padded_sharded_param_size: torch.Size
    sharded_post_forward_size: torch.Size
    contiguous_sharded_post_forward_stride: tuple[int, ...]
    _sharded_param_data: torch.Tensor
    sharded_param: nn.Parameter
    _sharded_post_forward_param_data: torch.Tensor | None
    _sharded_post_forward_param: nn.Parameter | None
    _unsharded_param: nn.Parameter
    unsharded_accumulated_grad: torch.Tensor | None
    _sharding_spec: DTensorSpec
    _tp_spec: DTensorSpec
    all_gather_outputs: list[torch.Tensor]
    _extensions_data: ExtensionsData
    _unsharded_inner_tensors: list[torch.Tensor]
    def __init__(
        self,
        param: nn.Parameter,
        module_info: ParamModuleInfo,
        mesh_info: FSDPMeshInfo,
        post_forward_mesh_info: FSDPMeshInfo | None,
        device: torch.device,
        shard_placement_fn: Callable[[nn.Parameter], Shard | None] | None,
        mp_policy: MixedPrecisionPolicy,
        offload_policy: OffloadPolicy,
    ) -> None: ...
    def init_dtype_attrs(self, mp_policy: MixedPrecisionPolicy): ...
    def init_all_gather_outputs(
        self,
        all_gather_input_numels: list[int],
        all_gather_input_dtypes: list[torch.dtype],
        world_size: int,
        device: torch.device,
        force_recreate: bool = ...,
    ): ...
    def init_unsharded_param(self):
        """
        [Note: Invariants for torch.compile Traceable FSDP2]
        1. Under compile, we always re-populate the content of `self._unsharded_param`
           per AllGather using the slow path.
        2. Under compile, we always recreate `self.all_gather_outputs` per AllGather.
           This is to ensure the buffer creation is internal to the graph and
           avoid `self.all_gather_outputs` being captured as a graph input.
        3. Under compile, at the end of `free_unsharded_param()`, we always clean up
           `self.all_gather_outputs` and `self._unsharded_inner_tensors`,
           to avoid them being captured as graph output.

        With these invariants, only these tensors will be inputs to the graph:
        - Sharded parameters
        - Placeholders for the `self._unsharded_param` nn.Parameter
        """
    def to_sharded(self) -> None: ...
    def to_sharded_post_forward(self) -> None: ...
    def to_unsharded(self) -> None: ...
    def to_sharded_dtensor(self, tensor: torch.Tensor) -> DTensor:
        """
        Converts a local tensor representing either the sharded parameter or
        sharded gradient to DTensor.
        """
    def to_sharded_post_forward_dtensor(self, tensor: torch.Tensor) -> DTensor: ...
    def to_accumulated_grad_if_needed(self) -> None: ...
    def accumulate_unsharded_grad_if_needed(self) -> None: ...
    def alloc_all_gather_outputs(self) -> None: ...
    def free_unsharded_param(self) -> None: ...
    @property
    def all_gather_inputs(self) -> list[torch.Tensor]: ...
    @property
    def unsharded_param(self) -> nn.Parameter: ...
    @property
    def unsharded_grad_data(self) -> torch.Tensor: ...
    @property
    def unsharded_accumulated_grad_data(self) -> torch.Tensor: ...
    @property
    def shard_mesh(self): ...
    @property
    def shard_mesh_from_root(self): ...
    def reset_sharded_param(self): ...

def alloc_storage(tensor: torch.Tensor) -> None: ...
def free_storage(tensor: torch.Tensor) -> None: ...
def unsafe_setattr_param(module: nn.Module, param_name: str, param: nn.Parameter) -> None: ...
def set_requires_grad_if_needed(src_tensor: torch.Tensor, dst_tensor: torch.Tensor) -> None: ...
