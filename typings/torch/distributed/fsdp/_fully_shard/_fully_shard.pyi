import torch
import torch.nn as nn
from typing import Callable, Optional, TYPE_CHECKING, Union, overload
from typing_extensions import deprecated
from torch.distributed._composable import contract
from ._fsdp_api import AllGather, MixedPrecisionPolicy, OffloadPolicy, ReduceScatter
from ._fsdp_param_group import FSDPParamGroup
from ._fsdp_state import FSDPState
from torch.distributed.tensor import DeviceMesh, Shard

if TYPE_CHECKING: ...
__all__ = ["fully_shard", "FSDPModule", "UnshardHandle", "register_fsdp_forward_method"]
cls_to_fsdp_cls: dict[type, type] = ...

@overload
def fully_shard(
    module: nn.Module,
    *,
    mesh: Optional[DeviceMesh] = ...,
    reshard_after_forward: Union[bool, int] = ...,
    shard_placement_fn: Optional[Callable[[nn.Parameter], Optional[Shard]]] = ...,
    mp_policy: MixedPrecisionPolicy = ...,
    offload_policy: OffloadPolicy = ...,
    ignored_params: Optional[set[nn.Parameter]] = ...,
) -> FSDPModule: ...
@overload
def fully_shard(
    module: list[nn.Module],
    *,
    mesh: Optional[DeviceMesh] = ...,
    reshard_after_forward: Union[bool, int] = ...,
    shard_placement_fn: Optional[Callable[[nn.Parameter], Optional[Shard]]] = ...,
    mp_policy: MixedPrecisionPolicy = ...,
    offload_policy: OffloadPolicy = ...,
    ignored_params: Optional[set[nn.Parameter]] = ...,
) -> list[FSDPModule]: ...
@contract(state_cls=FSDPState)
def fully_shard(
    module,
    *,
    mesh: Optional[DeviceMesh] = ...,
    reshard_after_forward: Optional[Union[bool, int]] = ...,
    shard_placement_fn: Optional[Callable[[nn.Parameter], Optional[Shard]]] = ...,
    mp_policy: MixedPrecisionPolicy = ...,
    offload_policy: OffloadPolicy = ...,
    ignored_params: Optional[set[nn.Parameter]] = ...,
): ...

class FSDPModule:
    def __new__(cls, *args, **kwargs): ...
    def reshard(self) -> None: ...
    def unshard(self, async_op: bool = ...) -> Optional[UnshardHandle]: ...
    def set_is_last_backward(self, is_last_backward: bool) -> None: ...
    def set_requires_gradient_sync(self, requires_gradient_sync: bool, *, recurse: bool = ...) -> None: ...
    def set_requires_all_reduce(self, requires_all_reduce: bool, *, recurse: bool = ...) -> None: ...
    def set_reshard_after_forward(self, reshard_after_forward: bool, recurse: bool = ...) -> None: ...
    def set_reshard_after_backward(self, reshard_after_backward: bool, *, recurse: bool = ...) -> None: ...
    def set_modules_to_forward_prefetch(self, modules: list[FSDPModule]) -> None: ...
    def set_modules_to_backward_prefetch(self, modules: list[FSDPModule]) -> None: ...
    def set_custom_all_gather(self, comm: AllGather) -> None: ...
    def set_custom_reduce_scatter(self, comm: ReduceScatter) -> None: ...
    def set_all_reduce_hook(
        self, hook: Callable[[torch.Tensor], None], *, stream: Optional[torch.cuda.Stream] = ...
    ):  # -> None:

        ...
    def set_post_optim_event(self, event: torch.Event) -> None: ...
    @deprecated("Use `set_gradient_divide_factor` instead")
    def set_reduce_scatter_divide_factor(self, factor: float) -> None: ...
    def set_gradient_divide_factor(self, factor: float) -> None: ...
    def set_force_sum_reduction_for_comms(self, enable: bool) -> None: ...
    def set_unshard_in_backward(self, unshard_in_backward: bool) -> None: ...
    def set_allocate_memory_from_process_group_for_comm(self, enable: bool) -> None: ...

class UnshardHandle:
    def wait(self) -> None: ...

class _UnshardHandleImpl(UnshardHandle):
    def __init__(self, fsdp_param_group: Optional[FSDPParamGroup]) -> None: ...
    def wait(self):  # -> None:
        ...

def register_fsdp_forward_method(module: nn.Module, method_name: str) -> None: ...
