from torch.autograd import Function

def broadcast(tensor, src, group=...) -> Any | None:
    """
    Broadcasts the tensor to the whole group.

    ``tensor`` must have the same number of elements in all processes
    participating in the collective.

    Arguments:
        tensor (Tensor): Data to be sent if ``src`` is the rank of current
            process.
        src (int): Source rank.
        group (ProcessGroup, optional): The process group to work on.

    Returns:
        Tensor: Received tensor from the broadcast op.
    """

def gather(tensor, dst=..., group=...) -> Any | None:
    """
    Gathers a list of tensors in a single process.

    Arguments:
        tensor (Tensor): Input tensor.
        dst (int, optional): Destination rank (default is 0).
        group (ProcessGroup, optional): The process group to work on.

    Returns:
        tuple[Tensor]: List of appropriately-sized tensors with the gathered data.
    """

def scatter(tensors, src=..., group=...) -> Any | None:
    """
    Scatters a list of tensors to all processes in a group.

    Each process will receive exactly one tensor and store its data in the
    ``tensor`` argument.

    Arguments:
        tensors (list[Tensor]): List of tensors to scatter on the source rank.
            Receivers must pass ``None`.
        src (int, optional): Source rank (default is 0).
        group (ProcessGroup, optional): The process group to work on.

    Returns:
        Tensor: Output tensor from the scatter operation.
    """

def reduce(tensor, dst, op=..., group=...) -> Any | None:
    """
    Reduces the tensor data across all machines.

    Only the process with rank ``dst`` is going to receive the final result.

    Arguments:
        tensor (Tensor): Input of the collective.
        dst (int): Destination rank.
        op (optional): One of the values from
            ``torch.distributed.ReduceOp``
            enum.  Specifies an operation used for element-wise reductions.
        group (ProcessGroup, optional): The process group to work on.

    Returns:
        Tensor: Output of the collective.
    """

def reduce_scatter(output, input_list, op=..., group=...) -> Any | None:
    """
    Reduces, then scatters a list of tensors to all processes in a group.

    Arguments:
        output (Tensor): Output tensor.
        input_list (list[Tensor]): List of tensors to reduce and scatter.
        op (optional): One of the values from
            ``torch.distributed.ReduceOp``
            enum.  Specifies an operation used for element-wise reductions.
        group (ProcessGroup, optional): The process group to work on.

    Returns:
        Tensor: Output of the collective.
    """

def all_gather(tensor, group=...) -> Any | None:
    """
    Gathers tensors from the whole group in a list.

    Arguments:
        tensor (Tensor): Tensor to be broadcast from current process.
        group (ProcessGroup, optional): The process group to work on.

    Returns:
        tuple([Tensor]): Output of the collective.
    """

def all_to_all(output_tensor_list, input_tensor_list, group=...) -> Any | None:
    """
    Each process scatters list of input tensors to all processes in a group and return gathered list of tensors in output list.

    Arguments:
        output_tensor_list (list[Tensor]): list of tensors to gather one per rank.
        input_tensor_list (list[Tensor]): List of tensors to scatter one per rank.
        group (ProcessGroup, optional): The process group to work on.

    Returns:
        tuple([Tensor]): Output of the collective.
    """

def all_to_all_single(output, input, output_split_sizes=..., input_split_sizes=..., group=...) -> Any | None:
    """
    Each process splits input tensor and then scatters the split list to all processes in a group.

    Then concatenate the received tensors from all the processes in the group and return single output tensor.

    Arguments:
        output (Tensor): Gathered concatenated output tensor.
        input (Tensor): Input tensor to scatter.
        output_split_sizes: (list[Int], optional): Output split sizes for dim 0
            if specified None or empty, dim 0 of ``output`` tensor must divide
            equally by ``world_size``.
        input_split_sizes: (list[Int], optional): Input split sizes for dim 0
            if specified None or empty, dim 0 of ``input`` tensor must divide
            equally by ``world_size``.

    Returns:
        Tensor: Output of the collective.
    """

def all_reduce(tensor, op=..., group=...) -> Any | None:
    """
    Reduces the tensor data across all machines in such a way that all get the final result.

    After the call the returned tensor is going to be bitwise
    identical in all processes.

    Arguments:
        tensor (Tensor): Input of the collective.
        op (optional): One of the values from
            ``torch.distributed.ReduceOp``
            enum.  Specifies an operation used for element-wise reductions.
        group (ProcessGroup, optional): The process group to work on.

    Returns:
        Tensor: Output of the collective
    """

class _Broadcast(Function):
    @staticmethod
    def forward(ctx, src, group, tensor): ...
    @staticmethod
    def backward(ctx, grad_output) -> tuple[None, None, Any | None]: ...

class _Gather(Function):
    @staticmethod
    def forward(ctx, dst, group, tensor) -> tuple[Tensor, ...]: ...
    @staticmethod
    def backward(ctx, *grad_outputs) -> tuple[None, None, Any | None]: ...

class _Scatter(Function):
    @staticmethod
    def forward(ctx, src, group, *tensors) -> Tensor: ...
    @staticmethod
    def backward(ctx, grad_output) -> Any: ...

class _Reduce(Function):
    @staticmethod
    def forward(ctx, src, op, group, tensor): ...
    @staticmethod
    def backward(ctx, grad_output) -> tuple[None, None, None, Any | None]: ...

class _Reduce_Scatter(Function):
    @staticmethod
    def forward(ctx, op, group, tensor, *input_tensor_list): ...
    @staticmethod
    def backward(ctx, grad_output) -> Any: ...

class _AllGather(Function):
    @staticmethod
    def forward(ctx, group, tensor) -> tuple[Tensor, ...]: ...
    @staticmethod
    def backward(ctx, *grad_outputs) -> tuple[None, Any | Tensor | None]: ...

class _AllGatherBase(Function):
    @staticmethod
    def forward(ctx, output_tensor, input_tensor, group): ...
    @staticmethod
    def backward(ctx, grad_output) -> tuple[None, Tensor, None]: ...

class _AlltoAll(Function):
    @staticmethod
    def forward(ctx, group, out_tensor_list, *tensors) -> tuple[Any, ...]: ...
    @staticmethod
    def backward(ctx, *grad_outputs) -> Any: ...

class _AlltoAllSingle(Function):
    @staticmethod
    def forward(ctx, group, output, output_split_sizes, input_split_sizes, input): ...
    @staticmethod
    def backward(ctx, grad_output) -> tuple[None, None, None, None, Any | None]: ...

class _AllReduce(Function):
    @staticmethod
    def forward(ctx, op, group, tensor): ...
    @staticmethod
    def backward(ctx, grad_output) -> tuple[None, None, Any | None]: ...
