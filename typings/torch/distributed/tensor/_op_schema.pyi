from collections.abc import Sequence
from dataclasses import dataclass
from functools import cached_property
from typing import Optional, TypeAlias, Union
from warnings import deprecated

from torch._ops import OpOverload
from torch.distributed.device_mesh import DeviceMesh
from torch.distributed.tensor._dtensor_spec import DTensorSpec
from torch.distributed.tensor.placement_types import Placement
from torch.utils._cxx_pytree import TreeSpec

"""
DTensor operator schema definitions and utilities.

This module defines the core data structures and utilities for describing and managing
distributed tensor operations in PyTorch's DTensor system. It provides the foundational
schema types used for sharding propagation, operator strategy selection, and distributed
execution planning.

Key components:
- OpSpec: Describes acceptable sharding placements for operations
- OpStrategy: Represents the possible sharding strategies for an operator
- TupleStrategy: Container for multiple strategies when ops have tuple/list of tensors input
- OpSchema: Describes operator input/output schemas with DTensorSpecs
- OutputSharding: Manages output sharding specifications and redistribution
- RuntimeSchemaInfo: Runtime execution metadata for operators
- OpInfo: Complete runtime operator execution information

These schema definitions enable the DTensor system to:
1. Propagate tensor sharding information to the operator outputs
2. Greedily select sharding strategies for distributed operations
3. Plan and execute tensor redistributions when needed
4. Cache sharding decisions for performance optimization
"""
type ArgsType = tuple[object, ...]
type KwargsType = dict[str, object]
type PlacementList = list[Placement | None]
type OutputSpecType = DTensorSpec | Sequence[DTensorSpec | None] | None

@dataclass
class OpSpec:
    output_specs: DTensorSpec | tuple[DTensorSpec | None, ...]
    input_specs: Sequence[DTensorSpec] | None = ...
    redistribute_cost: list[list[float]] | None = ...
    @cached_property
    def output_spec(self) -> DTensorSpec: ...
    @cached_property
    def mesh(self):  # -> DeviceMesh:
        ...
    def input_spec(self, index: int = ...) -> DTensorSpec: ...

class StrategyType: ...

class OpStrategy(StrategyType):
    def __init__(self, strategies: list[OpSpec]) -> None: ...
    def max_num_shards(self) -> int: ...
    @property
    def mesh(self):  # -> DeviceMesh:
        ...
    @property
    def mesh_shape(self):  # -> tuple[int, ...]:
        ...
    @property
    def ndim(self):  # -> int:
        ...
    @property
    def shape(self):  # -> Size:
        ...

class TupleStrategy(StrategyType):
    def __init__(self, children: Sequence[StrategyType]) -> None: ...
    @property
    @deprecated("TupleStrategy.childs is deprecated, use TupleStrategy.children instead.", category=FutureWarning)
    def childs(self) -> Sequence[StrategyType]: ...
    def child_mesh(self, index: int) -> DeviceMesh: ...

@dataclass
class RuntimeSchemaInfo:
    static_argnum: int = ...
    static_kwargkey: list[str] | None = ...
    needs_pytree: bool = ...

@dataclass
class OpSchema:
    op: OpOverload
    args_schema: ArgsType
    kwargs_schema: KwargsType
    schema_info: RuntimeSchemaInfo | None = ...
    _comparison_key: tuple[object, ...] | None = ...
    @property
    def args_spec(self) -> tuple[DTensorSpec, ...]: ...
    @property
    def args_strategy(self) -> tuple[OpStrategy, ...]: ...
    def __post_init__(self) -> None: ...
    def arg_type_tensor_or_tensor_list_like(self, arg: object) -> bool: ...
    def return_type_tuple_tensor_like(self) -> bool: ...
    def return_type_list_tensor_like(self) -> bool: ...
    def return_type_tensor(self) -> bool: ...
    def get_mesh_from_args(self, validate: bool = ...) -> DeviceMesh: ...
    def is_inplace_op(self) -> bool: ...
    def is_out_variant_op(self) -> bool: ...
    def is_view_op(self) -> bool: ...
    def __hash__(self) -> int: ...
    def __eq__(self, other: object) -> bool: ...
    def gen_fake_args(self) -> ArgsType: ...
    def gen_fake_kwargs(self) -> KwargsType: ...

@dataclass
class OutputSharding:
    output_spec: OutputSpecType
    redistribute_schema: OpSchema | None = ...
    needs_redistribute: bool = ...
    use_val_from_redistribute_schema: bool = ...
    @cached_property
    def mesh(self):  # -> DeviceMesh:
        ...

@dataclass
class OpInfo:
    compute_mesh: DeviceMesh
    schema: OpSchema
    flat_args_schema: list[object]
    local_args: Sequence[object]
    local_kwargs: dict[str, object]
    args_tree_spec: TreeSpec | None = ...
    output_sharding: OutputSharding | None = ...
