from collections.abc import Sequence
from dataclasses import dataclass
from functools import cached_property
from warnings import deprecated

from torch._ops import OpOverload
from torch.distributed.device_mesh import DeviceMesh
from torch.distributed.tensor._dtensor_spec import DTensorSpec
from torch.distributed.tensor.placement_types import Placement
from torch.utils._cxx_pytree import TreeSpec

type ArgsType = tuple[object, ...]
type KwargsType = dict[str, object]
type PlacementList = list[Placement | None]
type OutputSpecType = DTensorSpec | Sequence[DTensorSpec | None] | None

@dataclass
class OpSpec:
    output_specs: DTensorSpec | tuple[DTensorSpec | None, ...]
    input_specs: Sequence[DTensorSpec] | None = ...
    redistribute_cost: list[list[float]] | None = ...
    @cached_property
    def output_spec(self) -> DTensorSpec: ...
    @cached_property
    def mesh(self): ...
    def input_spec(self, index: int = ...) -> DTensorSpec: ...

class StrategyType: ...

class OpStrategy(StrategyType):
    def __init__(self, strategies: list[OpSpec]) -> None: ...
    def max_num_shards(self) -> int: ...
    @property
    def mesh(self): ...
    @property
    def mesh_shape(self): ...
    @property
    def ndim(self): ...
    @property
    def shape(self): ...

class TupleStrategy(StrategyType):
    def __init__(self, children: Sequence[StrategyType]) -> None: ...
    @property
    @deprecated("TupleStrategy.childs is deprecated, use TupleStrategy.children instead.", category=FutureWarning)
    def childs(self) -> Sequence[StrategyType]: ...
    def child_mesh(self, index: int) -> DeviceMesh: ...

@dataclass
class RuntimeSchemaInfo:
    static_argnum: int = ...
    static_kwargkey: list[str] | None = ...
    needs_pytree: bool = ...

@dataclass
class OpSchema:
    op: OpOverload
    args_schema: ArgsType
    kwargs_schema: KwargsType
    schema_info: RuntimeSchemaInfo | None = ...
    _comparison_key: tuple[object, ...] | None = ...
    @property
    def args_spec(self) -> tuple[DTensorSpec, ...]: ...
    @property
    def args_strategy(self) -> tuple[OpStrategy, ...]: ...
    def __post_init__(self) -> None: ...
    def arg_type_tensor_or_tensor_list_like(self, arg: object) -> bool: ...
    def return_type_tuple_tensor_like(self) -> bool: ...
    def return_type_list_tensor_like(self) -> bool: ...
    def return_type_tensor(self) -> bool: ...
    def get_mesh_from_args(self, validate: bool = ...) -> DeviceMesh: ...
    def is_inplace_op(self) -> bool: ...
    def is_out_variant_op(self) -> bool: ...
    def is_view_op(self) -> bool: ...
    def __hash__(self) -> int: ...
    def __eq__(self, other: object) -> bool: ...
    def gen_fake_args(self) -> ArgsType: ...
    def gen_fake_kwargs(self) -> KwargsType: ...

@dataclass
class OutputSharding:
    output_spec: OutputSpecType
    redistribute_schema: OpSchema | None = ...
    needs_redistribute: bool = ...
    use_val_from_redistribute_schema: bool = ...
    @cached_property
    def mesh(self): ...

@dataclass
class OpInfo:
    compute_mesh: DeviceMesh
    schema: OpSchema
    flat_args_schema: list[object]
    local_args: Sequence[object]
    local_kwargs: dict[str, object]
    args_tree_spec: TreeSpec | None = ...
    output_sharding: OutputSharding | None = ...
