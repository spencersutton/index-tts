import functools
import os
from collections.abc import Callable
from types import ModuleType
from typing import Any, NamedTuple, Optional, TypeAlias, Union

import torch
import torch.utils._pytree as pytree
from torch._C import ScriptObject
from torch._library.fake_class_registry import FakeScriptObject

from ._compatibility import compatibility
from .graph import Graph
from .graph_module import GraphModule
from .node import Argument
from .proxy import TracerBase

log = ...
HAS_VARSTUFF = ...
_orig_module_call: Callable = ...
_orig_module_getattr: Callable = ...
_proxyable_classes: dict[type, None] = ...
_is_fx_tracing_flag = ...
type _ConstantAttributeType = torch.Tensor | torch.ScriptObject | FakeScriptObject | pytree.TreeSpec
_constant_attribute_types = ...

@functools.lru_cache
def is_fx_tracing_warning():  # -> None:
    ...
def is_fx_tracing():  # -> bool:
    ...
def is_fx_symbolic_tracing():  # -> bool:
    ...

@compatibility(is_backward_compatible=True)
class ProxyableClassMeta(type):
    def __init__(cls, name, bases, attrs) -> None: ...
    def __call__(cls, *args, **kwargs): ...

@compatibility(is_backward_compatible=False)
class PHBase:
    def __repr__(self):  # -> Literal['PH']:
        ...

PH = ...

@compatibility(is_backward_compatible=False)
class PHWithMeta(PHBase):
    def __init__(self, ph_key: str | None = ...) -> None: ...

@compatibility(is_backward_compatible=True)
class Tracer(TracerBase):
    @compatibility(is_backward_compatible=True)
    def __init__(
        self,
        autowrap_modules: tuple[ModuleType] = ...,
        autowrap_functions: tuple[Callable, ...] = ...,
        param_shapes_constant: bool = ...,
    ) -> None: ...

    _qualname_counter: dict[str, int] = ...
    @compatibility(is_backward_compatible=True)
    def get_fresh_qualname(self, prefix: str) -> str: ...
    @compatibility(is_backward_compatible=True)
    def create_arg(self, a: Any) -> Argument: ...
    @compatibility(is_backward_compatible=True)
    def is_leaf_module(self, m: torch.nn.Module, module_qualified_name: str) -> bool: ...
    @compatibility(is_backward_compatible=True)
    def path_of_module(self, mod: torch.nn.Module) -> str: ...
    @compatibility(is_backward_compatible=True)
    def call_module(
        self, m: torch.nn.Module, forward: Callable[..., Any], args: tuple[Any, ...], kwargs: dict[str, Any]
    ) -> Any: ...
    @compatibility(is_backward_compatible=False)
    def getattr(self, attr: str, attr_val: Any, parameter_proxy_cache: dict[str, Any]):  # -> Parameter | Tensor | Any:

        ...
    @compatibility(is_backward_compatible=False)
    def create_args_for_root(
        self, root_fn, is_module, concrete_args=...
    ):  # -> tuple[Any, list[Any]] | tuple[Callable[..., list[Any]], list[Any]] | tuple[FunctionType | Any, list[Any]]:

        ...
    @compatibility(is_backward_compatible=True)
    def trace(
        self, root: torch.nn.Module | Callable[..., Any], concrete_args: dict[str, Any] | None = ...
    ) -> Graph: ...
    def __deepcopy__(self, memo):  # -> Tracer:
        ...

_wrapped_fns_to_patch: dict[tuple[int, str], dict] = ...
_wrapped_methods_to_patch: list[tuple[type, str]] = ...
if os.environ.get("FX_PATCH_GETITEM") == "1": ...

class _PatchedFn(NamedTuple):
    frame_dict: Any
    fn_name: str
    orig_fn: Any
    new_fn: Any
    def revert(self): ...
    def patch(self): ...

class _PatchedFnSetItem(_PatchedFn):
    def revert(self):  # -> None:
        ...
    def patch(self):  # -> None:
        ...

class _PatchedFnDel(_PatchedFn):
    def revert(self):  # -> None:
        ...
    def patch(self):  # -> None:
        ...

class _PatchedFnSetAttr(_PatchedFn):
    def revert(self):  # -> None:
        ...
    def patch(self):  # -> None:
        ...

class _Patcher:
    def __init__(self) -> None: ...
    def patch(self, frame_dict: dict[str, Any], name: str, new_fn: Callable, deduplicate: bool = ...):  # -> None:

        ...
    def patch_method(self, cls: type, name: str, new_fn: Callable, deduplicate: bool = ...):  # -> None:

        ...
    def visit_once(self, thing: Any):  # -> bool:

        ...
    def revert_all_patches(self):  # -> list[_PatchedFn]:

        ...
    def reapply_all_patches(self):  # -> list[_PatchedFn]:

        ...
    def __enter__(self):  # -> Self:
        ...
    def __exit__(self, exc_type, exc_val, exc_tb):  # -> None:

        ...

CURRENT_PATCHER: _Patcher | None = ...

@compatibility(is_backward_compatible=True)
def wrap(fn_or_name: str | Callable):  # -> Callable[..., Any] | str:

    ...
@compatibility(is_backward_compatible=True)
def symbolic_trace(
    root: torch.nn.Module | Callable[..., Any], concrete_args: dict[str, Any] | None = ...
) -> GraphModule: ...
