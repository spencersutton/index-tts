from collections.abc import Iterable
from typing import Any

import torch
from torch import fx, nn

__all__ = [
    "MklSubgraph",
    "UnionFind",
    "extract_subgraph",
    "fuse",
    "gen_mkl_autotuner",
    "matches_module_pattern",
    "modules_to_mkldnn",
    "optimize_for_inference",
    "remove_dropout",
    "replace_node_module",
    "reset_modules",
    "use_mkl_length",
]

def matches_module_pattern(pattern: Iterable[type], node: fx.Node, modules: dict[str, Any]) -> bool: ...
def replace_node_module(node: fx.Node, modules: dict[str, Any], new_module: torch.nn.Module) -> None: ...
def fuse(model: torch.nn.Module, inplace=..., no_trace=...) -> torch.nn.Module:
    """
    Fuses convolution/BN and linear/BN layers for inference purposes.
    Will deepcopy your model by default, but can modify the model inplace as well.
    """

def remove_dropout(model: nn.Module) -> nn.Module:
    """Removes all dropout layers from the module."""

def extract_subgraph(
    orig_module: nn.Module, nodes: list[fx.Node], inputs: list[fx.Node], outputs: list[fx.Node]
) -> GraphModule:
    """Given lists of nodes from an existing graph that represent a subgraph, returns a submodule that executes that subgraph."""

mkldnn_supported = ...
mkldnn_supported_unknown = ...
mkldnn_map = ...

def modules_to_mkldnn(nodes: list[fx.Node], modules: dict[str, nn.Module]) -> dict[Module, Module]:
    """
    For each node, if it's a module that can be preconverted into MKLDNN,
    then we do so and create a mapping to allow us to convert from the MKLDNN
    version of the module to the original.
    """

def reset_modules(nodes: list[fx.Node], modules: dict[str, nn.Module], old_modules: dict[nn.Module, nn.Module]) -> None:
    """
    Maps each module that's been changed with `modules_to_mkldnn` back to its
    original.
    """

class MklSubgraph:
    def __init__(self, fx_graph: fx.Graph) -> None: ...

def gen_mkl_autotuner(example_inputs, iters=..., warmup=...) -> Callable[..., bool]:
    """
    This generates a heuristic that can be passed into `optimize_for_inference` that
    determines whether a subgraph should be run in MKL by running it with the example_inputs.

    Example usage:
        heuristic = gen_mkl_autotuner(example_inputs, iters=10)
        fast_model = optimization.optimize_for_inference(model, heuristic)
    """

def use_mkl_length(graph: MklSubgraph) -> bool:
    """
    This is a heuristic that can be passed into `optimize_for_inference` that
    determines whether a subgraph should be run in MKL by checking if there
    are more than 2 nodes in it
    """

class UnionFind:
    def __init__(self, n) -> None: ...
    def make_set(self, v: int) -> None: ...
    def find(self, v: int) -> int: ...
    def join(self, a: int, b: int) -> int | None: ...

def optimize_for_inference(
    model: torch.nn.Module, pass_config: dict[str, Any] | None = ..., tracer: type[fx.Tracer] = ...
) -> torch.nn.Module:
    """
    Performs a set of optimization passes to optimize a model for the
    purposes of inference. Specifically, the passes that are run are:
    1. Conv/BN fusion
    2. Dropout removal
    3. MKL layout optimizations

    The third optimization takes a function `use_mkl_heuristic` that's used
    to determine whether a subgraph should be explicitly run in MKL layout.

    Note: As FX does not currently handle aliasing, this pass currently
    assumes nothing aliases. If that isn't true, use at your own risk.
    """
