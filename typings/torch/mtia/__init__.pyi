import torch
from collections.abc import Callable
from typing import Any, TypeAlias
from torch import Tensor, device as _device
from .memory import *

_device_t: TypeAlias = _device | str | int
Event = torch.Event
Stream = torch.Stream
_initialized = ...
_queued_calls: list[tuple[Callable[[], None], list[str]]] = ...
_tls = ...
_initialization_lock = ...
_lazy_seed_tracker = ...
if hasattr(torch._C, "_mtia_exchangeDevice"):
    _exchange_device = ...
if hasattr(torch._C, "_mtia_maybeExchangeDevice"):
    _maybe_exchange_device = ...

def init() -> None: ...
def is_initialized() -> bool: ...

class DeferredMtiaCallError(Exception): ...

def is_available() -> bool: ...
def synchronize(device: _device_t | None = ...) -> None: ...
def device_count() -> int: ...
def current_device() -> int: ...
def current_stream(device: _device_t | None = ...) -> Stream: ...
def default_stream(device: _device_t | None = ...) -> Stream: ...
def record_memory_history(enabled: str | None = ..., stacks: str = ..., max_entries: int = ...) -> None: ...
def snapshot() -> dict[str, Any]: ...
def attach_out_of_memory_observer(observer: Callable[[int, int, int, int], None]) -> None: ...
def is_bf16_supported(including_emulation: bool = ...) -> Literal[True]: ...
def get_device_capability(device: _device_t | None = ...) -> tuple[int, int]: ...
def empty_cache() -> None: ...
def set_stream(stream: Stream) -> None: ...
def set_device(device: _device_t) -> None: ...
def get_device_properties(device: _device_t | None = ...) -> dict[str, Any]: ...

class device:
    def __init__(self, device: Any) -> None: ...
    def __enter__(self) -> None: ...
    def __exit__(self, type: Any, value: Any, traceback: Any) -> Literal[False]: ...

class StreamContext:
    cur_stream: torch.mtia.Stream | None
    def __init__(self, stream: torch.mtia.Stream | None) -> None: ...
    def __enter__(self) -> None: ...
    def __exit__(self, type: Any, value: Any, traceback: Any) -> None: ...

def stream(stream: torch.mtia.Stream | None) -> StreamContext: ...
def get_rng_state(device: int | str | torch.device = ...) -> Tensor: ...
def set_rng_state(new_state: Tensor, device: int | str | torch.device = ...) -> None: ...

__all__ = [
    "attach_out_of_memory_observer",
    "current_device",
    "current_stream",
    "default_stream",
    "device",
    "device_count",
    "empty_cache",
    "get_device_capability",
    "get_device_properties",
    "get_rng_state",
    "init",
    "is_available",
    "is_bf16_supported",
    "is_initialized",
    "max_memory_allocated",
    "memory_allocated",
    "memory_stats",
    "record_memory_history",
    "reset_peak_memory_stats",
    "set_device",
    "set_rng_state",
    "set_stream",
    "snapshot",
    "stream",
    "synchronize",
]
