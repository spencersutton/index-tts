import torch
from torch import Tensor

from .module import Module

__all__ = [
    "CELU",
    "ELU",
    "GELU",
    "GLU",
    "SELU",
    "Hardshrink",
    "Hardsigmoid",
    "Hardswish",
    "Hardtanh",
    "LeakyReLU",
    "LogSigmoid",
    "LogSoftmax",
    "Mish",
    "MultiheadAttention",
    "PReLU",
    "RReLU",
    "ReLU",
    "ReLU6",
    "SiLU",
    "Sigmoid",
    "Softmax",
    "Softmax2d",
    "Softmin",
    "Softplus",
    "Softshrink",
    "Softsign",
    "Tanh",
    "Tanhshrink",
    "Threshold",
]

class Threshold(Module):
    __constants__ = ...
    threshold: float
    value: float
    inplace: bool
    def __init__(self, threshold: float, value: float, inplace: bool = ...) -> None: ...
    def forward(self, input: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...

class ReLU(Module):
    __constants__ = ...
    inplace: bool
    def __init__(self, inplace: bool = ...) -> None: ...
    def forward(self, input: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...

class RReLU(Module):
    __constants__ = ...
    lower: float
    upper: float
    inplace: bool
    def __init__(self, lower: float = ..., upper: float = ..., inplace: bool = ...) -> None: ...
    def forward(self, input: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...

class Hardtanh(Module):
    __constants__ = ...
    min_val: float
    max_val: float
    inplace: bool
    def __init__(
        self,
        min_val: float = ...,
        max_val: float = ...,
        inplace: bool = ...,
        min_value: float | None = ...,
        max_value: float | None = ...,
    ) -> None: ...
    def forward(self, input: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...

class ReLU6(Hardtanh):
    def __init__(self, inplace: bool = ...) -> None: ...
    def extra_repr(self) -> str: ...

class Sigmoid(Module):
    def forward(self, input: Tensor) -> Tensor: ...

class Hardsigmoid(Module):
    __constants__ = ...
    inplace: bool
    def __init__(self, inplace: bool = ...) -> None: ...
    def forward(self, input: Tensor) -> Tensor: ...

class Tanh(Module):
    def forward(self, input: Tensor) -> Tensor: ...

class SiLU(Module[[Tensor], Tensor]):
    __constants__ = ...
    inplace: bool
    def __init__(self, inplace: bool = ...) -> None: ...
    def forward(self, input: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...

class Mish(Module):
    __constants__ = ...
    inplace: bool
    def __init__(self, inplace: bool = ...) -> None: ...
    def forward(self, input: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...

class Hardswish(Module):
    __constants__ = ...
    inplace: bool
    def __init__(self, inplace: bool = ...) -> None: ...
    def forward(self, input: Tensor) -> Tensor: ...

class ELU(Module):
    __constants__ = ...
    alpha: float
    inplace: bool
    def __init__(self, alpha: float = ..., inplace: bool = ...) -> None: ...
    def forward(self, input: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...

class CELU(Module):
    __constants__ = ...
    alpha: float
    inplace: bool
    def __init__(self, alpha: float = ..., inplace: bool = ...) -> None: ...
    def forward(self, input: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...

class SELU(Module):
    __constants__ = ...
    inplace: bool
    def __init__(self, inplace: bool = ...) -> None: ...
    def forward(self, input: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...

class GLU(Module):
    __constants__ = ...
    dim: int
    def __init__(self, dim: int = ...) -> None: ...
    def forward(self, input: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...

class GELU(Module):
    __constants__ = ...
    approximate: str
    def __init__(self, approximate: str = ...) -> None: ...
    def forward(self, input: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...

class Hardshrink(Module):
    __constants__ = ...
    lambd: float
    def __init__(self, lambd: float = ...) -> None: ...
    def forward(self, input: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...

class LeakyReLU(Module):
    __constants__ = ...
    inplace: bool
    negative_slope: float
    def __init__(self, negative_slope: float = ..., inplace: bool = ...) -> None: ...
    def forward(self, input: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...

class LogSigmoid(Module):
    def forward(self, input: Tensor) -> Tensor: ...

class Softplus(Module):
    __constants__ = ...
    beta: float
    threshold: float
    def __init__(self, beta: float = ..., threshold: float = ...) -> None: ...
    def forward(self, input: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...

class Softshrink(Module):
    __constants__ = ...
    lambd: float
    def __init__(self, lambd: float = ...) -> None: ...
    def forward(self, input: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...

class MultiheadAttention(Module):
    __constants__ = ...
    bias_k: torch.Tensor | None
    bias_v: torch.Tensor | None
    def __init__(
        self,
        embed_dim,
        num_heads,
        dropout=...,
        bias=...,
        add_bias_kv=...,
        add_zero_attn=...,
        kdim=...,
        vdim=...,
        batch_first=...,
        device=...,
        dtype=...,
    ) -> None: ...
    def __setstate__(self, state) -> None: ...
    def forward(
        self,
        query: Tensor,
        key: Tensor,
        value: Tensor,
        key_padding_mask: Tensor | None = ...,
        need_weights: bool = ...,
        attn_mask: Tensor | None = ...,
        average_attn_weights: bool = ...,
        is_causal: bool = ...,
    ) -> tuple[Tensor, Tensor | None]: ...
    def merge_masks(
        self, attn_mask: Tensor | None, key_padding_mask: Tensor | None, query: Tensor
    ) -> tuple[Tensor | None, int | None]: ...

class PReLU(Module):
    __constants__ = ...
    num_parameters: int
    def __init__(self, num_parameters: int = ..., init: float = ..., device=..., dtype=...) -> None: ...
    def reset_parameters(self) -> None: ...
    def forward(self, input: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...

class Softsign(Module):
    def forward(self, input: Tensor) -> Tensor: ...

class Tanhshrink(Module):
    def forward(self, input: Tensor) -> Tensor: ...

class Softmin(Module):
    __constants__ = ...
    dim: int | None
    def __init__(self, dim: int | None = ...) -> None: ...
    def __setstate__(self, state) -> None: ...
    def forward(self, input: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...

class Softmax(Module):
    __constants__ = ...
    dim: int | None
    def __init__(self, dim: int | None = ...) -> None: ...
    def __setstate__(self, state) -> None: ...
    def forward(self, input: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...

class Softmax2d(Module):
    def forward(self, input: Tensor) -> Tensor: ...

class LogSoftmax(Module):
    __constants__ = ...
    dim: int | None
    def __init__(self, dim: int | None = ...) -> None: ...
    def __setstate__(self, state) -> None: ...
    def forward(self, input: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...
