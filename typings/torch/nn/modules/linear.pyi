from typing import Any

from torch import Tensor
from torch.nn.parameter import UninitializedParameter

from indextts.util import patch_call

from .lazy import LazyModuleMixin
from .module import Module

__all__ = ["Bilinear", "Identity", "LazyLinear", "Linear"]

class Identity(Module):
    def __init__(self, *args: Any, **kwargs: Any) -> None: ...
    def forward(self, input: Tensor) -> Tensor: ...
    @patch_call(forward)
    def __call__(self) -> None: ...

class Linear(Module[[Tensor], Tensor]):
    bias: Tensor | None
    __constants__ = ...
    in_features: int
    out_features: int
    weight: Tensor
    def __init__(self, in_features: int, out_features: int, bias: bool = ..., device=..., dtype=...) -> None: ...
    def reset_parameters(self) -> None: ...
    def forward(self, input: Tensor) -> Tensor: ...
    def __call__(self, input: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...
    @patch_call(forward)
    def __call__(self) -> None: ...

class NonDynamicallyQuantizableLinear(Linear):
    def __init__(self, in_features: int, out_features: int, bias: bool = ..., device=..., dtype=...) -> None: ...

class Bilinear(Module):
    __constants__ = ...
    in1_features: int
    in2_features: int
    out_features: int
    weight: Tensor
    def __init__(
        self, in1_features: int, in2_features: int, out_features: int, bias: bool = ..., device=..., dtype=...
    ) -> None: ...
    def reset_parameters(self) -> None: ...
    def forward(self, input1: Tensor, input2: Tensor) -> Tensor: ...
    def extra_repr(self) -> str: ...

class LazyLinear(LazyModuleMixin, Linear):
    cls_to_become = ...
    weight: UninitializedParameter
    bias: UninitializedParameter
    def __init__(self, out_features: int, bias: bool = ..., device=..., dtype=...) -> None: ...
    def reset_parameters(self) -> None: ...
    def initialize_parameters(self, input) -> None: ...
