import torch
from collections import OrderedDict
from collections.abc import Callable, Iterable
from typing import Any, ParamSpec, Self, TypeVar, overload, TypeAlias
from torch.utils.hooks import RemovableHandle

_P = ParamSpec("_P")
Args: TypeAlias = tuple[Any, ...]
Kwargs: TypeAlias = dict[str, Any]
StateDict: TypeAlias = dict[str, Any]
DeviceDict: TypeAlias = dict[torch.device | None, torch.Tensor]
DeviceDtypeDict: TypeAlias = dict[tuple[torch.device, torch.dtype] | None, torch.Tensor]
GlobalOptimizerPreHook: TypeAlias = Callable[[Optimizer, Args, Kwargs], tuple[Args, Kwargs] | None]
GlobalOptimizerPostHook: TypeAlias = Callable[[Optimizer, Args, Kwargs], None]
__all__ = ["Optimizer", "register_optimizer_step_post_hook", "register_optimizer_step_pre_hook"]
_global_optimizer_pre_hooks: dict[int, GlobalOptimizerPreHook] = ...
_global_optimizer_post_hooks: dict[int, GlobalOptimizerPostHook] = ...
_foreach_supported_types = ...

class _RequiredParameter: ...

required = ...
_params_doc = ...
_foreach_doc = ...
_fused_doc = ...
_capturable_doc = ...
_differentiable_doc = ...
_maximize_doc = ...

def register_optimizer_step_pre_hook(hook: GlobalOptimizerPreHook) -> RemovableHandle: ...
def register_optimizer_step_post_hook(hook: GlobalOptimizerPostHook) -> RemovableHandle: ...

ParamsT: TypeAlias = Iterable[torch.Tensor] | Iterable[dict[str, Any]] | Iterable[tuple[str, torch.Tensor]]
R = TypeVar("R")
T = TypeVar("T")

class Optimizer:
    OptimizerPreHook: TypeAlias = Callable[[Self, Args, Kwargs], tuple[Args, Kwargs] | None]
    OptimizerPostHook: TypeAlias = Callable[[Self, Args, Kwargs], None]
    _optimizer_step_pre_hooks: dict[int, OptimizerPreHook]
    _optimizer_step_post_hooks: dict[int, OptimizerPostHook]
    _optimizer_state_dict_pre_hooks: OrderedDict[int, Callable[[Optimizer], None]]
    _optimizer_state_dict_post_hooks: OrderedDict[int, Callable[[Optimizer, StateDict], StateDict | None]]
    _optimizer_load_state_dict_pre_hooks: OrderedDict[int, Callable[[Optimizer, StateDict], StateDict | None]]
    _optimizer_load_state_dict_post_hooks: OrderedDict[int, Callable[[Optimizer], None]]
    def __init__(self, params: ParamsT, defaults: dict[str, Any]) -> None: ...
    def __getstate__(self) -> dict[str, Any]: ...
    def __setstate__(self, state: dict[str, Any]) -> None: ...
    @staticmethod
    def profile_hook_step(func: Callable[_P, R]) -> Callable[_P, R]: ...
    def register_step_pre_hook(self, hook: OptimizerPreHook) -> RemovableHandle: ...
    def register_step_post_hook(self, hook: OptimizerPostHook) -> RemovableHandle: ...
    def register_state_dict_pre_hook(
        self, hook: Callable[[Optimizer], None], prepend: bool = ...
    ) -> RemovableHandle: ...
    def register_state_dict_post_hook(
        self, hook: Callable[[Optimizer, StateDict], StateDict | None], prepend: bool = ...
    ) -> RemovableHandle: ...
    @torch._disable_dynamo
    def state_dict(self) -> StateDict: ...
    def register_load_state_dict_pre_hook(
        self, hook: Callable[[Optimizer, StateDict], StateDict | None], prepend: bool = ...
    ) -> RemovableHandle: ...
    def register_load_state_dict_post_hook(
        self, hook: Callable[[Optimizer], None], prepend: bool = ...
    ) -> RemovableHandle: ...
    @torch._disable_dynamo
    def load_state_dict(self, state_dict: StateDict) -> None: ...
    @torch._disable_dynamo
    def zero_grad(self, set_to_none: bool = ...) -> None: ...
    @overload
    def step(self, closure: None = ...) -> None: ...
    @overload
    def step(self, closure: Callable[[], float]) -> float: ...
    def step(self, closure: Callable[[], float] | None = ...) -> float | None: ...
    @torch._disable_dynamo
    def add_param_group(self, param_group: dict[str, Any]) -> None: ...
