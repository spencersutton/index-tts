import threading
import torch
from typing import Any, Self, TYPE_CHECKING, TypeVar
from torch._prims_common import DeviceLikeType
from torch.types import _bool, _int

if TYPE_CHECKING: ...
__all__ = ["TypedStorage", "UntypedStorage"]
HAS_NUMPY = ...
_share_memory_lock = ...
_share_memory_map: dict[int, threading.RLock] = ...
T = TypeVar("T", bound=_StorageBase | TypedStorage)

class _StorageBase:
    _cdata: Any
    is_sparse: _bool = ...
    is_sparse_csr: _bool = ...
    device: torch.device
    _fake_device: torch.device | None = ...
    _checkpoint_offset: int | None = ...
    def __init__(self, *args, **kwargs) -> None: ...
    def __len__(self) -> _int: ...
    def __getitem__(self, idx): ...
    def __setitem__(self, *args, **kwargs) -> None: ...
    def copy_(self, source: T, non_blocking: _bool | None = ...) -> T: ...
    def new(self) -> _StorageBase | TypedStorage: ...
    def nbytes(self) -> _int: ...
    def size(self) -> _int: ...
    def type(self, dtype: str | None = ..., non_blocking: _bool = ...) -> _StorageBase | TypedStorage: ...
    def cuda(self, device=..., non_blocking=...) -> _StorageBase | TypedStorage: ...
    def hpu(self, device=..., non_blocking=...) -> _StorageBase | TypedStorage: ...
    def element_size(self) -> _int: ...
    def get_device(self) -> _int: ...
    def data_ptr(self) -> _int: ...
    def resizable(self) -> _bool: ...
    @classmethod
    def from_buffer(cls, *args, **kwargs) -> Self: ...
    def resize_(self, size: _int): ...
    def is_shared(self) -> _bool: ...
    @property
    def is_cuda(self): ...
    @property
    def is_hpu(self): ...
    @classmethod
    def from_file(cls, filename, shared, nbytes) -> _StorageBase | TypedStorage: ...
    def __iter__(self) -> Generator[Any, None, None]: ...
    def __copy__(self) -> Self: ...
    def __deepcopy__(self, memo) -> Self: ...
    def __reduce__(self) -> tuple[Callable[..., Any], tuple[bytes]]: ...
    def __sizeof__(self) -> int: ...
    def clone(self) -> Self: ...
    def tolist(self) -> list[Any]: ...
    def cpu(self) -> Self: ...
    def mps(self) -> Self: ...
    def to(self, *, device: DeviceLikeType, non_blocking: _bool = ...) -> UntypedStorage | Any: ...
    def double(self) -> TypedStorage: ...
    def float(self) -> TypedStorage: ...
    def half(self) -> TypedStorage: ...
    def long(self) -> TypedStorage: ...
    def int(self) -> TypedStorage: ...
    def short(self) -> TypedStorage: ...
    def char(self) -> TypedStorage: ...
    def byte(self) -> TypedStorage: ...
    def bool(self) -> TypedStorage: ...
    def bfloat16(self) -> TypedStorage: ...
    def complex_double(self) -> TypedStorage: ...
    def complex_float(self) -> TypedStorage: ...
    def float8_e5m2(self) -> TypedStorage: ...
    def float8_e4m3fn(self) -> TypedStorage: ...
    def float8_e5m2fnuz(self) -> TypedStorage: ...
    def float8_e4m3fnuz(self) -> TypedStorage: ...
    def is_pinned(self, device: str | torch.device = ...) -> bool: ...
    def pin_memory(self, device: str | torch.device = ...) -> UntypedStorage: ...
    def share_memory_(self) -> Self: ...
    def untyped(self) -> Self: ...
    def byteswap(self, dtype) -> None: ...

class UntypedStorage(torch._C.StorageBase, _StorageBase):
    def __getitem__(self, *args, **kwargs): ...
    @property
    def is_cuda(self) -> bool: ...
    @property
    def is_hpu(self) -> bool: ...
    @property
    def filename(self) -> str | None: ...
    @_share_memory_lock_protected
    def share_memory_(self, *args, **kwargs) -> Self: ...

_always_warn_typed_storage_removal = ...

class TypedStorage:
    is_sparse: _bool = ...
    _fake_device: torch.device | None = ...
    dtype: torch.dtype
    @property
    def filename(self) -> str | None: ...
    def fill_(self, value) -> Self: ...
    def __new__(cls, *args, wrap_storage=..., dtype=..., device=..., _internal=...) -> Self | TypedStorage: ...
    def __init__(self, *args, device=..., dtype=..., wrap_storage=..., _internal=...) -> None: ...
    @property
    def is_cuda(self) -> bool: ...
    @property
    def is_hpu(self) -> bool: ...
    def untyped(self) -> UntypedStorage: ...
    def __len__(self) -> int: ...
    def __setitem__(self, idx, value) -> None: ...
    def __getitem__(self, idx) -> Number: ...
    def copy_(self, source: T, non_blocking: bool | None = ...) -> Self: ...
    def nbytes(self) -> int: ...
    def type(self, dtype: str | None = ..., non_blocking: bool = ...) -> _StorageBase | TypedStorage | str: ...
    def cuda(self, device=..., non_blocking=...) -> Self: ...
    def hpu(self, device=..., non_blocking=...) -> Self: ...
    def to(self, *, device: DeviceLikeType, non_blocking: bool = ...) -> Self: ...
    def element_size(self): ...
    def get_device(self) -> _int: ...
    def __iter__(self) -> Generator[Number | Any, None, None]: ...
    def __copy__(self) -> Self: ...
    def __deepcopy__(self, memo) -> Self: ...
    def __sizeof__(self) -> int: ...
    def clone(self) -> Self: ...
    def tolist(self) -> list[Number | Any]: ...
    def cpu(self) -> Self: ...
    def is_pinned(self, device: str | torch.device = ...) -> bool: ...
    def pin_memory(self, device: str | torch.device = ...) -> Self: ...
    def share_memory_(self) -> Self: ...
    @property
    def device(self) -> device: ...
    def size(self): ...
    def pickle_storage_type(self) -> str: ...
    def __reduce__(self) -> tuple[Callable[..., Any], tuple[bytes]]: ...
    def data_ptr(self) -> int: ...
    def resizable(self) -> bool: ...
    def resize_(self, size) -> None: ...
    @classmethod
    def from_buffer(cls, *args, **kwargs) -> TypedStorage: ...
    def double(self) -> TypedStorage: ...
    def float(self) -> TypedStorage: ...
    def half(self) -> TypedStorage: ...
    def long(self) -> TypedStorage: ...
    def int(self) -> TypedStorage: ...
    def short(self) -> TypedStorage: ...
    def char(self) -> TypedStorage: ...
    def byte(self) -> TypedStorage: ...
    def bool(self) -> TypedStorage: ...
    def bfloat16(self) -> TypedStorage: ...
    def complex_double(self) -> TypedStorage: ...
    def complex_float(self) -> TypedStorage: ...
    def float8_e5m2(self) -> TypedStorage: ...
    def float8_e4m3fn(self) -> TypedStorage: ...
    def float8_e5m2fnuz(self) -> TypedStorage: ...
    def float8_e4m3fnuz(self) -> TypedStorage: ...
    @classmethod
    def from_file(cls, filename, shared, size) -> Self: ...
    def is_shared(self) -> bool: ...

class _LegacyStorageMeta(type):
    dtype: torch.dtype
    def __instancecheck__(cls, instance) -> bool: ...

class _LegacyStorage(TypedStorage, metaclass=_LegacyStorageMeta): ...
