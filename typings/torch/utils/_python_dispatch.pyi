import functools
from collections.abc import Sequence
from dataclasses import dataclass
from typing import Protocol, TypeIs, overload

import torch

_is_in_torch_dispatch_mode = ...
_is_in_non_infra_torch_dispatch_mode = ...
_is_in_any_mode_without_ignore_compile_internals = ...

def is_in_torch_dispatch_mode(include_infra_modes=...) -> bool: ...
def is_in_any_mode_without_ignore_compile_internals() -> bool: ...

class TorchDispatchMode:
    supports_higher_order_operators = ...
    def __init__(self, _dispatch_key=...) -> None: ...
    def __torch_dispatch__(self, func, types, args=..., kwargs=...): ...
    def __enter__(self): ...
    def __exit__(self, exc_type, exc_val, exc_tb): ...
    @classmethod
    def push(cls, *args, **kwargs): ...
    @classmethod
    def is_infra_mode(cls): ...
    @classmethod
    def ignore_compile_internals(cls): ...

class BaseTorchDispatchMode(TorchDispatchMode):
    def __torch_dispatch__(self, func, types, args=..., kwargs=...): ...

class TensorWithFlatten(Protocol):
    def __tensor_flatten__(self) -> tuple[Sequence[str], object]: ...
    @staticmethod
    def __tensor_unflatten__(
        inner_tensors: int, flatten_spec: int, outer_size: int, outer_stride: int
    ) -> torch.Tensor: ...

    shape: torch._C.Size
    @overload
    def stride(self, dim: None = ...) -> tuple[int, ...]: ...
    @overload
    def stride(self, dim: int) -> int: ...
    @overload
    def size(self, dim: None = ...) -> tuple[int, ...]: ...
    @overload
    def size(self, dim: int) -> int: ...
    def storage_offset(self) -> int: ...
    def dim(self) -> int: ...
    @overload
    def to(
        self,
        dtype: torch.types._dtype,
        non_blocking: bool = ...,
        copy: bool = ...,
        *,
        memory_format: torch.memory_format | None = ...,
    ) -> torch.Tensor: ...
    @overload
    def to(
        self,
        device: torch._prims_common.DeviceLikeType | None = ...,
        dtype: torch.types._dtype | None = ...,
        non_blocking: bool = ...,
        copy: bool = ...,
        *,
        memory_format: torch.memory_format | None = ...,
    ) -> torch.Tensor: ...
    @overload
    def to(
        self,
        other: torch.Tensor,
        non_blocking: bool = ...,
        copy: bool = ...,
        *,
        memory_format: torch.memory_format | None = ...,
    ) -> torch.Tensor: ...

def is_traceable_wrapper_subclass(t: object) -> TypeIs[TensorWithFlatten]: ...
def is_traceable_wrapper_subclass_type(t: type) -> TypeIs[type[TensorWithFlatten]]: ...
def transform_subclass(t, callback, outer_size=..., outer_stride=...): ...

@dataclass
class AliasInfo:
    alias_set: set[str]
    is_write: bool
    name: str | None

@dataclass
class SchemaInfo:
    args: list[AliasInfo]
    outs: list[AliasInfo]
    int_tags: list[int]

@functools.cache
def get_alias_info(func) -> SchemaInfo: ...

_TORCH_TAG_INPLACE_VIEW_INT = ...

def return_and_correct_aliasing(func, args, kwargs, out): ...
