from collections import UserList
from collections.abc import Callable, Iterable, Iterator
from typing import Any, Literal, TypeVar

from torch.utils.data import Dataset, IterableDataset
from torch.utils.data.datapipes._hook_iterator import _SnapshotState
from torch.utils.data.datapipes._typing import _DataPipeMeta, _IterDataPipeMeta

_T = TypeVar("_T")
_T_co = TypeVar("_T_co", covariant=True)
UNTRACABLE_DATAFRAME_PIPES: Any

class DataChunk(UserList[_T]):
    items: list[_T]
    def __init__(self, items: Iterable[_T]) -> None: ...
    def as_str(self, indent: str = ...) -> str: ...
    def __iter__(self) -> Iterator[_T]: ...
    def raw_iterator(self) -> Iterator[_T]: ...

class MapDataPipe(Dataset[_T_co], metaclass=_DataPipeMeta):
    functions: dict[str, Callable] = ...
    reduce_ex_hook: Callable | None = ...
    getstate_hook: Callable | None = ...
    str_hook: Callable | None = ...
    repr_hook: Callable | None = ...
    def __getattr__(self, attribute_name: Any): ...
    @classmethod
    def register_function(cls, function_name: Any, function: Any) -> None: ...
    @classmethod
    def register_datapipe_as_function(cls, function_name: Any, cls_to_register: Any): ...
    def __getstate__(self): ...
    def __reduce_ex__(self, *args: Any, **kwargs: Any): ...
    @classmethod
    def set_getstate_hook(cls, hook_fn: Any) -> None: ...
    @classmethod
    def set_reduce_ex_hook(cls, hook_fn: Any) -> None: ...
    def batch(
        self,
        batch_size: int,
        drop_last: bool = ...,
        wrapper_class: type[DataChunk] = ...,
    ) -> MapDataPipe: ...
    def concat(self, *datapipes: MapDataPipe) -> MapDataPipe: ...
    def map(self, fn: Callable = ...) -> MapDataPipe: ...
    def shuffle(self, *, indices: list | None = ...) -> IterDataPipe: ...
    def zip(self, *datapipes: MapDataPipe[_T_co]) -> MapDataPipe: ...

class IterDataPipe(IterableDataset[_T_co], metaclass=_IterDataPipeMeta):
    functions: dict[str, Callable] = ...
    reduce_ex_hook: Callable | None = ...
    getstate_hook: Callable | None = ...
    str_hook: Callable | None = ...
    repr_hook: Callable | None = ...
    _number_of_samples_yielded: int = ...
    _snapshot_state: _SnapshotState = ...
    _fast_forward_iterator: Iterator | None = ...
    def __getattr__(self, attribute_name: Any): ...
    @classmethod
    def register_function(cls, function_name: Any, function: Any) -> None: ...
    @classmethod
    def register_datapipe_as_function(
        cls,
        function_name: Any,
        cls_to_register: Any,
        enable_df_api_tracing: bool = ...,
    ): ...
    def __getstate__(self): ...
    def __reduce_ex__(self, *args: Any, **kwargs: Any): ...
    @classmethod
    def set_getstate_hook(cls, hook_fn: Any) -> None: ...
    @classmethod
    def set_reduce_ex_hook(cls, hook_fn: Any) -> None: ...
    def batch(
        self,
        batch_size: int,
        drop_last: bool = ...,
        wrapper_class: type[DataChunk] = ...,
    ) -> IterDataPipe: ...
    def collate(
        self,
        conversion: Callable[..., Any] | dict[str | Any, Callable | Any] | None = ...,
        collate_fn: Callable | None = ...,
    ) -> IterDataPipe: ...
    def concat(self, *datapipes: IterDataPipe) -> IterDataPipe: ...
    def demux(
        self,
        num_instances: int,
        classifier_fn: Callable[[_T_co], int | None],
        drop_none: bool = ...,
        buffer_size: int = ...,
    ) -> list[IterDataPipe]: ...
    def filter(self, filter_fn: Callable, input_col=...) -> IterDataPipe: ...
    def fork(
        self,
        num_instances: int,
        buffer_size: int = ...,
        copy: Literal["shallow", "deep"] | None = ...,
    ) -> list[IterDataPipe]: ...
    def groupby(
        self,
        group_key_fn: Callable[[_T_co], Any],
        *,
        keep_key: bool = ...,
        buffer_size: int = ...,
        group_size: int | None = ...,
        guaranteed_group_size: int | None = ...,
        drop_remaining: bool = ...,
    ) -> IterDataPipe: ...
    def list_files(
        self,
        masks: str | list[str] = ...,
        *,
        recursive: bool = ...,
        abspath: bool = ...,
        non_deterministic: bool = ...,
        length: int = ...,
    ) -> IterDataPipe: ...
    def map(self, fn: Callable, input_col=..., output_col=...) -> IterDataPipe: ...
    def mux(self, *datapipes) -> IterDataPipe: ...
    def open_files(self, mode: str = ..., encoding: str | None = ..., length: int = ...) -> IterDataPipe: ...
    def read_from_stream(self, chunk: int | None = ...) -> IterDataPipe: ...
    def routed_decode(self, *handlers: Callable, key_fn: Callable = ...) -> IterDataPipe: ...
    def sharding_filter(self, sharding_group_filter=...) -> IterDataPipe: ...
    def shuffle(self, *, buffer_size: int = ..., unbatch_level: int = ...) -> IterDataPipe: ...
    def unbatch(self, unbatch_level: int = ...) -> IterDataPipe: ...
    def zip(self, *datapipes: IterDataPipe) -> IterDataPipe: ...

class DFIterDataPipe(IterDataPipe):
    def __iter__(self): ...

class _DataPipeSerializationWrapper:
    def __init__(self, datapipe) -> None: ...
    def __getstate__(self): ...
    def __setstate__(self, state): ...
    def __len__(self) -> int: ...

class _IterDataPipeSerializationWrapper(_DataPipeSerializationWrapper, IterDataPipe):
    def __iter__(self): ...

class _MapDataPipeSerializationWrapper(_DataPipeSerializationWrapper, MapDataPipe):
    def __getitem__(self, idx): ...
