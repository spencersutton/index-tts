import torch
from torch import Tensor, nn

"""
This type stub file was generated by pyright.
"""
__all__ = ["Tacotron2"]

class _LocationLayer(nn.Module):
    def __init__(self, attention_n_filter: int, attention_kernel_size: int, attention_hidden_dim: int) -> None: ...
    def forward(self, attention_weights_cat: Tensor) -> Tensor: ...

class _Attention(nn.Module):
    def __init__(
        self,
        attention_rnn_dim: int,
        encoder_embedding_dim: int,
        attention_hidden_dim: int,
        attention_location_n_filter: int,
        attention_location_kernel_size: int,
    ) -> None: ...
    def forward(
        self,
        attention_hidden_state: Tensor,
        memory: Tensor,
        processed_memory: Tensor,
        attention_weights_cat: Tensor,
        mask: Tensor,
    ) -> tuple[Tensor, Tensor]: ...

class _Prenet(nn.Module):
    def __init__(self, in_dim: int, out_sizes: list[int]) -> None: ...
    def forward(self, x: Tensor) -> Tensor: ...

class _Postnet(nn.Module):
    def __init__(
        self, n_mels: int, postnet_embedding_dim: int, postnet_kernel_size: int, postnet_n_convolution: int
    ) -> None: ...
    def forward(self, x: Tensor) -> Tensor: ...

class _Encoder(nn.Module):
    def __init__(self, encoder_embedding_dim: int, encoder_n_convolution: int, encoder_kernel_size: int) -> None: ...
    def forward(self, x: Tensor, input_lengths: Tensor) -> Tensor: ...

class _Decoder(nn.Module):
    def __init__(
        self,
        n_mels: int,
        n_frames_per_step: int,
        encoder_embedding_dim: int,
        decoder_rnn_dim: int,
        decoder_max_step: int,
        decoder_dropout: float,
        decoder_early_stopping: bool,
        attention_rnn_dim: int,
        attention_hidden_dim: int,
        attention_location_n_filter: int,
        attention_location_kernel_size: int,
        attention_dropout: float,
        prenet_dim: int,
        gate_threshold: float,
    ) -> None: ...
    def decode(
        self,
        decoder_input: Tensor,
        attention_hidden: Tensor,
        attention_cell: Tensor,
        decoder_hidden: Tensor,
        decoder_cell: Tensor,
        attention_weights: Tensor,
        attention_weights_cum: Tensor,
        attention_context: Tensor,
        memory: Tensor,
        processed_memory: Tensor,
        mask: Tensor,
    ) -> tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]: ...
    def forward(
        self, memory: Tensor, mel_specgram_truth: Tensor, memory_lengths: Tensor
    ) -> tuple[Tensor, Tensor, Tensor]: ...
    @torch.jit.export
    def infer(self, memory: Tensor, memory_lengths: Tensor) -> tuple[Tensor, Tensor, Tensor, Tensor]: ...

class Tacotron2(nn.Module):
    def __init__(
        self,
        mask_padding: bool = ...,
        n_mels: int = ...,
        n_symbol: int = ...,
        n_frames_per_step: int = ...,
        symbol_embedding_dim: int = ...,
        encoder_embedding_dim: int = ...,
        encoder_n_convolution: int = ...,
        encoder_kernel_size: int = ...,
        decoder_rnn_dim: int = ...,
        decoder_max_step: int = ...,
        decoder_dropout: float = ...,
        decoder_early_stopping: bool = ...,
        attention_rnn_dim: int = ...,
        attention_hidden_dim: int = ...,
        attention_location_n_filter: int = ...,
        attention_location_kernel_size: int = ...,
        attention_dropout: float = ...,
        prenet_dim: int = ...,
        postnet_n_convolution: int = ...,
        postnet_kernel_size: int = ...,
        postnet_embedding_dim: int = ...,
        gate_threshold: float = ...,
    ) -> None: ...
    def forward(
        self, tokens: Tensor, token_lengths: Tensor, mel_specgram: Tensor, mel_specgram_lengths: Tensor
    ) -> tuple[Tensor, Tensor, Tensor, Tensor]: ...
    @torch.jit.export
    def infer(self, tokens: Tensor, lengths: Tensor | None = ...) -> tuple[Tensor, Tensor, Tensor]: ...
