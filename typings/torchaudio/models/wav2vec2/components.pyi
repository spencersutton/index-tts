"""
This type stub file was generated by pyright.
"""

from typing import List, Optional, Tuple

import torch
from torch import Tensor, nn
from torch.nn import Module

_LG = ...

class LayerNorm(nn.LayerNorm):
    """Layer norm with transpose"""
    def forward(self, input: Tensor) -> Tensor: ...

class ConvLayerBlock(Module):
    """Convolution unit of FeatureExtractor"""
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int,
        stride: int,
        bias: bool,
        layer_norm: Module | None,
    ) -> None: ...
    def forward(self, x: Tensor, length: Tensor | None) -> tuple[Tensor, Tensor | None]:
        """
        Args:
            x (Tensor): Shape: ``[batch, in_channels, in_frame]``.
            length (Tensor or None, optional): Shape ``[batch, ]``.
        Returns:
            Tensor: Shape ``[batch, out_channels, out_frames]``.
            Optional[Tensor]: Shape ``[batch, ]``.
        """

class FeatureExtractor(Module):
    """Extract features from audio

    Args:
        conv_layers (nn.ModuleList):
            convolution layers
    """
    def __init__(self, conv_layers: nn.ModuleList) -> None: ...
    def forward(self, x: Tensor, length: Tensor | None) -> tuple[Tensor, Tensor | None]:
        """
        Args:
            x (Tensor):
                Input Tensor representing a batch of audio,
                shape: ``[batch, time]``.
            length (Tensor or None, optional):
                Valid length of each input sample. shape: ``[batch, ]``.

        Returns:
            Tensor:
                The resulting feature, shape: ``[batch, frame, feature]``
            Optional[Tensor]:
                Valid length of each output sample. shape: ``[batch, ]``.
        """

class FeatureProjection(Module):
    """Layer that connects FeatureExtractor and Encoder

    Projects features to encoder dimension.

    Args:
        in_features (int): Input feature dim.
        out_features (int): Output feature dim.
        dropout (float): Dropout probability.
    """
    def __init__(self, in_features: int, out_features: int, dropout: float) -> None: ...
    def forward(self, x):  # -> Any:
        """
        Args:
            x (Tensor):
                Feature Tensor. shape: ``[batch, frame, in_feature]``
        Returns:
            Tensor: Projected features. ``[batch, frame, out_feature]``.
        """

class ConvolutionalPositionalEmbedding(Module):
    """Positional embedding which is placed at the beginning of Transformer.

    Args:
        embed_dim (int): Feature dimension of the input Tensor.
        kernel_size (int): The number of frames to be use.
        groups (int): The number of groups in feature dimensions.
    """
    def __init__(self, embed_dim: int, kernel_size: int, groups: int) -> None: ...
    def __prepare_scriptable__(self):  # -> Self:
        ...
    def forward(self, x):  # -> Tensor:
        """
        Args:
            x (Tensor): shape ``[batch, frame, feature]``.

        Returns:
            Tensor: The resulting feature. Shape ``[batch, frame, feature]``.
        """

class SelfAttention(Module):
    """Multihead Self Attention module

    Args:
        embed_dim (int): Total dimension of the model.
        num_heads (int): The number of heads.
        dropout (float, optional):
            Dropout probability on attn_output_weights. Default: ``0.0``
    """
    def __init__(self, embed_dim: int, num_heads: int, dropout: float = ...) -> None: ...
    def forward(
        self,
        x: Tensor,
        attention_mask: Tensor | None = ...,
        position_bias: Tensor | None = ...,
        key_padding_mask: Tensor | None = ...,
    ) -> tuple[Tensor, Tensor | None]:
        """
        Args:
            x (Tensor): shape: ``[batch_size, sequence_length, embed_dim]``.
            attention_mask (Tensor or ``None``, optional):
                shape: ``[batch_size, 1, sequence_length, sequence_length]``
            position_bias: Not used. Only for the compatibility with :py:class:`WavLMSelfAttention`.
            key_padding_mask (Tensor or ``None``): Not used. Only for the compatibility with
                :py:class:`WavLMSelfAttention`.
        Returns:
            (Tensor, ``None``): The resulting attention output and ``None`` (necessary for compatibility
                with :py:class:`WavLMSelAttention`).
                Attention output shape: ``[batch, sequence_length, embed_dim]``.
        """

class FeedForward(Module):
    """Layer that follows attention layer in encoder layer."""
    def __init__(
        self, io_features: int, intermediate_features: int, intermediate_dropout: float, output_dropout: float
    ) -> None: ...
    def forward(self, x):  # -> Any:
        """
        Args:
            x (Tensor): shape: `(batch, sequence_length, io_features)`
        Returns:
            x (Tensor): shape: `(batch, sequence_length, io_features)`
        """

class EncoderLayer(Module):
    """A layer unit in encoder. Combines multihead self attention and feed forward."""
    def __init__(self, attention: Module, dropout: float, layer_norm_first: bool, feed_forward: Module) -> None: ...
    def forward(
        self,
        x: Tensor,
        attention_mask: Tensor | None = ...,
        position_bias: Tensor | None = ...,
        key_padding_mask: Tensor | None = ...,
    ) -> tuple[Tensor, Tensor | None]:
        """
        Args:
            x (Tensor): Input of shape ``(batch, sequence_length, embed_dim)``.
            attention_mask (Tensor or ``None``, optional): attention mask
                of shape ``(batch, 1, sequence_length, sequence_length)``. (Default: ``None``)
            position_bias (Tensor or ``None``, optional): position bias of shape
                ``(batch_size * num_heads, src_len, src_len)``.
                Only necessary for WavLM model, ``None`` otherwise. (Default: ``None``)
            key_padding_mask (Tensor or ``None``, optional): key padding mask of shape ``(batch_size, src_len)``.
                Only used for WavLM model, ignored otherwise. (Default: ``None``)
        Returns:
            (x, position_bias): Shapes are the same as in the input. Position bias is only relevant for WaLM model,
                ``None`` otherwise.
        """

class Transformer(Module):
    def __init__(
        self, pos_conv_embed: Module, dropout: float, layers: Module, layer_norm_first: bool, layer_drop: float
    ) -> None: ...
    def forward(self, x: Tensor, attention_mask: Tensor | None = ..., position_bias: Tensor | None = ...) -> Tensor: ...
    def get_intermediate_outputs(
        self, x: Tensor, attention_mask: Tensor | None = ..., num_layers: int | None = ...
    ) -> list[Tensor]: ...

class Encoder(Module):
    def __init__(self, feature_projection: Module, transformer: Module) -> None: ...
    def forward(self, features: Tensor, lengths: Tensor | None = ...) -> Tensor: ...
    def extract_features(
        self, features: Tensor, lengths: Tensor | None = ..., num_layers: int | None = ...
    ) -> list[Tensor]: ...

class MaskGenerator(Module):
    """Generate the masks for masked prediction.
    Args:
        encoder_embed_dim (int): The dimension of the transformer embedding output.
        mask_prob (float): Probability for each token to be chosen as start of the span to be masked.
            This will be multiplied by number of timesteps divided by length of mask span to mask
            approximately this percentage of all elements. However due to overlaps, the actual number
            will be smaller (unless no_overlap is True).
        mask_selection (str): How to choose the mask length.
            Options: [``static``, ``uniform``, ``normal``, ``poisson``].
        mask_other (float): Secondary mask argument (used for more complex distributions).
        mask_length (int): The lengths of the mask.
        no_mask_overlap (bool):  Whether to allow masks to overlap.
        mask_min_space (int):  Minimum space between spans (if no overlap is enabled).
        mask_channel_prob (float): The probability of replacing a feature with 0.
        mask_channel_selection (str): How to choose the mask length for channel masking.
            Options: [``static``, ``uniform``, ``normal``, ``poisson``].
        mask_channel_other (float): Secondary mask argument for channel masking(used for more complex distributions).
        mask_channel_length (int): Minimum space between spans (if no overlap is enabled) for channel masking.
        no_mask_channel_overlap (bool):  Whether to allow channel masks to overlap.
        mask_channel_min_space (int): Minimum space between spans for channel masking(if no overlap is enabled).
    """
    def __init__(
        self,
        encoder_embed_dim: int,
        mask_prob: float,
        mask_selection: str,
        mask_other: float,
        mask_length: int,
        no_mask_overlap: bool,
        mask_min_space: int,
        mask_channel_prob: float,
        mask_channel_selection: str,
        mask_channel_other: float,
        mask_channel_length: int,
        no_mask_channel_overlap: bool,
        mask_channel_min_space: int,
    ) -> None: ...
    def forward(self, x: Tensor, padding_mask: Tensor | None) -> Tensor:
        """
        Args:
            x (Tensor): The encoded representations after feature extraction module.
            padding_mask (Tensor or None): The padding mask of the same dimension as shape,
                which will prevent masking padded elements.

        Returns:
            Tensor: The feature representations after masking.
            Tensor: The generated mask indices.
        """

class LogitGenerator(Module):
    """Generate the logits of masked and unmasked inputs.
    Args:
        encoder_embed_dim (int): The dimension of the transformer embedding output.
        num_classes (int): The number of classes in the labels.
        final_dim (int): Project final representations and targets to `final_dim`.
        skip_masked (bool): If True, skip computing losses over masked frames.
        skip_nomask (bool): If True, skip computing losses over unmasked frames.
    """
    def __init__(
        self, encoder_embed_dim: int, num_classes: int, final_dim: int, skip_masked: bool, skip_nomask: bool
    ) -> None: ...
    def forward(self, x: Tensor, label: Tensor, mask_m: Tensor, mask_u: Tensor) -> tuple[Tensor, Tensor]:
        """
        Args:
            x (Tensor): The feature representation of the last transformer layer.
            label (Tensor): The label Tensor of dimension `[batch, frame]`.
            mask_m (Tensor): The masked indices of dimension `[batch, frame]`.
            mask_u (Tensor): The unmasked indices of dimension `[batch, frame]`.

        Returns:
            Tensor: The logits of masked frames. Tensor of dimension `[masked_frame, final_dim]`.
            Tensor: The logits of unmasked frames. Tensor of dimension `[unmasked_frame, final_dim]`.
        """

class GradMultiply(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, scale): ...
    @staticmethod
    def backward(ctx, grad):  # -> tuple[Any, None]:
        ...
