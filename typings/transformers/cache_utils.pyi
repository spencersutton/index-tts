import os
from abc import ABC, abstractmethod
from collections.abc import Callable, Iterable
from dataclasses import dataclass
from typing import Any

import torch

from .configuration_utils import PretrainedConfig
from .utils import is_hqq_available, is_torch_greater_or_equal

if is_hqq_available(): ...
logger = ...

class CacheLayerMixin(ABC):
    is_compileable = ...
    def __init__(self) -> None: ...
    @abstractmethod
    def update(
        self, key_states: torch.Tensor, value_states: torch.Tensor, cache_kwargs: dict[str, Any] | None = ...
    ) -> tuple[torch.Tensor, torch.Tensor]: ...
    @abstractmethod
    def get_seq_length(self, cache_position=...) -> int: ...
    @abstractmethod
    def get_max_cache_shape(self) -> int: ...
    @abstractmethod
    def get_mask_sizes(self, cache_position: torch.Tensor) -> tuple[int, int]: ...
    def reset(self) -> None: ...
    def reorder_cache(self, beam_idx: torch.LongTensor) -> tuple[torch.Tensor, torch.Tensor]: ...

class DynamicLayer(CacheLayerMixin):
    is_sliding = ...
    def update(
        self, key_states: torch.Tensor, value_states: torch.Tensor, cache_kwargs: dict[str, Any] | None = ...
    ) -> tuple[torch.Tensor, torch.Tensor]: ...
    def get_seq_length(self, cache_position=...) -> int: ...
    def get_max_cache_shape(self) -> int: ...
    def reorder_cache(self, beam_idx: torch.LongTensor) -> None: ...
    def crop(self, max_length: int) -> None: ...
    def batch_repeat_interleave(self, repeats: int) -> None: ...
    def batch_select_indices(self, indices: torch.Tensor) -> None: ...
    def get_mask_sizes(self, cache_position: torch.Tensor) -> tuple[int, int]: ...
    @classmethod
    def from_tensors(cls, keys: torch.Tensor, values: torch.Tensor) -> DynamicLayer: ...

class StaticLayer(CacheLayerMixin):
    is_compileable = ...
    is_sliding = ...
    def __init__(
        self,
        max_cache_len: int,
        batch_size: int,
        num_heads: int,
        head_dim: int,
        dtype: torch.dtype = ...,
        device: str = ...,
        sliding_window: int | None = ...,
    ) -> None: ...
    def get_max_cache_shape(self) -> int: ...
    def update(
        self, key_states: torch.Tensor, value_states: torch.Tensor, cache_kwargs: dict[str, Any] | None = ...
    ) -> tuple[torch.Tensor, torch.Tensor]: ...
    def get_seq_length(self, cache_position=...) -> int: ...
    def reorder_cache(self, beam_idx: torch.LongTensor) -> None: ...
    def get_mask_sizes(self, cache_position: torch.Tensor) -> tuple[int, int]: ...

class SlidingWindowLayer(StaticLayer):
    is_sliding = ...
    def __init__(self, sliding_window, *args, **kwargs) -> None: ...
    def update(
        self, key_states: torch.Tensor, value_states: torch.Tensor, cache_kwargs: dict[str, Any] | None = ...
    ) -> tuple[torch.Tensor, torch.Tensor]: ...
    def get_mask_sizes(self, cache_position: torch.Tensor) -> tuple[int, int]: ...

class ChunkedSlidingLayer(SlidingWindowLayer):
    def __init__(self, *args, **kwargs) -> None: ...
    def update(
        self, key_states: torch.Tensor, value_states: torch.Tensor, cache_kwargs: dict[str, Any] | None = ...
    ) -> tuple[torch.Tensor, torch.Tensor]: ...
    def reset(self) -> None: ...
    def get_mask_sizes(self, cache_position: torch.Tensor) -> tuple[int, int]: ...

class CacheProcessor:
    def __init__(self, cache: Cache, **kwargs) -> None: ...
    def pre_update(
        self,
        cache: Cache,
        key_states: torch.Tensor,
        value_states: torch.Tensor,
        layer_idx: int,
        cache_kwargs: dict[str, Any] | None = ...,
    ) -> tuple[torch.Tensor, torch.Tensor]: ...
    def post_update(
        self,
        cache: Cache,
        key_tensors: torch.Tensor,
        value_tensors: torch.Tensor,
        layer_idx: int,
        cache_kwargs: dict[str, Any] | None = ...,
    ) -> tuple[torch.Tensor, torch.Tensor]: ...

class OffloadedCacheProcessor(CacheProcessor):
    def __init__(self, cache: Cache, offload_device: str | torch.device = ..., **kwargs) -> None: ...
    def pre_update(
        self,
        cache: Cache,
        key_states: torch.Tensor,
        value_states: torch.Tensor,
        layer_idx: int,
        cache_kwargs: dict[str, Any] | None = ...,
    ) -> tuple[torch.Tensor, torch.Tensor]: ...

class QuantizedCacheProcessor(CacheProcessor):
    def __init__(
        self,
        cache: Cache,
        backend: str = ...,
        nbits: int = ...,
        axis_key: int = ...,
        axis_value: int = ...,
        q_group_size: int = ...,
        residual_length: int = ...,
        compute_dtype: torch.dtype = ...,
        device: str = ...,
    ) -> None: ...
    def validate(self):  # -> None:

        ...
    def post_update(
        self,
        cache: Cache,
        key_tensors: torch.Tensor,
        value_tensors: torch.Tensor,
        layer_idx: int,
        cache_kwargs: dict[str, Any] | None = ...,
    ) -> tuple[torch.Tensor, torch.Tensor]: ...

class QuantoQuantizedCacheProcessor(QuantizedCacheProcessor):
    def __init__(
        self,
        cache: Cache,
        backend: str = ...,
        nbits: int = ...,
        axis_key: int = ...,
        axis_value: int = ...,
        q_group_size: int = ...,
        residual_length: int = ...,
        compute_dtype: torch.dtype = ...,
        device: str = ...,
    ) -> None: ...

class HQQQuantizedCacheProcessor(QuantizedCacheProcessor):
    def __init__(
        self,
        cache: Cache,
        backend: str = ...,
        nbits: int = ...,
        axis_key: int = ...,
        axis_value: int = ...,
        q_group_size: int = ...,
        residual_length: int = ...,
        compute_dtype: torch.dtype = ...,
        device: str = ...,
    ) -> None: ...

def apply_processors(
    fn: Callable[..., tuple[torch.Tensor, torch.Tensor]],
) -> Callable[..., tuple[torch.Tensor, torch.Tensor]]: ...

class KeyValuesWrapper:
    def __init__(self, layers, cache_type=...) -> None: ...
    def __getitem__(self, idx):  # -> list[Any] | Any:
        ...
    def __setitem__(self, idx, value) -> None:  # -> None:
        ...
    def __len__(self) -> int:  # -> int:
        ...
    def __iter__(self):  # -> Generator[Any, Any, None]:
        ...
    def __bool__(self) -> bool:  # -> bool:
        ...

class Cache:
    def __init__(
        self,
        layer_classes: list[type[CacheLayerMixin]] | type[CacheLayerMixin],
        config: PretrainedConfig | None = ...,
        cache_processor: str | type[CacheProcessor] | None = ...,
        max_batch_size: int | None = ...,
        max_cache_len: int | None = ...,
        device: torch.device | str | None = ...,
        dtype: torch.dtype | None = ...,
        layer_device_map: dict[int, torch.device] | None = ...,
        tp_size: int | None = ...,
        **kwargs,
    ) -> None: ...
    def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor]: ...
    def __iter__(self):  # -> Generator[tuple[Any | None, Any | None], Any, None]:

        ...
    def __len__(self) -> int:  # -> int:

        ...
    def append_new_layers(self, layer_idx: int) -> None: ...
    @apply_processors
    def update(
        self,
        key_states: torch.Tensor,
        value_states: torch.Tensor,
        layer_idx: int,
        cache_kwargs: dict[str, Any] | None = ...,
    ) -> tuple[torch.Tensor, torch.Tensor]: ...
    def get_seq_length(self, layer_idx: int = ..., cache_position=...) -> int: ...
    def get_mask_sizes(self, cache_position: torch.Tensor, layer_idx: int) -> tuple[int, int]: ...
    @property
    def key_cache(self) -> KeyValuesWrapper: ...
    @property
    def value_cache(self) -> KeyValuesWrapper: ...
    def get_max_cache_shape(self, layer_idx: int = ...) -> int: ...
    def reset(self):  # -> None:

        ...
    def reorder_cache(self, beam_idx: torch.LongTensor):  # -> None:

        ...
    def crop(self, max_length: int):  # -> None:

        ...
    def batch_repeat_interleave(self, repeats: int):  # -> None:

        ...
    def batch_select_indices(self, indices: torch.Tensor):  # -> None:

        ...
    @property
    def max_batch_size(self) -> int: ...
    @property
    def max_cache_len(self) -> int: ...
    @property
    def is_compileable(self) -> bool: ...
    @property
    def is_sliding(self) -> list[bool]: ...

class DynamicCache(Cache):
    def __init__(
        self, ddp_cache_data: Iterable[tuple[torch.Tensor, torch.Tensor]] | None = ..., *args, **kwargs
    ) -> None: ...
    def to_legacy_cache(self) -> tuple[tuple[torch.Tensor, torch.Tensor], ...]: ...
    @classmethod
    def from_legacy_cache(cls, past_key_values: tuple[tuple[torch.FloatTensor, torch.FloatTensor], ...]) -> Cache: ...

if is_torch_greater_or_equal("2.3"): ...

class OffloadedCache(DynamicCache):
    def __init__(self) -> None: ...

class StaticCache(Cache):
    def __init__(self, *args, **kwargs) -> None: ...

class OffloadedStaticCache(StaticCache):
    def __init__(self, *args, **kwargs) -> None: ...

class SlidingWindowCache(Cache):
    def __init__(self, *args, **kwargs) -> None: ...

class HybridCache(Cache):
    def __init__(self, config: PretrainedConfig, *args, **kwargs) -> None: ...

class HybridChunkedCache(HybridCache): ...

class OffloadedHybridCache(HybridChunkedCache):
    def __init__(self, *args, **kwargs) -> None: ...

class QuantizedCache(DynamicCache):
    def __init__(self, backend, **kwargs) -> None: ...

class QuantoQuantizedCache(QuantizedCache):
    def __init__(self, **kwargs) -> None: ...

class HQQQuantizedCache(QuantizedCache):
    def __init__(self, backend=..., **kwargs) -> None: ...

class EncoderDecoderCache(Cache):
    is_compileable = ...
    def __init__(self, self_attention_cache: Cache, cross_attention_cache: Cache) -> None: ...
    def __iter__(self):  # -> Generator[tuple[Any | None, Any | None, Any | None, Any | None], Any, None]:

        ...
    def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]: ...
    def __len__(self) -> int:  # -> int:

        ...
    def to_legacy_cache(self) -> tuple[tuple[torch.Tensor]]: ...
    @classmethod
    def from_legacy_cache(
        cls, past_key_values: tuple[tuple[torch.FloatTensor, torch.FloatTensor], ...]
    ) -> EncoderDecoderCache: ...
    def get_seq_length(self, layer_idx: int | None = ..., cache_position=...) -> int: ...
    def reset(self):  # -> None:
        ...
    def reorder_cache(self, beam_idx: torch.LongTensor):  # -> None:

        ...
    def check_dynamic_cache(self, method: str):  # -> None:
        ...
    def crop(self, maximum_length: int):  # -> None:

        ...
    def batch_split(self, full_batch_size: int, split_size: int) -> list[EncoderDecoderCache]: ...
    def batch_repeat_interleave(self, repeats: int):  # -> None:

        ...
    def batch_select_indices(self, indices: torch.Tensor):  # -> None:

        ...
    def get_max_cache_shape(self) -> int: ...
    def get_mask_sizes(self, cache_position: torch.Tensor, layer_idx: int) -> tuple[int, int]: ...

def parse_processor_args(processor_class: type[CacheProcessor] | None, kwargs: dict) -> tuple[dict, dict]: ...
def parse_layer_args_from_model_config(
    config: PretrainedConfig | None,
    batch_size: int | None = ...,
    max_cache_len: int | None = ...,
    device: torch.device | str | None = ...,
    dtype: torch.dtype | None = ...,
    layer_device_map: dict[int, torch.device] | None = ...,
    tp_size: int | None = ...,
    max_batch_size: int | None = ...,
) -> dict: ...

LAYER_CLASS_MAP: dict[str, type[CacheLayerMixin]] = ...
PROCESSOR_CLASS_MAP: dict[str, type[CacheProcessor]] = ...

class SinkCache(Cache):
    def __init__(self, **kwargs) -> None: ...

@dataclass
class CacheConfig:
    cache_implementation: None
    def __post_init__(self):  # -> None:
        ...
    @classmethod
    def from_dict(cls, config_dict, **kwargs):  # -> Self:

        ...
    def to_json_file(self, json_file_path: str | os.PathLike):  # -> None:

        ...
    def to_dict(self) -> dict[str, Any]: ...
    def __iter__(self):  # -> Generator[tuple[str, Any], Any, None]:

        ...
    def to_json_string(self):  # -> str:

        ...
    def update(self, **kwargs):  # -> dict[str, Any]:

        ...

@dataclass
class QuantizedCacheConfig(CacheConfig):
    def __init__(
        self,
        backend: str = ...,
        nbits: int | None = ...,
        axis_key: int | None = ...,
        axis_value: int | None = ...,
        q_group_size: int | None = ...,
        residual_length: int | None = ...,
        compute_dtype: torch.dtype | None = ...,
        device: str | None = ...,
    ) -> None: ...
    def validate(self):  # -> None:

        ...

@dataclass
class StaticCacheConfig(CacheConfig):
    cache_implementation = ...
    def __init__(self, batch_size: int, max_cache_len: int, device=...) -> None: ...
    def initialise_cache_layer(self, layer_idx, key_states):  # -> None:

        ...

def __getattr__(name: str) -> Any: ...
