"""
This type stub file was generated by pyright.
"""

import os
import torch
from abc import ABC, abstractmethod
from collections.abc import Iterable
from dataclasses import dataclass
from typing import Any, Callable, Optional, Union
from .configuration_utils import PretrainedConfig
from .utils import is_hqq_available, is_torch_greater_or_equal

if is_hqq_available(): ...
logger = ...

class CacheLayerMixin(ABC):
    is_compileable = ...
    def __init__(self) -> None: ...
    @abstractmethod
    def update(
        self, key_states: torch.Tensor, value_states: torch.Tensor, cache_kwargs: Optional[dict[str, Any]] = ...
    ) -> tuple[torch.Tensor, torch.Tensor]: ...
    @abstractmethod
    def get_seq_length(self, cache_position=...) -> int: ...
    @abstractmethod
    def get_max_cache_shape(self) -> int: ...
    @abstractmethod
    def get_mask_sizes(self, cache_position: torch.Tensor) -> tuple[int, int]: ...
    def reset(self) -> None: ...
    def reorder_cache(self, beam_idx: torch.LongTensor) -> tuple[torch.Tensor, torch.Tensor]: ...

class DynamicLayer(CacheLayerMixin):
    is_sliding = ...
    def update(
        self, key_states: torch.Tensor, value_states: torch.Tensor, cache_kwargs: Optional[dict[str, Any]] = ...
    ) -> tuple[torch.Tensor, torch.Tensor]: ...
    def get_seq_length(self, cache_position=...) -> int: ...
    def get_max_cache_shape(self) -> int: ...
    def reorder_cache(self, beam_idx: torch.LongTensor) -> None: ...
    def crop(self, max_length: int) -> None: ...
    def batch_repeat_interleave(self, repeats: int) -> None: ...
    def batch_select_indices(self, indices: torch.Tensor) -> None: ...
    def get_mask_sizes(self, cache_position: torch.Tensor) -> tuple[int, int]: ...
    @classmethod
    def from_tensors(cls, keys: torch.Tensor, values: torch.Tensor) -> DynamicLayer: ...

class StaticLayer(CacheLayerMixin):
    is_compileable = ...
    is_sliding = ...
    def __init__(
        self,
        max_cache_len: int,
        batch_size: int,
        num_heads: int,
        head_dim: int,
        dtype: torch.dtype = ...,
        device: str = ...,
        sliding_window: Optional[int] = ...,
    ) -> None: ...
    def get_max_cache_shape(self) -> int: ...
    def update(
        self, key_states: torch.Tensor, value_states: torch.Tensor, cache_kwargs: Optional[dict[str, Any]] = ...
    ) -> tuple[torch.Tensor, torch.Tensor]: ...
    def get_seq_length(self, cache_position=...) -> int: ...
    def reorder_cache(self, beam_idx: torch.LongTensor) -> None: ...
    def get_mask_sizes(self, cache_position: torch.Tensor) -> tuple[int, int]: ...

class SlidingWindowLayer(StaticLayer):
    is_sliding = ...
    def __init__(self, sliding_window, *args, **kwargs) -> None: ...
    def update(
        self, key_states: torch.Tensor, value_states: torch.Tensor, cache_kwargs: Optional[dict[str, Any]] = ...
    ) -> tuple[torch.Tensor, torch.Tensor]: ...
    def get_mask_sizes(self, cache_position: torch.Tensor) -> tuple[int, int]: ...

class ChunkedSlidingLayer(SlidingWindowLayer):
    def __init__(self, *args, **kwargs) -> None: ...
    def update(
        self, key_states: torch.Tensor, value_states: torch.Tensor, cache_kwargs: Optional[dict[str, Any]] = ...
    ) -> tuple[torch.Tensor, torch.Tensor]: ...
    def reset(self) -> None: ...
    def get_mask_sizes(self, cache_position: torch.Tensor) -> tuple[int, int]: ...

class CacheProcessor:
    def __init__(self, cache: Cache, **kwargs) -> None: ...
    def pre_update(
        self,
        cache: Cache,
        key_states: torch.Tensor,
        value_states: torch.Tensor,
        layer_idx: int,
        cache_kwargs: Optional[dict[str, Any]] = ...,
    ) -> tuple[torch.Tensor, torch.Tensor]: ...
    def post_update(
        self,
        cache: Cache,
        key_tensors: torch.Tensor,
        value_tensors: torch.Tensor,
        layer_idx: int,
        cache_kwargs: Optional[dict[str, Any]] = ...,
    ) -> tuple[torch.Tensor, torch.Tensor]: ...

class OffloadedCacheProcessor(CacheProcessor):
    def __init__(self, cache: Cache, offload_device: Union[str, torch.device] = ..., **kwargs) -> None: ...
    def pre_update(
        self,
        cache: Cache,
        key_states: torch.Tensor,
        value_states: torch.Tensor,
        layer_idx: int,
        cache_kwargs: Optional[dict[str, Any]] = ...,
    ) -> tuple[torch.Tensor, torch.Tensor]: ...

class QuantizedCacheProcessor(CacheProcessor):
    def __init__(
        self,
        cache: Cache,
        backend: str = ...,
        nbits: int = ...,
        axis_key: int = ...,
        axis_value: int = ...,
        q_group_size: int = ...,
        residual_length: int = ...,
        compute_dtype: torch.dtype = ...,
        device: str = ...,
    ) -> None: ...
    def validate(self): ...
    def post_update(
        self,
        cache: Cache,
        key_tensors: torch.Tensor,
        value_tensors: torch.Tensor,
        layer_idx: int,
        cache_kwargs: Optional[dict[str, Any]] = ...,
    ) -> tuple[torch.Tensor, torch.Tensor]: ...

class QuantoQuantizedCacheProcessor(QuantizedCacheProcessor):
    def __init__(
        self,
        cache: Cache,
        backend: str = ...,
        nbits: int = ...,
        axis_key: int = ...,
        axis_value: int = ...,
        q_group_size: int = ...,
        residual_length: int = ...,
        compute_dtype: torch.dtype = ...,
        device: str = ...,
    ) -> None: ...

class HQQQuantizedCacheProcessor(QuantizedCacheProcessor):
    def __init__(
        self,
        cache: Cache,
        backend: str = ...,
        nbits: int = ...,
        axis_key: int = ...,
        axis_value: int = ...,
        q_group_size: int = ...,
        residual_length: int = ...,
        compute_dtype: torch.dtype = ...,
        device: str = ...,
    ) -> None: ...

def apply_processors(
    fn: Callable[..., tuple[torch.Tensor, torch.Tensor]],
) -> Callable[..., tuple[torch.Tensor, torch.Tensor]]: ...

class KeyValuesWrapper:
    def __init__(self, layers, cache_type=...) -> None: ...
    def __getitem__(self, idx): ...
    def __setitem__(self, idx, value): ...
    def __len__(self): ...
    def __iter__(self): ...
    def __bool__(self): ...

class Cache:
    def __init__(
        self,
        layer_classes: Union[list[type[CacheLayerMixin]], type[CacheLayerMixin]],
        config: Optional[PretrainedConfig] = ...,
        cache_processor: Optional[Union[str, type[CacheProcessor]]] = ...,
        max_batch_size: Optional[int] = ...,
        max_cache_len: Optional[int] = ...,
        device: Union[torch.device, str, None] = ...,
        dtype: Optional[torch.dtype] = ...,
        layer_device_map: Optional[dict[int, torch.device]] = ...,
        tp_size: Optional[int] = ...,
        **kwargs,
    ) -> None: ...
    def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor]: ...
    def __iter__(self): ...
    def __len__(self): ...
    def __repr__(self): ...
    def append_new_layers(self, layer_idx: int) -> None: ...
    @apply_processors
    def update(
        self,
        key_states: torch.Tensor,
        value_states: torch.Tensor,
        layer_idx: int,
        cache_kwargs: Optional[dict[str, Any]] = ...,
    ) -> tuple[torch.Tensor, torch.Tensor]: ...
    def get_seq_length(self, layer_idx: int = ..., cache_position=...) -> int: ...
    def get_mask_sizes(self, cache_position: torch.Tensor, layer_idx: int) -> tuple[int, int]: ...
    @property
    def key_cache(self) -> KeyValuesWrapper: ...
    @property
    def value_cache(self) -> KeyValuesWrapper: ...
    def get_max_cache_shape(self, layer_idx: int = ...) -> int: ...
    def reset(self): ...
    def reorder_cache(self, beam_idx: torch.LongTensor): ...
    def crop(self, max_length: int): ...
    def batch_repeat_interleave(self, repeats: int): ...
    def batch_select_indices(self, indices: torch.Tensor): ...
    @property
    def max_batch_size(self) -> int: ...
    @property
    def max_cache_len(self) -> int: ...
    @property
    def is_compileable(self) -> bool: ...
    @property
    def is_sliding(self) -> list[bool]: ...

class DynamicCache(Cache):
    def __init__(
        self, ddp_cache_data: Optional[Iterable[tuple[torch.Tensor, torch.Tensor]]] = ..., *args, **kwargs
    ) -> None: ...
    def to_legacy_cache(self) -> tuple[tuple[torch.Tensor, torch.Tensor], ...]: ...
    @classmethod
    def from_legacy_cache(cls, past_key_values: tuple[tuple[torch.FloatTensor, torch.FloatTensor], ...]) -> Cache: ...

if is_torch_greater_or_equal("2.3"): ...

class OffloadedCache(DynamicCache):
    def __init__(self) -> None: ...

class StaticCache(Cache):
    def __init__(self, *args, **kwargs) -> None: ...

class OffloadedStaticCache(StaticCache):
    def __init__(self, *args, **kwargs) -> None: ...

class SlidingWindowCache(Cache):
    def __init__(self, *args, **kwargs) -> None: ...

class HybridCache(Cache):
    def __init__(self, config: PretrainedConfig, *args, **kwargs) -> None: ...

class HybridChunkedCache(HybridCache): ...

class OffloadedHybridCache(HybridChunkedCache):
    def __init__(self, *args, **kwargs) -> None: ...

class QuantizedCache(DynamicCache):
    def __init__(self, backend, **kwargs) -> None: ...

class QuantoQuantizedCache(QuantizedCache):
    def __init__(self, **kwargs) -> None: ...

class HQQQuantizedCache(QuantizedCache):
    def __init__(self, backend=..., **kwargs) -> None: ...

class EncoderDecoderCache(Cache):
    is_compileable = ...
    def __init__(self, self_attention_cache: Cache, cross_attention_cache: Cache) -> None: ...
    def __iter__(self): ...
    def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]: ...
    def __len__(self): ...
    def to_legacy_cache(self) -> tuple[tuple[torch.Tensor]]: ...
    @classmethod
    def from_legacy_cache(
        cls, past_key_values: tuple[tuple[torch.FloatTensor, torch.FloatTensor], ...]
    ) -> EncoderDecoderCache: ...
    def get_seq_length(self, layer_idx: Optional[int] = ..., cache_position=...) -> int: ...
    def reset(self): ...
    def reorder_cache(self, beam_idx: torch.LongTensor): ...
    def check_dynamic_cache(self, method: str): ...
    def crop(self, maximum_length: int): ...
    def batch_split(self, full_batch_size: int, split_size: int) -> list[EncoderDecoderCache]: ...
    def batch_repeat_interleave(self, repeats: int): ...
    def batch_select_indices(self, indices: torch.Tensor): ...
    def get_max_cache_shape(self) -> int: ...
    def get_mask_sizes(self, cache_position: torch.Tensor, layer_idx: int) -> tuple[int, int]: ...

def parse_processor_args(processor_class: Optional[type[CacheProcessor]], kwargs: dict) -> tuple[dict, dict]: ...
def parse_layer_args_from_model_config(
    config: Optional[PretrainedConfig],
    batch_size: Optional[int] = ...,
    max_cache_len: Optional[int] = ...,
    device: Union[torch.device, str, None] = ...,
    dtype: Optional[torch.dtype] = ...,
    layer_device_map: Optional[dict[int, torch.device]] = ...,
    tp_size: Optional[int] = ...,
    max_batch_size: Optional[int] = ...,
) -> dict: ...

LAYER_CLASS_MAP: dict[str, type[CacheLayerMixin]] = ...
PROCESSOR_CLASS_MAP: dict[str, type[CacheProcessor]] = ...

class SinkCache(Cache):
    def __init__(self, **kwargs) -> None: ...

@dataclass
class CacheConfig:
    cache_implementation: None
    def __post_init__(self): ...
    @classmethod
    def from_dict(cls, config_dict, **kwargs): ...
    def to_json_file(self, json_file_path: Union[str, os.PathLike]): ...
    def to_dict(self) -> dict[str, Any]: ...
    def __iter__(self): ...
    def __repr__(self): ...
    def to_json_string(self): ...
    def update(self, **kwargs): ...

@dataclass
class QuantizedCacheConfig(CacheConfig):
    def __init__(
        self,
        backend: str = ...,
        nbits: Optional[int] = ...,
        axis_key: Optional[int] = ...,
        axis_value: Optional[int] = ...,
        q_group_size: Optional[int] = ...,
        residual_length: Optional[int] = ...,
        compute_dtype: Optional[torch.dtype] = ...,
        device: Optional[str] = ...,
    ) -> None: ...
    def validate(self): ...

@dataclass
class StaticCacheConfig(CacheConfig):
    cache_implementation = ...
    def __init__(self, batch_size: int, max_cache_len: int, device=...) -> None: ...
    def initialise_cache_layer(self, layer_idx, key_states): ...

def __getattr__(name: str) -> Any: ...
