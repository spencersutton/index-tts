"""
This type stub file was generated by pyright.
"""

from typing import Optional
from tokenizers import Tokenizer

logger = ...

def import_protobuf(error_message=...): ...
def generate_merges(vocab, vocab_scores): ...

class SentencePieceExtractor:
    def __init__(self, model: str) -> None: ...
    def extract(self, vocab_scores=...) -> tuple[dict[str, int], list[tuple]]: ...

class GemmaSentencePieceExtractor(SentencePieceExtractor):
    def extract(self, vocab_scores=...) -> tuple[dict[str, int], list[tuple]]: ...

def check_number_comma(piece: str) -> bool: ...

class Converter:
    def __init__(self, original_tokenizer) -> None: ...
    def converted(self) -> Tokenizer: ...

class BertConverter(Converter):
    def converted(self) -> Tokenizer: ...

class SplinterConverter(Converter):
    def converted(self) -> Tokenizer: ...

class FunnelConverter(Converter):
    def converted(self) -> Tokenizer: ...

class MPNetConverter(Converter):
    def converted(self) -> Tokenizer: ...

class OpenAIGPTConverter(Converter):
    def converted(self) -> Tokenizer: ...

class GPT2Converter(Converter):
    def converted(
        self, vocab: Optional[dict[str, int]] = ..., merges: Optional[list[tuple[str, str]]] = ...
    ) -> Tokenizer: ...

class HerbertConverter(Converter):
    def converted(self) -> Tokenizer: ...

class Qwen2Converter(Converter):
    def converted(
        self, vocab: Optional[dict[str, int]] = ..., merges: Optional[list[tuple[str, str]]] = ...
    ) -> Tokenizer: ...

class RobertaConverter(Converter):
    def converted(self) -> Tokenizer: ...

class RoFormerConverter(Converter):
    def converted(self) -> Tokenizer: ...

class DebertaConverter(Converter):
    def converted(self) -> Tokenizer: ...

class SpmConverter(Converter):
    handle_byte_fallback = ...
    SpmExtractor = SentencePieceExtractor
    special_tokens = ...
    def __init__(self, *args) -> None: ...
    def vocab(self, proto): ...
    def unk_id(self, proto): ...
    def tokenizer(self, proto): ...
    def normalizer(self, proto): ...
    def pre_tokenizer(self, replacement, add_prefix_space): ...
    def post_processor(self): ...
    def decoder(self, replacement, add_prefix_space): ...
    def converted(self) -> Tokenizer: ...

class AlbertConverter(SpmConverter):
    def vocab(self, proto): ...
    def normalizer(self, proto): ...
    def post_processor(self): ...

class BarthezConverter(SpmConverter):
    def unk_id(self, proto): ...
    def post_processor(self): ...

class CamembertConverter(SpmConverter):
    def vocab(self, proto): ...
    def unk_id(self, proto): ...
    def post_processor(self): ...

class DebertaV2Converter(SpmConverter):
    def pre_tokenizer(self, replacement, add_prefix_space): ...
    def normalizer(self, proto): ...
    def post_processor(self): ...

class MBartConverter(SpmConverter):
    def vocab(self, proto): ...
    def unk_id(self, proto): ...
    def post_processor(self): ...

class MBart50Converter(SpmConverter):
    def vocab(self, proto): ...
    def unk_id(self, proto): ...
    def post_processor(self): ...

class NllbConverter(SpmConverter):
    def vocab(self, proto): ...
    def unk_id(self, proto): ...
    def post_processor(self): ...

class SeamlessM4TConverter(SpmConverter):
    def vocab(self, proto): ...
    def unk_id(self, proto): ...
    def post_processor(self): ...

class XLMRobertaConverter(SpmConverter):
    def vocab(self, proto): ...
    def unk_id(self, proto): ...
    def post_processor(self): ...

class XLNetConverter(SpmConverter):
    def vocab(self, proto): ...
    def normalizer(self, proto): ...
    def post_processor(self): ...

class ReformerConverter(SpmConverter): ...

class RemBertConverter(SpmConverter):
    def normalizer(self, proto): ...
    def post_processor(self): ...

class BertGenerationConverter(SpmConverter): ...

class PegasusConverter(SpmConverter):
    def vocab(self, proto): ...
    def unk_id(self, proto): ...
    def pre_tokenizer(self, replacement, add_prefix_space): ...
    def post_processor(self): ...

class T5Converter(SpmConverter):
    def vocab(self, proto): ...
    def post_processor(self): ...

class UdopConverter(SpmConverter):
    def post_processor(self): ...

class WhisperConverter(Converter):
    def converted(self) -> Tokenizer: ...

class BigBirdConverter(SpmConverter):
    def post_processor(self): ...

class CLIPConverter(Converter):
    def converted(self) -> Tokenizer: ...

class LayoutLMv2Converter(Converter):
    def converted(self) -> Tokenizer: ...

class BlenderbotConverter(Converter):
    def converted(self) -> Tokenizer: ...

class XGLMConverter(SpmConverter):
    def vocab(self, proto): ...
    def unk_id(self, proto): ...
    def post_processor(self): ...

class GemmaConverter(SpmConverter):
    handle_byte_fallback = ...
    SpmExtractor = GemmaSentencePieceExtractor
    special_tokens = ...
    def normalizer(self, proto): ...
    def vocab(self, proto): ...
    def pre_tokenizer(self, replacement, add_prefix_space): ...
    def unk_id(self, proto): ...
    def decoder(self, replacement, add_prefix_space): ...

class LlamaConverter(SpmConverter):
    handle_byte_fallback = ...
    def vocab(self, proto): ...
    def unk_id(self, proto): ...
    def decoder(self, replacement, add_prefix_space): ...
    def normalizer(self, proto): ...
    def pre_tokenizer(self, replacement, add_prefix_space): ...
    def post_processor(self): ...

class MarkupLMConverter(Converter):
    def converted(self) -> Tokenizer: ...

class MoshiConverter(SpmConverter):
    handle_byte_fallback = ...
    def __init__(self, vocab_file, model_max_length=..., **kwargs) -> None: ...
    def normalizer(self, proto): ...
    def decoder(self, replacement, add_prefix_space): ...
    def pre_tokenizer(self, replacement, add_prefix_space): ...

class HeliumConverter(SpmConverter):
    handle_byte_fallback = ...
    def __init__(self, vocab_file=..., *args) -> None: ...
    def tokenizer(self, proto): ...
    def vocab(self, proto): ...
    def unk_id(self, proto): ...
    def decoder(self, replacement, add_prefix_space): ...
    def normalizer(self, proto): ...
    def pre_tokenizer(self, replacement, add_prefix_space): ...
    def post_processor(self): ...

def bytes_to_unicode(): ...

class TikTokenConverter:
    def __init__(
        self, vocab_file=..., pattern=..., add_prefix_space=..., additional_special_tokens=..., *args, **kwargs
    ) -> None: ...
    def extract_vocab_merges_from_model(self, tiktoken_url: str): ...
    def tokenizer(self): ...
    def converted(self) -> Tokenizer: ...

SLOW_TO_FAST_CONVERTERS = ...

def convert_slow_tokenizer(transformer_tokenizer, from_tiktoken=...) -> Tokenizer: ...
