from tokenizers import Tokenizer

"""
Utilities to convert slow tokenizers in their fast tokenizers counterparts.

All the conversions are grouped here to gather SentencePiece dependencies outside of the fast tokenizers files and
allow to make our dependency on SentencePiece optional.
"""
logger = ...

def import_protobuf(error_message=...):  # -> Any:
    ...
def generate_merges(vocab, vocab_scores):  # -> list[tuple[Any, Any]]:
    ...

class SentencePieceExtractor:
    def __init__(self, model: str) -> None: ...
    def extract(self, vocab_scores=...) -> tuple[dict[str, int], list[tuple]]: ...

class GemmaSentencePieceExtractor(SentencePieceExtractor):
    def extract(self, vocab_scores=...) -> tuple[dict[str, int], list[tuple]]: ...

def check_number_comma(piece: str) -> bool: ...

class Converter:
    def __init__(self, original_tokenizer) -> None: ...
    def converted(self) -> Tokenizer: ...

class BertConverter(Converter):
    def converted(self) -> Tokenizer: ...

class SplinterConverter(Converter):
    def converted(self) -> Tokenizer: ...

class FunnelConverter(Converter):
    def converted(self) -> Tokenizer: ...

class MPNetConverter(Converter):
    def converted(self) -> Tokenizer: ...

class OpenAIGPTConverter(Converter):
    def converted(self) -> Tokenizer: ...

class GPT2Converter(Converter):
    def converted(
        self, vocab: dict[str, int] | None = ..., merges: list[tuple[str, str]] | None = ...
    ) -> Tokenizer: ...

class HerbertConverter(Converter):
    def converted(self) -> Tokenizer: ...

class Qwen2Converter(Converter):
    def converted(
        self, vocab: dict[str, int] | None = ..., merges: list[tuple[str, str]] | None = ...
    ) -> Tokenizer: ...

class RobertaConverter(Converter):
    def converted(self) -> Tokenizer: ...

class RoFormerConverter(Converter):
    def converted(self) -> Tokenizer: ...

class DebertaConverter(Converter):
    def converted(self) -> Tokenizer: ...

class SpmConverter(Converter):
    handle_byte_fallback = ...
    SpmExtractor = SentencePieceExtractor
    special_tokens = ...
    def __init__(self, *args) -> None: ...
    def vocab(self, proto):  # -> list[tuple[Any, Any]]:
        ...
    def unk_id(self, proto): ...
    def tokenizer(self, proto):  # -> Tokenizer:
        ...
    def normalizer(self, proto):  # -> Sequence:
        ...
    def pre_tokenizer(self, replacement, add_prefix_space):  # -> Metaspace:
        ...
    def post_processor(self):  # -> None:
        ...
    def decoder(self, replacement, add_prefix_space):  # -> Metaspace:
        ...
    def converted(self) -> Tokenizer: ...

class AlbertConverter(SpmConverter):
    def vocab(self, proto):  # -> list[tuple[Any, Any]]:
        ...
    def normalizer(self, proto):  # -> Sequence:
        ...
    def post_processor(self):  # -> TemplateProcessing:
        ...

class BarthezConverter(SpmConverter):
    def unk_id(self, proto):  # -> Literal[3]:
        ...
    def post_processor(self):  # -> TemplateProcessing:
        ...

class CamembertConverter(SpmConverter):
    def vocab(self, proto):  # -> list[tuple[str, float] | tuple[str, int]]:
        ...
    def unk_id(self, proto):  # -> Literal[3]:
        ...
    def post_processor(self):  # -> TemplateProcessing:
        ...

class DebertaV2Converter(SpmConverter):
    def pre_tokenizer(self, replacement, add_prefix_space):  # -> Sequence:
        ...
    def normalizer(self, proto):  # -> Sequence:
        ...
    def post_processor(self):  # -> TemplateProcessing:
        ...

class MBartConverter(SpmConverter):
    def vocab(self, proto):  # -> list[tuple[str, float]]:
        ...
    def unk_id(self, proto):  # -> Literal[3]:
        ...
    def post_processor(self):  # -> TemplateProcessing:
        ...

class MBart50Converter(SpmConverter):
    def vocab(self, proto):  # -> list[tuple[str, float]]:
        ...
    def unk_id(self, proto):  # -> Literal[3]:
        ...
    def post_processor(self):  # -> TemplateProcessing:
        ...

class NllbConverter(SpmConverter):
    def vocab(self, proto):  # -> list[tuple[str, float]]:
        ...
    def unk_id(self, proto):  # -> Literal[3]:
        ...
    def post_processor(self):  # -> TemplateProcessing:
        ...

class SeamlessM4TConverter(SpmConverter):
    def vocab(self, proto):  # -> list[tuple[str, float]]:
        ...
    def unk_id(self, proto): ...
    def post_processor(self):  # -> TemplateProcessing:
        ...

class XLMRobertaConverter(SpmConverter):
    def vocab(self, proto):  # -> list[tuple[str, float]]:
        ...
    def unk_id(self, proto):  # -> Literal[3]:
        ...
    def post_processor(self):  # -> TemplateProcessing:
        ...

class XLNetConverter(SpmConverter):
    def vocab(self, proto):  # -> list[tuple[Any, Any]]:
        ...
    def normalizer(self, proto):  # -> Sequence:
        ...
    def post_processor(self):  # -> TemplateProcessing:
        ...

class ReformerConverter(SpmConverter): ...

class RemBertConverter(SpmConverter):
    def normalizer(self, proto):  # -> Sequence:
        ...
    def post_processor(self):  # -> TemplateProcessing:
        ...

class BertGenerationConverter(SpmConverter): ...

class PegasusConverter(SpmConverter):
    def vocab(self, proto):  # -> list[tuple[Any, float]]:
        ...
    def unk_id(self, proto): ...
    def pre_tokenizer(self, replacement, add_prefix_space):  # -> Sequence:
        ...
    def post_processor(self):  # -> TemplateProcessing:
        ...

class T5Converter(SpmConverter):
    def vocab(self, proto):  # -> list[tuple[Any, Any]]:
        ...
    def post_processor(self):  # -> TemplateProcessing:
        ...

class UdopConverter(SpmConverter):
    def post_processor(self):  # -> TemplateProcessing:
        ...

class WhisperConverter(Converter):
    def converted(self) -> Tokenizer: ...

class BigBirdConverter(SpmConverter):
    def post_processor(self):  # -> TemplateProcessing:
        ...

class CLIPConverter(Converter):
    def converted(self) -> Tokenizer: ...

class LayoutLMv2Converter(Converter):
    def converted(self) -> Tokenizer: ...

class BlenderbotConverter(Converter):
    def converted(self) -> Tokenizer: ...

class XGLMConverter(SpmConverter):
    def vocab(self, proto):  # -> list[tuple[str, float]]:
        ...
    def unk_id(self, proto):  # -> Literal[3]:
        ...
    def post_processor(self):  # -> TemplateProcessing:
        ...

class GemmaConverter(SpmConverter):
    handle_byte_fallback = ...
    SpmExtractor = GemmaSentencePieceExtractor
    special_tokens = ...
    def normalizer(self, proto):  # -> Replace:
        ...
    def vocab(self, proto):  # -> list[tuple[Any, float]]:
        ...
    def pre_tokenizer(self, replacement, add_prefix_space):  # -> Split:
        ...
    def unk_id(self, proto):  # -> Literal[3]:
        ...
    def decoder(self, replacement, add_prefix_space):  # -> Sequence:
        ...

class LlamaConverter(SpmConverter):
    handle_byte_fallback = ...
    def vocab(self, proto):  # -> list[tuple[Any, float]]:
        ...
    def unk_id(self, proto):  # -> Literal[0]:
        ...
    def decoder(self, replacement, add_prefix_space):  # -> Sequence:
        ...
    def normalizer(self, proto):  # -> Sequence | None:
        ...
    def pre_tokenizer(self, replacement, add_prefix_space):  # -> Metaspace | None:
        ...
    def post_processor(self):  # -> None:
        ...

class MarkupLMConverter(Converter):
    def converted(self) -> Tokenizer: ...

class MoshiConverter(SpmConverter):
    handle_byte_fallback = ...
    def __init__(self, vocab_file, model_max_length=..., **kwargs) -> None: ...
    def normalizer(self, proto):  # -> Sequence:
        ...
    def decoder(self, replacement, add_prefix_space):  # -> Sequence:
        ...
    def pre_tokenizer(self, replacement, add_prefix_space):  # -> Metaspace:
        ...

class HeliumConverter(SpmConverter):
    handle_byte_fallback = ...
    def __init__(self, vocab_file=..., *args) -> None: ...
    def tokenizer(self, proto):  # -> Tokenizer:
        ...
    def vocab(self, proto):  # -> list[Any]:
        ...
    def unk_id(self, proto):  # -> Literal[0]:
        ...
    def decoder(self, replacement, add_prefix_space):  # -> Sequence:
        ...
    def normalizer(self, proto):  # -> Sequence:
        ...
    def pre_tokenizer(self, replacement, add_prefix_space):  # -> Sequence:
        ...
    def post_processor(self):  # -> TemplateProcessing:
        ...

def bytes_to_unicode():  # -> dict[int, str]:

    ...

class TikTokenConverter:
    def __init__(
        self, vocab_file=..., pattern=..., add_prefix_space=..., additional_special_tokens=..., *args, **kwargs
    ) -> None: ...
    def extract_vocab_merges_from_model(self, tiktoken_url: str):  # -> tuple[dict[Any, Any], list[tuple[str, str]]]:
        ...
    def tokenizer(self):  # -> Tokenizer:
        ...
    def converted(self) -> Tokenizer: ...

SLOW_TO_FAST_CONVERTERS = ...

def convert_slow_tokenizer(transformer_tokenizer, from_tiktoken=...) -> Tokenizer: ...
