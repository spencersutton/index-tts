from collections.abc import Callable
from dataclasses import dataclass
from typing import Any, NewType

from ..tokenization_utils_base import PreTrainedTokenizerBase
from ..utils import PaddingStrategy

InputDataClass = NewType("InputDataClass", Any)
DataCollator = NewType("DataCollator", Callable[[list[InputDataClass]], dict[str, Any]])

class DataCollatorMixin:
    def __call__(self, features, return_tensors=...): ...

def pad_without_fast_tokenizer_warning(tokenizer, *pad_args, **pad_kwargs): ...
def default_data_collator(features: list[InputDataClass], return_tensors=...) -> dict[str, Any]: ...

@dataclass
class DefaultDataCollator(DataCollatorMixin):
    return_tensors: str = ...
    def __call__(self, features: list[dict[str, Any]], return_tensors=...) -> dict[str, Any]: ...

def torch_default_data_collator(features: list[InputDataClass]) -> dict[str, Any]: ...
def tf_default_data_collator(features: list[InputDataClass]) -> dict[str, Any]: ...
def numpy_default_data_collator(features: list[InputDataClass]) -> dict[str, Any]: ...

@dataclass
class DataCollatorWithPadding:
    tokenizer: PreTrainedTokenizerBase
    padding: bool | str | PaddingStrategy = ...
    max_length: int | None = ...
    pad_to_multiple_of: int | None = ...
    return_tensors: str = ...
    def __call__(self, features: list[dict[str, Any]]) -> dict[str, Any]: ...

@dataclass
class DataCollatorForTokenClassification(DataCollatorMixin):
    tokenizer: PreTrainedTokenizerBase
    padding: bool | str | PaddingStrategy = ...
    max_length: int | None = ...
    pad_to_multiple_of: int | None = ...
    label_pad_token_id: int = ...
    return_tensors: str = ...
    def torch_call(self, features):  # -> BatchEncoding:
        ...
    def tf_call(self, features):  # -> BatchEncoding | dict[Any, Any]:
        ...
    def numpy_call(self, features):  # -> BatchEncoding | dict[Any, NDArray[signedinteger[_64Bit]]]:
        ...

@dataclass
class DataCollatorForMultipleChoice(DataCollatorMixin):
    tokenizer: PreTrainedTokenizerBase
    padding: bool | str | PaddingStrategy = ...
    max_length: int | None = ...
    pad_to_multiple_of: int | None = ...
    return_tensors: str = ...
    def torch_call(self, examples: list[dict[str, Any]]):  # -> dict[Any, Any]:
        ...
    def tf_call(self, features):  # -> dict[Any, Any]:
        ...

@dataclass
class DataCollatorForSeq2Seq:
    tokenizer: PreTrainedTokenizerBase
    model: Any | None = ...
    padding: bool | str | PaddingStrategy = ...
    max_length: int | None = ...
    pad_to_multiple_of: int | None = ...
    label_pad_token_id: int = ...
    return_tensors: str = ...
    def __call__(self, features, return_tensors=...):  # -> BatchEncoding:
        ...

@dataclass
class DataCollatorForLanguageModeling(DataCollatorMixin):
    tokenizer: PreTrainedTokenizerBase
    mlm: bool = ...
    mlm_probability: float | None = ...
    mask_replace_prob: float = ...
    random_replace_prob: float = ...
    pad_to_multiple_of: int | None = ...
    tf_experimental_compile: bool = ...
    return_tensors: str = ...
    seed: int | None = ...
    def __post_init__(self):  # -> None:
        ...
    def get_generator(self, seed):  # -> torch._C.Generator | numpy.random._generator.Generator:
        ...
    def create_rng(self):  # -> None:
        ...
    @staticmethod
    def tf_bernoulli(shape, probability, generator=...): ...
    def tf_mask_tokens(
        self, inputs: Any, vocab_size, mask_token_id, special_tokens_mask: Any | None = ...
    ) -> tuple[Any, Any]: ...
    def tf_call(self, examples: list[list[int] | Any | dict[str, Any]]) -> dict[str, Any]: ...
    def torch_call(self, examples: list[list[int] | Any | dict[str, Any]]) -> dict[str, Any]: ...
    def torch_mask_tokens(self, inputs: Any, special_tokens_mask: Any | None = ...) -> tuple[Any, Any]: ...
    def numpy_call(self, examples: list[list[int] | Any | dict[str, Any]]) -> dict[str, Any]: ...
    def numpy_mask_tokens(self, inputs: Any, special_tokens_mask: Any | None = ...) -> tuple[Any, Any]: ...

@dataclass
class DataCollatorForWholeWordMask(DataCollatorForLanguageModeling):
    def torch_call(self, examples: list[list[int] | Any | dict[str, Any]]) -> dict[str, Any]: ...
    def tf_call(self, examples: list[list[int] | Any | dict[str, Any]]) -> dict[str, Any]: ...
    def numpy_call(self, examples: list[list[int] | Any | dict[str, Any]]) -> dict[str, Any]: ...
    def torch_mask_tokens(self, inputs: Any, mask_labels: Any) -> tuple[Any, Any]: ...
    def tf_mask_tokens(self, inputs: Any, mask_labels: Any) -> tuple[Any, Any]: ...
    def numpy_mask_tokens(self, inputs: Any, mask_labels: Any) -> tuple[Any, Any]: ...

def tolist(x):  # -> list[Any]:
    ...

@dataclass
class DataCollatorForSOP(DataCollatorForLanguageModeling):
    def __init__(self, *args, **kwargs) -> None: ...
    def __call__(self, examples: list[dict[str, Any]]) -> dict[str, Any]: ...
    def mask_tokens(self, inputs: Any) -> tuple[Any, Any, Any]: ...

@dataclass
class DataCollatorForPermutationLanguageModeling(DataCollatorMixin):
    tokenizer: PreTrainedTokenizerBase
    plm_probability: float = ...
    max_span_length: int = ...
    return_tensors: str = ...
    def torch_call(self, examples: list[list[int] | Any | dict[str, Any]]) -> dict[str, Any]: ...
    def tf_call(self, examples: list[list[int] | Any | dict[str, Any]]) -> dict[str, Any]: ...
    def numpy_call(self, examples: list[list[int] | Any | dict[str, Any]]) -> dict[str, Any]: ...
    def torch_mask_tokens(self, inputs: Any) -> tuple[Any, Any, Any, Any]: ...
    def tf_mask_tokens(self, inputs: Any) -> tuple[Any, Any, Any, Any]: ...
    def numpy_mask_tokens(self, inputs: Any) -> tuple[Any, Any, Any, Any]: ...

@dataclass
class DataCollatorWithFlattening(DefaultDataCollator):
    def __init__(
        self,
        *args,
        return_position_ids=...,
        separator_id=...,
        return_flash_attn_kwargs=...,
        return_seq_idx=...,
        **kwargs,
    ) -> None: ...
    def __call__(self, features, return_tensors=..., separator_id=...):  # -> dict[str, list[Any]]:
        ...
