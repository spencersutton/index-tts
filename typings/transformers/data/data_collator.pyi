"""
This type stub file was generated by pyright.
"""

from dataclasses import dataclass
from typing import Any, Callable, NewType, Optional, Union
from ..tokenization_utils_base import PreTrainedTokenizerBase
from ..utils import PaddingStrategy

InputDataClass = NewType("InputDataClass", Any)
DataCollator = NewType("DataCollator", Callable[[list[InputDataClass]], dict[str, Any]])

class DataCollatorMixin:
    def __call__(self, features, return_tensors=...): ...

def pad_without_fast_tokenizer_warning(tokenizer, *pad_args, **pad_kwargs): ...
def default_data_collator(features: list[InputDataClass], return_tensors=...) -> dict[str, Any]: ...

@dataclass
class DefaultDataCollator(DataCollatorMixin):
    return_tensors: str = ...
    def __call__(self, features: list[dict[str, Any]], return_tensors=...) -> dict[str, Any]: ...

def torch_default_data_collator(features: list[InputDataClass]) -> dict[str, Any]: ...
def tf_default_data_collator(features: list[InputDataClass]) -> dict[str, Any]: ...
def numpy_default_data_collator(features: list[InputDataClass]) -> dict[str, Any]: ...

@dataclass
class DataCollatorWithPadding:
    tokenizer: PreTrainedTokenizerBase
    padding: Union[bool, str, PaddingStrategy] = ...
    max_length: Optional[int] = ...
    pad_to_multiple_of: Optional[int] = ...
    return_tensors: str = ...
    def __call__(self, features: list[dict[str, Any]]) -> dict[str, Any]: ...

@dataclass
class DataCollatorForTokenClassification(DataCollatorMixin):
    tokenizer: PreTrainedTokenizerBase
    padding: Union[bool, str, PaddingStrategy] = ...
    max_length: Optional[int] = ...
    pad_to_multiple_of: Optional[int] = ...
    label_pad_token_id: int = ...
    return_tensors: str = ...
    def torch_call(self, features): ...
    def tf_call(self, features): ...
    def numpy_call(self, features): ...

@dataclass
class DataCollatorForMultipleChoice(DataCollatorMixin):
    tokenizer: PreTrainedTokenizerBase
    padding: Union[bool, str, PaddingStrategy] = ...
    max_length: Optional[int] = ...
    pad_to_multiple_of: Optional[int] = ...
    return_tensors: str = ...
    def torch_call(self, examples: list[dict[str, Any]]): ...
    def tf_call(self, features): ...

@dataclass
class DataCollatorForSeq2Seq:
    tokenizer: PreTrainedTokenizerBase
    model: Optional[Any] = ...
    padding: Union[bool, str, PaddingStrategy] = ...
    max_length: Optional[int] = ...
    pad_to_multiple_of: Optional[int] = ...
    label_pad_token_id: int = ...
    return_tensors: str = ...
    def __call__(self, features, return_tensors=...): ...

@dataclass
class DataCollatorForLanguageModeling(DataCollatorMixin):
    tokenizer: PreTrainedTokenizerBase
    mlm: bool = ...
    mlm_probability: Optional[float] = ...
    mask_replace_prob: float = ...
    random_replace_prob: float = ...
    pad_to_multiple_of: Optional[int] = ...
    tf_experimental_compile: bool = ...
    return_tensors: str = ...
    seed: Optional[int] = ...
    def __post_init__(self): ...
    def get_generator(self, seed): ...
    def create_rng(self): ...
    @staticmethod
    def tf_bernoulli(shape, probability, generator=...): ...
    def tf_mask_tokens(
        self, inputs: Any, vocab_size, mask_token_id, special_tokens_mask: Optional[Any] = ...
    ) -> tuple[Any, Any]: ...
    def tf_call(self, examples: list[Union[list[int], Any, dict[str, Any]]]) -> dict[str, Any]: ...
    def torch_call(self, examples: list[Union[list[int], Any, dict[str, Any]]]) -> dict[str, Any]: ...
    def torch_mask_tokens(self, inputs: Any, special_tokens_mask: Optional[Any] = ...) -> tuple[Any, Any]: ...
    def numpy_call(self, examples: list[Union[list[int], Any, dict[str, Any]]]) -> dict[str, Any]: ...
    def numpy_mask_tokens(self, inputs: Any, special_tokens_mask: Optional[Any] = ...) -> tuple[Any, Any]: ...

@dataclass
class DataCollatorForWholeWordMask(DataCollatorForLanguageModeling):
    def torch_call(self, examples: list[Union[list[int], Any, dict[str, Any]]]) -> dict[str, Any]: ...
    def tf_call(self, examples: list[Union[list[int], Any, dict[str, Any]]]) -> dict[str, Any]: ...
    def numpy_call(self, examples: list[Union[list[int], Any, dict[str, Any]]]) -> dict[str, Any]: ...
    def torch_mask_tokens(self, inputs: Any, mask_labels: Any) -> tuple[Any, Any]: ...
    def tf_mask_tokens(self, inputs: Any, mask_labels: Any) -> tuple[Any, Any]: ...
    def numpy_mask_tokens(self, inputs: Any, mask_labels: Any) -> tuple[Any, Any]: ...

def tolist(x): ...

@dataclass
class DataCollatorForSOP(DataCollatorForLanguageModeling):
    def __init__(self, *args, **kwargs) -> None: ...
    def __call__(self, examples: list[dict[str, Any]]) -> dict[str, Any]: ...
    def mask_tokens(self, inputs: Any) -> tuple[Any, Any, Any]: ...

@dataclass
class DataCollatorForPermutationLanguageModeling(DataCollatorMixin):
    tokenizer: PreTrainedTokenizerBase
    plm_probability: float = ...
    max_span_length: int = ...
    return_tensors: str = ...
    def torch_call(self, examples: list[Union[list[int], Any, dict[str, Any]]]) -> dict[str, Any]: ...
    def tf_call(self, examples: list[Union[list[int], Any, dict[str, Any]]]) -> dict[str, Any]: ...
    def numpy_call(self, examples: list[Union[list[int], Any, dict[str, Any]]]) -> dict[str, Any]: ...
    def torch_mask_tokens(self, inputs: Any) -> tuple[Any, Any, Any, Any]: ...
    def tf_mask_tokens(self, inputs: Any) -> tuple[Any, Any, Any, Any]: ...
    def numpy_mask_tokens(self, inputs: Any) -> tuple[Any, Any, Any, Any]: ...

@dataclass
class DataCollatorWithFlattening(DefaultDataCollator):
    def __init__(
        self,
        *args,
        return_position_ids=...,
        separator_id=...,
        return_flash_attn_kwargs=...,
        return_seq_idx=...,
        **kwargs,
    ) -> None: ...
    def __call__(self, features, return_tensors=..., separator_id=...): ...
