from dataclasses import dataclass
from enum import Enum

from torch.utils.data import Dataset

from ...tokenization_utils_base import PreTrainedTokenizerBase
from ..processors.utils import InputFeatures

logger = ...

@dataclass
class GlueDataTrainingArguments:
    task_name: str = ...
    data_dir: str = ...
    max_seq_length: int = ...
    overwrite_cache: bool = ...
    def __post_init__(self):  # -> None:
        ...

class Split(Enum):
    train = ...
    dev = ...
    test = ...

class GlueDataset(Dataset):
    args: GlueDataTrainingArguments
    output_mode: str
    features: list[InputFeatures]
    def __init__(
        self,
        args: GlueDataTrainingArguments,
        tokenizer: PreTrainedTokenizerBase,
        limit_length: int | None = ...,
        mode: str | Split = ...,
        cache_dir: str | None = ...,
    ) -> None: ...
    def __len__(self) -> int:  # -> int:
        ...
    def __getitem__(self, i) -> InputFeatures: ...
    def get_labels(self):  # -> list[str] | list[None]:
        ...
