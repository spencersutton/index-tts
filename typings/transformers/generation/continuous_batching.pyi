import queue
import threading
from abc import ABC, abstractmethod
from dataclasses import dataclass
from enum import Enum

import torch

from ..configuration_utils import PretrainedConfig
from ..generation.configuration_utils import GenerationConfig
from ..utils.metrics import attach_tracer, traced

class RequestStatus(Enum):
    PENDING = ...
    PREFILLING = ...
    PREFILLING_SPLIT = ...
    SPLIT_PENDING_REMAINDER = ...
    DECODING = ...
    FINISHED = ...
    FAILED = ...

logger = ...

@dataclass
class GenerationOutput:
    request_id: str
    prompt_ids: list[int] = ...
    generated_tokens: list[int] = ...
    logprobs: list[float] = ...
    error: str | None = ...
    status: RequestStatus = ...
    created_time: float = ...
    next_token: int | None = ...

@dataclass
class RequestState:
    request_id: str
    prompt_ids: list[int] | None = ...
    full_prompt_ids: list[int] | None = ...
    remaining_prompt_ids: list[int] = ...
    static_outputs: list[int] = ...
    allocated_blocks: list[int] = ...
    position_offset: int = ...
    status: RequestStatus = ...
    max_new_tokens: int = ...
    eos_token_id: int = ...
    created_time: float = ...
    error: str | None = ...
    next_token: str | None = ...
    def current_len(self) -> int: ...
    def generated_len(self) -> int: ...
    @traced
    def update_with_token(self, token_id: int) -> bool: ...
    def to_generation_output(self):  # -> GenerationOutput:

        ...

@attach_tracer()
class PagedAttentionCache:
    def __init__(
        self,
        config: PretrainedConfig,
        generation_config: GenerationConfig,
        device: torch.device,
        dtype: torch.dtype = ...,
        num_requests: int = ...,
        layer_device_map: dict[int, str | torch.device | int] | None = ...,
        tp_size: int | None = ...,
    ) -> None: ...
    @traced
    def allocate_blocks(self, n_blocks: int, request_id: str) -> list[int]: ...
    @traced
    def free_blocks(self, request_id: str) -> None: ...
    def get_num_free_blocks(self) -> int: ...
    def get_block_table(self, request_id: str) -> list[int]: ...
    @traced
    def update(
        self, key_states: torch.Tensor, value_states: torch.Tensor, layer_idx: int, read_index, write_index, **kwargs
    ) -> tuple[torch.Tensor, torch.Tensor]: ...

class Scheduler(ABC):
    def __init__(self, cache: PagedAttentionCache, retain_cache_on_finish: bool = ...) -> None: ...
    @abstractmethod
    def add_waiting_request(self, state: RequestState):  # -> None:

        ...
    @abstractmethod
    def schedule_batch(self, token_budget: int) -> list[RequestState]: ...
    @traced
    def has_pending_requests(self) -> bool: ...
    @abstractmethod
    def finish_request(self, request_id: str, evict_from_cache: bool = ...):  # -> None:

        ...
    @traced
    def get_active_request_static_outputs(self, request_id: str) -> list[int]: ...

@attach_tracer()
class FIFOScheduler(Scheduler):
    @traced
    def add_waiting_request(self, state: RequestState):  # -> None:

        ...
    @traced
    def schedule_batch(self, token_budget: int) -> list[RequestState]: ...
    @traced
    def finish_request(self, request_id: str, evict_from_cache: bool = ...):  # -> None:
        ...

@attach_tracer()
class PrefillFirstScheduler(Scheduler):
    @traced
    def add_waiting_request(self, state: RequestState):  # -> None:

        ...
    @traced
    def schedule_batch(self, token_budget: int) -> list[RequestState]: ...
    @traced
    def finish_request(self, request_id: str, evict_from_cache: bool = ...):  # -> None:
        ...

def get_device_and_memory():  # -> tuple[device, Any | int | None, int, int]:
    ...
@traced(standalone=True)
def compute_optimal_blocks(
    max_num_tokens, block_size, head_dim, num_heads, num_layers, max_memory_percent=..., num_blocks=..., dtype=...
):  # -> tuple[int, int]:
    ...

@dataclass
class PagedAttentionArgs:
    input_ids: torch.Tensor
    attention_mask: torch.Tensor
    position_ids: torch.Tensor
    cumulative_seqlens_q: torch.Tensor
    cumulative_seqlens_k: torch.Tensor
    max_seqlen_q: int
    max_seqlen_k: int
    write_index: torch.Tensor
    read_index: torch.Tensor
    logits_indices: torch.Tensor
    block_tables: dict[str, list[int]]
    cache: PagedAttentionCache
    use_cache: bool = ...

@traced
def create_document_mask(cumulative_seqlens_q, cumulative_seqlens_k):  # -> Tensor:
    ...

@attach_tracer()
class ContinuousBatchProcessor:
    def __init__(
        self,
        cache: PagedAttentionCache,
        config: PretrainedConfig,
        generation_config: GenerationConfig,
        input_queue: queue.Queue,
        output_queue: queue.Queue,
        stop_event: threading.Event,
        model_device: torch.device,
        model_dtype: torch.dtype,
        scheduler: Scheduler,
        streaming: bool = ...,
        manual_eviction: bool = ...,
    ) -> None: ...
    @traced(standalone=True)
    def setup_static_tensors(self):  # -> None:
        ...
    @traced
    @torch.no_grad()
    def reset_static_tensors(self):  # -> None:

        ...
    def get_model_kwargs(self) -> PagedAttentionArgs: ...
    @traced
    def prepare_next_batch(self):  # -> None:

        ...
    @traced
    def update_batch(self):  # -> None:

        ...
    @traced
    def has_pending_requests(self) -> bool: ...
    @traced
    def handle_batch_error(self, error):  # -> None:

        ...
    @traced
    def fail_all_requests(self, error):  # -> None:

        ...

SCHEDULER_MAPPING = ...

@attach_tracer()
class ContinuousBatchingManager:
    def __init__(
        self,
        model,
        generation_config: GenerationConfig,
        manual_eviction: bool = ...,
        max_queue_size=...,
        streaming: bool = ...,
    ) -> None: ...
    @traced
    def start(self):  # -> None:

        ...
    def is_running(self):  # -> bool:

        ...
    def stop(self, block: bool = ..., timeout: float | None = ...):  # -> None:

        ...
    def join(self, timeout: float | None = ...):  # -> None:

        ...
    def add_request(
        self, input_ids: list[int], request_id: str | None = ..., max_new_tokens: int | None = ...
    ) -> str: ...
    def add_requests(self, inputs: list[list[int]], **kwargs):  # -> None:
        ...
    def get_result(self, timeout=...) -> GenerationOutput | None: ...
    def __iter__(self):  # -> Generator[GenerationOutput, Any, None]:

        ...
    @traced
    def warmup(self, batch_processor):  # -> None:
        ...
    @traced
    def evict_request_from_cache(self, request_id: str):  # -> None:

        ...

class ContinuousMixin:
    def init_continuous_batching(
        self,
        generation_config: GenerationConfig | None = ...,
        manual_eviction: bool = ...,
        max_queue_size: int = ...,
        streaming: bool = ...,
    ) -> ContinuousBatchingManager: ...
    @traced
    @torch.inference_mode()
    def generate_batch(
        self,
        inputs: list[list[int]],
        generation_config: GenerationConfig | None = ...,
        progress_bar: bool = ...,
        **kwargs,
    ) -> list[list[int]]: ...
