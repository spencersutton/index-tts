"""
This type stub file was generated by pyright.
"""

import queue
import threading
import torch
from abc import ABC, abstractmethod
from dataclasses import dataclass
from enum import Enum
from typing import Optional, Union
from ..configuration_utils import PretrainedConfig
from ..generation.configuration_utils import GenerationConfig
from ..utils.metrics import attach_tracer, traced

class RequestStatus(Enum):
    PENDING = ...
    PREFILLING = ...
    PREFILLING_SPLIT = ...
    SPLIT_PENDING_REMAINDER = ...
    DECODING = ...
    FINISHED = ...
    FAILED = ...

logger = ...

@dataclass
class GenerationOutput:
    request_id: str
    prompt_ids: list[int] = ...
    generated_tokens: list[int] = ...
    logprobs: list[float] = ...
    error: Optional[str] = ...
    status: RequestStatus = ...
    created_time: float = ...
    next_token: Optional[int] = ...

@dataclass
class RequestState:
    request_id: str
    prompt_ids: Optional[list[int]] = ...
    full_prompt_ids: Optional[list[int]] = ...
    remaining_prompt_ids: list[int] = ...
    static_outputs: list[int] = ...
    allocated_blocks: list[int] = ...
    position_offset: int = ...
    status: RequestStatus = ...
    max_new_tokens: int = ...
    eos_token_id: int = ...
    created_time: float = ...
    error: Optional[str] = ...
    next_token: Optional[str] = ...
    def current_len(self) -> int: ...
    def generated_len(self) -> int: ...
    @traced
    def update_with_token(self, token_id: int) -> bool: ...
    def __repr__(self): ...
    def to_generation_output(self): ...

@attach_tracer()
class PagedAttentionCache:
    def __init__(
        self,
        config: PretrainedConfig,
        generation_config: GenerationConfig,
        device: torch.device,
        dtype: torch.dtype = ...,
        num_requests: int = ...,
        layer_device_map: Optional[dict[int, Union[str, torch.device, int]]] = ...,
        tp_size: Optional[int] = ...,
    ) -> None: ...
    @traced
    def allocate_blocks(self, n_blocks: int, request_id: str) -> list[int]: ...
    @traced
    def free_blocks(self, request_id: str) -> None: ...
    def get_num_free_blocks(self) -> int: ...
    def get_block_table(self, request_id: str) -> list[int]: ...
    @traced
    def update(
        self, key_states: torch.Tensor, value_states: torch.Tensor, layer_idx: int, read_index, write_index, **kwargs
    ) -> tuple[torch.Tensor, torch.Tensor]: ...

class Scheduler(ABC):
    def __init__(self, cache: PagedAttentionCache, retain_cache_on_finish: bool = ...) -> None: ...
    @abstractmethod
    def add_waiting_request(self, state: RequestState): ...
    @abstractmethod
    def schedule_batch(self, token_budget: int) -> list[RequestState]: ...
    @traced
    def has_pending_requests(self) -> bool: ...
    @abstractmethod
    def finish_request(self, request_id: str, evict_from_cache: bool = ...): ...
    @traced
    def get_active_request_static_outputs(self, request_id: str) -> list[int]: ...

@attach_tracer()
class FIFOScheduler(Scheduler):
    @traced
    def add_waiting_request(self, state: RequestState): ...
    @traced
    def schedule_batch(self, token_budget: int) -> list[RequestState]: ...
    @traced
    def finish_request(self, request_id: str, evict_from_cache: bool = ...): ...

@attach_tracer()
class PrefillFirstScheduler(Scheduler):
    @traced
    def add_waiting_request(self, state: RequestState): ...
    @traced
    def schedule_batch(self, token_budget: int) -> list[RequestState]: ...
    @traced
    def finish_request(self, request_id: str, evict_from_cache: bool = ...): ...

def get_device_and_memory(): ...
@traced(standalone=True)
def compute_optimal_blocks(
    max_num_tokens, block_size, head_dim, num_heads, num_layers, max_memory_percent=..., num_blocks=..., dtype=...
): ...

@dataclass
class PagedAttentionArgs:
    input_ids: torch.Tensor
    attention_mask: torch.Tensor
    position_ids: torch.Tensor
    cumulative_seqlens_q: torch.Tensor
    cumulative_seqlens_k: torch.Tensor
    max_seqlen_q: int
    max_seqlen_k: int
    write_index: torch.Tensor
    read_index: torch.Tensor
    logits_indices: torch.Tensor
    block_tables: dict[str, list[int]]
    cache: PagedAttentionCache
    use_cache: bool = ...

@traced
def create_document_mask(cumulative_seqlens_q, cumulative_seqlens_k): ...

@attach_tracer()
class ContinuousBatchProcessor:
    def __init__(
        self,
        cache: PagedAttentionCache,
        config: PretrainedConfig,
        generation_config: GenerationConfig,
        input_queue: queue.Queue,
        output_queue: queue.Queue,
        stop_event: threading.Event,
        model_device: torch.device,
        model_dtype: torch.dtype,
        scheduler: Scheduler,
        streaming: bool = ...,
        manual_eviction: bool = ...,
    ) -> None: ...
    @traced(standalone=True)
    def setup_static_tensors(self): ...
    @traced
    @torch.no_grad()
    def reset_static_tensors(self): ...
    def get_model_kwargs(self) -> PagedAttentionArgs: ...
    def __repr__(self): ...
    @traced
    def prepare_next_batch(self): ...
    @traced
    def update_batch(self): ...
    @traced
    def has_pending_requests(self) -> bool: ...
    @traced
    def handle_batch_error(self, error): ...
    @traced
    def fail_all_requests(self, error): ...

SCHEDULER_MAPPING = ...

@attach_tracer()
class ContinuousBatchingManager:
    def __init__(
        self,
        model,
        generation_config: GenerationConfig,
        manual_eviction: bool = ...,
        max_queue_size=...,
        streaming: bool = ...,
    ) -> None: ...
    @traced
    def start(self): ...
    def is_running(self): ...
    def stop(self, block: bool = ..., timeout: Optional[float] = ...): ...
    def join(self, timeout: Optional[float] = ...): ...
    def add_request(
        self, input_ids: list[int], request_id: Optional[str] = ..., max_new_tokens: Optional[int] = ...
    ) -> str: ...
    def add_requests(self, inputs: list[list[int]], **kwargs): ...
    def get_result(self, timeout=...) -> Optional[GenerationOutput]: ...
    def __iter__(self): ...
    @traced
    def warmup(self, batch_processor): ...
    @traced
    def evict_request_from_cache(self, request_id: str): ...

class ContinuousMixin:
    def init_continuous_batching(
        self,
        generation_config: Optional[GenerationConfig] = ...,
        manual_eviction: bool = ...,
        max_queue_size: int = ...,
        streaming: bool = ...,
    ) -> ContinuousBatchingManager: ...
    @traced
    @torch.inference_mode()
    def generate_batch(
        self,
        inputs: list[list[int]],
        generation_config: Optional[GenerationConfig] = ...,
        progress_bar: bool = ...,
        **kwargs,
    ) -> list[list[int]]: ...
