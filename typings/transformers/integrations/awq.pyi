from ..utils import is_torch_available

"AWQ (Activation aware Weight Quantization) integration file"
if is_torch_available(): ...
logger = ...
AWQ_FUSED_MAPPINGS = ...
AWQ_SCALES_MAPPINGS = ...

def replace_quantization_scales(model, model_type): ...
def replace_with_awq_linear(
    model, modules_to_not_convert=..., quantization_config=..., current_key_name=..., has_been_replaced=...
) -> bool: ...
def get_modules_to_fuse(
    model, quantization_config
):  # -> dict[str, list[str] | bool] | dict[str, list[str] | bool | float]:

    ...
def fuse_awq_modules(model, quantization_config): ...
def post_init_awq_exllama_modules(model, exllama_config): ...
def post_init_awq_ipex_modules(model): ...
