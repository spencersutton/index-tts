from ..utils import is_accelerate_available, is_torch_available

"""
Integration with Deepspeed
"""
if is_torch_available(): ...
logger = ...

def is_deepspeed_available():  # -> bool | None:
    ...

if is_accelerate_available() and is_deepspeed_available(): ...

class HfDeepSpeedConfig:
    def __init__(self, config_file_or_dict) -> None: ...

class HfTrainerDeepSpeedConfig(HfDeepSpeedConfig):
    def __init__(self, config_file_or_dict) -> None: ...
    def dtype(self):  # -> dtype:
        ...
    def is_auto(self, ds_key_long):  # -> Literal[False]:
        ...
    def fill_match(self, ds_key_long, hf_val, hf_key=..., must_match=...):  # -> None:

        ...

    fill_only = ...
    def trainer_config_process(self, args, auto_find_batch_size=...):  # -> None:

        ...
    def trainer_config_finalize(self, args, model, num_training_steps):  # -> None:

        ...

_hf_deepspeed_config_weak_ref = ...

def set_hf_deepspeed_config(hf_deepspeed_config_obj):  # -> None:
    ...
def unset_hf_deepspeed_config():  # -> None:
    ...
def is_deepspeed_zero3_enabled():  # -> Literal[False]:
    ...
def deepspeed_config():  # -> None:
    ...
def deepspeed_optim_sched(
    trainer, hf_deepspeed_config, args, num_training_steps, model_parameters
):  # -> tuple[Any, Any]:

    ...
def deepspeed_init(trainer, num_training_steps, inference=...):  # -> tuple[Any | None, Any | None]:

    ...
def deepspeed_load_checkpoint(deepspeed_engine, checkpoint_path, load_module_strict=...):  # -> None:
    ...
