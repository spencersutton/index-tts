"""
This type stub file was generated by pyright.
"""

import torch
from typing import Callable, Optional
from ..modeling_utils import PreTrainedModel

class TorchExportableModuleForVLM:
    def __init__(self, model, max_batch_size: int = ..., max_cache_len: int = ...) -> None: ...
    def export_vision_encoder(self): ...
    def export_connector(self): ...
    def export_text_decoder(self): ...
    def export(self, **kwargs): ...
    def forward(self, pixel_values, input_ids, cache_position): ...
    def generate(
        self, pixel_values=..., input_ids=..., max_new_tokens=..., do_sample=..., temperature=..., **kwargs
    ): ...

class TorchExportableModuleForDecoderOnlyLM(torch.nn.Module):
    def __init__(self, model: PreTrainedModel, max_batch_size: int = ..., max_cache_len: int = ...) -> None: ...
    def forward(self, input_ids: torch.Tensor, cache_position: torch.Tensor) -> torch.Tensor: ...
    def export(
        self,
        input_ids: Optional[torch.Tensor] = ...,
        cache_position: Optional[torch.Tensor] = ...,
        dynamic_shapes: Optional[dict] = ...,
        strict: Optional[bool] = ...,
    ) -> torch.export.ExportedProgram: ...
    @staticmethod
    def generate(
        exported_program: torch.export.ExportedProgram,
        tokenizer,
        prompt: str,
        max_new_tokens: int = ...,
        do_sample: bool = ...,
        temperature: float = ...,
        top_k: int = ...,
        top_p: float = ...,
        device: str = ...,
    ) -> str: ...

class TorchExportableModuleWithStaticCache(torch.nn.Module):
    def __init__(self, model: PreTrainedModel, max_batch_size: int = ..., max_cache_len: int = ...) -> None: ...
    def forward(self, input_ids: torch.Tensor, cache_position: torch.Tensor): ...
    @staticmethod
    def generate(
        exported_program: torch.export.ExportedProgram, prompt_token_ids: torch.Tensor, max_new_tokens: int
    ) -> torch.Tensor: ...

class TorchExportableModuleWithHybridCache(torch.nn.Module):
    def __init__(self, model: PreTrainedModel, max_batch_size: int = ..., max_cache_len: int = ...) -> None: ...
    def forward(self, input_ids: torch.Tensor, cache_position: torch.Tensor) -> torch.Tensor: ...

def convert_and_export_with_cache(
    model: PreTrainedModel,
    example_input_ids: Optional[torch.Tensor] = ...,
    example_cache_position: Optional[torch.Tensor] = ...,
    dynamic_shapes: Optional[dict] = ...,
    strict: Optional[bool] = ...,
): ...

class Seq2SeqLMEncoderExportableModule(torch.nn.Module):
    def __init__(self, encoder_model) -> None: ...
    def forward(self, input_ids): ...

class Seq2SeqLMDecoderExportableModuleWithStaticCache(torch.nn.Module):
    def __init__(self, model, max_static_cache_length, batch_size) -> None: ...
    def forward(self, decoder_input_ids, encoder_hidden_states, cache_position): ...

class Seq2SeqLMExportableModule(torch.nn.Module):
    def __init__(
        self, model, batch_size=..., max_hidden_seq_length=..., cache_implementation=..., max_cache_length=...
    ) -> None: ...
    def export(self, encoder_input_ids=..., decoder_input_ids=..., encoder_hidden_states=..., cache_position=...): ...
    def generate(self, prompt_token_ids, max_new_tokens): ...

def export_with_dynamic_cache(
    model: PreTrainedModel,
    example_input_ids: Optional[torch.Tensor] = ...,
    example_attention_mask: Optional[torch.Tensor] = ...,
): ...
def sdpa_mask_without_vmap(
    batch_size: int,
    cache_position: torch.Tensor,
    kv_length: int,
    kv_offset: int = ...,
    mask_function: Optional[Callable] = ...,
    attention_mask: Optional[torch.Tensor] = ...,
    local_size: Optional[int] = ...,
    allow_is_causal_skip: bool = ...,
    allow_torch_fix: bool = ...,
    **kwargs,
) -> Optional[torch.Tensor]: ...
