"""
This type stub file was generated by pyright.
"""

import torch
from typing import Optional, Union
from ..utils import is_torch_flex_attn_available
from torch.nn.attention.flex_attention import BlockMask

"""
Partially inspired by torchtune's flex attention implementation

Citation:
@software{torchtune,
  title = {torchtune: PyTorch's finetuning library},
  author = {torchtune maintainers and contributors},
  url = {https//github.com/pytorch/torchtune},
  license = {BSD-3-Clause},
  month = apr,
  year = {2024}
}
"""
if is_torch_flex_attn_available(): ...
logger = ...

class WrappedFlexAttention:
    """
    We are doing a singleton class so that flex attention is compiled once when it's first called.
    """

    _instance = ...
    _is_flex_compiled = ...
    _compiled_flex_attention = ...
    def __new__(cls, *args, **kwargs):  # -> Self:
        ...
    @torch.compiler.disable(recursive=False)
    def __init__(self, training) -> None:
        """
        Initialize or update the singleton instance.
        """
        ...

    def __call__(self):  # -> Callable[..., Tensor | tuple[Tensor, Tensor] | tuple[Tensor, AuxOutput]] | None:
        ...

def compile_friendly_flex_attention(
    query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, training=..., **kwargs
) -> torch.Tensor: ...

Offset = Union[torch.Tensor, int]

def make_flex_block_causal_mask(
    attention_mask_2d: torch.Tensor,
    attention_chunk_size: Optional[int] = ...,
    query_length=...,
    key_length=...,
    offsets: Optional[tuple[Offset, Offset]] = ...,
    is_causal: Optional[bool] = ...,
) -> BlockMask:
    """
    IMPORTANT NOTICE: This function is deprecated in favor of using the mask primitives in `masking_utils.py`,
    and will be removed in a future version without warnings. New code should not use it. It is only kept here
    for BC for now, while models using it are being patched accordingly.

    Create a block (causal) document mask for a batch of sequences, both packed and unpacked.
    Create Block (causal) logic and passing it into :func:`torch.nn.attention.flex_attention.create_block_mask`.
    The resultant BlockMask is a compressed representation of the full (causal) block
    mask. BlockMask is essential for performant computation of flex attention.
    See: https://pytorch.org/blog/flexattention/

    Args:
        attention_mask_2d (torch.Tensor): Attention mask for packed and padded sequences
        of shape (batch_size, total_seq_len). e.g.

        For unpacked sequence:
        [[1, 1, 1, 1, 0, 0, 0],
         [1, 1, 1, 1, 1, 0, 0]]

        For packed sequence:
        [[1, 1, 1, 2, 2, 2, 0],
         [1, 1, 2, 2, 2, 3, 3]]

    Returns:
        BlockMask
    """
    ...

def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,
    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)
    """
    ...

def flex_attention_forward(
    module: torch.nn.Module,
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: Union[torch.Tensor, BlockMask],
    scaling: Optional[float] = ...,
    softcap: Optional[float] = ...,
    head_mask: Optional[torch.Tensor] = ...,
    s_aux: Optional[torch.Tensor] = ...,
    **kwargs,
) -> tuple[torch.Tensor, torch.Tensor]: ...
