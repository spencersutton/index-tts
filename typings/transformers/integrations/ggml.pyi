"""
This type stub file was generated by pyright.
"""

from tokenizers import Tokenizer
from ..convert_slow_tokenizer import GPT2Converter, GemmaConverter, LlamaConverter, Qwen2Converter, T5Converter

logger = ...
GGUF_CONFIG_MAPPING = ...
GGUF_TOKENIZER_MAPPING = ...

class GGUFTokenizerSkeleton:
    def __init__(self, dict_) -> None: ...

class GGUFLlamaConverter(LlamaConverter):
    def __init__(self, tokenizer_dict) -> None: ...
    def vocab(self, proto): ...
    def merges(self, proto): ...
    def tokenizer(self, proto): ...
    def decoder(self, replacement, add_prefix_space): ...
    def converted(self): ...

class GGUFQwen2Converter(Qwen2Converter):
    def __init__(self, tokenizer_dict) -> None: ...
    def converted(self) -> Tokenizer: ...

class GGUFPhi3Converter(LlamaConverter):
    def __init__(self, tokenizer_dict) -> None: ...
    def vocab(self, proto): ...
    def merges(self, proto): ...
    def tokenizer(self, proto): ...
    def decoder(self, replacement, add_prefix_space): ...
    def converted(self) -> Tokenizer: ...

class GGUFGPTConverter(GPT2Converter):
    def __init__(self, tokenizer_dict) -> None: ...
    def converted(self) -> Tokenizer: ...

class GGUFT5Converter(T5Converter):
    def __init__(self, tokenizer_dict) -> None: ...
    def vocab(self, proto): ...
    def normalizer(self, proto): ...
    def post_processor(self): ...
    def converted(self) -> Tokenizer: ...

class GGUFGemmaConverter(GemmaConverter):
    def __init__(self, tokenizer_dict) -> None: ...
    def vocab(self, proto): ...
    def normalizer(self, proto): ...
    def decoder(self, replacement, add_prefix_space): ...
    def converted(self) -> Tokenizer: ...

GGUF_TO_FAST_CONVERTERS = ...

def convert_gguf_tokenizer(architecture, tokenizer_dict) -> Tokenizer: ...
