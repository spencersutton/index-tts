import torch

logger = ...
_is_torch_greater_or_equal_than_2_5 = ...
_is_torch_greater_or_equal_than_2_8 = ...
_is_torch_xpu_available = ...

def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor: ...
def use_gqa_in_sdpa(attention_mask: torch.Tensor | None, key: torch.Tensor) -> bool: ...
def sdpa_attention_forward(
    module: torch.nn.Module,
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: torch.Tensor | None,
    dropout: float = ...,
    scaling: float | None = ...,
    is_causal: bool | None = ...,
    **kwargs,
) -> tuple[torch.Tensor, None]: ...
