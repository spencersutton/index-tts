"""
This type stub file was generated by pyright.
"""

import torch
import mamba_ssm
from torch.cuda.amp import custom_bwd, custom_fwd

if hasattr(mamba_ssm.ops.triton, "layernorm"): ...
else: ...

class SelectiveScanFn(torch.autograd.Function):
    @staticmethod
    def forward(
        ctx, u, delta, A, B, C, D=..., z=..., delta_bias=..., delta_softplus=..., return_last_state=...
    ):  # -> tuple[Any, Any]:
        ...
    @staticmethod
    def backward(
        ctx, dout, *args
    ):  # -> tuple[Any, Any, Any, Any, Any, Any | None, Any | None, Any | None, None, None]:
        ...

def rms_norm_forward(x, weight, bias, eps=..., is_rms_norm=...): ...
def selective_scan_fn(
    u, delta, A, B, C, D=..., z=..., delta_bias=..., delta_softplus=..., return_last_state=...
):  # -> Any | None:
    """if return_last_state is True, returns (out, last_state)
    last_state has shape (batch, dim, dstate). Note that the gradient of the last state is
    not considered in the backward pass.
    """
    ...

def selective_scan_ref(
    u, delta, A, B, C, D=..., z=..., delta_bias=..., delta_softplus=..., return_last_state=...
):  # -> Tensor | tuple[Tensor | Any, Any | None]:
    """
    u: r(B D L)
    delta: r(B D L)
    A: c(D N) or r(D N)
    B: c(D N) or r(B N L) or r(B N 2L) or r(B G N L) or (B G N L)
    C: c(D N) or r(B N L) or r(B N 2L) or r(B G N L) or (B G N L)
    D: r(D)
    z: r(B D L)
    delta_bias: r(D), fp32

    out: r(B D L)
    last_state (optional): r(B D dstate) or c(B D dstate)
    """
    ...

class MambaInnerFn(torch.autograd.Function):
    @staticmethod
    @custom_fwd
    def forward(
        ctx,
        xz,
        conv1d_weight,
        conv1d_bias,
        x_proj_weight,
        delta_proj_weight,
        out_proj_weight,
        out_proj_bias,
        A,
        B=...,
        C=...,
        D=...,
        delta_bias=...,
        B_proj_bias=...,
        C_proj_bias=...,
        delta_softplus=...,
        checkpoint_lvl=...,
        b_rms_weight=...,
        c_rms_weight=...,
        dt_rms_weight=...,
        b_c_dt_rms_eps=...,
    ):
        """
        xz: (batch, dim, seqlen)
        """
        ...

    @staticmethod
    @custom_bwd
    def backward(
        ctx, dout
    ):  # -> tuple[Tensor, Any, Any | None, Tensor, Tensor, Tensor, Any | None, Any, Any | None, Any | None, Any | None, Any | None, Any | None, Any | None, None, None, None, None, None, None]:
        ...

def mamba_inner_fn(
    xz,
    conv1d_weight,
    conv1d_bias,
    x_proj_weight,
    delta_proj_weight,
    out_proj_weight,
    out_proj_bias,
    A,
    B=...,
    C=...,
    D=...,
    delta_bias=...,
    B_proj_bias=...,
    C_proj_bias=...,
    delta_softplus=...,
    checkpoint_lvl=...,
    b_rms_weight=...,
    c_rms_weight=...,
    dt_rms_weight=...,
    b_c_dt_rms_eps=...,
):  # -> Any | None:
    ...
