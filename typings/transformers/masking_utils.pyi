"""
This type stub file was generated by pyright.
"""

import torch
from typing import Callable, Optional, Union
from .cache_utils import Cache
from .configuration_utils import PretrainedConfig
from .utils.generic import GeneralInterface
from .utils.import_utils import is_torch_flex_attn_available
from torch.nn.attention.flex_attention import BlockMask

if is_torch_flex_attn_available(): ...
else:
    BlockMask = ...
_is_torch_greater_or_equal_than_2_5 = ...
_is_torch_greater_or_equal_than_2_6 = ...
_is_torch_xpu_available = ...
if _is_torch_greater_or_equal_than_2_6: ...

def and_masks(*mask_functions: list[Callable]) -> Callable: ...
def or_masks(*mask_functions: list[Callable]) -> Callable: ...
def causal_mask_function(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool: ...
def sliding_window_overlay(sliding_window: int) -> Callable: ...
def chunked_overlay(chunk_size: int) -> Callable: ...
def sliding_window_causal_mask_function(sliding_window: int) -> Callable: ...
def chunked_causal_mask_function(chunk_size: int) -> Callable: ...
def padding_mask_function(padding_mask: torch.Tensor) -> Callable: ...
def packed_sequence_mask_function(packed_sequence_mask: torch.Tensor) -> Callable: ...
def add_offsets_to_mask_function(mask_function: Callable, q_offset: int, kv_offset: int) -> Callable: ...
def prepare_padding_mask(
    attention_mask: Optional[torch.Tensor], kv_length: int, kv_offset: int, _slice: bool = ...
) -> Optional[torch.Tensor]: ...
def sdpa_mask_recent_torch(
    batch_size: int,
    cache_position: torch.Tensor,
    kv_length: int,
    kv_offset: int = ...,
    mask_function: Callable = ...,
    attention_mask: Optional[torch.Tensor] = ...,
    local_size: Optional[int] = ...,
    allow_is_causal_skip: bool = ...,
    **kwargs,
) -> Optional[torch.Tensor]: ...
def sdpa_mask_older_torch(
    batch_size: int,
    cache_position: torch.Tensor,
    kv_length: int,
    kv_offset: int = ...,
    mask_function: Callable = ...,
    attention_mask: Optional[torch.Tensor] = ...,
    local_size: Optional[int] = ...,
    allow_is_causal_skip: bool = ...,
    allow_torch_fix: bool = ...,
    **kwargs,
) -> Optional[torch.Tensor]: ...

sdpa_mask = ...

def eager_mask(
    batch_size: int,
    cache_position: torch.Tensor,
    kv_length: int,
    kv_offset: int = ...,
    mask_function: Callable = ...,
    attention_mask: Optional[torch.Tensor] = ...,
    dtype: torch.dtype = ...,
    **kwargs,
) -> torch.Tensor: ...
def flash_attention_mask(
    batch_size: int,
    cache_position: torch.Tensor,
    kv_length: int,
    kv_offset: int = ...,
    mask_function: Callable = ...,
    attention_mask: Optional[torch.Tensor] = ...,
    **kwargs,
): ...
def flex_attention_mask(
    batch_size: int,
    cache_position: torch.Tensor,
    kv_length: int,
    kv_offset: int = ...,
    mask_function: Callable = ...,
    attention_mask: Optional[torch.Tensor] = ...,
    **kwargs,
) -> BlockMask: ...

class AttentionMaskInterface(GeneralInterface):
    _global_mapping = ...

ALL_MASK_ATTENTION_FUNCTIONS: AttentionMaskInterface = ...

def find_packed_sequence_indices(position_ids: torch.Tensor) -> torch.Tensor: ...
def create_causal_mask(
    config: PretrainedConfig,
    input_embeds: torch.Tensor,
    attention_mask: Optional[torch.Tensor],
    cache_position: torch.Tensor,
    past_key_values: Optional[Cache],
    position_ids: Optional[torch.Tensor] = ...,
    or_mask_function: Optional[Callable] = ...,
    and_mask_function: Optional[Callable] = ...,
) -> Optional[Union[torch.Tensor, BlockMask]]: ...
def create_sliding_window_causal_mask(
    config: PretrainedConfig,
    input_embeds: torch.Tensor,
    attention_mask: Optional[torch.Tensor],
    cache_position: torch.Tensor,
    past_key_values: Optional[Cache],
    position_ids: Optional[torch.Tensor] = ...,
    or_mask_function: Optional[Callable] = ...,
    and_mask_function: Optional[Callable] = ...,
) -> Optional[Union[torch.Tensor, BlockMask]]: ...
def create_chunked_causal_mask(
    config: PretrainedConfig,
    input_embeds: torch.Tensor,
    attention_mask: Optional[torch.Tensor],
    cache_position: torch.Tensor,
    past_key_values: Optional[Cache],
    position_ids: Optional[torch.Tensor] = ...,
    or_mask_function: Optional[Callable] = ...,
    and_mask_function: Optional[Callable] = ...,
) -> Optional[Union[torch.Tensor, BlockMask]]: ...

LAYER_PATTERN_TO_MASK_FUNCTION_MAPPING = ...

def create_masks_for_generate(
    config: PretrainedConfig,
    input_embeds: torch.Tensor,
    attention_mask: Optional[torch.Tensor],
    cache_position: torch.Tensor,
    past_key_values: Optional[Cache],
    position_ids: Optional[torch.Tensor] = ...,
    or_mask_function: Optional[Callable] = ...,
    and_mask_function: Optional[Callable] = ...,
    **kwargs,
): ...

GREEN = ...
YELLOW = ...
RESET = ...
BLACK_SQUARE = ...
WHITE_SQUARE = ...
GREY_SQUARE = ...
LOW_TRIANGLE = ...
UPPER_TRIANGLE = ...

def get_style(style): ...

YELLOW_SQUARE = ...
GREEN_SQUARE = ...

def tensor_to_mask_visual(original_tensor: torch.Tensor, grid_size=..., style=...) -> str: ...

class AttentionMask(torch.Tensor):
    def __new__(cls, data, style=...): ...
    def __init__(self, data) -> None: ...
    def to_string(self, grid_size=..., limit=...): ...
    def __repr__(self): ...
    @classmethod
    def from_tensor(cls, tensor: torch.Tensor, style: Optional[str] = ...) -> AttentionMask: ...
