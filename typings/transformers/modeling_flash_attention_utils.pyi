from typing import TypedDict

import torch

logger = ...

def flash_attn_supports_top_left_mask():  # -> bool:
    ...
def is_flash_attn_available():  # -> bool:
    ...

_flash_fn = ...
_flash_varlen_fn = ...
_pad_fn = ...
_unpad_fn = ...
_process_flash_kwargs_fn = ...
_hf_api_to_flash_mapping = ...

def lazy_import_flash_attention(
    implementation: str | None,
):  # -> tuple[tuple[Any | Callable[..., Any] | None, Any | Callable[..., Any] | None, Any | Callable[..., Tensor] | None, Any | Callable[..., tuple[Any, Tensor, Any, Any, Any]] | None], partial[dict[str, bool | float | None]]]:

    ...
def prepare_fa_kwargs_from_position_ids(
    position_ids, is_packed_sequence: bool = ...
):  # -> tuple[tuple[Tensor, Tensor], tuple[Number, Number]]:

    ...
def fa_peft_integration_check(
    q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, target_dtype: torch.dtype | None = ...
):  # -> tuple[Tensor, Tensor, Tensor]:

    ...

class FlashAttentionKwargs(TypedDict, total=False):
    cumulative_seqlens_q: torch.LongTensor | None
    cumulative_seqlens_k: torch.LongTensor | None
    max_length_q: int | None
    max_length_k: int | None
