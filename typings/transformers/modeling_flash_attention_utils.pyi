"""
This type stub file was generated by pyright.
"""

import torch
from typing import Optional, TypedDict

logger = ...

def flash_attn_supports_top_left_mask():  # -> bool:
    ...
def is_flash_attn_available():  # -> bool:
    ...

_flash_fn = ...
_flash_varlen_fn = ...
_pad_fn = ...
_unpad_fn = ...
_process_flash_kwargs_fn = ...
_hf_api_to_flash_mapping = ...

def lazy_import_flash_attention(
    implementation: Optional[str],
):  # -> tuple[tuple[Any | Callable[..., Any] | None, Any | Callable[..., Any] | None, Any | Callable[..., Tensor] | None, Any | Callable[..., tuple[Any, Tensor, Any, Any, Any]] | None], partial[dict[str, bool | float | None]]]:
    """
    Lazy loading flash attention and returning the respective functions + flags back

    NOTE: For fullgraph, this needs to be called before compile while no fullgraph can
          can work without preloading. See `_check_and_adjust_attn_implementation` in `modeling_utils`.
    """
    ...

def prepare_fa_kwargs_from_position_ids(
    position_ids, is_packed_sequence: bool = ...
):  # -> tuple[tuple[Tensor, Tensor], tuple[Number, Number]]:
    """
    This function returns all the necessary kwargs to call `flash_attn_varlen_func`
    extracted from position_ids. The `position_ids` can be either packed sequence or
    the usual padded position ids, for example in inference time.

    Arguments:
        position_ids (`torch.Tensor`):
            Boolean or int tensor of shape (batch_size, sequence_length), 1 means valid and 0 means not valid.
        is_packed_sequence (`bool`, *optional*, defaults to `True`):
            Whether the input position ids are a packed sequence or not.

    Return:
        (cu_seqlens_q, cu_seqlens_k) (`tuple[int]`):
            The cumulative sequence lengths for the target (query) and source (key, value), used to index into
            ragged (unpadded) tensors. `cu_seqlens` shape is (batch_size + 1,).
        (max_seqlen_in_batch_q, max_seqlen_in_batch_k) (`tuple[int]`):
            Maximum sequence length in batch (`max_seqlen_in_batch_q` for the target sequence i.e. query,
            `max_seqlen_in_batch_k` for the source sequence i.e. key/value).
    """
    ...

def fa_peft_integration_check(
    q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, target_dtype: Optional[torch.dtype] = ...
):  # -> tuple[Tensor, Tensor, Tensor]:
    """
    PEFT usually casts the layer norms in float32 for training stability reasons
    therefore the input hidden states gets silently casted in float32. Hence, we need
    cast them back in float16 / bfloat16 just to be sure everything works as expected.
    This might slowdown training & inference so it is recommended to not cast the LayerNorms!
    """
    ...

class FlashAttentionKwargs(TypedDict, total=False):
    """
    Keyword arguments for Flash Attention with Compile.

    Attributes:
        cumulative_seqlens_q (`torch.LongTensor`, *optional*)
            Gets cumulative sequence length for query state.
        cumulative_seqlens_k (`torch.LongTensor`, *optional*)
            Gets cumulative sequence length for key state.
        max_length_q (`int`, *optional*):
            Maximum sequence length for query state.
        max_length_k (`int`, *optional*):
            Maximum sequence length for key state.
    """

    cumulative_seqlens_q: Optional[torch.LongTensor]
    cumulative_seqlens_k: Optional[torch.LongTensor]
    max_length_q: Optional[int]
    max_length_k: Optional[int]
    ...
