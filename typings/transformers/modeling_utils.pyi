"""
This type stub file was generated by pyright.
"""

import os
import torch
from abc import abstractmethod
from contextlib import contextmanager
from enum import Enum
from functools import wraps
from typing import Any, Callable, Optional, Self, TypeVar, Union
from torch import Tensor, nn
from transformers.utils import is_torchao_available
from .configuration_utils import PretrainedConfig
from .generation import CompileConfig
from .integrations import PeftAdapterMixin
from .quantizers import HfQuantizer
from .utils import (
    PushToHubMixin,
    is_accelerate_available,
    is_kernels_available,
    is_peft_available,
    is_safetensors_available,
    is_torch_greater_or_equal,
)
from .utils.generic import GeneralInterface, OutputRecorder
from .utils.import_utils import is_sagemaker_mp_enabled

if is_torchao_available(): ...
XLA_USE_BF16 = ...
XLA_DOWNCAST_BF16 = ...
if is_accelerate_available():
    accelerate_version = ...
if is_safetensors_available(): ...
if is_kernels_available(): ...
logger = ...
_init_weights = ...
_is_quantized = ...
_is_ds_init_called = ...
_torch_distributed_available = ...
_is_dtensor_available = ...
if _is_dtensor_available: ...

def is_fsdp_enabled(): ...
def is_local_dist_rank_0(): ...

if is_sagemaker_mp_enabled():
    IS_SAGEMAKER_MP_POST_1_10 = ...
else:
    IS_SAGEMAKER_MP_POST_1_10 = ...
if is_peft_available(): ...
SpecificPreTrainedModelType = TypeVar("SpecificPreTrainedModelType", bound=PreTrainedModel)
TORCH_INIT_FUNCTIONS = ...
VLMS = ...

@contextmanager
def no_init_weights(): ...
@contextmanager
def set_quantized_state(): ...
@contextmanager
def set_zero3_state(): ...
def restore_default_torch_dtype(func): ...
def get_torch_context_manager_or_global_device(): ...
def get_parameter_device(parameter: Union[nn.Module, ModuleUtilsMixin]): ...
def get_parameter_dtype(parameter: Union[nn.Module, ModuleUtilsMixin]): ...
def get_state_dict_dtype(state_dict): ...
def load_sharded_checkpoint(model, folder, strict=..., prefer_safe=...): ...

str_to_torch_dtype = ...
if is_torch_greater_or_equal("2.3.0"): ...

def load_state_dict(
    checkpoint_file: Union[str, os.PathLike],
    is_quantized: bool = ...,
    map_location: Optional[Union[str, torch.device]] = ...,
    weights_only: bool = ...,
): ...
def set_initialized_submodules(model, state_dict_keys): ...
def load_shard_file(args): ...
def load_shard_files_with_threadpool(args_list): ...

class PipelineParallel(Enum):
    inputs: 0
    outputs: 1
    ...

class ModuleUtilsMixin:
    def add_memory_hooks(self): ...
    def reset_memory_hooks_state(self): ...
    @property
    def device(self) -> torch.device: ...
    @property
    def dtype(self) -> torch.dtype: ...
    def invert_attention_mask(self, encoder_attention_mask: Tensor) -> Tensor: ...
    @staticmethod
    def create_extended_attention_mask_for_decoder(input_shape, attention_mask, device=...): ...
    def get_extended_attention_mask(
        self, attention_mask: Tensor, input_shape: tuple[int], device: torch.device = ..., dtype: torch.float = ...
    ) -> Tensor: ...
    def get_head_mask(
        self, head_mask: Optional[Tensor], num_hidden_layers: int, is_attention_chunked: bool = ...
    ) -> Tensor: ...
    def num_parameters(self, only_trainable: bool = ..., exclude_embeddings: bool = ...) -> int: ...
    def estimate_tokens(self, input_dict: dict[str, Union[torch.Tensor, Any]]) -> int: ...
    def floating_point_ops(
        self, input_dict: dict[str, Union[torch.Tensor, Any]], exclude_embeddings: bool = ...
    ) -> int: ...

class EmbeddingAccessMixin:
    _input_embed_layer = ...
    def get_input_embeddings(self) -> nn.Module: ...
    def set_input_embeddings(self, value: nn.Module): ...
    def get_output_embeddings(self): ...
    def set_output_embeddings(self, new_embeddings): ...

class PreTrainedModel(nn.Module, EmbeddingAccessMixin, ModuleUtilsMixin, PushToHubMixin, PeftAdapterMixin):
    config_class = ...
    base_model_prefix = ...
    main_input_name = ...
    model_tags = ...
    _checkpoint_conversion_mapping = ...
    _auto_class = ...
    _no_split_modules = ...
    _skip_keys_device_placement = ...
    _keep_in_fp32_modules = ...
    _keep_in_fp32_modules_strict = ...
    _keys_to_ignore_on_load_missing = ...
    _keys_to_ignore_on_load_unexpected = ...
    _keys_to_ignore_on_save = ...
    _tied_weights_keys = ...
    is_parallelizable = ...
    supports_gradient_checkpointing = ...
    _is_stateful = ...
    _supports_flash_attn = ...
    _supports_sdpa = ...
    _supports_flex_attn = ...
    _can_compile_fullgraph = ...
    _tp_plan = ...
    _tp_size = ...
    _pp_plan = ...
    _supports_attention_backend = ...
    _can_record_outputs = ...
    @property
    @torch._dynamo.allow_in_graph
    def can_record_outputs(self) -> dict[str, OutputRecorder]: ...
    @property
    def dummy_inputs(self) -> dict[str, torch.Tensor]: ...
    @property
    def framework(self) -> str: ...
    def __init_subclass__(cls, **kwargs): ...
    def __init__(self, config: PretrainedConfig, *inputs, **kwargs) -> None: ...
    def post_init(self): ...
    def dequantize(self): ...
    def add_model_tags(self, tags: Union[list[str], str]) -> None: ...
    @property
    def base_model(self) -> nn.Module: ...
    @classmethod
    def can_generate(cls) -> bool: ...
    def get_correct_attn_implementation(self, _requested_attention: str, is_init_check: bool = ...) -> str: ...
    def set_attn_implementation(self, attn_implementation: Union[str, dict]): ...
    def enable_input_require_grads(self): ...
    def disable_input_require_grads(self): ...
    @torch.no_grad()
    def initialize_weights(self): ...
    def tie_weights(self): ...
    def resize_token_embeddings(
        self, new_num_tokens: Optional[int] = ..., pad_to_multiple_of: Optional[int] = ..., mean_resizing: bool = ...
    ) -> nn.Embedding: ...
    def resize_position_embeddings(self, new_num_position_embeddings: int): ...
    def get_position_embeddings(self) -> Union[nn.Embedding, tuple[nn.Embedding]]: ...
    def init_weights(self): ...
    def prune_heads(self, heads_to_prune: dict[int, list[int]]): ...
    def gradient_checkpointing_enable(self, gradient_checkpointing_kwargs=...): ...
    def gradient_checkpointing_disable(self): ...
    @property
    def is_gradient_checkpointing(self) -> bool: ...
    def save_pretrained(
        self,
        save_directory: Union[str, os.PathLike],
        is_main_process: bool = ...,
        state_dict: Optional[dict] = ...,
        save_function: Callable = ...,
        push_to_hub: bool = ...,
        max_shard_size: Union[int, str] = ...,
        safe_serialization: bool = ...,
        variant: Optional[str] = ...,
        token: Optional[Union[str, bool]] = ...,
        save_peft_format: bool = ...,
        **kwargs,
    ): ...
    @wraps(PushToHubMixin.push_to_hub)
    def push_to_hub(self, *args, **kwargs): ...
    def get_memory_footprint(self, return_buffers=...): ...
    @wraps(torch.nn.Module.cuda)
    def cuda(self, *args, **kwargs): ...
    @wraps(torch.nn.Module.to)
    def to(self, *args, **kwargs): ...
    def half(self, *args): ...
    def float(self, *args): ...
    @classmethod
    def get_init_context(cls, is_quantized: bool, _is_ds_init_called: bool): ...
    @classmethod
    @restore_default_torch_dtype
    def from_pretrained(
        cls,
        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],
        *model_args,
        config: Optional[Union[PretrainedConfig, str, os.PathLike]] = ...,
        cache_dir: Optional[Union[str, os.PathLike]] = ...,
        ignore_mismatched_sizes: bool = ...,
        force_download: bool = ...,
        local_files_only: bool = ...,
        token: Optional[Union[str, bool]] = ...,
        revision: str = ...,
        use_safetensors: Optional[bool] = ...,
        weights_only: bool = ...,
        **kwargs,
    ) -> Self: ...
    def retrieve_modules_from_names(self, names, add_prefix=..., remove_prefix=...): ...
    @classmethod
    def register_for_auto_class(cls, auto_class=...): ...
    def to_bettertransformer(self) -> PreTrainedModel: ...
    def reverse_bettertransformer(self): ...
    def warn_if_padding_and_no_attention_mask(self, input_ids, attention_mask): ...
    @property
    def supports_tp_plan(self): ...
    @property
    def tp_size(self): ...
    @property
    def supports_pp_plan(self): ...
    @property
    def loss_function(self): ...
    @loss_function.setter
    def loss_function(self, value): ...
    def get_compiled_call(self, compile_config: Optional[CompileConfig]) -> Callable: ...
    @classmethod
    def is_backend_compatible(cls): ...
    def get_parameter_or_buffer(self, target: str): ...

if PreTrainedModel.push_to_hub.__doc__ is not None: ...

def unwrap_model(model: nn.Module, recursive: bool = ...) -> nn.Module: ...
def expand_device_map(device_map, param_names): ...
def is_accelerator_device(device: Union[str, int, torch.device]) -> bool: ...
def caching_allocator_warmup(
    model: PreTrainedModel, expanded_device_map: dict, hf_quantizer: Optional[HfQuantizer]
): ...
def get_disk_only_shard_files(device_map, weight_map): ...

class AttentionInterface(GeneralInterface):
    _global_mapping = ...

ALL_ATTENTION_FUNCTIONS: AttentionInterface = ...

class PreTrainedAudioTokenizerBase(PreTrainedModel):
    @abstractmethod
    def encode(self, input_values: torch.Tensor, *args, **kwargs): ...
    @abstractmethod
    def decode(self, audio_codes: torch.Tensor, *args, **kwargs): ...
