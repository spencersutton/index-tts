"""
This type stub file was generated by pyright.
"""

import flax
import flax.linen as nn
import jax
import jax.numpy as jnp
import numpy as np
from typing import Callable, Optional
from flax.core.frozen_dict import FrozenDict
from ...modeling_flax_utils import FlaxPreTrainedModel
from ...utils import ModelOutput, add_start_docstrings, add_start_docstrings_to_model_forward
from .configuration_albert import AlbertConfig

logger = ...
_CHECKPOINT_FOR_DOC = ...
_CONFIG_FOR_DOC = ...

@flax.struct.dataclass
class FlaxAlbertForPreTrainingOutput(ModelOutput):
    prediction_logits: jnp.ndarray = ...
    sop_logits: jnp.ndarray = ...
    hidden_states: Optional[tuple[jnp.ndarray]] = ...
    attentions: Optional[tuple[jnp.ndarray]] = ...

ALBERT_START_DOCSTRING = ...
ALBERT_INPUTS_DOCSTRING = ...

class FlaxAlbertEmbeddings(nn.Module):
    config: AlbertConfig
    dtype: jnp.dtype = ...
    def setup(self): ...
    def __call__(self, input_ids, token_type_ids, position_ids, deterministic: bool = ...): ...

class FlaxAlbertSelfAttention(nn.Module):
    config: AlbertConfig
    dtype: jnp.dtype = ...
    def setup(self): ...
    def __call__(self, hidden_states, attention_mask, deterministic=..., output_attentions: bool = ...): ...

class FlaxAlbertLayer(nn.Module):
    config: AlbertConfig
    dtype: jnp.dtype = ...
    def setup(self): ...
    def __call__(self, hidden_states, attention_mask, deterministic: bool = ..., output_attentions: bool = ...): ...

class FlaxAlbertLayerCollection(nn.Module):
    config: AlbertConfig
    dtype: jnp.dtype = ...
    def setup(self): ...
    def __call__(
        self,
        hidden_states,
        attention_mask,
        deterministic: bool = ...,
        output_attentions: bool = ...,
        output_hidden_states: bool = ...,
    ): ...

class FlaxAlbertLayerCollections(nn.Module):
    config: AlbertConfig
    dtype: jnp.dtype = ...
    layer_index: Optional[str] = ...
    def setup(self): ...
    def __call__(
        self,
        hidden_states,
        attention_mask,
        deterministic: bool = ...,
        output_attentions: bool = ...,
        output_hidden_states: bool = ...,
    ): ...

class FlaxAlbertLayerGroups(nn.Module):
    config: AlbertConfig
    dtype: jnp.dtype = ...
    def setup(self): ...
    def __call__(
        self,
        hidden_states,
        attention_mask,
        deterministic: bool = ...,
        output_attentions: bool = ...,
        output_hidden_states: bool = ...,
        return_dict: bool = ...,
    ): ...

class FlaxAlbertEncoder(nn.Module):
    config: AlbertConfig
    dtype: jnp.dtype = ...
    def setup(self): ...
    def __call__(
        self,
        hidden_states,
        attention_mask,
        deterministic: bool = ...,
        output_attentions: bool = ...,
        output_hidden_states: bool = ...,
        return_dict: bool = ...,
    ): ...

class FlaxAlbertOnlyMLMHead(nn.Module):
    config: AlbertConfig
    dtype: jnp.dtype = ...
    bias_init: Callable[..., np.ndarray] = ...
    def setup(self): ...
    def __call__(self, hidden_states, shared_embedding=...): ...

class FlaxAlbertSOPHead(nn.Module):
    config: AlbertConfig
    dtype: jnp.dtype = ...
    def setup(self): ...
    def __call__(self, pooled_output, deterministic=...): ...

class FlaxAlbertPreTrainedModel(FlaxPreTrainedModel):
    config_class = AlbertConfig
    base_model_prefix = ...
    module_class: nn.Module = ...
    def __init__(
        self,
        config: AlbertConfig,
        input_shape: tuple = ...,
        seed: int = ...,
        dtype: jnp.dtype = ...,
        _do_init: bool = ...,
        **kwargs,
    ) -> None: ...
    def init_weights(self, rng: jax.random.PRNGKey, input_shape: tuple, params: FrozenDict = ...) -> FrozenDict: ...
    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format("batch_size, sequence_length"))
    def __call__(
        self,
        input_ids,
        attention_mask=...,
        token_type_ids=...,
        position_ids=...,
        params: Optional[dict] = ...,
        dropout_rng: jax.random.PRNGKey = ...,
        train: bool = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ): ...

class FlaxAlbertModule(nn.Module):
    config: AlbertConfig
    dtype: jnp.dtype = ...
    add_pooling_layer: bool = ...
    def setup(self): ...
    def __call__(
        self,
        input_ids,
        attention_mask,
        token_type_ids: Optional[np.ndarray] = ...,
        position_ids: Optional[np.ndarray] = ...,
        deterministic: bool = ...,
        output_attentions: bool = ...,
        output_hidden_states: bool = ...,
        return_dict: bool = ...,
    ): ...

@add_start_docstrings(..., ALBERT_START_DOCSTRING)
class FlaxAlbertModel(FlaxAlbertPreTrainedModel):
    module_class = ...

class FlaxAlbertForPreTrainingModule(nn.Module):
    config: AlbertConfig
    dtype: jnp.dtype = ...
    def setup(self): ...
    def __call__(
        self,
        input_ids,
        attention_mask,
        token_type_ids,
        position_ids,
        deterministic: bool = ...,
        output_attentions: bool = ...,
        output_hidden_states: bool = ...,
        return_dict: bool = ...,
    ): ...

@add_start_docstrings(..., ALBERT_START_DOCSTRING)
class FlaxAlbertForPreTraining(FlaxAlbertPreTrainedModel):
    module_class = ...

FLAX_ALBERT_FOR_PRETRAINING_DOCSTRING = ...

class FlaxAlbertForMaskedLMModule(nn.Module):
    config: AlbertConfig
    dtype: jnp.dtype = ...
    def setup(self): ...
    def __call__(
        self,
        input_ids,
        attention_mask,
        token_type_ids,
        position_ids,
        deterministic: bool = ...,
        output_attentions: bool = ...,
        output_hidden_states: bool = ...,
        return_dict: bool = ...,
    ): ...

@add_start_docstrings(..., ALBERT_START_DOCSTRING)
class FlaxAlbertForMaskedLM(FlaxAlbertPreTrainedModel):
    module_class = ...

class FlaxAlbertForSequenceClassificationModule(nn.Module):
    config: AlbertConfig
    dtype: jnp.dtype = ...
    def setup(self): ...
    def __call__(
        self,
        input_ids,
        attention_mask,
        token_type_ids,
        position_ids,
        deterministic: bool = ...,
        output_attentions: bool = ...,
        output_hidden_states: bool = ...,
        return_dict: bool = ...,
    ): ...

@add_start_docstrings(..., ALBERT_START_DOCSTRING)
class FlaxAlbertForSequenceClassification(FlaxAlbertPreTrainedModel):
    module_class = ...

class FlaxAlbertForMultipleChoiceModule(nn.Module):
    config: AlbertConfig
    dtype: jnp.dtype = ...
    def setup(self): ...
    def __call__(
        self,
        input_ids,
        attention_mask,
        token_type_ids,
        position_ids,
        deterministic: bool = ...,
        output_attentions: bool = ...,
        output_hidden_states: bool = ...,
        return_dict: bool = ...,
    ): ...

@add_start_docstrings(..., ALBERT_START_DOCSTRING)
class FlaxAlbertForMultipleChoice(FlaxAlbertPreTrainedModel):
    module_class = ...

class FlaxAlbertForTokenClassificationModule(nn.Module):
    config: AlbertConfig
    dtype: jnp.dtype = ...
    def setup(self): ...
    def __call__(
        self,
        input_ids,
        attention_mask,
        token_type_ids,
        position_ids,
        deterministic: bool = ...,
        output_attentions: bool = ...,
        output_hidden_states: bool = ...,
        return_dict: bool = ...,
    ): ...

@add_start_docstrings(..., ALBERT_START_DOCSTRING)
class FlaxAlbertForTokenClassification(FlaxAlbertPreTrainedModel):
    module_class = ...

class FlaxAlbertForQuestionAnsweringModule(nn.Module):
    config: AlbertConfig
    dtype: jnp.dtype = ...
    def setup(self): ...
    def __call__(
        self,
        input_ids,
        attention_mask,
        token_type_ids,
        position_ids,
        deterministic: bool = ...,
        output_attentions: bool = ...,
        output_hidden_states: bool = ...,
        return_dict: bool = ...,
    ): ...

@add_start_docstrings(..., ALBERT_START_DOCSTRING)
class FlaxAlbertForQuestionAnswering(FlaxAlbertPreTrainedModel):
    module_class = ...

__all__ = [
    "FlaxAlbertPreTrainedModel",
    "FlaxAlbertModel",
    "FlaxAlbertForPreTraining",
    "FlaxAlbertForMaskedLM",
    "FlaxAlbertForSequenceClassification",
    "FlaxAlbertForMultipleChoice",
    "FlaxAlbertForTokenClassification",
    "FlaxAlbertForQuestionAnswering",
]
