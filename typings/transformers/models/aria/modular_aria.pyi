"""
This type stub file was generated by pyright.
"""

import numpy as np
import torch
from collections.abc import Iterable
from typing import Optional, Union
from ...cache_utils import Cache
from ...configuration_utils import PretrainedConfig
from ...image_processing_utils import BaseImageProcessor, BatchFeature
from ...image_transforms import PaddingMode
from ...image_utils import ChannelDimension, ImageInput, PILImageResampling
from ...modeling_flash_attention_utils import FlashAttentionKwargs
from ...modeling_utils import PreTrainedModel
from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack
from ...tokenization_utils import PreTokenizedInput, TextInput
from ...utils import TensorType, TransformersKwargs, auto_docstring, can_return_tuple
from ...utils.import_utils import is_torch_available
from ..auto import AutoTokenizer
from ..llama.configuration_llama import LlamaConfig
from ..llama.modeling_llama import (
    LlamaAttention,
    LlamaDecoderLayer,
    LlamaForCausalLM,
    LlamaMLP,
    LlamaModel,
    LlamaPreTrainedModel,
    LlamaRMSNorm,
)
from ..llava.modeling_llava import (
    LlavaCausalLMOutputWithPast,
    LlavaForConditionalGeneration,
    LlavaModel,
    LlavaModelOutputWithPast,
)
from torch import nn

logger = ...
if is_torch_available(): ...

def sequential_experts_gemm(token_states, expert_weights, tokens_per_expert): ...

class AriaTextConfig(LlamaConfig):
    model_type = ...
    base_config_key = ...
    def __init__(
        self,
        intermediate_size: int = ...,
        moe_num_experts: int = ...,
        moe_topk: int = ...,
        moe_num_shared_experts: int = ...,
        pad_token_id=...,
        **super_kwargs,
    ) -> None: ...

class AriaConfig(PretrainedConfig):
    model_type = ...
    attribute_map = ...
    sub_configs = ...
    def __init__(
        self,
        vision_config=...,
        vision_feature_layer: int = ...,
        text_config: AriaTextConfig = ...,
        projector_patch_to_query_dict: Optional[dict] = ...,
        image_token_index: int = ...,
        initializer_range: float = ...,
        **kwargs,
    ) -> None: ...

class AriaTextRMSNorm(LlamaRMSNorm): ...

class AriaProjectorMLP(nn.Module):
    def __init__(self, in_features, hidden_features, output_dim) -> None: ...
    def forward(self, hidden_states): ...

class AriaCrossAttention(nn.Module):
    def __init__(self, config: AriaConfig, dropout_rate: float = ...) -> None: ...
    def forward(self, key_value_states, hidden_states, attn_mask=...): ...

class AriaProjector(nn.Module):
    def __init__(self, config: AriaConfig) -> None: ...
    def forward(self, key_value_states: torch.Tensor, attn_mask: Optional[torch.Tensor] = ...): ...

class AriaImageProcessor(BaseImageProcessor):
    model_input_names = ...
    def __init__(
        self,
        image_mean: Optional[list[float]] = ...,
        image_std: Optional[list[float]] = ...,
        max_image_size: int = ...,
        min_image_size: int = ...,
        split_resolutions: Optional[list[tuple[int, int]]] = ...,
        split_image: Optional[bool] = ...,
        do_convert_rgb: Optional[bool] = ...,
        do_rescale: bool = ...,
        rescale_factor: float = ...,
        do_normalize: Optional[bool] = ...,
        resample: PILImageResampling = ...,
        **kwargs,
    ) -> None: ...
    def preprocess(
        self,
        images: Union[ImageInput, list[ImageInput]],
        image_mean: Optional[Union[float, list[float]]] = ...,
        image_std: Optional[Union[float, list[float]]] = ...,
        max_image_size: Optional[int] = ...,
        min_image_size: Optional[int] = ...,
        split_image: Optional[bool] = ...,
        do_convert_rgb: Optional[bool] = ...,
        do_rescale: Optional[bool] = ...,
        rescale_factor: Optional[float] = ...,
        do_normalize: Optional[bool] = ...,
        resample: PILImageResampling = ...,
        return_tensors: Optional[Union[str, TensorType]] = ...,
        data_format: Optional[ChannelDimension] = ...,
        input_data_format: Optional[Union[str, ChannelDimension]] = ...,
    ): ...
    def pad(
        self,
        image: np.ndarray,
        padding: Union[int, tuple[int, int], Iterable[tuple[int, int]]],
        mode: PaddingMode = ...,
        constant_values: Union[float, Iterable[float]] = ...,
        data_format: Optional[Union[str, ChannelDimension]] = ...,
        input_data_format: Optional[Union[str, ChannelDimension]] = ...,
    ) -> np.ndarray: ...
    def get_image_patches(
        self,
        image: np.array,
        grid_pinpoints: list[tuple[int, int]],
        patch_size: int,
        resample: PILImageResampling,
        data_format: ChannelDimension,
        input_data_format: ChannelDimension,
    ) -> list[np.array]: ...
    def get_number_of_image_patches(self, height: int, width: int, images_kwargs=...): ...

class AriaProcessorKwargs(ProcessingKwargs, total=False):
    _defaults = ...

class AriaProcessor(ProcessorMixin):
    attributes = ...
    image_processor_class = ...
    tokenizer_class = ...
    def __init__(
        self,
        image_processor=...,
        tokenizer: Union[AutoTokenizer, str] = ...,
        chat_template: Optional[str] = ...,
        size_conversion: Optional[dict[Union[float, int], int]] = ...,
    ) -> None: ...
    def __call__(
        self,
        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]],
        images: Optional[ImageInput] = ...,
        audio=...,
        videos=...,
        **kwargs: Unpack[AriaProcessorKwargs],
    ) -> BatchFeature: ...
    def batch_decode(self, *args, **kwargs): ...
    def decode(self, *args, **kwargs): ...
    @property
    def model_input_names(self): ...

class AriaSharedExpertsMLP(LlamaMLP):
    def __init__(self, config: AriaTextConfig) -> None: ...

class AriaGroupedExpertsGemm(nn.Module):
    def __init__(self, in_features, out_features, groups) -> None: ...
    def forward(self, input, tokens_per_expert): ...

class AriaGroupedExpertsMLP(nn.Module):
    def __init__(self, config: AriaTextConfig) -> None: ...
    def forward(self, permuted_tokens, tokens_per_expert): ...

class AriaTextMoELayer(nn.Module):
    def __init__(self, config: AriaTextConfig) -> None: ...
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor: ...

class AriaTextAttention(LlamaAttention):
    def __init__(self, config: AriaTextConfig, layer_idx: int) -> None: ...

class AriaTextDecoderLayer(LlamaDecoderLayer):
    def __init__(self, config: AriaTextConfig, layer_idx: int) -> None: ...

@auto_docstring
class AriaTextPreTrainedModel(PreTrainedModel):
    config: AriaTextConfig
    base_model_prefix = ...
    _no_split_modules = ...
    supports_gradient_checkpointing = ...
    _skip_keys_device_placement = ...
    _supports_flash_attn = ...
    _supports_sdpa = ...
    _supports_attention_backend = ...
    _can_record_outputs = ...

class AriaPreTrainedModel(LlamaPreTrainedModel):
    config: AriaConfig
    base_model_prefix = ...
    _can_compile_fullgraph = ...
    _supports_attention_backend = ...

class AriaTextModel(LlamaModel):
    def __init__(self, config: AriaTextConfig) -> None: ...

class AriaTextForCausalLM(AriaTextPreTrainedModel, LlamaForCausalLM):
    _tied_weights_keys = ...
    def __init__(self, config: AriaTextConfig) -> None: ...
    @auto_docstring
    def forward(self, **super_kwargs): ...

class AriaCausalLMOutputWithPast(LlavaCausalLMOutputWithPast): ...
class AriaModelOutputWithPast(LlavaModelOutputWithPast): ...

class AriaModel(LlavaModel):
    def __init__(self, config: AriaConfig) -> None: ...
    def get_image_features(
        self,
        pixel_values: torch.FloatTensor,
        pixel_mask: Optional[torch.FloatTensor] = ...,
        vision_feature_layer: int = ...,
    ): ...
    def forward(
        self,
        input_ids: torch.LongTensor = ...,
        pixel_values: torch.FloatTensor = ...,
        pixel_mask: torch.LongTensor = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        position_ids: Optional[torch.LongTensor] = ...,
        past_key_values: Optional[Cache] = ...,
        inputs_embeds: Optional[torch.FloatTensor] = ...,
        use_cache: Optional[bool] = ...,
        cache_position: Optional[torch.LongTensor] = ...,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> Union[tuple, AriaModelOutputWithPast]: ...

@auto_docstring(custom_intro=...)
class AriaForConditionalGeneration(LlavaForConditionalGeneration):
    def get_image_features(
        self,
        pixel_values: torch.FloatTensor,
        pixel_mask: Optional[torch.FloatTensor] = ...,
        vision_feature_layer: int = ...,
    ): ...
    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: torch.LongTensor = ...,
        pixel_values: torch.FloatTensor = ...,
        pixel_mask: torch.LongTensor = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        position_ids: Optional[torch.LongTensor] = ...,
        past_key_values: Optional[Cache] = ...,
        inputs_embeds: Optional[torch.FloatTensor] = ...,
        labels: Optional[torch.LongTensor] = ...,
        use_cache: Optional[bool] = ...,
        logits_to_keep: Union[int, torch.Tensor] = ...,
        cache_position: Optional[torch.LongTensor] = ...,
        **kwargs: Unpack[TransformersKwargs],
    ) -> Union[tuple, AriaCausalLMOutputWithPast]: ...
    def prepare_inputs_for_generation(
        self,
        input_ids,
        past_key_values=...,
        inputs_embeds=...,
        pixel_values=...,
        pixel_mask=...,
        attention_mask=...,
        cache_position=...,
        logits_to_keep=...,
        **kwargs,
    ): ...

__all__ = [
    "AriaConfig",
    "AriaTextConfig",
    "AriaImageProcessor",
    "AriaProcessor",
    "AriaForConditionalGeneration",
    "AriaPreTrainedModel",
    "AriaTextPreTrainedModel",
    "AriaTextModel",
    "AriaModel",
    "AriaTextForCausalLM",
]
