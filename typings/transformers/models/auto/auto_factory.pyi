"""
This type stub file was generated by pyright.
"""

import os
from collections import OrderedDict
from collections.abc import Iterator
from typing import Any, TypeVar, Union

from ...configuration_utils import PretrainedConfig
from ...utils import is_torch_available

"""Factory function to build auto-model classes."""
if is_torch_available(): ...
logger = ...
_T = TypeVar("_T")
_LazyAutoMappingValue = tuple[type[Any] | None, type[Any] | None]
CLASS_DOCSTRING = ...
FROM_CONFIG_DOCSTRING = ...
FROM_PRETRAINED_TORCH_DOCSTRING = ...
FROM_PRETRAINED_TF_DOCSTRING = ...
FROM_PRETRAINED_FLAX_DOCSTRING = ...

class _BaseAutoModelClass:
    _model_mapping = ...
    def __init__(self, *args, **kwargs) -> None: ...
    @classmethod
    def from_config(cls, config, **kwargs):  # -> Any:
        ...
    @classmethod
    def from_pretrained(cls, pretrained_model_name_or_path: str | os.PathLike[str], *model_args, **kwargs):  # -> Any:
        ...
    @classmethod
    def register(cls, config_class, model_class, exist_ok=...) -> None:
        """
        Register a new model for this class.

        Args:
            config_class ([`PretrainedConfig`]):
                The configuration corresponding to the model to register.
            model_class ([`PreTrainedModel`]):
                The model to register.
        """
        ...

class _BaseAutoBackboneClass(_BaseAutoModelClass):
    _model_mapping = ...
    @classmethod
    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):  # -> Any:
        ...

def insert_head_doc(docstring, head_doc: str = ...): ...
def auto_class_update(cls, checkpoint_for_example: str = ..., head_doc: str = ...): ...
def get_values(model_mapping):  # -> list[Any]:
    ...
def getattribute_from_module(
    module, attr
):  # -> tuple[tuple[Any, ...] | Any | None, ...] | Any | tuple[Any, ...] | None:
    ...
def add_generation_mixin_to_remote_model(model_class):  # -> Any:
    """
    Adds `GenerationMixin` to the inheritance of `model_class`, if `model_class` is a PyTorch model.

    This function is used for backwards compatibility purposes: in v4.45, we've started a deprecation cycle to make
    `PreTrainedModel` stop inheriting from `GenerationMixin`. Without this function, older models dynamically loaded
    from the Hub may not have the `generate` method after we remove the inheritance.
    """
    ...

class _LazyAutoMapping(OrderedDict[type[PretrainedConfig], _LazyAutoMappingValue]):
    """
    " A mapping config to object (model or tokenizer for instance) that will load keys and values when it is accessed.

    Args:
        - config_mapping: The map model type to config class
        - model_mapping: The map model type to model (or tokenizer) class
    """
    def __init__(self, config_mapping, model_mapping) -> None: ...
    def __len__(self) -> int: ...
    def __getitem__(self, key: type[PretrainedConfig]) -> _LazyAutoMappingValue: ...
    def keys(self) -> list[type[PretrainedConfig]]: ...
    def get(self, key: type[PretrainedConfig], default: _T) -> _LazyAutoMappingValue | _T: ...
    def __bool__(self) -> bool: ...
    def values(self) -> list[_LazyAutoMappingValue]: ...
    def items(self) -> list[tuple[type[PretrainedConfig], _LazyAutoMappingValue]]: ...
    def __iter__(self) -> Iterator[type[PretrainedConfig]]: ...
    def __contains__(self, item: type) -> bool: ...
    def register(self, key: type[PretrainedConfig], value: _LazyAutoMappingValue, exist_ok=...) -> None:
        """
        Register a new model in this mapping.
        """
        ...

__all__ = ["get_values"]
