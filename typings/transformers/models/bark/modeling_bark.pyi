"""
This type stub file was generated by pyright.
"""

import torch
from typing import Optional, Union
from torch import nn
from ...generation import GenerationMixin
from ...modeling_flash_attention_utils import is_flash_attn_available
from ...modeling_layers import GradientCheckpointingLayer
from ...modeling_outputs import CausalLMOutputWithPast, MaskedLMOutput
from ...modeling_utils import PreTrainedModel
from ...utils import auto_docstring
from .configuration_bark import BarkCoarseConfig, BarkConfig, BarkFineConfig, BarkSemanticConfig, BarkSubModelConfig
from .generation_configuration_bark import (
    BarkCoarseGenerationConfig,
    BarkFineGenerationConfig,
    BarkSemanticGenerationConfig,
)

if is_flash_attn_available(): ...
logger = ...

class BarkSelfAttention(nn.Module):
    def __init__(self, config, is_causal=..., layer_idx=...) -> None: ...
    def forward(
        self,
        hidden_states,
        attention_mask=...,
        past_key_values=...,
        head_mask=...,
        use_cache=...,
        output_attentions=...,
        cache_position=...,
    ): ...

class BarkSelfFlashAttention2(BarkSelfAttention):
    def __init__(self, *args, **kwargs) -> None: ...
    def forward(
        self,
        hidden_states,
        attention_mask=...,
        past_key_values=...,
        head_mask=...,
        use_cache=...,
        output_attentions=...,
        cache_position=...,
    ): ...

BARK_ATTENTION_CLASSES = ...

class BarkMLP(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states): ...

class BarkBlock(GradientCheckpointingLayer):
    def __init__(self, config, is_causal=..., layer_idx=...) -> None: ...
    def forward(
        self,
        hidden_states,
        past_key_values=...,
        attention_mask=...,
        head_mask=...,
        use_cache=...,
        output_attentions=...,
        cache_position=...,
    ): ...

@auto_docstring
class BarkPreTrainedModel(PreTrainedModel):
    config: BarkConfig
    supports_gradient_checkpointing = ...
    _supports_flash_attn = ...
    def __init__(self, *inputs, **kwargs) -> None: ...
    @property
    def device(self) -> torch.device: ...

class BarkCausalModel(BarkPreTrainedModel, GenerationMixin):
    config: BarkSubModelConfig
    def __init__(self, config) -> None: ...
    def get_output_embeddings(self): ...
    def get_input_embeddings(self): ...
    def set_input_embeddings(self, new_embeddings): ...
    def prepare_inputs_for_generation(
        self,
        input_ids,
        attention_mask=...,
        input_embeds=...,
        past_key_values=...,
        position_ids=...,
        use_cache=...,
        cache_position=...,
        **kwargs,
    ): ...
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.Tensor] = ...,
        past_key_values: Optional[tuple[torch.FloatTensor]] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        position_ids: Optional[torch.Tensor] = ...,
        head_mask: Optional[torch.Tensor] = ...,
        labels: Optional[torch.LongTensor] = ...,
        input_embeds: Optional[torch.Tensor] = ...,
        use_cache: Optional[bool] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
        cache_position: Optional[torch.Tensor] = ...,
    ) -> Union[tuple[torch.Tensor], CausalLMOutputWithPast]: ...

@auto_docstring(custom_intro=...)
class BarkSemanticModel(BarkCausalModel):
    base_model_prefix = ...
    config: BarkSemanticConfig
    def generate(
        self,
        input_ids: torch.Tensor,
        semantic_generation_config: BarkSemanticGenerationConfig = ...,
        history_prompt: Optional[dict[str, torch.Tensor]] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        **kwargs,
    ) -> torch.LongTensor: ...

@auto_docstring(custom_intro=...)
class BarkCoarseModel(BarkCausalModel):
    base_model_prefix = ...
    config: BarkCoarseConfig
    def preprocess_histories(
        self,
        max_coarse_history: int,
        semantic_to_coarse_ratio: int,
        batch_size: int,
        semantic_generation_config: int,
        codebook_size: int,
        history_prompt: Optional[dict[str, torch.Tensor]] = ...,
    ): ...
    def generate(
        self,
        semantic_output: torch.Tensor,
        semantic_generation_config: BarkSemanticGenerationConfig = ...,
        coarse_generation_config: BarkCoarseGenerationConfig = ...,
        codebook_size: int = ...,
        history_prompt: Optional[dict[str, torch.Tensor]] = ...,
        return_output_lengths: Optional[bool] = ...,
        **kwargs,
    ) -> Union[torch.LongTensor, tuple[torch.LongTensor, torch.LongTensor]]: ...

@auto_docstring(custom_intro=...)
class BarkFineModel(BarkPreTrainedModel):
    base_model_prefix = ...
    config: BarkFineConfig
    main_input_name = ...
    def __init__(self, config) -> None: ...
    def get_input_embeddings(self): ...
    def set_input_embeddings(self, new_embeddings): ...
    def get_output_embeddings(self): ...
    def set_output_embeddings(self, new_output_embeddings): ...
    def resize_token_embeddings(
        self, new_num_tokens: Optional[int] = ..., pad_to_multiple_of: Optional[int] = ..., mean_resizing: bool = ...
    ) -> nn.Embedding: ...
    def tie_weights(self): ...
    @auto_docstring
    def forward(
        self,
        codebook_idx: int,
        input_ids: Optional[torch.Tensor] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        position_ids: Optional[torch.Tensor] = ...,
        head_mask: Optional[torch.Tensor] = ...,
        labels: Optional[torch.LongTensor] = ...,
        input_embeds: Optional[torch.Tensor] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[tuple[torch.Tensor], MaskedLMOutput]: ...
    @torch.no_grad()
    def generate(
        self,
        coarse_output: torch.Tensor,
        semantic_generation_config: BarkSemanticGenerationConfig = ...,
        coarse_generation_config: BarkCoarseGenerationConfig = ...,
        fine_generation_config: BarkFineGenerationConfig = ...,
        codebook_size: int = ...,
        history_prompt: Optional[dict[str, torch.Tensor]] = ...,
        **kwargs,
    ) -> torch.LongTensor: ...

@auto_docstring(custom_intro=...)
class BarkModel(BarkPreTrainedModel):
    config: BarkConfig
    def __init__(self, config) -> None: ...
    @classmethod
    def can_generate(cls) -> bool: ...
    @property
    def device(self) -> torch.device: ...
    def enable_cpu_offload(self, accelerator_id: Optional[int] = ..., **kwargs): ...
    def codec_decode(self, fine_output, output_lengths=...): ...
    @torch.no_grad()
    def generate(
        self,
        input_ids: Optional[torch.Tensor] = ...,
        history_prompt: Optional[dict[str, torch.Tensor]] = ...,
        return_output_lengths: Optional[bool] = ...,
        **kwargs,
    ) -> torch.LongTensor: ...

__all__ = [
    "BarkFineModel",
    "BarkSemanticModel",
    "BarkCoarseModel",
    "BarkModel",
    "BarkPreTrainedModel",
    "BarkCausalModel",
]
