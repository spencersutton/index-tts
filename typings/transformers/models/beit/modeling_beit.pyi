from dataclasses import dataclass

import torch
from torch import Tensor, nn

from ...modeling_layers import GradientCheckpointingLayer
from ...modeling_outputs import (
    BackboneOutput,
    BaseModelOutput,
    BaseModelOutputWithPooling,
    ImageClassifierOutput,
    MaskedLMOutput,
    SemanticSegmenterOutput,
)
from ...modeling_utils import PreTrainedModel
from ...pytorch_utils import compile_compatible_method_lru_cache
from ...utils.backbone_utils import BackboneMixin
from .configuration_beit import BeitConfig

"""PyTorch BEiT model."""
logger = ...

@dataclass
class BeitModelOutputWithPooling(BaseModelOutputWithPooling): ...

def drop_path(input: torch.Tensor, drop_prob: float = ..., training: bool = ...) -> torch.Tensor: ...

class BeitDropPath(nn.Module):
    def __init__(self, drop_prob: float | None = ...) -> None: ...
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor: ...
    def extra_repr(self) -> str: ...

class BeitEmbeddings(nn.Module):
    def __init__(self, config: BeitConfig) -> None: ...
    def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor: ...
    def forward(
        self,
        pixel_values: torch.Tensor,
        bool_masked_pos: torch.BoolTensor | None = ...,
        interpolate_pos_encoding: bool | None = ...,
    ) -> torch.Tensor: ...

class BeitPatchEmbeddings(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, pixel_values: torch.Tensor) -> torch.Tensor: ...

class BeitSelfAttention(nn.Module):
    def __init__(self, config: BeitConfig, window_size: tuple | None = ...) -> None: ...
    def forward(
        self,
        hidden_states: torch.Tensor,
        head_mask: torch.Tensor | None = ...,
        output_attentions: bool = ...,
        relative_position_bias: torch.Tensor | None = ...,
        interpolate_pos_encoding: bool = ...,
        resolution: tuple[int] | None = ...,
    ) -> tuple[torch.Tensor] | tuple[torch.Tensor, torch.Tensor]: ...

class BeitSdpaSelfAttention(BeitSelfAttention):
    def forward(
        self,
        hidden_states: torch.Tensor,
        head_mask: torch.Tensor | None = ...,
        output_attentions: bool = ...,
        relative_position_bias: torch.Tensor | None = ...,
        interpolate_pos_encoding: bool = ...,
        resolution: tuple[int] | None = ...,
    ) -> tuple[torch.Tensor] | tuple[torch.Tensor, torch.Tensor]: ...

class BeitSelfOutput(nn.Module):
    def __init__(self, config: BeitConfig) -> None: ...
    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor, gamma=...) -> torch.Tensor: ...

BEIT_SELF_ATTENTION_CLASSES = ...

class BeitAttention(nn.Module):
    def __init__(self, config: BeitConfig, window_size: tuple | None = ...) -> None: ...
    def prune_heads(self, heads):  # -> None:
        ...
    def forward(
        self,
        hidden_states: torch.Tensor,
        head_mask: torch.Tensor | None = ...,
        output_attentions: bool = ...,
        relative_position_bias: torch.Tensor | None = ...,
        interpolate_pos_encoding: bool = ...,
        resolution: tuple[int] | None = ...,
    ) -> tuple[torch.Tensor] | tuple[torch.Tensor, torch.Tensor]: ...

class BeitIntermediate(nn.Module):
    def __init__(self, config: BeitConfig) -> None: ...
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor: ...

class BeitOutput(nn.Module):
    def __init__(self, config: BeitConfig) -> None: ...
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor: ...

class BeitLayer(GradientCheckpointingLayer):
    def __init__(self, config: BeitConfig, window_size: tuple | None = ..., drop_path_rate: float = ...) -> None: ...
    def forward(
        self,
        hidden_states: torch.Tensor,
        head_mask: torch.Tensor | None = ...,
        output_attentions: bool = ...,
        relative_position_bias: torch.Tensor | None = ...,
        interpolate_pos_encoding: bool = ...,
        resolution: tuple[int, int] | None = ...,
    ) -> tuple[torch.Tensor] | tuple[torch.Tensor, torch.Tensor]: ...

class BeitRelativePositionBias(nn.Module):
    def __init__(self, config: BeitConfig, window_size: tuple) -> None: ...
    @compile_compatible_method_lru_cache(maxsize=10)
    def generate_relative_position_index(self, window_size: tuple[int, int]) -> torch.Tensor: ...
    def forward(self, window_size, interpolate_pos_encoding: bool = ..., dim_size=...) -> torch.Tensor: ...

class BeitEncoder(nn.Module):
    def __init__(self, config: BeitConfig, window_size: tuple | None = ...) -> None: ...
    def forward(
        self,
        hidden_states: torch.Tensor,
        head_mask: torch.Tensor | None = ...,
        output_attentions: bool = ...,
        output_hidden_states: bool = ...,
        interpolate_pos_encoding: bool = ...,
        resolution: tuple[int, int] | None = ...,
        return_dict: bool = ...,
    ) -> tuple | BaseModelOutput: ...

class BeitPreTrainedModel(PreTrainedModel):
    config: BeitConfig
    base_model_prefix = ...
    main_input_name = ...
    supports_gradient_checkpointing = ...
    _no_split_modules = ...
    _keys_to_ignore_on_load_unexpected = ...
    _supports_sdpa = ...

class BeitModel(BeitPreTrainedModel):
    def __init__(self, config: BeitConfig, add_pooling_layer: bool = ...) -> None: ...
    def get_input_embeddings(self):  # -> BeitPatchEmbeddings:
        ...
    def forward(
        self,
        pixel_values: torch.Tensor,
        bool_masked_pos: torch.BoolTensor | None = ...,
        head_mask: torch.Tensor | None = ...,
        output_attentions: bool | None = ...,
        output_hidden_states: bool | None = ...,
        interpolate_pos_encoding: bool = ...,
        return_dict: bool | None = ...,
    ) -> tuple | BeitModelOutputWithPooling: ...

class BeitPooler(nn.Module):
    def __init__(self, config: BeitConfig) -> None: ...
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor: ...

class BeitForMaskedImageModeling(BeitPreTrainedModel):
    def __init__(self, config: BeitConfig) -> None: ...
    def get_output_embeddings(self):  # -> None:
        ...
    def forward(
        self,
        pixel_values: torch.Tensor | None = ...,
        bool_masked_pos: torch.BoolTensor | None = ...,
        head_mask: torch.Tensor | None = ...,
        labels: torch.Tensor | None = ...,
        output_attentions: bool | None = ...,
        output_hidden_states: bool | None = ...,
        interpolate_pos_encoding: bool = ...,
        return_dict: bool | None = ...,
    ) -> tuple | MaskedLMOutput: ...

class BeitForImageClassification(BeitPreTrainedModel):
    def __init__(self, config: BeitConfig) -> None: ...
    def forward(
        self,
        pixel_values: torch.Tensor | None = ...,
        head_mask: torch.Tensor | None = ...,
        labels: torch.Tensor | None = ...,
        output_attentions: bool | None = ...,
        output_hidden_states: bool | None = ...,
        interpolate_pos_encoding: bool = ...,
        return_dict: bool | None = ...,
    ) -> tuple | ImageClassifierOutput: ...

class BeitConvModule(nn.Module):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int | tuple[int, int],
        padding: int | tuple[int, int] | str = ...,
        bias: bool = ...,
        dilation: int | tuple[int, int] = ...,
    ) -> None: ...
    def forward(self, input: torch.Tensor) -> torch.Tensor: ...

class BeitPyramidPoolingBlock(nn.Module):
    def __init__(self, pool_scale: int, in_channels: int, channels: int) -> None: ...
    def forward(self, input: torch.Tensor) -> torch.Tensor: ...

class BeitPyramidPoolingModule(nn.Module):
    def __init__(self, pool_scales: tuple[int, ...], in_channels: int, channels: int, align_corners: bool) -> None: ...
    def forward(self, x: torch.Tensor) -> list[torch.Tensor]: ...

class BeitUperHead(nn.Module):
    def __init__(self, config: BeitConfig) -> None: ...
    def psp_forward(self, inputs):  # -> Any:
        ...
    def forward(self, encoder_hidden_states: torch.Tensor) -> torch.Tensor: ...

class BeitFCNHead(nn.Module):
    def __init__(
        self,
        config: BeitConfig,
        in_index: int = ...,
        kernel_size: int = ...,
        dilation: int | tuple[int, int] = ...,
    ) -> None: ...
    def forward(self, encoder_hidden_states: torch.Tensor) -> torch.Tensor: ...

class BeitForSemanticSegmentation(BeitPreTrainedModel):
    def __init__(self, config: BeitConfig) -> None: ...
    def compute_loss(self, logits, auxiliary_logits, labels):  # -> Any:
        ...
    def forward(
        self,
        pixel_values: torch.Tensor | None = ...,
        head_mask: torch.Tensor | None = ...,
        labels: torch.Tensor | None = ...,
        output_attentions: bool | None = ...,
        output_hidden_states: bool | None = ...,
        interpolate_pos_encoding: bool = ...,
        return_dict: bool | None = ...,
    ) -> tuple | SemanticSegmenterOutput: ...

class BeitBackbone(BeitPreTrainedModel, BackboneMixin):
    def __init__(self, config) -> None: ...
    def get_input_embeddings(self):  # -> BeitPatchEmbeddings:
        ...
    def forward(
        self,
        pixel_values: Tensor,
        output_hidden_states: bool | None = ...,
        output_attentions: bool | None = ...,
        return_dict: bool | None = ...,
    ) -> BackboneOutput: ...

__all__ = [
    "BeitBackbone",
    "BeitForImageClassification",
    "BeitForMaskedImageModeling",
    "BeitForSemanticSegmentation",
    "BeitModel",
    "BeitPreTrainedModel",
]
