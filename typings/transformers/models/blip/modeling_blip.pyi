"""
This type stub file was generated by pyright.
"""

import torch
from dataclasses import dataclass
from typing import Any, Optional, Union
from torch import nn
from ...generation import GenerationMixin
from ...modeling_layers import GradientCheckpointingLayer
from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling
from ...modeling_utils import PreTrainedModel
from ...utils import ModelOutput, auto_docstring
from .configuration_blip import BlipConfig, BlipTextConfig, BlipVisionConfig

logger = ...

def contrastive_loss(logits: torch.Tensor) -> torch.Tensor: ...
def blip_loss(similarity: torch.Tensor) -> torch.Tensor: ...

@dataclass
@auto_docstring(custom_intro=...)
class BlipForConditionalGenerationModelOutput(ModelOutput):
    loss: Optional[tuple[torch.FloatTensor]] = ...
    logits: Optional[tuple[torch.FloatTensor]] = ...
    image_embeds: Optional[torch.FloatTensor] = ...
    last_hidden_state: Optional[torch.FloatTensor] = ...
    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = ...
    attentions: Optional[tuple[torch.FloatTensor, ...]] = ...
    @property
    def decoder_logits(self): ...

@dataclass
@auto_docstring(custom_intro=...)
class BlipTextVisionModelOutput(ModelOutput):
    loss: Optional[torch.FloatTensor] = ...
    image_embeds: Optional[torch.FloatTensor] = ...
    last_hidden_state: Optional[torch.FloatTensor] = ...
    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = ...
    attentions: Optional[tuple[torch.FloatTensor, ...]] = ...

@dataclass
@auto_docstring(custom_intro=...)
class BlipImageTextMatchingModelOutput(ModelOutput):
    itm_score: Optional[torch.FloatTensor] = ...
    loss: Optional[torch.FloatTensor] = ...
    image_embeds: Optional[torch.FloatTensor] = ...
    last_hidden_state: Optional[torch.FloatTensor] = ...
    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = ...
    vision_pooler_output: Optional[torch.FloatTensor] = ...
    attentions: Optional[tuple[torch.FloatTensor, ...]] = ...
    question_embeds: Optional[tuple[torch.FloatTensor]] = ...

@dataclass
@auto_docstring
class BlipOutput(ModelOutput):
    loss: Optional[torch.FloatTensor] = ...
    logits_per_image: Optional[torch.FloatTensor] = ...
    logits_per_text: Optional[torch.FloatTensor] = ...
    text_embeds: Optional[torch.FloatTensor] = ...
    image_embeds: Optional[torch.FloatTensor] = ...
    text_model_output: BaseModelOutputWithPooling = ...
    vision_model_output: BaseModelOutputWithPooling = ...
    def to_tuple(self) -> tuple[Any]: ...

class BlipVisionEmbeddings(nn.Module):
    def __init__(self, config: BlipVisionConfig) -> None: ...
    def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor: ...
    def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding: bool = ...) -> torch.Tensor: ...

class BlipTextEmbeddings(nn.Module):
    def __init__(self, config: BlipTextConfig) -> None: ...
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = ...,
        position_ids: Optional[torch.LongTensor] = ...,
        inputs_embeds: Optional[torch.FloatTensor] = ...,
    ) -> torch.Tensor: ...

class BlipAttention(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(
        self,
        hidden_states: torch.Tensor,
        head_mask: Optional[torch.Tensor] = ...,
        output_attentions: Optional[bool] = ...,
    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]: ...

class BlipMLP(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor: ...

class BlipEncoderLayer(GradientCheckpointingLayer):
    def __init__(self, config: BlipConfig) -> None: ...
    def forward(
        self, hidden_states: torch.Tensor, attention_mask: torch.Tensor, output_attentions: Optional[bool] = ...
    ) -> tuple[torch.FloatTensor]: ...

@auto_docstring
class BlipPreTrainedModel(PreTrainedModel):
    config: BlipConfig
    base_model_prefix = ...
    supports_gradient_checkpointing = ...
    _no_split_modules = ...
    _skip_keys_device_placement = ...

class BlipEncoder(nn.Module):
    def __init__(self, config: BlipConfig) -> None: ...
    def forward(
        self,
        inputs_embeds,
        attention_mask: Optional[torch.Tensor] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[tuple, BaseModelOutput]: ...

class BlipVisionModel(BlipPreTrainedModel):
    main_input_name = ...
    config: BlipVisionConfig
    def __init__(self, config: BlipVisionConfig) -> None: ...
    @auto_docstring
    def forward(
        self,
        pixel_values: Optional[torch.FloatTensor] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
        interpolate_pos_encoding: bool = ...,
    ) -> Union[tuple, BaseModelOutputWithPooling]: ...
    def get_input_embeddings(self): ...

@auto_docstring(custom_intro=...)
class BlipModel(BlipPreTrainedModel):
    config: BlipConfig
    def __init__(self, config: BlipConfig) -> None: ...
    def get_input_embeddings(self): ...
    def set_input_embeddings(self, value): ...
    @auto_docstring
    def get_text_features(
        self,
        input_ids: Optional[torch.Tensor] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        position_ids: Optional[torch.Tensor] = ...,
        return_dict: Optional[bool] = ...,
    ) -> torch.FloatTensor: ...
    @auto_docstring
    def get_image_features(
        self,
        pixel_values: Optional[torch.FloatTensor] = ...,
        return_dict: Optional[bool] = ...,
        interpolate_pos_encoding: bool = ...,
    ) -> torch.FloatTensor: ...
    @auto_docstring
    def get_multimodal_features(
        self,
        input_ids: Optional[torch.LongTensor] = ...,
        pixel_values: Optional[torch.FloatTensor] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        return_dict: Optional[bool] = ...,
        interpolate_pos_encoding: bool = ...,
    ) -> torch.FloatTensor: ...
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = ...,
        pixel_values: Optional[torch.FloatTensor] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        position_ids: Optional[torch.LongTensor] = ...,
        return_loss: Optional[bool] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
        interpolate_pos_encoding: bool = ...,
    ) -> Union[tuple, BlipOutput]: ...

@auto_docstring(custom_intro=...)
class BlipForConditionalGeneration(BlipPreTrainedModel, GenerationMixin):
    config: BlipConfig
    _tied_weights_keys = ...
    main_input_name = ...
    def __init__(self, config: BlipConfig) -> None: ...
    def get_input_embeddings(self): ...
    def set_input_embeddings(self, value): ...
    @auto_docstring
    def forward(
        self,
        pixel_values: torch.FloatTensor,
        input_ids: Optional[torch.LongTensor] = ...,
        attention_mask: Optional[torch.LongTensor] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        labels: Optional[torch.LongTensor] = ...,
        return_dict: Optional[bool] = ...,
        interpolate_pos_encoding: bool = ...,
    ) -> Union[tuple, BlipForConditionalGenerationModelOutput]: ...
    @torch.no_grad()
    def generate(
        self,
        pixel_values: torch.FloatTensor,
        input_ids: Optional[torch.LongTensor] = ...,
        attention_mask: Optional[torch.LongTensor] = ...,
        interpolate_pos_encoding: bool = ...,
        **generate_kwargs,
    ) -> torch.LongTensor: ...

@auto_docstring(custom_intro=...)
class BlipForQuestionAnswering(BlipPreTrainedModel, GenerationMixin):
    config: BlipConfig
    _tied_weights_keys = ...
    def __init__(self, config: BlipConfig) -> None: ...
    def set_input_embeddings(self, value): ...
    def get_input_embeddings(self): ...
    @auto_docstring
    def forward(
        self,
        input_ids: torch.LongTensor,
        pixel_values: torch.FloatTensor,
        decoder_input_ids: Optional[torch.LongTensor] = ...,
        decoder_attention_mask: Optional[torch.LongTensor] = ...,
        attention_mask: Optional[torch.LongTensor] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        labels: Optional[torch.LongTensor] = ...,
        return_dict: Optional[bool] = ...,
        interpolate_pos_encoding: bool = ...,
    ) -> Union[tuple, BlipTextVisionModelOutput]: ...
    @torch.no_grad()
    def generate(
        self,
        input_ids: torch.LongTensor,
        pixel_values: torch.FloatTensor,
        attention_mask: Optional[torch.LongTensor] = ...,
        interpolate_pos_encoding: bool = ...,
        **generate_kwargs,
    ) -> torch.LongTensor: ...

@auto_docstring(custom_intro=...)
class BlipForImageTextRetrieval(BlipPreTrainedModel):
    config: BlipConfig
    def __init__(self, config: BlipConfig) -> None: ...
    def get_input_embeddings(self): ...
    def set_input_embeddings(self, value): ...
    @auto_docstring
    def forward(
        self,
        input_ids: torch.LongTensor,
        pixel_values: torch.FloatTensor,
        use_itm_head: Optional[bool] = ...,
        attention_mask: Optional[torch.LongTensor] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
        interpolate_pos_encoding: bool = ...,
    ) -> Union[tuple, BlipTextVisionModelOutput]: ...

__all__ = [
    "BlipModel",
    "BlipPreTrainedModel",
    "BlipForConditionalGeneration",
    "BlipForQuestionAnswering",
    "BlipVisionModel",
    "BlipTextModel",
    "BlipForImageTextRetrieval",
]
