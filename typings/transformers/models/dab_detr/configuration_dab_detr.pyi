"""
This type stub file was generated by pyright.
"""

from ...configuration_utils import PretrainedConfig

"""DAB-DETR model configuration"""
logger = ...

class DabDetrConfig(PretrainedConfig):
    r"""
    This is the configuration class to store the configuration of a [`DabDetrModel`]. It is used to instantiate
    a DAB-DETR model according to the specified arguments, defining the model architecture. Instantiating a
    configuration with the defaults will yield a similar configuration to that of the DAB-DETR
    [IDEA-Research/dab_detr-base](https://huggingface.co/IDEA-Research/dab_detr-base) architecture.

    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the
    documentation from [`PretrainedConfig`] for more information.

    Args:
        use_timm_backbone (`bool`, *optional*, defaults to `True`):
            Whether or not to use the `timm` library for the backbone. If set to `False`, will use the [`AutoBackbone`]
            API.
        backbone_config (`PretrainedConfig` or `dict`, *optional*):
            The configuration of the backbone model. Only used in case `use_timm_backbone` is set to `False` in which
            case it will default to `ResNetConfig()`.
        backbone (`str`, *optional*, defaults to `"resnet50"`):
            Name of backbone to use when `backbone_config` is `None`. If `use_pretrained_backbone` is `True`, this
            will load the corresponding pretrained weights from the timm or transformers library. If `use_pretrained_backbone`
            is `False`, this loads the backbone's config and uses that to initialize the backbone with random weights.
        use_pretrained_backbone (`bool`, *optional*, defaults to `True`):
            Whether to use pretrained weights for the backbone.
        backbone_kwargs (`dict`, *optional*):
            Keyword arguments to be passed to AutoBackbone when loading from a checkpoint
            e.g. `{'out_indices': (0, 1, 2, 3)}`. Cannot be specified if `backbone_config` is set.
        num_queries (`int`, *optional*, defaults to 300):
            Number of object queries, i.e. detection slots. This is the maximal number of objects
            [`DabDetrModel`] can detect in a single image. For COCO, we recommend 100 queries.
        encoder_layers (`int`, *optional*, defaults to 6):
            Number of encoder layers.
        encoder_ffn_dim (`int`, *optional*, defaults to 2048):
            Dimension of the "intermediate" (often named feed-forward) layer in encoder.
        encoder_attention_heads (`int`, *optional*, defaults to 8):
            Number of attention heads for each attention layer in the Transformer encoder.
        decoder_layers (`int`, *optional*, defaults to 6):
            Number of decoder layers.
        decoder_ffn_dim (`int`, *optional*, defaults to 2048):
            Dimension of the "intermediate" (often named feed-forward) layer in decoder.
        decoder_attention_heads (`int`, *optional*, defaults to 8):
            Number of attention heads for each attention layer in the Transformer decoder.
        is_encoder_decoder (`bool`, *optional*, defaults to `True`):
            Indicates whether the transformer model architecture is an encoder-decoder or not.
        activation_function (`str` or `function`, *optional*, defaults to `"prelu"`):
            The non-linear activation function (function or string) in the encoder and pooler. If string, `"gelu"`,
            `"relu"`, `"silu"` and `"gelu_new"` are supported.
        hidden_size (`int`, *optional*, defaults to 256):
            This parameter is a general dimension parameter, defining dimensions for components such as the encoder layer and projection parameters in the decoder layer, among others.
        dropout (`float`, *optional*, defaults to 0.1):
            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.
        attention_dropout (`float`, *optional*, defaults to 0.0):
            The dropout ratio for the attention probabilities.
        activation_dropout (`float`, *optional*, defaults to 0.0):
            The dropout ratio for activations inside the fully connected layer.
        init_std (`float`, *optional*, defaults to 0.02):
            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.
        init_xavier_std (`float`, *optional*, defaults to 1.0):
            The scaling factor used for the Xavier initialization gain in the HM Attention map module.
        auxiliary_loss (`bool`, *optional*, defaults to `False`):
            Whether auxiliary decoding losses (loss at each decoder layer) are to be used.
        dilation (`bool`, *optional*, defaults to `False`):
            Whether to replace stride with dilation in the last convolutional block (DC5). Only supported when `use_timm_backbone` = `True`.
        class_cost (`float`, *optional*, defaults to 2):
            Relative weight of the classification error in the Hungarian matching cost.
        bbox_cost (`float`, *optional*, defaults to 5):
            Relative weight of the L1 error of the bounding box coordinates in the Hungarian matching cost.
        giou_cost (`float`, *optional*, defaults to 2):
            Relative weight of the generalized IoU loss of the bounding box in the Hungarian matching cost.
        cls_loss_coefficient (`float`, *optional*, defaults to 2):
            Relative weight of the classification loss in the object detection loss function.
        bbox_loss_coefficient (`float`, *optional*, defaults to 5):
            Relative weight of the L1 bounding box loss in the object detection loss.
        giou_loss_coefficient (`float`, *optional*, defaults to 2):
            Relative weight of the generalized IoU loss in the object detection loss.
        focal_alpha (`float`, *optional*, defaults to 0.25):
            Alpha parameter in the focal loss.
        temperature_height (`int`, *optional*, defaults to 20):
            Temperature parameter to tune the flatness of positional attention (HEIGHT)
        temperature_width (`int`, *optional*, defaults to 20):
            Temperature parameter to tune the flatness of positional attention (WIDTH)
        query_dim (`int`, *optional*, defaults to 4):
            Query dimension parameter represents the size of the output vector.
        random_refpoints_xy (`bool`, *optional*, defaults to `False`):
            Whether to fix the x and y coordinates of the anchor boxes with random initialization.
        keep_query_pos (`bool`, *optional*, defaults to `False`):
            Whether to concatenate the projected positional embedding from the object query into the original query (key) in every decoder layer.
        num_patterns (`int`, *optional*, defaults to 0):
            Number of pattern embeddings.
        normalize_before (`bool`, *optional*, defaults to `False`):
            Whether we use a normalization layer in the Encoder or not.
        sine_position_embedding_scale (`float`, *optional*, defaults to 'None'):
            Scaling factor applied to the normalized positional encodings.
        initializer_bias_prior_prob (`float`, *optional*):
            The prior probability used by the bias initializer to initialize biases for `enc_score_head` and `class_embed`.
            If `None`, `prior_prob` computed as `prior_prob = 1 / (num_labels + 1)` while initializing model weights.


    Examples:

    ```python
    >>> from transformers import DabDetrConfig, DabDetrModel

    >>> # Initializing a DAB-DETR IDEA-Research/dab_detr-base style configuration
    >>> configuration = DabDetrConfig()

    >>> # Initializing a model (with random weights) from the IDEA-Research/dab_detr-base style configuration
    >>> model = DabDetrModel(configuration)

    >>> # Accessing the model configuration
    >>> configuration = model.config
    ```"""

    model_type = ...
    keys_to_ignore_at_inference = ...
    attribute_map = ...
    def __init__(
        self,
        use_timm_backbone=...,
        backbone_config=...,
        backbone=...,
        use_pretrained_backbone=...,
        backbone_kwargs=...,
        num_queries=...,
        encoder_layers=...,
        encoder_ffn_dim=...,
        encoder_attention_heads=...,
        decoder_layers=...,
        decoder_ffn_dim=...,
        decoder_attention_heads=...,
        is_encoder_decoder=...,
        activation_function=...,
        hidden_size=...,
        dropout=...,
        attention_dropout=...,
        activation_dropout=...,
        init_std=...,
        init_xavier_std=...,
        auxiliary_loss=...,
        dilation=...,
        class_cost=...,
        bbox_cost=...,
        giou_cost=...,
        cls_loss_coefficient=...,
        bbox_loss_coefficient=...,
        giou_loss_coefficient=...,
        focal_alpha=...,
        temperature_height=...,
        temperature_width=...,
        query_dim=...,
        random_refpoints_xy=...,
        keep_query_pos=...,
        num_patterns=...,
        normalize_before=...,
        sine_position_embedding_scale=...,
        initializer_bias_prior_prob=...,
        **kwargs,
    ) -> None: ...
    @property
    def sub_configs(self):  # -> dict[str, type[None] | type[Any] | type[PretrainedConfig]]:
        ...

__all__ = ["DabDetrConfig"]
