from torch import nn

from ...modeling_layers import GradientCheckpointingLayer
from ...modeling_outputs import Wav2Vec2BaseModelOutput
from ...modeling_utils import PreTrainedModel
from ..wav2vec2.modeling_wav2vec2 import (
    Wav2Vec2Adapter,
    Wav2Vec2Encoder,
    Wav2Vec2FeatureEncoder,
    Wav2Vec2FeatureProjection,
    Wav2Vec2ForAudioFrameClassification,
    Wav2Vec2ForCTC,
    Wav2Vec2ForSequenceClassification,
    Wav2Vec2ForXVector,
    Wav2Vec2Model,
    Wav2Vec2PreTrainedModel,
    Wav2Vec2SamePadLayer,
)
from .configuration_data2vec_audio import Data2VecAudioConfig

"""PyTorch Data2VecText model."""

class Data2VecAudioConvLayer(GradientCheckpointingLayer):
    def __init__(self, config, layer_id=...) -> None: ...
    def forward(self, hidden_states): ...

class Data2VecAudioPadLayer(Wav2Vec2SamePadLayer): ...

class Data2VecAudioPositionalConvLayer(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states): ...

class Data2VecAudioPositionalConvEmbedding(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states):  # -> Any:
        ...

class Data2VecAudioFeatureEncoder(Wav2Vec2FeatureEncoder, nn.Module):
    def __init__(self, config) -> None: ...

class Data2VecAudioFeatureProjection(Wav2Vec2FeatureProjection): ...
class Data2VecAudioEncoder(Wav2Vec2Encoder): ...
class Data2VecAudioAdapter(Wav2Vec2Adapter): ...

class Data2VecAudioPreTrainedModel(PreTrainedModel, Wav2Vec2PreTrainedModel):
    config: Data2VecAudioConfig
    base_model_prefix = ...
    main_input_name = ...
    supports_gradient_checkpointing = ...
    _supports_flash_attn = ...
    _supports_sdpa = ...
    _supports_flex_attn = ...
    def init_adapter_layers(self): ...
    def load_adapter(self): ...

Data2VecAudioBaseModelOutput = Wav2Vec2BaseModelOutput

class Data2VecAudioModel(Data2VecAudioPreTrainedModel, Wav2Vec2Model):
    def __init__(self, config: Data2VecAudioConfig) -> None: ...
    def freeze_feature_extractor(self): ...
    def freeze_feature_encoder(self):  # -> None:

        ...
    def forward(self, **super_kwargs):  # -> tuple[Any, ...] | Wav2Vec2BaseModelOutput:
        ...

class Data2VecAudioForCTC(Data2VecAudioPreTrainedModel, Wav2Vec2ForCTC):
    def __init__(self, config) -> None: ...
    def freeze_base_model(self): ...
    def tie_weights(self): ...
    def forward(self, **super_kwargs):  # -> tuple[Any, ...] | CausalLMOutput:
        ...

class Data2VecAudioForSequenceClassification(Wav2Vec2ForSequenceClassification): ...
class Data2VecAudioForAudioFrameClassification(Wav2Vec2ForAudioFrameClassification): ...
class Data2VecAudioForXVector(Wav2Vec2ForXVector): ...

__all__ = [
    "Data2VecAudioForAudioFrameClassification",
    "Data2VecAudioForCTC",
    "Data2VecAudioForSequenceClassification",
    "Data2VecAudioForXVector",
    "Data2VecAudioModel",
    "Data2VecAudioPreTrainedModel",
]
