"""
This type stub file was generated by pyright.
"""

import torch
from dataclasses import dataclass
from typing import Optional, Union
from torch import Tensor, nn
from torch.autograd import Function
from torch.autograd.function import once_differentiable
from ....file_utils import (
    ModelOutput,
    add_start_docstrings,
    add_start_docstrings_to_model_forward,
    is_scipy_available,
    is_vision_available,
    replace_return_docstrings,
)
from ....modeling_layers import GradientCheckpointingLayer
from ....modeling_utils import PreTrainedModel
from ....utils import is_accelerate_available, is_torchvision_available
from .configuration_deta import DetaConfig

logger = ...
MultiScaleDeformableAttention = ...

def load_cuda_kernels(): ...

class MultiScaleDeformableAttentionFunction(Function):
    @staticmethod
    def forward(
        context,
        value,
        value_spatial_shapes,
        value_level_start_index,
        sampling_locations,
        attention_weights,
        im2col_step,
    ): ...
    @staticmethod
    @once_differentiable
    def backward(context, grad_output): ...

if is_accelerate_available(): ...
if is_vision_available(): ...
if is_torchvision_available(): ...
if is_scipy_available(): ...
logger = ...
_CONFIG_FOR_DOC = ...
_CHECKPOINT_FOR_DOC = ...

@dataclass
class DetaDecoderOutput(ModelOutput):
    last_hidden_state: Optional[torch.FloatTensor] = ...
    intermediate_hidden_states: Optional[torch.FloatTensor] = ...
    intermediate_reference_points: Optional[torch.FloatTensor] = ...
    hidden_states: Optional[tuple[torch.FloatTensor]] = ...
    attentions: Optional[tuple[torch.FloatTensor]] = ...
    cross_attentions: Optional[tuple[torch.FloatTensor]] = ...

@dataclass
class DetaModelOutput(ModelOutput):
    init_reference_points: Optional[torch.FloatTensor] = ...
    last_hidden_state: Optional[torch.FloatTensor] = ...
    intermediate_hidden_states: Optional[torch.FloatTensor] = ...
    intermediate_reference_points: Optional[torch.FloatTensor] = ...
    decoder_hidden_states: Optional[tuple[torch.FloatTensor]] = ...
    decoder_attentions: Optional[tuple[torch.FloatTensor]] = ...
    cross_attentions: Optional[tuple[torch.FloatTensor]] = ...
    encoder_last_hidden_state: Optional[torch.FloatTensor] = ...
    encoder_hidden_states: Optional[tuple[torch.FloatTensor]] = ...
    encoder_attentions: Optional[tuple[torch.FloatTensor]] = ...
    enc_outputs_class: Optional[torch.FloatTensor] = ...
    enc_outputs_coord_logits: Optional[torch.FloatTensor] = ...
    output_proposals: Optional[torch.FloatTensor] = ...

@dataclass
class DetaObjectDetectionOutput(ModelOutput):
    loss: Optional[torch.FloatTensor] = ...
    loss_dict: Optional[dict] = ...
    logits: Optional[torch.FloatTensor] = ...
    pred_boxes: Optional[torch.FloatTensor] = ...
    auxiliary_outputs: Optional[list[dict]] = ...
    init_reference_points: Optional[torch.FloatTensor] = ...
    last_hidden_state: Optional[torch.FloatTensor] = ...
    intermediate_hidden_states: Optional[torch.FloatTensor] = ...
    intermediate_reference_points: Optional[torch.FloatTensor] = ...
    decoder_hidden_states: Optional[tuple[torch.FloatTensor]] = ...
    decoder_attentions: Optional[tuple[torch.FloatTensor]] = ...
    cross_attentions: Optional[tuple[torch.FloatTensor]] = ...
    encoder_last_hidden_state: Optional[torch.FloatTensor] = ...
    encoder_hidden_states: Optional[tuple[torch.FloatTensor]] = ...
    encoder_attentions: Optional[tuple[torch.FloatTensor]] = ...
    enc_outputs_class: Optional = ...
    enc_outputs_coord_logits: Optional = ...
    output_proposals: Optional[torch.FloatTensor] = ...

def inverse_sigmoid(x, eps=...): ...

class DetaFrozenBatchNorm2d(nn.Module):
    def __init__(self, n) -> None: ...
    def forward(self, x): ...

def replace_batch_norm(model): ...

class DetaBackboneWithPositionalEncodings(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, pixel_values: torch.Tensor, pixel_mask: torch.Tensor): ...

class DetaSinePositionEmbedding(nn.Module):
    def __init__(self, embedding_dim=..., temperature=..., normalize=..., scale=...) -> None: ...
    def forward(self, pixel_values, pixel_mask): ...

class DetaLearnedPositionEmbedding(nn.Module):
    def __init__(self, embedding_dim=...) -> None: ...
    def forward(self, pixel_values, pixel_mask=...): ...

def build_position_encoding(config): ...
def multi_scale_deformable_attention(
    value: Tensor, value_spatial_shapes: Tensor, sampling_locations: Tensor, attention_weights: Tensor
) -> Tensor: ...

class DetaMultiscaleDeformableAttention(nn.Module):
    def __init__(self, config: DetaConfig, num_heads: int, n_points: int) -> None: ...
    def with_pos_embed(self, tensor: torch.Tensor, position_embeddings: Optional[Tensor]): ...
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = ...,
        encoder_hidden_states=...,
        encoder_attention_mask=...,
        position_embeddings: Optional[torch.Tensor] = ...,
        reference_points=...,
        spatial_shapes=...,
        level_start_index=...,
        output_attentions: bool = ...,
    ): ...

class DetaMultiheadAttention(nn.Module):
    def __init__(self, embed_dim: int, num_heads: int, dropout: float = ..., bias: bool = ...) -> None: ...
    def with_pos_embed(self, tensor: torch.Tensor, position_embeddings: Optional[Tensor]): ...
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = ...,
        position_embeddings: Optional[torch.Tensor] = ...,
        output_attentions: bool = ...,
    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]: ...

class DetaEncoderLayer(nn.Module):
    def __init__(self, config: DetaConfig) -> None: ...
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: torch.Tensor,
        position_embeddings: Optional[torch.Tensor] = ...,
        reference_points=...,
        spatial_shapes=...,
        level_start_index=...,
        output_attentions: bool = ...,
    ): ...

class DetaDecoderLayer(GradientCheckpointingLayer):
    def __init__(self, config: DetaConfig) -> None: ...
    def forward(
        self,
        hidden_states: torch.Tensor,
        position_embeddings: Optional[torch.Tensor] = ...,
        reference_points=...,
        spatial_shapes=...,
        level_start_index=...,
        encoder_hidden_states: Optional[torch.Tensor] = ...,
        encoder_attention_mask: Optional[torch.Tensor] = ...,
        output_attentions: Optional[bool] = ...,
    ): ...

class DetaPreTrainedModel(PreTrainedModel):
    config: DetaConfig
    base_model_prefix = ...
    main_input_name = ...
    _no_split_modules = ...
    supports_gradient_checkpointing = ...

DETA_START_DOCSTRING = ...
DETA_INPUTS_DOCSTRING = ...

class DetaEncoder(DetaPreTrainedModel):
    def __init__(self, config: DetaConfig) -> None: ...
    @staticmethod
    def get_reference_points(spatial_shapes, valid_ratios, device): ...
    def forward(
        self,
        inputs_embeds=...,
        attention_mask=...,
        position_embeddings=...,
        spatial_shapes=...,
        level_start_index=...,
        valid_ratios=...,
        output_attentions=...,
        output_hidden_states=...,
        return_dict=...,
    ): ...

class DetaDecoder(DetaPreTrainedModel):
    def __init__(self, config: DetaConfig) -> None: ...
    def forward(
        self,
        inputs_embeds=...,
        encoder_hidden_states=...,
        encoder_attention_mask=...,
        position_embeddings=...,
        reference_points=...,
        spatial_shapes=...,
        level_start_index=...,
        valid_ratios=...,
        output_attentions=...,
        output_hidden_states=...,
        return_dict=...,
    ): ...

@add_start_docstrings(..., DETA_START_DOCSTRING)
class DetaModel(DetaPreTrainedModel):
    def __init__(self, config: DetaConfig) -> None: ...
    def get_encoder(self): ...
    def get_decoder(self): ...
    def freeze_backbone(self): ...
    def unfreeze_backbone(self): ...
    def get_valid_ratio(self, mask, dtype=...): ...
    def get_proposal_pos_embed(self, proposals): ...
    def gen_encoder_output_proposals(self, enc_output, padding_mask, spatial_shapes): ...
    @add_start_docstrings_to_model_forward(DETA_INPUTS_DOCSTRING)
    @replace_return_docstrings(output_type=DetaModelOutput, config_class=_CONFIG_FOR_DOC)
    def forward(
        self,
        pixel_values: torch.FloatTensor,
        pixel_mask: Optional[torch.LongTensor] = ...,
        decoder_attention_mask: Optional[torch.FloatTensor] = ...,
        encoder_outputs: Optional[torch.FloatTensor] = ...,
        inputs_embeds: Optional[torch.FloatTensor] = ...,
        decoder_inputs_embeds: Optional[torch.FloatTensor] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[tuple[torch.FloatTensor], DetaModelOutput]: ...

@add_start_docstrings(..., DETA_START_DOCSTRING)
class DetaForObjectDetection(DetaPreTrainedModel):
    _tied_weights_keys = ...
    _no_split_modules = ...
    def __init__(self, config: DetaConfig) -> None: ...
    @add_start_docstrings_to_model_forward(DETA_INPUTS_DOCSTRING)
    @replace_return_docstrings(output_type=DetaObjectDetectionOutput, config_class=_CONFIG_FOR_DOC)
    def forward(
        self,
        pixel_values: torch.FloatTensor,
        pixel_mask: Optional[torch.LongTensor] = ...,
        decoder_attention_mask: Optional[torch.FloatTensor] = ...,
        encoder_outputs: Optional[torch.FloatTensor] = ...,
        inputs_embeds: Optional[torch.FloatTensor] = ...,
        decoder_inputs_embeds: Optional[torch.FloatTensor] = ...,
        labels: Optional[list[dict]] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[tuple[torch.FloatTensor], DetaObjectDetectionOutput]: ...

def dice_loss(inputs, targets, num_boxes): ...
def sigmoid_focal_loss(inputs, targets, num_boxes, alpha: float = ..., gamma: float = ...): ...

class DetaLoss(nn.Module):
    def __init__(
        self, matcher, num_classes, focal_alpha, losses, num_queries, assign_first_stage=..., assign_second_stage=...
    ) -> None: ...
    def loss_labels(self, outputs, targets, indices, num_boxes): ...
    @torch.no_grad()
    def loss_cardinality(self, outputs, targets, indices, num_boxes): ...
    def loss_boxes(self, outputs, targets, indices, num_boxes): ...
    def get_loss(self, loss, outputs, targets, indices, num_boxes): ...
    def forward(self, outputs, targets): ...

class DetaMLPPredictionHead(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers) -> None: ...
    def forward(self, x): ...

class DetaHungarianMatcher(nn.Module):
    def __init__(self, class_cost: float = ..., bbox_cost: float = ..., giou_cost: float = ...) -> None: ...
    @torch.no_grad()
    def forward(self, outputs, targets): ...

def box_area(boxes: Tensor) -> Tensor: ...
def box_iou(boxes1, boxes2): ...
def generalized_box_iou(boxes1, boxes2): ...
def nonzero_tuple(x): ...

class DetaMatcher:
    def __init__(self, thresholds: list[float], labels: list[int], allow_low_quality_matches: bool = ...) -> None: ...
    def __call__(self, match_quality_matrix): ...
    def set_low_quality_matches_(self, match_labels, match_quality_matrix): ...

def subsample_labels(labels: torch.Tensor, num_samples: int, positive_fraction: float, bg_label: int): ...
def sample_topk_per_gt(pr_inds, gt_inds, iou, k): ...

class DetaStage2Assigner(nn.Module):
    def __init__(self, num_queries, max_k=...) -> None: ...
    def forward(self, outputs, targets, return_cost_matrix=...): ...
    def postprocess_indices(self, pr_inds, gt_inds, iou): ...

class DetaStage1Assigner(nn.Module):
    def __init__(self, t_low=..., t_high=..., max_k=...) -> None: ...
    def forward(self, outputs, targets): ...
    def postprocess_indices(self, pr_inds, gt_inds, iou): ...

__all__ = ["DetaForObjectDetection", "DetaModel", "DetaPreTrainedModel"]
