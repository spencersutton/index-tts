"""
This type stub file was generated by pyright.
"""

import torch
import torch.nn as nn
from typing import Optional, Union
from ....modeling_outputs import MoECausalLMOutputWithPast, MoEModelOutputWithPastAndCrossAttentions
from ....modeling_utils import PreTrainedModel
from ....utils import add_start_docstrings, add_start_docstrings_to_model_forward
from .configuration_gptsan_japanese import GPTSanJapaneseConfig

logger = ...
_CONFIG_FOR_DOC = ...
_CHECKPOINT_FOR_DOC = ...

def router_z_loss_func(router_logits: torch.Tensor) -> float: ...
def load_balancing_loss_func(router_probs: torch.Tensor, expert_indices: torch.Tensor) -> float: ...

class GPTSanJapaneseDenseActDense(nn.Module):
    def __init__(self, config: GPTSanJapaneseConfig, ext_layer=...) -> None: ...
    def forward(self, hidden_states): ...

class GPTSanJapaneseTop1Router(nn.Module):
    def __init__(self, config: GPTSanJapaneseConfig) -> None: ...
    def forward(self, hidden_states: torch.Tensor) -> tuple: ...

class GPTSanJapaneseSparseMLP(nn.Module):
    def __init__(self, config: GPTSanJapaneseConfig, expert_class: nn.Module = ...) -> None: ...
    def forward(self, hidden_states): ...

class GPTSanJapaneseLayerSparseFF(nn.Module):
    def __init__(self, config: GPTSanJapaneseConfig) -> None: ...
    def forward(self, hidden_states, output_router_logits): ...

class GPTSanJapaneseLayerDenseFF(nn.Module):
    def __init__(self, config: GPTSanJapaneseConfig) -> None: ...
    def forward(self, hidden_states): ...

class GPTSanJapaneseAttention(nn.Module):
    def __init__(
        self,
        embed_dim: int,
        num_heads: int,
        dropout: float = ...,
        is_decoder: bool = ...,
        bias: bool = ...,
        is_causal: bool = ...,
        config: Optional[GPTSanJapaneseConfig] = ...,
    ) -> None: ...
    def forward(
        self,
        hidden_states: torch.Tensor,
        key_value_states: Optional[torch.Tensor] = ...,
        past_key_value: Optional[tuple[torch.Tensor]] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        layer_head_mask: Optional[torch.Tensor] = ...,
        output_attentions: bool = ...,
    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]: ...

class GPTSanJapaneseLayerSelfAttention(nn.Module):
    def __init__(self, config, has_relative_attention_bias=...) -> None: ...
    def forward(
        self,
        hidden_states: Optional[tuple[torch.FloatTensor]],
        past_key_value: Optional[tuple[torch.Tensor]] = ...,
        attention_mask: Optional[torch.FloatTensor] = ...,
        head_mask: Optional[torch.FloatTensor] = ...,
        use_cache: Optional[bool] = ...,
        output_attentions: Optional[bool] = ...,
    ) -> tuple[Union[torch.Tensor, tuple[torch.Tensor]], ...]: ...

class GPTSanJapaneseBlock(nn.Module):
    def __init__(self, config, ext_layer=...) -> None: ...
    def forward(
        self,
        hidden_states: Optional[tuple[torch.FloatTensor]],
        past_key_value: Optional[tuple[torch.Tensor]] = ...,
        attention_mask: Optional[torch.FloatTensor] = ...,
        head_mask: Optional[torch.FloatTensor] = ...,
        use_cache: Optional[bool] = ...,
        output_attentions: Optional[bool] = ...,
        output_router_tuple: Optional[bool] = ...,
    ) -> tuple[Union[torch.Tensor, tuple[torch.Tensor]], ...]: ...

class GPTSanJapanesePreTrainedModel(PreTrainedModel):
    config: GPTSanJapaneseConfig
    base_model_prefix = ...
    supports_gradient_checkpointing = ...
    _no_split_modules = ...
    _skip_keys_device_placement = ...
    @property
    def dummy_inputs(self): ...

GPTSAN_JAPANESE_START_DOCSTRING = ...
GPTSAN_JAPANESE_INPUTS_DOCSTRING = ...

@add_start_docstrings(..., GPTSAN_JAPANESE_START_DOCSTRING)
class GPTSanJapaneseModel(GPTSanJapanesePreTrainedModel):
    def __init__(self, config: GPTSanJapaneseConfig) -> None: ...
    def set_input_embeddings(self, new_embeddings): ...
    @add_start_docstrings_to_model_forward(GPTSAN_JAPANESE_INPUTS_DOCSTRING)
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = ...,
        attention_mask: Optional[torch.FloatTensor] = ...,
        token_type_ids: Optional[torch.FloatTensor] = ...,
        spout: Optional[torch.FloatTensor] = ...,
        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = ...,
        head_mask: Optional[torch.FloatTensor] = ...,
        use_cache: Optional[bool] = ...,
        inputs_embeds: Optional[torch.FloatTensor] = ...,
        decoder_inputs_embeds: Optional[torch.FloatTensor] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
        output_router_logits: Optional[bool] = ...,
        num_precontext: Optional[torch.LongTensor] = ...,
    ) -> Union[MoEModelOutputWithPastAndCrossAttentions, tuple[torch.FloatTensor]]: ...

@add_start_docstrings(..., GPTSAN_JAPANESE_START_DOCSTRING)
class GPTSanJapaneseForConditionalGeneration(GPTSanJapanesePreTrainedModel):
    _tied_weights_keys = ...
    def __init__(self, config: GPTSanJapaneseConfig) -> None: ...
    @add_start_docstrings_to_model_forward(GPTSAN_JAPANESE_INPUTS_DOCSTRING)
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = ...,
        attention_mask: Optional[torch.FloatTensor] = ...,
        token_type_ids: Optional[torch.FloatTensor] = ...,
        spout: Optional[torch.FloatTensor] = ...,
        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = ...,
        head_mask: Optional[torch.FloatTensor] = ...,
        use_cache: Optional[bool] = ...,
        inputs_embeds: Optional[torch.FloatTensor] = ...,
        decoder_inputs_embeds: Optional[torch.FloatTensor] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
        output_router_logits: Optional[bool] = ...,
        labels: Optional[torch.LongTensor] = ...,
    ) -> Union[tuple[torch.FloatTensor], MoECausalLMOutputWithPast]: ...
    def prepare_inputs_for_generation(
        self,
        input_ids: torch.LongTensor,
        attention_mask: torch.FloatTensor,
        token_type_ids: Optional[torch.FloatTensor] = ...,
        spout: Optional[Union[list, torch.FloatTensor]] = ...,
        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = ...,
        **kwargs,
    ): ...
    def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor): ...
    def resize_token_embeddings(
        self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = ..., mean_resizing: bool = ...
    ) -> nn.Embedding: ...
    def get_input_embeddings(self): ...
    def set_input_embeddings(self, new_embeddings): ...

__all__ = ["GPTSanJapaneseForConditionalGeneration", "GPTSanJapaneseModel", "GPTSanJapanesePreTrainedModel"]
