"""
This type stub file was generated by pyright.
"""

import torch
import torch.nn as nn
from collections.abc import Iterable, Iterator
from typing import Optional, Union
from ....modeling_outputs import BaseModelOutputWithNoAttention, SequenceClassifierOutput
from ....modeling_utils import PreTrainedModel
from .configuration_graphormer import GraphormerConfig

logger = ...
_CHECKPOINT_FOR_DOC = ...
_CONFIG_FOR_DOC = ...

def quant_noise(module: nn.Module, p: float, block_size: int): ...

class LayerDropModuleList(nn.ModuleList):
    def __init__(self, p: float, modules: Optional[Iterable[nn.Module]] = ...) -> None: ...
    def __iter__(self) -> Iterator[nn.Module]: ...

class GraphormerGraphNodeFeature(nn.Module):
    def __init__(self, config: GraphormerConfig) -> None: ...
    def forward(
        self, input_nodes: torch.LongTensor, in_degree: torch.LongTensor, out_degree: torch.LongTensor
    ) -> torch.Tensor: ...

class GraphormerGraphAttnBias(nn.Module):
    def __init__(self, config: GraphormerConfig) -> None: ...
    def forward(
        self,
        input_nodes: torch.LongTensor,
        attn_bias: torch.Tensor,
        spatial_pos: torch.LongTensor,
        input_edges: torch.LongTensor,
        attn_edge_type: torch.LongTensor,
    ) -> torch.Tensor: ...

class GraphormerMultiheadAttention(nn.Module):
    def __init__(self, config: GraphormerConfig) -> None: ...
    def reset_parameters(self): ...
    def forward(
        self,
        query: torch.LongTensor,
        key: Optional[torch.Tensor],
        value: Optional[torch.Tensor],
        attn_bias: Optional[torch.Tensor],
        key_padding_mask: Optional[torch.Tensor] = ...,
        need_weights: bool = ...,
        attn_mask: Optional[torch.Tensor] = ...,
        before_softmax: bool = ...,
        need_head_weights: bool = ...,
    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]: ...
    def apply_sparse_mask(self, attn_weights: torch.Tensor, tgt_len: int, src_len: int, bsz: int) -> torch.Tensor: ...

class GraphormerGraphEncoderLayer(nn.Module):
    def __init__(self, config: GraphormerConfig) -> None: ...
    def build_fc(
        self, input_dim: int, output_dim: int, q_noise: float, qn_block_size: int
    ) -> Union[nn.Module, nn.Linear, nn.Embedding, nn.Conv2d]: ...
    def forward(
        self,
        input_nodes: torch.Tensor,
        self_attn_bias: Optional[torch.Tensor] = ...,
        self_attn_mask: Optional[torch.Tensor] = ...,
        self_attn_padding_mask: Optional[torch.Tensor] = ...,
    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]: ...

class GraphormerGraphEncoder(nn.Module):
    def __init__(self, config: GraphormerConfig) -> None: ...
    def forward(
        self,
        input_nodes: torch.LongTensor,
        input_edges: torch.LongTensor,
        attn_bias: torch.Tensor,
        in_degree: torch.LongTensor,
        out_degree: torch.LongTensor,
        spatial_pos: torch.LongTensor,
        attn_edge_type: torch.LongTensor,
        perturb=...,
        last_state_only: bool = ...,
        token_embeddings: Optional[torch.Tensor] = ...,
        attn_mask: Optional[torch.Tensor] = ...,
    ) -> tuple[Union[torch.Tensor, list[torch.LongTensor]], torch.Tensor]: ...

class GraphormerDecoderHead(nn.Module):
    def __init__(self, embedding_dim: int, num_classes: int) -> None: ...
    def forward(self, input_nodes: torch.Tensor, **unused) -> torch.Tensor: ...

class GraphormerPreTrainedModel(PreTrainedModel):
    config: GraphormerConfig
    base_model_prefix = ...
    main_input_name_nodes = ...
    main_input_name_edges = ...
    def normal_(self, data: torch.Tensor): ...
    def init_graphormer_params(self, module: Union[nn.Linear, nn.Embedding, GraphormerMultiheadAttention]): ...

class GraphormerModel(GraphormerPreTrainedModel):
    def __init__(self, config: GraphormerConfig) -> None: ...
    def reset_output_layer_parameters(self): ...
    def forward(
        self,
        input_nodes: torch.LongTensor,
        input_edges: torch.LongTensor,
        attn_bias: torch.Tensor,
        in_degree: torch.LongTensor,
        out_degree: torch.LongTensor,
        spatial_pos: torch.LongTensor,
        attn_edge_type: torch.LongTensor,
        perturb: Optional[torch.FloatTensor] = ...,
        masked_tokens: None = ...,
        return_dict: Optional[bool] = ...,
        **unused,
    ) -> Union[tuple[torch.LongTensor], BaseModelOutputWithNoAttention]: ...
    def max_nodes(self): ...

class GraphormerForGraphClassification(GraphormerPreTrainedModel):
    def __init__(self, config: GraphormerConfig) -> None: ...
    def forward(
        self,
        input_nodes: torch.LongTensor,
        input_edges: torch.LongTensor,
        attn_bias: torch.Tensor,
        in_degree: torch.LongTensor,
        out_degree: torch.LongTensor,
        spatial_pos: torch.LongTensor,
        attn_edge_type: torch.LongTensor,
        labels: Optional[torch.LongTensor] = ...,
        return_dict: Optional[bool] = ...,
        **unused,
    ) -> Union[tuple[torch.Tensor], SequenceClassifierOutput]: ...

__all__ = ["GraphormerForGraphClassification", "GraphormerModel", "GraphormerPreTrainedModel"]
