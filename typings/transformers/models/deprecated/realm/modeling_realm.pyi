"""
This type stub file was generated by pyright.
"""

import torch
from dataclasses import dataclass
from typing import Optional, Union
from torch import nn
from ....modeling_layers import GradientCheckpointingLayer
from ....modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, MaskedLMOutput, ModelOutput
from ....modeling_utils import PreTrainedModel
from ....utils import add_start_docstrings, add_start_docstrings_to_model_forward, replace_return_docstrings
from .configuration_realm import RealmConfig

logger = ...
_EMBEDDER_CHECKPOINT_FOR_DOC = ...
_ENCODER_CHECKPOINT_FOR_DOC = ...
_SCORER_CHECKPOINT_FOR_DOC = ...
_CONFIG_FOR_DOC = ...

def load_tf_weights_in_realm(model, config, tf_checkpoint_path): ...

class RealmEmbeddings(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = ...,
        token_type_ids: Optional[torch.LongTensor] = ...,
        position_ids: Optional[torch.LongTensor] = ...,
        inputs_embeds: Optional[torch.FloatTensor] = ...,
        past_key_values_length: int = ...,
    ) -> torch.Tensor: ...

class RealmSelfAttention(nn.Module):
    def __init__(self, config, position_embedding_type=...) -> None: ...
    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor: ...
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.FloatTensor] = ...,
        head_mask: Optional[torch.FloatTensor] = ...,
        encoder_hidden_states: Optional[torch.FloatTensor] = ...,
        encoder_attention_mask: Optional[torch.FloatTensor] = ...,
        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = ...,
        output_attentions: Optional[bool] = ...,
    ) -> tuple[torch.Tensor]: ...

class RealmSelfOutput(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor: ...

REALM_SELF_ATTENTION_CLASSES = ...

class RealmAttention(nn.Module):
    def __init__(self, config, position_embedding_type=...) -> None: ...
    def prune_heads(self, heads): ...
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.FloatTensor] = ...,
        head_mask: Optional[torch.FloatTensor] = ...,
        encoder_hidden_states: Optional[torch.FloatTensor] = ...,
        encoder_attention_mask: Optional[torch.FloatTensor] = ...,
        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = ...,
        output_attentions: Optional[bool] = ...,
    ) -> tuple[torch.Tensor]: ...

class RealmIntermediate(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor: ...

class RealmOutput(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor: ...

class RealmLayer(GradientCheckpointingLayer):
    def __init__(self, config) -> None: ...
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.FloatTensor] = ...,
        head_mask: Optional[torch.FloatTensor] = ...,
        encoder_hidden_states: Optional[torch.FloatTensor] = ...,
        encoder_attention_mask: Optional[torch.FloatTensor] = ...,
        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = ...,
        output_attentions: Optional[bool] = ...,
    ) -> tuple[torch.Tensor]: ...
    def feed_forward_chunk(self, attention_output): ...

class RealmEncoder(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.FloatTensor] = ...,
        head_mask: Optional[torch.FloatTensor] = ...,
        encoder_hidden_states: Optional[torch.FloatTensor] = ...,
        encoder_attention_mask: Optional[torch.FloatTensor] = ...,
        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = ...,
        use_cache: Optional[bool] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]: ...

class RealmPooler(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor: ...

@dataclass
class RealmEmbedderOutput(ModelOutput):
    projected_score: Optional[torch.FloatTensor] = ...
    hidden_states: Optional[tuple[torch.FloatTensor]] = ...
    attentions: Optional[tuple[torch.FloatTensor]] = ...

@dataclass
class RealmScorerOutput(ModelOutput):
    relevance_score: Optional[torch.FloatTensor] = ...
    query_score: Optional[torch.FloatTensor] = ...
    candidate_score: Optional[torch.FloatTensor] = ...

@dataclass
class RealmReaderOutput(ModelOutput):
    loss: Optional[torch.FloatTensor] = ...
    retriever_loss: Optional[torch.FloatTensor] = ...
    reader_loss: Optional[torch.FloatTensor] = ...
    retriever_correct: torch.BoolTensor = ...
    reader_correct: torch.BoolTensor = ...
    block_idx: Optional[torch.LongTensor] = ...
    candidate: Optional[torch.LongTensor] = ...
    start_pos: torch.int32 = ...
    end_pos: torch.int32 = ...
    hidden_states: Optional[tuple[torch.FloatTensor]] = ...
    attentions: Optional[tuple[torch.FloatTensor]] = ...

@dataclass
class RealmForOpenQAOutput(ModelOutput):
    reader_output: dict = ...
    predicted_answer_ids: Optional[torch.LongTensor] = ...

class RealmPredictionHeadTransform(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states): ...

class RealmLMPredictionHead(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states): ...

class RealmOnlyMLMHead(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, sequence_output): ...

class RealmScorerProjection(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states): ...

class RealmReaderProjection(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states, block_mask): ...

REALM_START_DOCSTRING = ...
REALM_INPUTS_DOCSTRING = ...

class RealmPreTrainedModel(PreTrainedModel):
    config: RealmConfig
    load_tf_weights = ...
    base_model_prefix = ...

class RealmBertModel(RealmPreTrainedModel):
    def __init__(self, config, add_pooling_layer=...) -> None: ...
    def get_input_embeddings(self): ...
    def set_input_embeddings(self, value): ...
    def forward(
        self,
        input_ids=...,
        attention_mask=...,
        token_type_ids=...,
        position_ids=...,
        head_mask=...,
        inputs_embeds=...,
        encoder_hidden_states=...,
        encoder_attention_mask=...,
        past_key_values=...,
        use_cache=...,
        output_attentions=...,
        output_hidden_states=...,
        return_dict=...,
    ): ...

@add_start_docstrings(..., REALM_START_DOCSTRING)
class RealmEmbedder(RealmPreTrainedModel):
    _tied_weights_keys = ...
    def __init__(self, config) -> None: ...
    def get_input_embeddings(self): ...
    def set_input_embeddings(self, value): ...
    @add_start_docstrings_to_model_forward(REALM_INPUTS_DOCSTRING.format("batch_size, sequence_length"))
    @replace_return_docstrings(output_type=RealmEmbedderOutput, config_class=_CONFIG_FOR_DOC)
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = ...,
        attention_mask: Optional[torch.FloatTensor] = ...,
        token_type_ids: Optional[torch.LongTensor] = ...,
        position_ids: Optional[torch.LongTensor] = ...,
        head_mask: Optional[torch.FloatTensor] = ...,
        inputs_embeds: Optional[torch.FloatTensor] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[tuple, RealmEmbedderOutput]: ...

@add_start_docstrings(..., REALM_START_DOCSTRING)
class RealmScorer(RealmPreTrainedModel):
    def __init__(self, config, query_embedder=...) -> None: ...
    @add_start_docstrings_to_model_forward(REALM_INPUTS_DOCSTRING.format("batch_size, sequence_length"))
    @replace_return_docstrings(output_type=RealmScorerOutput, config_class=_CONFIG_FOR_DOC)
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = ...,
        attention_mask: Optional[torch.FloatTensor] = ...,
        token_type_ids: Optional[torch.LongTensor] = ...,
        position_ids: Optional[torch.LongTensor] = ...,
        candidate_input_ids: Optional[torch.LongTensor] = ...,
        candidate_attention_mask: Optional[torch.FloatTensor] = ...,
        candidate_token_type_ids: Optional[torch.LongTensor] = ...,
        candidate_inputs_embeds: Optional[torch.FloatTensor] = ...,
        head_mask: Optional[torch.FloatTensor] = ...,
        inputs_embeds: Optional[torch.FloatTensor] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[tuple, RealmScorerOutput]: ...

@add_start_docstrings(..., REALM_START_DOCSTRING)
class RealmKnowledgeAugEncoder(RealmPreTrainedModel):
    _tied_weights_keys = ...
    def __init__(self, config) -> None: ...
    def get_input_embeddings(self): ...
    def set_input_embeddings(self, value): ...
    def get_output_embeddings(self): ...
    def set_output_embeddings(self, new_embeddings): ...
    @add_start_docstrings_to_model_forward(REALM_INPUTS_DOCSTRING.format("batch_size, num_candidates, sequence_length"))
    @replace_return_docstrings(output_type=MaskedLMOutput, config_class=_CONFIG_FOR_DOC)
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = ...,
        attention_mask: Optional[torch.FloatTensor] = ...,
        token_type_ids: Optional[torch.LongTensor] = ...,
        position_ids: Optional[torch.LongTensor] = ...,
        head_mask: Optional[torch.FloatTensor] = ...,
        inputs_embeds: Optional[torch.FloatTensor] = ...,
        relevance_score: Optional[torch.FloatTensor] = ...,
        labels: Optional[torch.LongTensor] = ...,
        mlm_mask: Optional[torch.LongTensor] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[tuple, MaskedLMOutput]: ...

@add_start_docstrings("The reader of REALM.", REALM_START_DOCSTRING)
class RealmReader(RealmPreTrainedModel):
    def __init__(self, config) -> None: ...
    @add_start_docstrings_to_model_forward(REALM_INPUTS_DOCSTRING.format("reader_beam_size, sequence_length"))
    @replace_return_docstrings(output_type=RealmReaderOutput, config_class=_CONFIG_FOR_DOC)
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = ...,
        attention_mask: Optional[torch.FloatTensor] = ...,
        token_type_ids: Optional[torch.LongTensor] = ...,
        position_ids: Optional[torch.LongTensor] = ...,
        head_mask: Optional[torch.FloatTensor] = ...,
        inputs_embeds: Optional[torch.FloatTensor] = ...,
        relevance_score: Optional[torch.FloatTensor] = ...,
        block_mask: Optional[torch.BoolTensor] = ...,
        start_positions: Optional[torch.LongTensor] = ...,
        end_positions: Optional[torch.LongTensor] = ...,
        has_answers: Optional[torch.BoolTensor] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[tuple, RealmReaderOutput]: ...

REALM_FOR_OPEN_QA_DOCSTRING = ...

@add_start_docstrings(..., REALM_START_DOCSTRING)
class RealmForOpenQA(RealmPreTrainedModel):
    def __init__(self, config, retriever=...) -> None: ...
    @property
    def searcher_beam_size(self): ...
    def block_embedding_to(self, device): ...
    @add_start_docstrings_to_model_forward(REALM_FOR_OPEN_QA_DOCSTRING.format("1, sequence_length"))
    @replace_return_docstrings(output_type=RealmForOpenQAOutput, config_class=_CONFIG_FOR_DOC)
    def forward(
        self,
        input_ids: Optional[torch.LongTensor],
        attention_mask: Optional[torch.FloatTensor] = ...,
        token_type_ids: Optional[torch.LongTensor] = ...,
        answer_ids: Optional[torch.LongTensor] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[tuple, RealmForOpenQAOutput]: ...

__all__ = [
    "RealmEmbedder",
    "RealmForOpenQA",
    "RealmKnowledgeAugEncoder",
    "RealmPreTrainedModel",
    "RealmReader",
    "RealmScorer",
    "load_tf_weights_in_realm",
]
