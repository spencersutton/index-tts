"""
This type stub file was generated by pyright.
"""

from typing import Optional
from ....tokenization_utils import PreTrainedTokenizer
from ....utils import is_sacremoses_available, is_torch_available, torch_only_method

if is_sacremoses_available(): ...
if is_torch_available(): ...
logger = ...
VOCAB_FILES_NAMES = ...
PRETRAINED_CORPUS_ARCHIVE_MAP = ...
CORPUS_NAME = ...
MATCH_NUMBERS = ...
DETOKENIZE_NUMBERS = ...

def tokenize_numbers(text_array: list[str]) -> list[str]: ...
def detokenize_numbers(text: str) -> str: ...

class TransfoXLTokenizer(PreTrainedTokenizer):
    vocab_files_names = ...
    model_input_names = ...
    def __init__(
        self,
        special=...,
        min_freq=...,
        max_size=...,
        lower_case=...,
        delimiter=...,
        vocab_file=...,
        pretrained_vocab_file: Optional[str] = ...,
        never_split=...,
        unk_token=...,
        eos_token=...,
        additional_special_tokens=...,
        language=...,
        **kwargs,
    ) -> None: ...
    @property
    def do_lower_case(self): ...
    def count_file(self, path, verbose=..., add_eos=...): ...
    def count_sents(self, sents, verbose=...): ...
    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = ...) -> tuple[str]: ...
    def build_vocab(self): ...
    @torch_only_method
    def encode_file(self, path, ordered=..., verbose=..., add_eos=..., add_double_eos=...): ...
    @torch_only_method
    def encode_sents(self, sents, ordered=..., verbose=...): ...
    def add_special(self, sym): ...
    def add_symbol(self, sym): ...
    def move_added_token(self, token: str, target_idx: int): ...
    def moses_punct_norm(self, text): ...
    def moses_tokenize(self, text): ...
    def moses_pipeline(self, text: str) -> list[str]: ...
    def convert_tokens_to_string(self, tokens): ...
    @torch_only_method
    def convert_to_tensor(self, symbols): ...
    @property
    def vocab_size(self): ...
    def get_vocab(self): ...

class LMOrderedIterator:
    def __init__(self, data, bsz, bptt, device=..., ext_len=...) -> None: ...
    def get_batch(self, i, bptt=...): ...
    def get_fixlen_iter(self, start=...): ...
    def get_varlen_iter(self, start=..., std=..., min_len=..., max_deviation=...): ...
    def __iter__(self): ...

class LMShuffledIterator:
    def __init__(self, data, bsz, bptt, device=..., ext_len=..., shuffle=...) -> None: ...
    def get_sent_stream(self): ...
    @torch_only_method
    def stream_iterator(self, sent_stream): ...
    def __iter__(self): ...

class LMMultiFileIterator(LMShuffledIterator):
    def __init__(self, paths, vocab, bsz, bptt, device=..., ext_len=..., shuffle=...) -> None: ...
    def get_sent_stream(self, path): ...
    def __iter__(self): ...

class TransfoXLCorpus:
    @classmethod
    @torch_only_method
    def from_pretrained(cls, pretrained_model_name_or_path, cache_dir=..., *inputs, **kwargs): ...
    def __init__(self, *args, **kwargs) -> None: ...
    def build_corpus(self, path, dataset): ...
    def get_iterator(self, split, *args, **kwargs): ...

@torch_only_method
def get_lm_corpus(datadir, dataset): ...

__all__ = ["TransfoXLCorpus", "TransfoXLTokenizer"]
