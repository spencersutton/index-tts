from ....tokenization_utils import PreTrainedTokenizer
from ....utils import is_sacremoses_available, is_torch_available, torch_only_method

"""
Tokenization classes for Transformer XL model. Adapted from https://github.com/kimiyoung/transformer-xl.
"""
if is_sacremoses_available(): ...
if is_torch_available(): ...
logger = ...
VOCAB_FILES_NAMES = ...
PRETRAINED_CORPUS_ARCHIVE_MAP = ...
CORPUS_NAME = ...
MATCH_NUMBERS = ...
DETOKENIZE_NUMBERS = ...

def tokenize_numbers(text_array: list[str]) -> list[str]: ...
def detokenize_numbers(text: str) -> str: ...

class TransfoXLTokenizer(PreTrainedTokenizer):
    vocab_files_names = ...
    model_input_names = ...
    def __init__(
        self,
        special=...,
        min_freq=...,
        max_size=...,
        lower_case=...,
        delimiter=...,
        vocab_file=...,
        pretrained_vocab_file: str | None = ...,
        never_split=...,
        unk_token=...,
        eos_token=...,
        additional_special_tokens=...,
        language=...,
        **kwargs,
    ) -> None: ...
    @property
    def do_lower_case(self):  # -> bool:
        ...
    def count_file(self, path, verbose=..., add_eos=...):  # -> list[Any]:
        ...
    def count_sents(self, sents, verbose=...):  # -> None:

        ...
    def save_vocabulary(self, save_directory: str, filename_prefix: str | None = ...) -> tuple[str]: ...
    def build_vocab(self):  # -> None:
        ...
    @torch_only_method
    def encode_file(self, path, ordered=..., verbose=..., add_eos=..., add_double_eos=...):  # -> Tensor | list[Any]:
        ...
    @torch_only_method
    def encode_sents(self, sents, ordered=..., verbose=...):  # -> Tensor | list[Any]:
        ...
    def add_special(self, sym):  # -> None:
        ...
    def add_symbol(self, sym):  # -> None:
        ...
    def move_added_token(self, token: str, target_idx: int):  # -> None:

        ...
    def moses_punct_norm(self, text): ...
    def moses_tokenize(self, text): ...
    def moses_pipeline(self, text: str) -> list[str]: ...
    def convert_tokens_to_string(self, tokens):  # -> str:

        ...
    @torch_only_method
    def convert_to_tensor(self, symbols):  # -> LongTensor:
        ...
    @property
    def vocab_size(self):  # -> int:
        ...
    def get_vocab(self):  # -> OrderedDict[Any, Any]:
        ...

class LMOrderedIterator:
    def __init__(self, data, bsz, bptt, device=..., ext_len=...) -> None: ...
    def get_batch(self, i, bptt=...):  # -> tuple[Any, Any, Any]:
        ...
    def get_fixlen_iter(self, start=...):  # -> Generator[tuple[Any, Any, Any], Any, None]:
        ...
    def get_varlen_iter(
        self, start=..., std=..., min_len=..., max_deviation=...
    ):  # -> Generator[tuple[Any, Any, Any], Any, None]:
        ...
    def __iter__(self):  # -> Generator[tuple[Any, Any, Any], Any, None]:
        ...

class LMShuffledIterator:
    def __init__(self, data, bsz, bptt, device=..., ext_len=..., shuffle=...) -> None: ...
    def get_sent_stream(self):  # -> Generator[Any, Any, None]:
        ...
    @torch_only_method
    def stream_iterator(self, sent_stream):  # -> Generator[tuple[Tensor, Tensor, Any], Any, None]:
        ...
    def __iter__(self):  # -> Generator[tuple[Tensor, Tensor, Any], Any, None]:
        ...

class LMMultiFileIterator(LMShuffledIterator):
    def __init__(self, paths, vocab, bsz, bptt, device=..., ext_len=..., shuffle=...) -> None: ...
    def get_sent_stream(self, path): ...
    def __iter__(self):  # -> Generator[tuple[Tensor, Tensor, Any], Any, None]:
        ...

class TransfoXLCorpus:
    @classmethod
    @torch_only_method
    def from_pretrained(cls, pretrained_model_name_or_path, cache_dir=..., *inputs, **kwargs):  # -> Self | None:

        ...
    def __init__(self, *args, **kwargs) -> None: ...
    def build_corpus(self, path, dataset):  # -> None:
        ...
    def get_iterator(self, split, *args, **kwargs):  # -> LMOrderedIterator | LMMultiFileIterator | LMShuffledIterator:
        ...

@torch_only_method
def get_lm_corpus(datadir, dataset):  # -> Any | TransfoXLCorpus:
    ...

__all__ = ["TransfoXLCorpus", "TransfoXLTokenizer"]
