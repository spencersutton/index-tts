"""
This type stub file was generated by pyright.
"""

import torch
from dataclasses import dataclass
from typing import Optional, Union
from torch import nn
from ....modeling_layers import GradientCheckpointingLayer
from ....modeling_outputs import SequenceClassifierOutput
from ....modeling_utils import PreTrainedModel
from ....utils import (
    ModelOutput,
    add_start_docstrings,
    add_start_docstrings_to_model_forward,
    replace_return_docstrings,
)
from .configuration_tvlt import TvltConfig

logger = ...
_CONFIG_FOR_DOC = ...
_CHECKPOINT_FOR_DOC = ...

@dataclass
class TvltModelOutput(ModelOutput):
    last_hidden_state: Optional[torch.FloatTensor] = ...
    last_pixel_hidden_state: Optional[torch.FloatTensor] = ...
    last_audio_hidden_state: Optional[torch.FloatTensor] = ...
    pixel_label_masks: Optional[torch.LongTensor] = ...
    audio_label_masks: Optional[torch.LongTensor] = ...
    pixel_ids_restore: Optional[torch.LongTensor] = ...
    audio_ids_restore: Optional[torch.LongTensor] = ...
    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = ...
    attentions: Optional[tuple[torch.FloatTensor, ...]] = ...

@dataclass
class TvltDecoderOutput(ModelOutput):
    logits: Optional[torch.FloatTensor] = ...
    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = ...
    attentions: Optional[tuple[torch.FloatTensor, ...]] = ...

@dataclass
class TvltForPreTrainingOutput(ModelOutput):
    loss: Optional[torch.FloatTensor] = ...
    matching_logits: Optional[torch.FloatTensor] = ...
    pixel_logits: Optional[torch.FloatTensor] = ...
    audio_logits: Optional[torch.FloatTensor] = ...
    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = ...
    attentions: Optional[tuple[torch.FloatTensor, ...]] = ...

def generate_pixel_mask_noise(pixel_values, pixel_mask=..., mask_ratio=...): ...
def generate_audio_mask_noise(audio_values, audio_mask=..., mask_ratio=..., mask_type=..., freq_len=...): ...
def random_masking(sequence, noise, len_keep, attention_masks=...): ...

class TvltPixelEmbeddings(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, pixel_values, attention_masks=...): ...

class TvltAudioEmbeddings(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, audio_values, attention_masks=...): ...

class TvltPixelPatchEmbeddings(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, pixel_values: torch.Tensor) -> torch.Tensor: ...

class TvltAudioPatchEmbeddings(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, audio_values: torch.Tensor) -> torch.Tensor: ...

class TvltSelfAttention(nn.Module):
    def __init__(self, config) -> None: ...
    def transpose_for_scores(self, x): ...
    def forward(self, hidden_states, attention_mask=..., head_mask=..., output_attentions=...): ...

class TvltSelfOutput(nn.Module):
    def __init__(self, config: TvltConfig) -> None: ...
    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor: ...

class TvltAttention(nn.Module):
    def __init__(self, config) -> None: ...
    def prune_heads(self, heads): ...
    def forward(self, hidden_states, attention_mask=..., head_mask=..., output_attentions=...): ...

class TvltIntermediate(nn.Module):
    def __init__(self, config: TvltConfig) -> None: ...
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor: ...

class TvltOutput(nn.Module):
    def __init__(self, config: TvltConfig) -> None: ...
    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor: ...

class TvltLayer(GradientCheckpointingLayer):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states, attention_mask=..., head_mask=..., output_attentions=...): ...

class TvltEncoder(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(
        self,
        hidden_states,
        attention_mask=...,
        head_mask=...,
        output_attentions=...,
        output_hidden_states=...,
        return_dict=...,
    ): ...

class TvltPreTrainedModel(PreTrainedModel):
    config: TvltConfig
    base_model_prefix = ...
    main_input_name = ...
    supports_gradient_checkpointing = ...

TVLT_START_DOCSTRING = ...
TVLT_INPUTS_DOCSTRING = ...

@add_start_docstrings(..., TVLT_START_DOCSTRING)
class TvltModel(TvltPreTrainedModel):
    def __init__(self, config) -> None: ...
    def get_input_embeddings(self): ...
    @add_start_docstrings_to_model_forward(TVLT_INPUTS_DOCSTRING)
    @replace_return_docstrings(output_type=TvltModelOutput, config_class=_CONFIG_FOR_DOC)
    def forward(
        self,
        pixel_values: torch.FloatTensor,
        audio_values: torch.FloatTensor,
        pixel_mask: Optional[torch.FloatTensor] = ...,
        audio_mask: Optional[torch.FloatTensor] = ...,
        mask_pixel: bool = ...,
        mask_audio: bool = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[tuple[torch.FloatTensor], TvltModelOutput]: ...

class TvltDecoder(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states, output_attentions=..., output_hidden_states=..., return_dict=...): ...

@add_start_docstrings(..., TVLT_START_DOCSTRING)
class TvltForPreTraining(TvltPreTrainedModel):
    def __init__(self, config) -> None: ...
    def patchify_pixel(self, pixel_values): ...
    def patchify_audio(self, audio_values): ...
    def pixel_mae_loss(self, pixel_values, pixel_predictions, mask): ...
    def audio_mae_loss(self, audio_values, audio_predictions, mask): ...
    def concatenate_mask(self, mask_token, sequence, ids_restore): ...
    @add_start_docstrings_to_model_forward(TVLT_INPUTS_DOCSTRING)
    @replace_return_docstrings(output_type=TvltForPreTrainingOutput, config_class=_CONFIG_FOR_DOC)
    def forward(
        self,
        pixel_values: torch.FloatTensor,
        audio_values: torch.FloatTensor,
        pixel_mask: Optional[torch.FloatTensor] = ...,
        audio_mask: Optional[torch.FloatTensor] = ...,
        labels: Optional[torch.LongTensor] = ...,
        pixel_values_mixed: Optional[torch.FloatTensor] = ...,
        pixel_mask_mixed: Optional[torch.FloatTensor] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[tuple[torch.FloatTensor], TvltForPreTrainingOutput]: ...

class TvltPooler(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states): ...

class TvltMatchingHead(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states): ...

class TvltMAEHead(nn.Module):
    def __init__(self, config, output_dim=...) -> None: ...
    def forward(self, hidden_states): ...

@add_start_docstrings(..., TVLT_START_DOCSTRING)
class TvltForAudioVisualClassification(TvltPreTrainedModel):
    def __init__(self, config) -> None: ...
    @add_start_docstrings_to_model_forward(TVLT_INPUTS_DOCSTRING)
    @replace_return_docstrings(output_type=SequenceClassifierOutput, config_class=_CONFIG_FOR_DOC)
    def forward(
        self,
        pixel_values: torch.FloatTensor,
        audio_values: torch.FloatTensor,
        pixel_mask: Optional[torch.FloatTensor] = ...,
        audio_mask: Optional[torch.FloatTensor] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
        labels: Optional[torch.LongTensor] = ...,
    ) -> Union[tuple[torch.FloatTensor], SequenceClassifierOutput]: ...

__all__ = ["TvltModel", "TvltForPreTraining", "TvltForAudioVisualClassification", "TvltPreTrainedModel"]
