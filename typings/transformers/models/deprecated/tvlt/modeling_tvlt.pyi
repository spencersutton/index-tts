from dataclasses import dataclass

import torch
from torch import nn

from ....modeling_layers import GradientCheckpointingLayer
from ....modeling_outputs import SequenceClassifierOutput
from ....modeling_utils import PreTrainedModel
from ....utils import (
    ModelOutput,
    add_start_docstrings,
    add_start_docstrings_to_model_forward,
    replace_return_docstrings,
)
from .configuration_tvlt import TvltConfig

"""PyTorch TVLT model."""
logger = ...
_CONFIG_FOR_DOC = ...
_CHECKPOINT_FOR_DOC = ...

@dataclass
class TvltModelOutput(ModelOutput):
    last_hidden_state: torch.FloatTensor | None = ...
    last_pixel_hidden_state: torch.FloatTensor | None = ...
    last_audio_hidden_state: torch.FloatTensor | None = ...
    pixel_label_masks: torch.LongTensor | None = ...
    audio_label_masks: torch.LongTensor | None = ...
    pixel_ids_restore: torch.LongTensor | None = ...
    audio_ids_restore: torch.LongTensor | None = ...
    hidden_states: tuple[torch.FloatTensor, ...] | None = ...
    attentions: tuple[torch.FloatTensor, ...] | None = ...

@dataclass
class TvltDecoderOutput(ModelOutput):
    logits: torch.FloatTensor | None = ...
    hidden_states: tuple[torch.FloatTensor, ...] | None = ...
    attentions: tuple[torch.FloatTensor, ...] | None = ...

@dataclass
class TvltForPreTrainingOutput(ModelOutput):
    loss: torch.FloatTensor | None = ...
    matching_logits: torch.FloatTensor | None = ...
    pixel_logits: torch.FloatTensor | None = ...
    audio_logits: torch.FloatTensor | None = ...
    hidden_states: tuple[torch.FloatTensor, ...] | None = ...
    attentions: tuple[torch.FloatTensor, ...] | None = ...

def generate_pixel_mask_noise(pixel_values, pixel_mask=..., mask_ratio=...):  # -> tuple[Tensor, int]:

    ...
def generate_audio_mask_noise(
    audio_values, audio_mask=..., mask_ratio=..., mask_type=..., freq_len=...
):  # -> tuple[Tensor | Any, int]:

    ...
def random_masking(
    sequence, noise, len_keep, attention_masks=...
):  # -> tuple[Tensor, Tensor | None, Any | Tensor, Tensor]:

    ...

class TvltPixelEmbeddings(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, pixel_values, attention_masks=...):  # -> tuple[Any, Any | None]:
        ...

class TvltAudioEmbeddings(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, audio_values, attention_masks=...):  # -> tuple[Any, Any | None]:
        ...

class TvltPixelPatchEmbeddings(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, pixel_values: torch.Tensor) -> torch.Tensor: ...

class TvltAudioPatchEmbeddings(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, audio_values: torch.Tensor) -> torch.Tensor: ...

class TvltSelfAttention(nn.Module):
    def __init__(self, config) -> None: ...
    def transpose_for_scores(self, x): ...
    def forward(
        self, hidden_states, attention_mask=..., head_mask=..., output_attentions=...
    ):  # -> tuple[Tensor, Any] | tuple[Tensor]:
        ...

class TvltSelfOutput(nn.Module):
    def __init__(self, config: TvltConfig) -> None: ...
    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor: ...

class TvltAttention(nn.Module):
    def __init__(self, config) -> None: ...
    def prune_heads(self, heads):  # -> None:
        ...
    def forward(self, hidden_states, attention_mask=..., head_mask=..., output_attentions=...):  # -> Any:
        ...

class TvltIntermediate(nn.Module):
    def __init__(self, config: TvltConfig) -> None: ...
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor: ...

class TvltOutput(nn.Module):
    def __init__(self, config: TvltConfig) -> None: ...
    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor: ...

class TvltLayer(GradientCheckpointingLayer):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states, attention_mask=..., head_mask=..., output_attentions=...):  # -> Any:
        ...

class TvltEncoder(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(
        self,
        hidden_states,
        attention_mask=...,
        head_mask=...,
        output_attentions=...,
        output_hidden_states=...,
        return_dict=...,
    ):  # -> tuple[Any | tuple[Any, ...] | tuple[()], ...] | BaseModelOutput:
        ...

class TvltPreTrainedModel(PreTrainedModel):
    config: TvltConfig
    base_model_prefix = ...
    main_input_name = ...
    supports_gradient_checkpointing = ...

TVLT_START_DOCSTRING = ...
TVLT_INPUTS_DOCSTRING = ...

@add_start_docstrings(
    ...,
    TVLT_START_DOCSTRING,
)
class TvltModel(TvltPreTrainedModel):
    def __init__(self, config) -> None: ...
    def get_input_embeddings(self):  # -> tuple[TvltPixelPatchEmbeddings, TvltAudioPatchEmbeddings]:
        ...
    @add_start_docstrings_to_model_forward(TVLT_INPUTS_DOCSTRING)
    @replace_return_docstrings(output_type=TvltModelOutput, config_class=_CONFIG_FOR_DOC)
    def forward(
        self,
        pixel_values: torch.FloatTensor,
        audio_values: torch.FloatTensor,
        pixel_mask: torch.FloatTensor | None = ...,
        audio_mask: torch.FloatTensor | None = ...,
        mask_pixel: bool = ...,
        mask_audio: bool = ...,
        output_attentions: bool | None = ...,
        output_hidden_states: bool | None = ...,
        return_dict: bool | None = ...,
    ) -> tuple[torch.FloatTensor] | TvltModelOutput: ...

class TvltDecoder(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(
        self, hidden_states, output_attentions=..., output_hidden_states=..., return_dict=...
    ):  # -> tuple[Any | tuple[Any, ...] | tuple[()], ...] | TvltDecoderOutput:
        ...

@add_start_docstrings(..., TVLT_START_DOCSTRING)
class TvltForPreTraining(TvltPreTrainedModel):
    def __init__(self, config) -> None: ...
    def patchify_pixel(self, pixel_values):  # -> Tensor:

        ...
    def patchify_audio(self, audio_values):  # -> Tensor:

        ...
    def pixel_mae_loss(self, pixel_values, pixel_predictions, mask): ...
    def audio_mae_loss(self, audio_values, audio_predictions, mask): ...
    def concatenate_mask(self, mask_token, sequence, ids_restore):  # -> Tensor:
        ...
    @add_start_docstrings_to_model_forward(TVLT_INPUTS_DOCSTRING)
    @replace_return_docstrings(output_type=TvltForPreTrainingOutput, config_class=_CONFIG_FOR_DOC)
    def forward(
        self,
        pixel_values: torch.FloatTensor,
        audio_values: torch.FloatTensor,
        pixel_mask: torch.FloatTensor | None = ...,
        audio_mask: torch.FloatTensor | None = ...,
        labels: torch.LongTensor | None = ...,
        pixel_values_mixed: torch.FloatTensor | None = ...,
        pixel_mask_mixed: torch.FloatTensor | None = ...,
        output_attentions: bool | None = ...,
        output_hidden_states: bool | None = ...,
        return_dict: bool | None = ...,
    ) -> tuple[torch.FloatTensor] | TvltForPreTrainingOutput: ...

class TvltPooler(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states):  # -> Any:
        ...

class TvltMatchingHead(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states):  # -> Any:
        ...

class TvltMAEHead(nn.Module):
    def __init__(self, config, output_dim=...) -> None: ...
    def forward(self, hidden_states):  # -> Any:
        ...

@add_start_docstrings(
    ...,
    TVLT_START_DOCSTRING,
)
class TvltForAudioVisualClassification(TvltPreTrainedModel):
    def __init__(self, config) -> None: ...
    @add_start_docstrings_to_model_forward(TVLT_INPUTS_DOCSTRING)
    @replace_return_docstrings(output_type=SequenceClassifierOutput, config_class=_CONFIG_FOR_DOC)
    def forward(
        self,
        pixel_values: torch.FloatTensor,
        audio_values: torch.FloatTensor,
        pixel_mask: torch.FloatTensor | None = ...,
        audio_mask: torch.FloatTensor | None = ...,
        output_attentions: bool | None = ...,
        output_hidden_states: bool | None = ...,
        return_dict: bool | None = ...,
        labels: torch.LongTensor | None = ...,
    ) -> tuple[torch.FloatTensor] | SequenceClassifierOutput: ...

__all__ = ["TvltForAudioVisualClassification", "TvltForPreTraining", "TvltModel", "TvltPreTrainedModel"]
