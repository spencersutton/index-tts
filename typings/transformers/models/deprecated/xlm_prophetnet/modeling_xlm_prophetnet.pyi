"""
This type stub file was generated by pyright.
"""

import torch
from dataclasses import dataclass
from typing import Optional, Union
from torch import Tensor, nn
from ....modeling_layers import GradientCheckpointingLayer
from ....modeling_outputs import BaseModelOutput
from ....modeling_utils import PreTrainedModel
from ....utils import (
    ModelOutput,
    add_start_docstrings,
    add_start_docstrings_to_model_forward,
    replace_return_docstrings,
)
from .configuration_xlm_prophetnet import XLMProphetNetConfig

logger = ...
_CONFIG_FOR_DOC = ...
XLM_PROPHETNET_START_DOCSTRING = ...
XLM_PROPHETNET_INPUTS_DOCSTRING = ...
XLM_PROPHETNET_STANDALONE_INPUTS_DOCSTRING = ...

def softmax(hidden_state, dim, onnx_trace=...): ...
def ngram_attention_bias(sequence_length, ngram, device, dtype): ...
def compute_relative_buckets(num_buckets, max_distance, relative_positions, is_bidirectional=...): ...
def compute_all_stream_relative_buckets(num_buckets, max_distance, position_ids): ...

@dataclass
class XLMProphetNetSeq2SeqLMOutput(ModelOutput):
    loss: Optional[torch.FloatTensor] = ...
    logits: Optional[torch.FloatTensor] = ...
    logits_ngram: Optional[torch.FloatTensor] = ...
    past_key_values: Optional[tuple[torch.FloatTensor]] = ...
    decoder_hidden_states: Optional[tuple[torch.FloatTensor]] = ...
    decoder_ngram_hidden_states: Optional[tuple[torch.FloatTensor]] = ...
    decoder_attentions: Optional[tuple[torch.FloatTensor]] = ...
    decoder_ngram_attentions: Optional[tuple[torch.FloatTensor]] = ...
    cross_attentions: Optional[tuple[torch.FloatTensor]] = ...
    encoder_last_hidden_state: Optional[torch.FloatTensor] = ...
    encoder_hidden_states: Optional[tuple[torch.FloatTensor]] = ...
    encoder_attentions: Optional[tuple[torch.FloatTensor]] = ...
    @property
    def decoder_cross_attentions(self): ...

@dataclass
class XLMProphetNetSeq2SeqModelOutput(ModelOutput):
    last_hidden_state: torch.FloatTensor
    last_hidden_state_ngram: Optional[torch.FloatTensor] = ...
    past_key_values: Optional[tuple[torch.FloatTensor]] = ...
    decoder_hidden_states: Optional[tuple[torch.FloatTensor]] = ...
    decoder_ngram_hidden_states: Optional[tuple[torch.FloatTensor]] = ...
    decoder_attentions: Optional[tuple[torch.FloatTensor]] = ...
    decoder_ngram_attentions: Optional[tuple[torch.FloatTensor]] = ...
    cross_attentions: Optional[tuple[torch.FloatTensor]] = ...
    encoder_last_hidden_state: Optional[torch.FloatTensor] = ...
    encoder_hidden_states: Optional[tuple[torch.FloatTensor]] = ...
    encoder_attentions: Optional[tuple[torch.FloatTensor]] = ...
    @property
    def decoder_cross_attentions(self): ...

@dataclass
class XLMProphetNetDecoderModelOutput(ModelOutput):
    last_hidden_state: torch.FloatTensor
    last_hidden_state_ngram: Optional[torch.FloatTensor] = ...
    past_key_values: Optional[tuple[torch.FloatTensor]] = ...
    hidden_states: Optional[tuple[torch.FloatTensor]] = ...
    hidden_states_ngram: Optional[tuple[torch.FloatTensor]] = ...
    attentions: Optional[tuple[torch.FloatTensor]] = ...
    ngram_attentions: Optional[tuple[torch.FloatTensor]] = ...
    cross_attentions: Optional[tuple[torch.FloatTensor]] = ...

@dataclass
class XLMProphetNetDecoderLMOutput(ModelOutput):
    loss: Optional[torch.FloatTensor] = ...
    logits: Optional[torch.FloatTensor] = ...
    logits_ngram: Optional[torch.FloatTensor] = ...
    past_key_values: Optional[tuple[torch.FloatTensor]] = ...
    hidden_states: Optional[tuple[torch.FloatTensor]] = ...
    hidden_states_ngram: Optional[tuple[torch.FloatTensor]] = ...
    attentions: Optional[tuple[torch.FloatTensor]] = ...
    ngram_attentions: Optional[tuple[torch.FloatTensor]] = ...
    cross_attentions: Optional[tuple[torch.FloatTensor]] = ...

class XLMProphetNetPreTrainedModel(PreTrainedModel):
    config: XLMProphetNetConfig
    base_model_prefix = ...
    supports_gradient_checkpointing = ...

class XLMProphetNetPositionalEmbeddings(nn.Embedding):
    def __init__(self, config: XLMProphetNetConfig) -> None: ...
    def forward(self, inputs_shape, device, attention_mask=..., past_key_values=..., position_ids=...): ...

class XLMProphetNetAttention(nn.Module):
    def __init__(self, config: XLMProphetNetConfig, num_attn_heads: int) -> None: ...
    def forward(
        self,
        hidden_states,
        key_value_states: Optional[Tensor] = ...,
        attention_mask: Optional[Tensor] = ...,
        layer_head_mask: Optional[Tensor] = ...,
        past_key_value: Optional[tuple[Tensor]] = ...,
        output_attentions: bool = ...,
    ) -> tuple[Tensor, Optional[Tensor]]: ...

class XLMProphetNetFeedForward(nn.Module):
    def __init__(self, config: XLMProphetNetConfig, ffn_dim: int) -> None: ...
    def forward(self, hidden_states): ...

class XLMProphetNetNgramSelfAttention(nn.Module):
    def __init__(self, config: XLMProphetNetConfig) -> None: ...
    def prepare_for_onnx_export_(self): ...
    def forward(
        self,
        hidden_states,
        past_key_value: Optional[tuple[Tensor]] = ...,
        attention_mask=...,
        layer_head_mask=...,
        extended_predict_attention_mask=...,
        main_relative_position_buckets=...,
        predict_relative_position_buckets=...,
        position_ids=...,
    ): ...
    def get_main_relative_pos_embeddings(
        self, hidden_states, attn_weights, position_ids, main_relative_position_buckets
    ): ...
    def get_predict_relative_pos_embeddings(
        self, hidden_states, attn_weights, position_ids, predict_relative_position_buckets
    ): ...

class XLMProphetNetEncoderLayer(GradientCheckpointingLayer):
    def __init__(self, config: XLMProphetNetConfig) -> None: ...
    def forward(self, hidden_states, attention_mask, layer_head_mask, output_attentions: bool = ...): ...

class XLMProphetNetDecoderLayer(GradientCheckpointingLayer):
    def __init__(self, config: XLMProphetNetConfig) -> None: ...
    def forward(
        self,
        hidden_states,
        attention_mask=...,
        encoder_hidden_states=...,
        encoder_attn_mask=...,
        layer_head_mask=...,
        cross_attn_layer_head_mask=...,
        extended_predict_attention_mask=...,
        main_relative_position_buckets=...,
        predict_relative_position_buckets=...,
        position_ids=...,
        past_key_value=...,
        use_cache: bool = ...,
        output_attentions: bool = ...,
    ): ...

@add_start_docstrings(..., XLM_PROPHETNET_START_DOCSTRING)
class XLMProphetNetEncoder(XLMProphetNetPreTrainedModel):
    def __init__(self, config: XLMProphetNetConfig, word_embeddings: nn.Embedding = ...) -> None: ...
    def get_input_embeddings(self): ...
    def set_input_embeddings(self, value): ...
    @add_start_docstrings_to_model_forward(XLM_PROPHETNET_STANDALONE_INPUTS_DOCSTRING)
    @replace_return_docstrings(output_type=BaseModelOutput, config_class=_CONFIG_FOR_DOC)
    def forward(
        self,
        input_ids: Optional[torch.Tensor] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        head_mask: Optional[torch.Tensor] = ...,
        inputs_embeds: Optional[torch.Tensor] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[tuple, BaseModelOutput]: ...

@add_start_docstrings(..., XLM_PROPHETNET_START_DOCSTRING)
class XLMProphetNetDecoder(XLMProphetNetPreTrainedModel):
    def __init__(self, config: XLMProphetNetConfig, word_embeddings: Optional[nn.Embedding] = ...) -> None: ...
    def get_input_embeddings(self): ...
    def set_input_embeddings(self, value): ...
    @add_start_docstrings_to_model_forward(XLM_PROPHETNET_STANDALONE_INPUTS_DOCSTRING)
    @replace_return_docstrings(output_type=XLMProphetNetDecoderModelOutput, config_class=_CONFIG_FOR_DOC)
    def forward(
        self,
        input_ids: Optional[torch.Tensor] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        encoder_hidden_states: Optional[torch.Tensor] = ...,
        encoder_attention_mask: Optional[torch.Tensor] = ...,
        head_mask: Optional[torch.Tensor] = ...,
        cross_attn_head_mask: Optional[torch.Tensor] = ...,
        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = ...,
        inputs_embeds: Optional[torch.Tensor] = ...,
        use_cache: Optional[bool] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[tuple, XLMProphetNetDecoderModelOutput]: ...
    def compute_buffered_relative_buckets(self, position_ids): ...
    def prepare_attention_mask(self, hidden_states, attention_mask): ...
    def prepare_predict_attention_mask(self, hidden_states, attention_mask): ...

@add_start_docstrings(..., XLM_PROPHETNET_START_DOCSTRING)
class XLMProphetNetModel(XLMProphetNetPreTrainedModel):
    _tied_weights_keys = ...
    def __init__(self, config: XLMProphetNetConfig) -> None: ...
    def get_input_embeddings(self): ...
    def set_input_embeddings(self, value): ...
    def get_encoder(self): ...
    def get_decoder(self): ...
    @add_start_docstrings_to_model_forward(XLM_PROPHETNET_INPUTS_DOCSTRING)
    @replace_return_docstrings(output_type=XLMProphetNetSeq2SeqModelOutput, config_class=_CONFIG_FOR_DOC)
    def forward(
        self,
        input_ids: Optional[torch.Tensor] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        decoder_input_ids: Optional[torch.Tensor] = ...,
        decoder_attention_mask: Optional[torch.BoolTensor] = ...,
        head_mask: Optional[torch.Tensor] = ...,
        decoder_head_mask: Optional[torch.Tensor] = ...,
        cross_attn_head_mask: Optional[torch.Tensor] = ...,
        encoder_outputs: Optional[tuple] = ...,
        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = ...,
        inputs_embeds: Optional[torch.Tensor] = ...,
        decoder_inputs_embeds: Optional[torch.Tensor] = ...,
        use_cache: Optional[bool] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[tuple, XLMProphetNetSeq2SeqModelOutput]: ...

@add_start_docstrings(..., XLM_PROPHETNET_START_DOCSTRING)
class XLMProphetNetForConditionalGeneration(XLMProphetNetPreTrainedModel):
    _tied_weights_keys = ...
    def __init__(self, config: XLMProphetNetConfig) -> None: ...
    def get_input_embeddings(self): ...
    @add_start_docstrings_to_model_forward(XLM_PROPHETNET_INPUTS_DOCSTRING)
    @replace_return_docstrings(output_type=XLMProphetNetSeq2SeqLMOutput, config_class=_CONFIG_FOR_DOC)
    def forward(
        self,
        input_ids: Optional[torch.Tensor] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        decoder_input_ids: Optional[torch.Tensor] = ...,
        decoder_attention_mask: Optional[torch.BoolTensor] = ...,
        head_mask: Optional[torch.Tensor] = ...,
        decoder_head_mask: Optional[torch.Tensor] = ...,
        cross_attn_head_mask: Optional[torch.Tensor] = ...,
        encoder_outputs: Optional[torch.Tensor] = ...,
        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = ...,
        inputs_embeds: Optional[torch.Tensor] = ...,
        decoder_inputs_embeds: Optional[torch.Tensor] = ...,
        labels: Optional[torch.Tensor] = ...,
        use_cache: Optional[bool] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[tuple, XLMProphetNetSeq2SeqLMOutput]: ...
    def prepare_inputs_for_generation(
        self,
        decoder_input_ids,
        past_key_values=...,
        attention_mask=...,
        head_mask=...,
        decoder_head_mask=...,
        cross_attn_head_mask=...,
        use_cache=...,
        encoder_outputs=...,
        **kwargs,
    ): ...
    def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor): ...
    def get_encoder(self): ...
    def get_decoder(self): ...

@add_start_docstrings(..., XLM_PROPHETNET_START_DOCSTRING)
class XLMProphetNetForCausalLM(XLMProphetNetPreTrainedModel):
    _tied_weights_keys = ...
    def __init__(self, config: XLMProphetNetConfig) -> None: ...
    def get_input_embeddings(self): ...
    def set_input_embeddings(self, value): ...
    def set_decoder(self, decoder): ...
    def get_decoder(self): ...
    @add_start_docstrings_to_model_forward(XLM_PROPHETNET_STANDALONE_INPUTS_DOCSTRING)
    @replace_return_docstrings(output_type=XLMProphetNetDecoderLMOutput, config_class=_CONFIG_FOR_DOC)
    def forward(
        self,
        input_ids: Optional[torch.Tensor] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        encoder_hidden_states: Optional[torch.Tensor] = ...,
        encoder_attention_mask: Optional[torch.Tensor] = ...,
        head_mask: Optional[torch.Tensor] = ...,
        cross_attn_head_mask: Optional[torch.Tensor] = ...,
        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = ...,
        inputs_embeds: Optional[torch.Tensor] = ...,
        labels: Optional[torch.Tensor] = ...,
        use_cache: Optional[bool] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[tuple, XLMProphetNetDecoderLMOutput]: ...
    def prepare_inputs_for_generation(
        self, input_ids, past_key_values=..., attention_mask=..., head_mask=..., use_cache=..., **kwargs
    ): ...

class XLMProphetNetDecoderWrapper(XLMProphetNetPreTrainedModel):
    def __init__(self, config: XLMProphetNetConfig) -> None: ...
    def forward(self, *args, **kwargs): ...

__all__ = [
    "XLMProphetNetDecoder",
    "XLMProphetNetEncoder",
    "XLMProphetNetForCausalLM",
    "XLMProphetNetForConditionalGeneration",
    "XLMProphetNetModel",
    "XLMProphetNetPreTrainedModel",
]
