"""
This type stub file was generated by pyright.
"""

import torch
from dataclasses import dataclass
from typing import Optional, Union
from torch import nn
from ...modeling_utils import PreTrainedModel
from ...utils import ModelOutput, auto_docstring
from .configuration_depth_pro import DepthProConfig

"""PyTorch DepthPro model."""
logger = ...

@dataclass
@auto_docstring(
    custom_intro="""
    Base class for DepthPro's outputs.
    """
)
class DepthProOutput(ModelOutput):
    r"""
    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, n_patches_per_batch, sequence_length, hidden_size)`):
        Sequence of hidden-states at the output of the last layer of the model.
    features (`Union[torch.FloatTensor, List[torch.FloatTensor]]`, *optional*):
        Features from encoders. Can be a single feature or a list of features.
    """

    last_hidden_state: torch.FloatTensor | None = ...
    features: torch.FloatTensor | list[torch.FloatTensor] = ...
    hidden_states: tuple[torch.FloatTensor, ...] | None = ...
    attentions: tuple[torch.FloatTensor, ...] | None = ...

@dataclass
@auto_docstring(
    custom_intro="""
    Base class for DepthProForDepthEstimation's output.
    """
)
class DepthProDepthEstimatorOutput(ModelOutput):
    r"""
    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):
        Classification (or regression if config.num_labels==1) loss.
    field_of_view (`torch.FloatTensor` of shape `(batch_size,)`, *optional*, returned when `use_fov_model` is provided):
        Field of View Scaler.
    """

    loss: torch.FloatTensor | None = ...
    predicted_depth: torch.FloatTensor | None = ...
    field_of_view: torch.FloatTensor | None = ...
    hidden_states: tuple[torch.FloatTensor, ...] | None = ...
    attentions: tuple[torch.FloatTensor, ...] | None = ...

def split_to_patches(pixel_values: torch.Tensor, patch_size: int, overlap_ratio: float) -> torch.Tensor:
    """Creates Patches from Batch."""
    ...

def reshape_features(hidden_states: torch.Tensor) -> torch.Tensor:
    """Discard class token and reshape 1D feature map to a 2D grid."""
    ...

def merge_patches(patches: torch.Tensor, batch_size: int, padding: int) -> torch.Tensor:
    """Merges smaller patches into image-like feature map."""
    ...

def reconstruct_feature_maps(
    hidden_state: torch.Tensor, batch_size: int, padding: int, output_size: tuple[float, float]
) -> torch.Tensor:
    """
    Reconstructs feature maps from the hidden state produced by any of the encoder. Converts the hidden state of shape
    `(n_patches_per_batch * batch_size, seq_len, hidden_size)` to feature maps of shape
    `(batch_size, hidden_size, output_size[0], output_size[1])`.

    Args:
        hidden_state (torch.Tensor): Input tensor of shape `(n_patches_per_batch * batch_size, seq_len, hidden_size)`
            representing the encoded patches.
        batch_size (int): The number of samples in a batch.
        padding (int): The amount of padding to be removed when merging patches.
        output_size (tuple[float, float]): The desired output size for the feature maps, specified as `(height, width)`.

    Returns:
        torch.Tensor: Reconstructed feature maps of shape `(batch_size, hidden_size, output_size[0], output_size[1])`.
    """
    ...

class DepthProPatchEncoder(nn.Module):
    def __init__(self, config: DepthProConfig) -> None: ...
    def forward(self, pixel_values: torch.Tensor, head_mask: torch.Tensor | None = ...) -> list[torch.Tensor]: ...

class DepthProImageEncoder(nn.Module):
    def __init__(self, config: DepthProConfig) -> None: ...
    def forward(
        self,
        pixel_values: torch.Tensor,
        head_mask: torch.Tensor | None = ...,
        output_attentions: bool = ...,
        output_hidden_states: bool = ...,
        return_dict: bool = ...,
    ) -> tuple | DepthProOutput: ...

class DepthProEncoder(nn.Module):
    def __init__(self, config: DepthProConfig) -> None: ...
    def forward(
        self,
        pixel_values: torch.Tensor,
        head_mask: torch.Tensor | None = ...,
        output_attentions: bool = ...,
        output_hidden_states: bool = ...,
        return_dict: bool = ...,
    ) -> tuple | DepthProOutput: ...

class DepthProFeatureUpsampleBlock(nn.Module):
    def __init__(
        self,
        config: DepthProConfig,
        input_dims: int,
        intermediate_dims: int,
        output_dims: int,
        n_upsample_layers: int,
        use_proj: bool = ...,
        bias: bool = ...,
    ) -> None: ...
    def forward(self, features: torch.Tensor) -> torch.Tensor: ...

class DepthProFeatureUpsample(nn.Module):
    def __init__(self, config: DepthProConfig) -> None: ...
    def forward(self, features: list[torch.Tensor]) -> list[torch.Tensor]: ...

class DepthProFeatureProjection(nn.Module):
    def __init__(self, config: DepthProConfig) -> None: ...
    def forward(self, features: list[torch.Tensor]) -> list[torch.Tensor]: ...

class DepthProNeck(nn.Module):
    def __init__(self, config: DepthProConfig) -> None: ...
    def forward(self, features: list[torch.Tensor]) -> list[torch.Tensor]: ...

@auto_docstring
class DepthProPreTrainedModel(PreTrainedModel):
    config: DepthProConfig
    base_model_prefix = ...
    main_input_name = ...
    supports_gradient_checkpointing = ...
    _supports_sdpa = ...
    _no_split_modules = ...
    _keys_to_ignore_on_load_unexpected = ...

@auto_docstring
class DepthProModel(DepthProPreTrainedModel):
    def __init__(self, config) -> None: ...
    def get_input_embeddings(self):  # -> Any:
        ...
    @auto_docstring
    def forward(
        self,
        pixel_values: torch.FloatTensor,
        head_mask: torch.FloatTensor | None = ...,
        output_attentions: bool | None = ...,
        output_hidden_states: bool | None = ...,
        return_dict: bool | None = ...,
    ) -> tuple | DepthProOutput:
        r"""
        Examples:

        ```python
        >>> import torch
        >>> from PIL import Image
        >>> import requests
        >>> from transformers import AutoProcessor, DepthProModel

        >>> url = "https://www.ilankelman.org/stopsigns/australia.jpg"
        >>> image = Image.open(requests.get(url, stream=True).raw)

        >>> checkpoint = "apple/DepthPro-hf"
        >>> processor = AutoProcessor.from_pretrained(checkpoint)
        >>> model = DepthProModel.from_pretrained(checkpoint)

        >>> # prepare image for the model
        >>> inputs = processor(images=image, return_tensors="pt")

        >>> with torch.no_grad():
        ...     output = model(**inputs)

        >>> output.last_hidden_state.shape
        torch.Size([1, 35, 577, 1024])
        ```"""
        ...

class DepthProPreActResidualLayer(nn.Module):
    """
    ResidualConvUnit, pre-activate residual unit.

    Args:
        config (`[DepthProConfig]`):
            Model configuration class defining the model architecture.
    """
    def __init__(self, config) -> None: ...
    def forward(self, hidden_state: torch.Tensor) -> torch.Tensor: ...

class DepthProFeatureFusionLayer(nn.Module):
    def __init__(self, config: DepthProConfig, use_deconv: bool = ...) -> None: ...
    def forward(self, hidden_state: torch.Tensor, residual: torch.Tensor | None = ...) -> torch.Tensor: ...

class DepthProFeatureFusionStage(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states: list[torch.Tensor]) -> list[torch.Tensor]: ...

class DepthProFovEncoder(nn.Module):
    def __init__(self, config: DepthProConfig) -> None: ...
    def forward(self, pixel_values: torch.Tensor, head_mask: torch.Tensor | None = ...) -> torch.Tensor: ...

class DepthProFovHead(nn.Module):
    def __init__(self, config: DepthProConfig) -> None: ...
    def forward(self, features: torch.Tensor) -> torch.Tensor: ...

class DepthProFovModel(nn.Module):
    def __init__(self, config: DepthProConfig) -> None: ...
    def forward(
        self, pixel_values: torch.Tensor, global_features: torch.Tensor, head_mask: torch.Tensor | None = ...
    ) -> torch.Tensor: ...

class DepthProDepthEstimationHead(nn.Module):
    """
    The DepthProDepthEstimationHead module serves as the output head for depth estimation tasks.
    This module comprises a sequence of convolutional and transposed convolutional layers
    that process the feature map from the fusion to produce a single-channel depth map.
    Key operations include dimensionality reduction and upsampling to match the input resolution.
    """
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor: ...

@auto_docstring(
    custom_intro="""
    DepthPro Model with a depth estimation head on top (consisting of 3 convolutional layers).
    """
)
class DepthProForDepthEstimation(DepthProPreTrainedModel):
    def __init__(self, config, use_fov_model=...) -> None:
        r"""
        use_fov_model (bool, *optional*):
            Whether to use the field of view model.
        """
        ...

    @auto_docstring
    def forward(
        self,
        pixel_values: torch.FloatTensor,
        head_mask: torch.FloatTensor | None = ...,
        labels: torch.LongTensor | None = ...,
        output_attentions: bool | None = ...,
        output_hidden_states: bool | None = ...,
        return_dict: bool | None = ...,
    ) -> tuple[torch.Tensor] | DepthProDepthEstimatorOutput:
        r"""
        labels (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*):
            Ground truth depth estimation maps for computing the loss.

        Examples:

        ```python
        >>> from transformers import AutoImageProcessor, DepthProForDepthEstimation
        >>> import torch
        >>> from PIL import Image
        >>> import requests

        >>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
        >>> image = Image.open(requests.get(url, stream=True).raw)

        >>> checkpoint = "apple/DepthPro-hf"
        >>> processor = AutoImageProcessor.from_pretrained(checkpoint)
        >>> model = DepthProForDepthEstimation.from_pretrained(checkpoint)

        >>> device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        >>> model.to(device)

        >>> # prepare image for the model
        >>> inputs = processor(images=image, return_tensors="pt").to(device)

        >>> with torch.no_grad():
        ...     outputs = model(**inputs)

        >>> # interpolate to original size
        >>> post_processed_output = processor.post_process_depth_estimation(
        ...     outputs, target_sizes=[(image.height, image.width)],
        ... )

        >>> # get the field of view (fov) predictions
        >>> field_of_view = post_processed_output[0]["field_of_view"]
        >>> focal_length = post_processed_output[0]["focal_length"]

        >>> # visualize the prediction
        >>> predicted_depth = post_processed_output[0]["predicted_depth"]
        >>> depth = predicted_depth * 255 / predicted_depth.max()
        >>> depth = depth.detach().cpu().numpy()
        >>> depth = Image.fromarray(depth.astype("uint8"))
        ```"""
        ...

__all__ = ["DepthProPreTrainedModel", "DepthProModel", "DepthProForDepthEstimation"]
