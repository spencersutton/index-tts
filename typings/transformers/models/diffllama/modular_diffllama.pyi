"""
This type stub file was generated by pyright.
"""

import torch
from typing import Optional
from torch import nn
from ...cache_utils import Cache
from ..gemma.modeling_gemma import GemmaForCausalLM
from ..llama.modeling_llama import (
    LlamaDecoderLayer,
    LlamaForQuestionAnswering,
    LlamaForSequenceClassification,
    LlamaForTokenClassification,
    LlamaModel,
    LlamaPreTrainedModel,
)
from ..mistral.modeling_mistral import MistralMLP
from .configuration_diffllama import DiffLlamaConfig

logger = ...
_CHECKPOINT_FOR_DOC = ...
_CONFIG_FOR_DOC = ...

class DiffLlamaMLP(MistralMLP): ...

def lambda_init_fn(layer_idx): ...

class DiffLlamaAttention(nn.Module):
    def __init__(self, config: DiffLlamaConfig, layer_idx: Optional[int] = ...) -> None: ...
    def forward(
        self,
        hidden_states: torch.Tensor,
        position_embeddings: tuple[torch.Tensor, torch.Tensor],
        attention_mask: Optional[torch.Tensor] = ...,
        position_ids: Optional[torch.LongTensor] = ...,
        past_key_value: Optional[Cache] = ...,
        use_cache: bool = ...,
        cache_position: Optional[torch.LongTensor] = ...,
        **kwargs,
    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]: ...

class DiffLlamaFlashAttention2(DiffLlamaAttention):
    def __init__(self, *args, **kwargs) -> None: ...
    def forward(
        self,
        hidden_states: torch.Tensor,
        position_embeddings: tuple[torch.Tensor, torch.Tensor],
        attention_mask: Optional[torch.LongTensor] = ...,
        position_ids: Optional[torch.LongTensor] = ...,
        past_key_value: Optional[Cache] = ...,
        use_cache: bool = ...,
        cache_position: Optional[torch.LongTensor] = ...,
    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]: ...

class DiffLlamaSdpaAttention(DiffLlamaAttention):
    def forward(
        self,
        hidden_states: torch.Tensor,
        position_embeddings: tuple[torch.Tensor, torch.Tensor],
        attention_mask: Optional[torch.Tensor] = ...,
        position_ids: Optional[torch.LongTensor] = ...,
        past_key_value: Optional[Cache] = ...,
        use_cache: bool = ...,
        cache_position: Optional[torch.LongTensor] = ...,
        **kwargs,
    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]: ...

DIFFLLAMA_ATTENTION_CLASSES = ...

class DiffLlamaDecoderLayer(LlamaDecoderLayer):
    def __init__(self, config: DiffLlamaConfig, layer_idx: int) -> None: ...

class DiffLlamaPreTrainedModel(LlamaPreTrainedModel):
    _supports_flex_attn = ...
    _supports_attention_backend = ...

class DiffLlamaModel(LlamaModel): ...
class DiffLlamaForCausalLM(GemmaForCausalLM): ...
class DiffLlamaForSequenceClassification(LlamaForSequenceClassification): ...
class DiffLlamaForQuestionAnswering(LlamaForQuestionAnswering): ...
class DiffLlamaForTokenClassification(LlamaForTokenClassification): ...

__all__ = [
    "DiffLlamaPreTrainedModel",
    "DiffLlamaModel",
    "DiffLlamaForCausalLM",
    "DiffLlamaForSequenceClassification",
    "DiffLlamaForQuestionAnswering",
    "DiffLlamaForTokenClassification",
]
