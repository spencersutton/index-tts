"""
This type stub file was generated by pyright.
"""

import torch
from typing import Optional, Union
from torch import nn
from ...configuration_utils import PretrainedConfig
from ...modeling_flash_attention_utils import is_flash_attn_available
from ...modeling_layers import GradientCheckpointingLayer
from ...modeling_outputs import (
    BaseModelOutput,
    MaskedLMOutput,
    MultipleChoiceModelOutput,
    QuestionAnsweringModelOutput,
    SequenceClassifierOutput,
    TokenClassifierOutput,
)
from ...modeling_utils import PreTrainedModel
from ...utils import auto_docstring
from .configuration_distilbert import DistilBertConfig

if is_flash_attn_available(): ...
logger = ...

def create_sinusoidal_embeddings(n_pos: int, dim: int, out: torch.Tensor): ...

class Embeddings(nn.Module):
    def __init__(self, config: PretrainedConfig) -> None: ...
    def forward(self, input_ids: torch.Tensor, input_embeds: Optional[torch.Tensor] = ...) -> torch.Tensor: ...

class MultiHeadSelfAttention(nn.Module):
    def __init__(self, config: PretrainedConfig) -> None: ...
    def prune_heads(self, heads: list[int]): ...
    def forward(
        self,
        query: torch.Tensor,
        key: torch.Tensor,
        value: torch.Tensor,
        mask: torch.Tensor,
        head_mask: Optional[torch.Tensor] = ...,
        output_attentions: bool = ...,
    ) -> tuple[torch.Tensor, ...]: ...

class DistilBertFlashAttention2(MultiHeadSelfAttention):
    def __init__(self, *args, **kwargs) -> None: ...
    def forward(
        self,
        query: torch.Tensor,
        key: torch.Tensor,
        value: torch.Tensor,
        mask: torch.Tensor,
        head_mask: Optional[torch.Tensor] = ...,
        output_attentions: bool = ...,
    ) -> tuple[torch.Tensor, ...]: ...

class DistilBertSdpaAttention(MultiHeadSelfAttention):
    def __init__(self, config: PretrainedConfig) -> None: ...
    def forward(
        self,
        query: torch.Tensor,
        key: torch.Tensor,
        value: torch.Tensor,
        mask: torch.Tensor,
        head_mask: Optional[torch.Tensor] = ...,
        output_attentions: bool = ...,
    ) -> tuple[torch.Tensor, ...]: ...

class FFN(nn.Module):
    def __init__(self, config: PretrainedConfig) -> None: ...
    def forward(self, input: torch.Tensor) -> torch.Tensor: ...
    def ff_chunk(self, input: torch.Tensor) -> torch.Tensor: ...

DISTILBERT_ATTENTION_CLASSES = ...

class TransformerBlock(GradientCheckpointingLayer):
    def __init__(self, config: PretrainedConfig) -> None: ...
    def forward(
        self,
        x: torch.Tensor,
        attn_mask: Optional[torch.Tensor] = ...,
        head_mask: Optional[torch.Tensor] = ...,
        output_attentions: bool = ...,
    ) -> tuple[torch.Tensor, ...]: ...

class Transformer(nn.Module):
    def __init__(self, config: PretrainedConfig) -> None: ...
    def forward(
        self,
        x: torch.Tensor,
        attn_mask: Optional[torch.Tensor] = ...,
        head_mask: Optional[torch.Tensor] = ...,
        output_attentions: bool = ...,
        output_hidden_states: bool = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[BaseModelOutput, tuple[torch.Tensor, ...]]: ...

@auto_docstring
class DistilBertPreTrainedModel(PreTrainedModel):
    config: DistilBertConfig
    load_tf_weights = ...
    base_model_prefix = ...
    supports_gradient_checkpointing = ...
    _supports_flash_attn = ...
    _supports_sdpa = ...

@auto_docstring
class DistilBertModel(DistilBertPreTrainedModel):
    def __init__(self, config: PretrainedConfig) -> None: ...
    def get_position_embeddings(self) -> nn.Embedding: ...
    def resize_position_embeddings(self, new_num_position_embeddings: int): ...
    def get_input_embeddings(self) -> nn.Embedding: ...
    def set_input_embeddings(self, new_embeddings: nn.Embedding): ...
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.Tensor] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        head_mask: Optional[torch.Tensor] = ...,
        inputs_embeds: Optional[torch.Tensor] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[BaseModelOutput, tuple[torch.Tensor, ...]]: ...

@auto_docstring(custom_intro=...)
class DistilBertForMaskedLM(DistilBertPreTrainedModel):
    _tied_weights_keys = ...
    def __init__(self, config: PretrainedConfig) -> None: ...
    def get_position_embeddings(self) -> nn.Embedding: ...
    def resize_position_embeddings(self, new_num_position_embeddings: int): ...
    def get_output_embeddings(self) -> nn.Module: ...
    def set_output_embeddings(self, new_embeddings: nn.Module): ...
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.Tensor] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        head_mask: Optional[torch.Tensor] = ...,
        inputs_embeds: Optional[torch.Tensor] = ...,
        labels: Optional[torch.LongTensor] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[MaskedLMOutput, tuple[torch.Tensor, ...]]: ...

@auto_docstring(custom_intro=...)
class DistilBertForSequenceClassification(DistilBertPreTrainedModel):
    def __init__(self, config: PretrainedConfig) -> None: ...
    def get_position_embeddings(self) -> nn.Embedding: ...
    def resize_position_embeddings(self, new_num_position_embeddings: int): ...
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.Tensor] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        head_mask: Optional[torch.Tensor] = ...,
        inputs_embeds: Optional[torch.Tensor] = ...,
        labels: Optional[torch.LongTensor] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[SequenceClassifierOutput, tuple[torch.Tensor, ...]]: ...

@auto_docstring
class DistilBertForQuestionAnswering(DistilBertPreTrainedModel):
    def __init__(self, config: PretrainedConfig) -> None: ...
    def get_position_embeddings(self) -> nn.Embedding: ...
    def resize_position_embeddings(self, new_num_position_embeddings: int): ...
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.Tensor] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        head_mask: Optional[torch.Tensor] = ...,
        inputs_embeds: Optional[torch.Tensor] = ...,
        start_positions: Optional[torch.Tensor] = ...,
        end_positions: Optional[torch.Tensor] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[QuestionAnsweringModelOutput, tuple[torch.Tensor, ...]]: ...

@auto_docstring
class DistilBertForTokenClassification(DistilBertPreTrainedModel):
    def __init__(self, config: PretrainedConfig) -> None: ...
    def get_position_embeddings(self) -> nn.Embedding: ...
    def resize_position_embeddings(self, new_num_position_embeddings: int): ...
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.Tensor] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        head_mask: Optional[torch.Tensor] = ...,
        inputs_embeds: Optional[torch.Tensor] = ...,
        labels: Optional[torch.LongTensor] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[TokenClassifierOutput, tuple[torch.Tensor, ...]]: ...

@auto_docstring
class DistilBertForMultipleChoice(DistilBertPreTrainedModel):
    def __init__(self, config: PretrainedConfig) -> None: ...
    def get_position_embeddings(self) -> nn.Embedding: ...
    def resize_position_embeddings(self, new_num_position_embeddings: int): ...
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.Tensor] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        head_mask: Optional[torch.Tensor] = ...,
        inputs_embeds: Optional[torch.Tensor] = ...,
        labels: Optional[torch.LongTensor] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[MultipleChoiceModelOutput, tuple[torch.Tensor, ...]]: ...

__all__ = [
    "DistilBertForMaskedLM",
    "DistilBertForMultipleChoice",
    "DistilBertForQuestionAnswering",
    "DistilBertForSequenceClassification",
    "DistilBertForTokenClassification",
    "DistilBertModel",
    "DistilBertPreTrainedModel",
]
