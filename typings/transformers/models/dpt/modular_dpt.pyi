"""
This type stub file was generated by pyright.
"""

import torch
from collections.abc import Iterable
from typing import Optional, TYPE_CHECKING, Union
from transformers.models.beit.image_processing_beit_fast import BeitImageProcessorFast
from ...image_processing_utils_fast import DefaultFastImageProcessorKwargs
from ...image_utils import SizeDict
from ...utils import TensorType, auto_docstring, is_torch_available, is_torchvision_v2_available
from ...modeling_outputs import DepthEstimatorOutput
from torchvision.transforms.v2 import functional as F
from torchvision.transforms import functional as F

if TYPE_CHECKING: ...
if is_torch_available(): ...
if is_torchvision_v2_available(): ...
else: ...

def get_resize_output_image_size(
    input_image: torch.Tensor, output_size: int | Iterable[int], keep_aspect_ratio: bool, multiple: int
) -> SizeDict: ...

class DPTFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):
    """
    ensure_multiple_of (`int`, *optional*, defaults to 1):
        If `do_resize` is `True`, the image is resized to a size that is a multiple of this value. Can be overidden
        by `ensure_multiple_of` in `preprocess`.
    do_pad (`bool`, *optional*, defaults to `False`):
        Whether to apply center padding. This was introduced in the DINOv2 paper, which uses the model in
        combination with DPT.
    size_divisor (`int`, *optional*):
        If `do_pad` is `True`, pads the image dimensions to be divisible by this value. This was introduced in the
        DINOv2 paper, which uses the model in combination with DPT.
    keep_aspect_ratio (`bool`, *optional*, defaults to `False`):
        If `True`, the image is resized to the largest possible size such that the aspect ratio is preserved. Can
        be overidden by `keep_aspect_ratio` in `preprocess`.
    do_reduce_labels (`bool`, *optional*, defaults to `self.do_reduce_labels`):
        Whether or not to reduce all label values of segmentation maps by 1. Usually used for datasets where 0
        is used for background, and background itself is not included in all classes of a dataset (e.g.
        ADE20k). The background label will be replaced by 255.
    """

    ensure_multiple_of: int | None
    size_divisor: int | None
    do_pad: bool | None
    keep_aspect_ratio: bool | None
    do_reduce_labels: bool | None
    ...

@auto_docstring
class DPTImageProcessorFast(BeitImageProcessorFast):
    resample = ...
    image_mean = ...
    image_std = ...
    size = ...
    do_resize = ...
    do_rescale = ...
    do_normalize = ...
    do_pad = ...
    rescale_factor = ...
    ensure_multiple_of = ...
    keep_aspect_ratio = ...
    do_reduce_labels = ...
    crop_size = ...
    do_center_crop = ...
    do_reduce_labels = ...
    valid_kwargs = DPTFastImageProcessorKwargs
    def resize(
        self,
        image: torch.Tensor,
        size: SizeDict,
        interpolation: F.InterpolationMode = ...,
        antialias: bool = ...,
        ensure_multiple_of: int | None = ...,
        keep_aspect_ratio: bool = ...,
    ) -> torch.Tensor:
        """
        Resize an image to `(size["height"], size["width"])`.

        Args:
            image (`torch.Tensor`):
                Image to resize.
            size (`SizeDict`):
                Dictionary in the format `{"height": int, "width": int}` specifying the size of the output image.
            interpolation (`InterpolationMode`, *optional*, defaults to `InterpolationMode.BILINEAR`):
                `InterpolationMode` filter to use when resizing the image e.g. `InterpolationMode.BICUBIC`.
            antialias (`bool`, *optional*, defaults to `True`):
                Whether to use antialiasing when resizing the image
            ensure_multiple_of (`int`, *optional*):
                If `do_resize` is `True`, the image is resized to a size that is a multiple of this value
            keep_aspect_ratio (`bool`, *optional*, defaults to `False`):
                If `True`, and `do_resize` is `True`, the image is resized to the largest possible size such that the aspect ratio is preserved.

        Returns:
            `torch.Tensor`: The resized image.
        """
        ...

    def pad_image(self, image: torch.Tensor, size_divisor: int = ...) -> torch.Tensor:
        r"""
        Center pad a batch of images to be a multiple of `size_divisor`.

        Args:
            image (`torch.Tensor`):
                Image to pad.  Can be a batch of images of dimensions (N, C, H, W) or a single image of dimensions (C, H, W).
            size_divisor (`int`):
                The width and height of the image will be padded to a multiple of this number.
        """
        ...

    def post_process_depth_estimation(
        self,
        outputs: DepthEstimatorOutput,
        target_sizes: TensorType | list[tuple[int, int]] | None | None = ...,
    ) -> list[dict[str, TensorType]]:
        """
        Converts the raw output of [`DepthEstimatorOutput`] into final depth predictions and depth PIL images.
        Only supports PyTorch.

        Args:
            outputs ([`DepthEstimatorOutput`]):
                Raw outputs of the model.
            target_sizes (`TensorType` or `List[Tuple[int, int]]`, *optional*):
                Tensor of shape `(batch_size, 2)` or list of tuples (`Tuple[int, int]`) containing the target size
                (height, width) of each image in the batch. If left to None, predictions will not be resized.

        Returns:
            `List[Dict[str, TensorType]]`: A list of dictionaries of tensors representing the processed depth
            predictions.
        """
        ...

__all__ = ["DPTImageProcessorFast"]
