from collections.abc import Iterable

import torch
from torchvision.transforms import functional as F
from torchvision.transforms.v2 import functional as F
from transformers.models.beit.image_processing_beit_fast import BeitImageProcessorFast

from ...image_processing_utils_fast import DefaultFastImageProcessorKwargs
from ...image_utils import SizeDict
from ...modeling_outputs import DepthEstimatorOutput
from ...utils import TensorType, is_torch_available, is_torchvision_v2_available

if is_torch_available(): ...
if is_torchvision_v2_available(): ...

def get_resize_output_image_size(
    input_image: torch.Tensor, output_size: int | Iterable[int], keep_aspect_ratio: bool, multiple: int
) -> SizeDict: ...

class DPTFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):
    ensure_multiple_of: int | None
    size_divisor: int | None
    do_pad: bool | None
    keep_aspect_ratio: bool | None
    do_reduce_labels: bool | None

class DPTImageProcessorFast(BeitImageProcessorFast):
    resample = ...
    image_mean = ...
    image_std = ...
    size = ...
    do_resize = ...
    do_rescale = ...
    do_normalize = ...
    do_pad = ...
    rescale_factor = ...
    ensure_multiple_of = ...
    keep_aspect_ratio = ...
    do_reduce_labels = ...
    crop_size = ...
    do_center_crop = ...
    valid_kwargs = DPTFastImageProcessorKwargs
    def resize(
        self,
        image: torch.Tensor,
        size: SizeDict,
        interpolation: F.InterpolationMode = ...,
        antialias: bool = ...,
        ensure_multiple_of: int | None = ...,
        keep_aspect_ratio: bool = ...,
    ) -> torch.Tensor: ...
    def pad_image(self, image: torch.Tensor, size_divisor: int = ...) -> torch.Tensor: ...
    def post_process_depth_estimation(
        self,
        outputs: DepthEstimatorOutput,
        target_sizes: TensorType | list[tuple[int, int]] | None = ...,
    ) -> list[dict[str, TensorType]]: ...

__all__ = ["DPTImageProcessorFast"]
