"""
This type stub file was generated by pyright.
"""

import flax
import flax.linen as nn
import jax
import jax.numpy as jnp
import numpy as np
from typing import Callable, Optional
from flax.core.frozen_dict import FrozenDict
from ...modeling_flax_utils import FlaxPreTrainedModel
from ...utils import ModelOutput, add_start_docstrings, add_start_docstrings_to_model_forward
from .configuration_electra import ElectraConfig

logger = ...
_CHECKPOINT_FOR_DOC = ...
_CONFIG_FOR_DOC = ...
remat = ...

@flax.struct.dataclass
class FlaxElectraForPreTrainingOutput(ModelOutput):
    logits: jnp.ndarray = ...
    hidden_states: Optional[tuple[jnp.ndarray]] = ...
    attentions: Optional[tuple[jnp.ndarray]] = ...

ELECTRA_START_DOCSTRING = ...
ELECTRA_INPUTS_DOCSTRING = ...

class FlaxElectraEmbeddings(nn.Module):
    config: ElectraConfig
    dtype: jnp.dtype = ...
    def setup(self): ...
    def __call__(self, input_ids, token_type_ids, position_ids, attention_mask, deterministic: bool = ...): ...

class FlaxElectraSelfAttention(nn.Module):
    config: ElectraConfig
    causal: bool = ...
    dtype: jnp.dtype = ...
    def setup(self): ...
    def __call__(
        self,
        hidden_states,
        attention_mask,
        layer_head_mask,
        key_value_states: Optional[jnp.ndarray] = ...,
        init_cache: bool = ...,
        deterministic=...,
        output_attentions: bool = ...,
    ): ...

class FlaxElectraSelfOutput(nn.Module):
    config: ElectraConfig
    dtype: jnp.dtype = ...
    def setup(self): ...
    def __call__(self, hidden_states, input_tensor, deterministic: bool = ...): ...

class FlaxElectraAttention(nn.Module):
    config: ElectraConfig
    causal: bool = ...
    dtype: jnp.dtype = ...
    def setup(self): ...
    def __call__(
        self,
        hidden_states,
        attention_mask,
        layer_head_mask,
        key_value_states=...,
        init_cache=...,
        deterministic=...,
        output_attentions: bool = ...,
    ): ...

class FlaxElectraIntermediate(nn.Module):
    config: ElectraConfig
    dtype: jnp.dtype = ...
    def setup(self): ...
    def __call__(self, hidden_states): ...

class FlaxElectraOutput(nn.Module):
    config: ElectraConfig
    dtype: jnp.dtype = ...
    def setup(self): ...
    def __call__(self, hidden_states, attention_output, deterministic: bool = ...): ...

class FlaxElectraLayer(nn.Module):
    config: ElectraConfig
    dtype: jnp.dtype = ...
    def setup(self): ...
    def __call__(
        self,
        hidden_states,
        attention_mask,
        layer_head_mask,
        encoder_hidden_states: Optional[jnp.ndarray] = ...,
        encoder_attention_mask: Optional[jnp.ndarray] = ...,
        init_cache: bool = ...,
        deterministic: bool = ...,
        output_attentions: bool = ...,
    ): ...

class FlaxElectraLayerCollection(nn.Module):
    config: ElectraConfig
    dtype: jnp.dtype = ...
    gradient_checkpointing: bool = ...
    def setup(self): ...
    def __call__(
        self,
        hidden_states,
        attention_mask,
        head_mask,
        encoder_hidden_states: Optional[jnp.ndarray] = ...,
        encoder_attention_mask: Optional[jnp.ndarray] = ...,
        init_cache: bool = ...,
        deterministic: bool = ...,
        output_attentions: bool = ...,
        output_hidden_states: bool = ...,
        return_dict: bool = ...,
    ): ...

class FlaxElectraEncoder(nn.Module):
    config: ElectraConfig
    dtype: jnp.dtype = ...
    gradient_checkpointing: bool = ...
    def setup(self): ...
    def __call__(
        self,
        hidden_states,
        attention_mask,
        head_mask,
        encoder_hidden_states: Optional[jnp.ndarray] = ...,
        encoder_attention_mask: Optional[jnp.ndarray] = ...,
        init_cache: bool = ...,
        deterministic: bool = ...,
        output_attentions: bool = ...,
        output_hidden_states: bool = ...,
        return_dict: bool = ...,
    ): ...

class FlaxElectraGeneratorPredictions(nn.Module):
    config: ElectraConfig
    dtype: jnp.dtype = ...
    def setup(self): ...
    def __call__(self, hidden_states): ...

class FlaxElectraDiscriminatorPredictions(nn.Module):
    config: ElectraConfig
    dtype: jnp.dtype = ...
    def setup(self): ...
    def __call__(self, hidden_states): ...

class FlaxElectraPreTrainedModel(FlaxPreTrainedModel):
    config_class = ElectraConfig
    base_model_prefix = ...
    module_class: nn.Module = ...
    def __init__(
        self,
        config: ElectraConfig,
        input_shape: tuple = ...,
        seed: int = ...,
        dtype: jnp.dtype = ...,
        _do_init: bool = ...,
        gradient_checkpointing: bool = ...,
        **kwargs,
    ) -> None: ...
    def enable_gradient_checkpointing(self): ...
    def init_weights(self, rng: jax.random.PRNGKey, input_shape: tuple, params: FrozenDict = ...) -> FrozenDict: ...
    def init_cache(self, batch_size, max_length): ...
    @add_start_docstrings_to_model_forward(ELECTRA_INPUTS_DOCSTRING.format("batch_size, sequence_length"))
    def __call__(
        self,
        input_ids,
        attention_mask=...,
        token_type_ids=...,
        position_ids=...,
        head_mask=...,
        encoder_hidden_states=...,
        encoder_attention_mask=...,
        params: Optional[dict] = ...,
        dropout_rng: jax.random.PRNGKey = ...,
        train: bool = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
        past_key_values: Optional[dict] = ...,
    ): ...

class FlaxElectraModule(nn.Module):
    config: ElectraConfig
    dtype: jnp.dtype = ...
    gradient_checkpointing: bool = ...
    def setup(self): ...
    def __call__(
        self,
        input_ids,
        attention_mask,
        token_type_ids,
        position_ids,
        head_mask: Optional[np.ndarray] = ...,
        encoder_hidden_states: Optional[jnp.ndarray] = ...,
        encoder_attention_mask: Optional[jnp.ndarray] = ...,
        init_cache: bool = ...,
        deterministic: bool = ...,
        output_attentions: bool = ...,
        output_hidden_states: bool = ...,
        return_dict: bool = ...,
    ): ...

@add_start_docstrings(..., ELECTRA_START_DOCSTRING)
class FlaxElectraModel(FlaxElectraPreTrainedModel):
    module_class = ...

class FlaxElectraTiedDense(nn.Module):
    embedding_size: int
    dtype: jnp.dtype = ...
    precision = ...
    bias_init: Callable[..., np.ndarray] = ...
    def setup(self): ...
    def __call__(self, x, kernel): ...

class FlaxElectraForMaskedLMModule(nn.Module):
    config: ElectraConfig
    dtype: jnp.dtype = ...
    gradient_checkpointing: bool = ...
    def setup(self): ...
    def __call__(
        self,
        input_ids,
        attention_mask=...,
        token_type_ids=...,
        position_ids=...,
        head_mask=...,
        deterministic: bool = ...,
        output_attentions: bool = ...,
        output_hidden_states: bool = ...,
        return_dict: bool = ...,
    ): ...

@add_start_docstrings(..., ELECTRA_START_DOCSTRING)
class FlaxElectraForMaskedLM(FlaxElectraPreTrainedModel):
    module_class = ...

class FlaxElectraForPreTrainingModule(nn.Module):
    config: ElectraConfig
    dtype: jnp.dtype = ...
    gradient_checkpointing: bool = ...
    def setup(self): ...
    def __call__(
        self,
        input_ids,
        attention_mask=...,
        token_type_ids=...,
        position_ids=...,
        head_mask=...,
        deterministic: bool = ...,
        output_attentions: bool = ...,
        output_hidden_states: bool = ...,
        return_dict: bool = ...,
    ): ...

@add_start_docstrings(..., ELECTRA_START_DOCSTRING)
class FlaxElectraForPreTraining(FlaxElectraPreTrainedModel):
    module_class = ...

FLAX_ELECTRA_FOR_PRETRAINING_DOCSTRING = ...

class FlaxElectraForTokenClassificationModule(nn.Module):
    config: ElectraConfig
    dtype: jnp.dtype = ...
    gradient_checkpointing: bool = ...
    def setup(self): ...
    def __call__(
        self,
        input_ids,
        attention_mask=...,
        token_type_ids=...,
        position_ids=...,
        head_mask=...,
        deterministic: bool = ...,
        output_attentions: bool = ...,
        output_hidden_states: bool = ...,
        return_dict: bool = ...,
    ): ...

@add_start_docstrings(..., ELECTRA_START_DOCSTRING)
class FlaxElectraForTokenClassification(FlaxElectraPreTrainedModel):
    module_class = ...

def identity(x, **kwargs): ...

class FlaxElectraSequenceSummary(nn.Module):
    config: ElectraConfig
    dtype: jnp.dtype = ...
    def setup(self): ...
    def __call__(self, hidden_states, cls_index=..., deterministic: bool = ...): ...

class FlaxElectraForMultipleChoiceModule(nn.Module):
    config: ElectraConfig
    dtype: jnp.dtype = ...
    gradient_checkpointing: bool = ...
    def setup(self): ...
    def __call__(
        self,
        input_ids,
        attention_mask=...,
        token_type_ids=...,
        position_ids=...,
        head_mask=...,
        deterministic: bool = ...,
        output_attentions: bool = ...,
        output_hidden_states: bool = ...,
        return_dict: bool = ...,
    ): ...

@add_start_docstrings(..., ELECTRA_START_DOCSTRING)
class FlaxElectraForMultipleChoice(FlaxElectraPreTrainedModel):
    module_class = ...

class FlaxElectraForQuestionAnsweringModule(nn.Module):
    config: ElectraConfig
    dtype: jnp.dtype = ...
    gradient_checkpointing: bool = ...
    def setup(self): ...
    def __call__(
        self,
        input_ids,
        attention_mask=...,
        token_type_ids=...,
        position_ids=...,
        head_mask=...,
        deterministic: bool = ...,
        output_attentions: bool = ...,
        output_hidden_states: bool = ...,
        return_dict: bool = ...,
    ): ...

@add_start_docstrings(..., ELECTRA_START_DOCSTRING)
class FlaxElectraForQuestionAnswering(FlaxElectraPreTrainedModel):
    module_class = ...

class FlaxElectraClassificationHead(nn.Module):
    config: ElectraConfig
    dtype: jnp.dtype = ...
    def setup(self): ...
    def __call__(self, hidden_states, deterministic: bool = ...): ...

class FlaxElectraForSequenceClassificationModule(nn.Module):
    config: ElectraConfig
    dtype: jnp.dtype = ...
    gradient_checkpointing: bool = ...
    def setup(self): ...
    def __call__(
        self,
        input_ids,
        attention_mask=...,
        token_type_ids=...,
        position_ids=...,
        head_mask=...,
        deterministic: bool = ...,
        output_attentions: bool = ...,
        output_hidden_states: bool = ...,
        return_dict: bool = ...,
    ): ...

@add_start_docstrings(..., ELECTRA_START_DOCSTRING)
class FlaxElectraForSequenceClassification(FlaxElectraPreTrainedModel):
    module_class = ...

class FlaxElectraForCausalLMModule(nn.Module):
    config: ElectraConfig
    dtype: jnp.dtype = ...
    gradient_checkpointing: bool = ...
    def setup(self): ...
    def __call__(
        self,
        input_ids,
        attention_mask: Optional[jnp.ndarray] = ...,
        token_type_ids: Optional[jnp.ndarray] = ...,
        position_ids: Optional[jnp.ndarray] = ...,
        head_mask: Optional[jnp.ndarray] = ...,
        encoder_hidden_states: Optional[jnp.ndarray] = ...,
        encoder_attention_mask: Optional[jnp.ndarray] = ...,
        init_cache: bool = ...,
        deterministic: bool = ...,
        output_attentions: bool = ...,
        output_hidden_states: bool = ...,
        return_dict: bool = ...,
    ): ...

@add_start_docstrings(..., ELECTRA_START_DOCSTRING)
class FlaxElectraForCausalLM(FlaxElectraPreTrainedModel):
    module_class = ...
    def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[jax.Array] = ...): ...
    def update_inputs_for_generation(self, model_outputs, model_kwargs): ...

__all__ = [
    "FlaxElectraForCausalLM",
    "FlaxElectraForMaskedLM",
    "FlaxElectraForMultipleChoice",
    "FlaxElectraForPreTraining",
    "FlaxElectraForQuestionAnswering",
    "FlaxElectraForSequenceClassification",
    "FlaxElectraForTokenClassification",
    "FlaxElectraModel",
    "FlaxElectraPreTrainedModel",
]
