"""
This type stub file was generated by pyright.
"""

import torch
from dataclasses import dataclass
from typing import Optional, Union
from torch import nn
from ...modeling_utils import PreTrainedModel
from ...utils import ModelOutput, auto_docstring
from .configuration_encodec import EncodecConfig

"""PyTorch EnCodec model."""
logger = ...

@dataclass
@auto_docstring
class EncodecOutput(ModelOutput):
    r"""
    audio_codes (`torch.LongTensor`  of shape `(nb_frames, batch_size, nb_quantizers, frame_len)`, *optional*):
        Discrete code embeddings computed using `model.encode`.
    audio_values (`torch.FloatTensor`  of shape `(batch_size, segment_length)`, *optional*):
        Decoded audio values, obtained using the decoder part of Encodec.
    """

    audio_codes: torch.LongTensor | None = ...
    audio_values: torch.FloatTensor | None = ...

@dataclass
@auto_docstring
class EncodecEncoderOutput(ModelOutput):
    r"""
    audio_codes (`torch.LongTensor`  of shape `(nb_frames, batch_size, nb_quantizers, frame_len)`, *optional*):
        Discrete code embeddings computed using `model.encode`.
    audio_scales (list of length `nb_frames` of `torch.Tensor` of shape `(batch_size, 1)`, *optional*):
        Scaling factor for each `audio_codes` input. This is used to unscale each chunk of audio when decoding.
    last_frame_pad_length (`int`, *optional*):
        The length of the padding in the last frame, if any. This is used to ensure that the encoded frames can be
        outputted as a tensor. This value should be passed during decoding to ensure padding is removed from the
        encoded frames.
    """

    audio_codes: torch.LongTensor | None = ...
    audio_scales: torch.FloatTensor | None = ...
    last_frame_pad_length: int | None = ...

@dataclass
@auto_docstring
class EncodecDecoderOutput(ModelOutput):
    r"""
    audio_values (`torch.FloatTensor`  of shape `(batch_size, segment_length)`, *optional*):
        Decoded audio values, obtained using the decoder part of Encodec.
    """

    audio_values: torch.FloatTensor | None = ...

class EncodecConv1d(nn.Module):
    """Conv1d with asymmetric or causal padding and normalization."""
    def __init__(
        self, config, in_channels: int, out_channels: int, kernel_size: int, stride: int = ..., dilation: int = ...
    ) -> None: ...
    def forward(self, hidden_states):  # -> Any:
        ...

class EncodecConvTranspose1d(nn.Module):
    """ConvTranspose1d with asymmetric or causal padding and normalization."""
    def __init__(self, config, in_channels: int, out_channels: int, kernel_size: int, stride: int = ...) -> None: ...
    def forward(self, hidden_states):  # -> Any:
        ...

class EncodecLSTM(nn.Module):
    """
    LSTM without worrying about the hidden state, nor the layout of the data. Expects input as convolutional layout.
    """
    def __init__(self, config: EncodecConfig, dimension: int) -> None: ...
    def forward(self, hidden_states): ...

class EncodecResnetBlock(nn.Module):
    """
    Residual block from SEANet model as used by EnCodec.
    """
    def __init__(self, config: EncodecConfig, dim: int, dilations: list[int]) -> None: ...
    def forward(self, hidden_states):  # -> Any:
        ...

class EncodecEncoder(nn.Module):
    """SEANet encoder as used by EnCodec."""
    def __init__(self, config: EncodecConfig) -> None: ...
    def forward(self, hidden_states):  # -> Any:
        ...

class EncodecDecoder(nn.Module):
    """SEANet decoder as used by EnCodec."""
    def __init__(self, config: EncodecConfig) -> None: ...
    def forward(self, hidden_states):  # -> Any:
        ...

class EncodecEuclideanCodebook(nn.Module):
    """Codebook with Euclidean distance."""
    def __init__(self, config: EncodecConfig) -> None: ...
    def quantize(self, hidden_states): ...
    def encode(self, hidden_states): ...
    def decode(self, embed_ind):  # -> Tensor:
        ...

class EncodecVectorQuantization(nn.Module):
    """
    Vector quantization implementation. Currently supports only euclidean distance.
    """
    def __init__(self, config: EncodecConfig) -> None: ...
    def encode(self, hidden_states): ...
    def decode(self, embed_ind):  # -> Tensor:
        ...

class EncodecResidualVectorQuantizer(nn.Module):
    """Residual Vector Quantizer."""
    def __init__(self, config: EncodecConfig) -> None: ...
    def get_num_quantizers_for_bandwidth(self, bandwidth: float | None = ...) -> int:
        """Return num_quantizers based on specified target bandwidth."""
        ...

    def encode(self, embeddings: torch.Tensor, bandwidth: float | None = ...) -> torch.Tensor:
        """
        Encode a given input tensor with the specified frame rate at the given bandwidth. The RVQ encode method sets
        the appropriate number of quantizers to use and returns indices for each quantizer.
        """
        ...

    def decode(self, codes: torch.Tensor) -> torch.Tensor:
        """Decode the given codes to the quantized representation."""
        ...

@auto_docstring
class EncodecPreTrainedModel(PreTrainedModel):
    config: EncodecConfig
    base_model_prefix = ...
    main_input_name = ...

@auto_docstring(
    custom_intro="""
    The EnCodec neural audio codec model.
    """
)
class EncodecModel(EncodecPreTrainedModel):
    def __init__(self, config: EncodecConfig) -> None: ...
    def get_encoder(self):  # -> EncodecEncoder:
        ...
    def get_decoder(self):  # -> EncodecDecoder:
        ...
    def encode(
        self,
        input_values: torch.Tensor,
        padding_mask: torch.Tensor | None = ...,
        bandwidth: float | None = ...,
        return_dict: bool | None = ...,
    ) -> tuple[torch.Tensor, torch.Tensor | None, int] | EncodecEncoderOutput:
        """
        Encodes the input audio waveform into discrete codes of shape
        `(nb_frames, batch_size, nb_quantizers, frame_len)`.

        - `nb_frames=1` if `self.config.chunk_length=None` (as the encoder is applied on the full audio), which is the
        case for the 24kHz model. Otherwise, `nb_frames=ceil(input_length/self.config.chunk_stride)`, which is the case
        for the 48kHz model.
        - `frame_len` is the length of each frame, which is equal to `ceil(input_length/self.config.hop_length)` if
        `self.config.chunk_length=None` (e.g., for the 24kHz model). Otherwise, if `self.config.chunk_length` is
        defined, `frame_len=self.config.chunk_length/self.config.hop_length`, e.g., the case for the 48kHz model with
        `frame_len=150`.

        Args:
            input_values (`torch.Tensor` of shape `(batch_size, channels, sequence_length)`):
                Float values of the input audio waveform.
            padding_mask (`torch.Tensor` of shape `(batch_size, channels, sequence_length)`):
                Padding mask used to pad the `input_values`.
            bandwidth (`float`, *optional*):
                The target bandwidth. Must be one of `config.target_bandwidths`. If `None`, uses the smallest possible
                bandwidth. bandwidth is represented as a thousandth of what it is, e.g. 6kbps bandwidth is represented
                as bandwidth == 6.0

        Returns:
            EncodecEncoderOutput dict or a tuple containing:
            - audio_codes (`torch.LongTensor`  of shape `(nb_frames, batch_size, nb_quantizers, frame_len)`, *optional*),
            - audio_scales (list of length `nb_frames` of `torch.Tensor` of shape `(batch_size, 1)`, *optional*),
            - last_frame_pad_length (`int`, *optional*).
        """
        ...

    def decode(
        self,
        audio_codes: torch.LongTensor,
        audio_scales: torch.Tensor,
        padding_mask: torch.Tensor | None = ...,
        return_dict: bool | None = ...,
        last_frame_pad_length: int | None = ...,
    ) -> tuple[torch.Tensor, torch.Tensor] | EncodecDecoderOutput:
        """
        Decodes the given frames into an output audio waveform.

        Note that the output might be a bit bigger than the input. In that case, any extra steps at the end can be
        trimmed.

        Args:
            audio_codes (`torch.LongTensor`  of shape `(nb_frames, batch_size, nb_quantizers, frame_len)`, *optional*):
                Discrete code embeddings computed using `model.encode`.
            audio_scales (list of length `nb_frames` of `torch.Tensor` of shape `(batch_size, 1)`, *optional*):
                Scaling factor for each `audio_codes` input.
            padding_mask (`torch.Tensor` of shape `(channels, sequence_length)`):
                Padding mask used to pad the `input_values`.
            return_dict (`bool`, *optional*):
                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
            last_frame_pad_length (`int`, *optional*):
                Integer representing the length of the padding in the last frame, which is removed during decoding.

        """
        ...

    @auto_docstring
    def forward(
        self,
        input_values: torch.FloatTensor,
        padding_mask: torch.BoolTensor | None = ...,
        bandwidth: float | None = ...,
        audio_codes: torch.LongTensor | None = ...,
        audio_scales: torch.Tensor | None = ...,
        return_dict: bool | None = ...,
        last_frame_pad_length: int | None = ...,
    ) -> tuple[torch.Tensor, torch.Tensor] | EncodecOutput:
        r"""
        input_values (`torch.FloatTensor` of shape `(batch_size, channels, sequence_length)`, *optional*):
            Raw audio input converted to Float and padded to the appropriate length in order to be encoded using chunks
            of length self.chunk_length and a stride of `config.chunk_stride`.
        padding_mask (`torch.BoolTensor` of shape `(batch_size, channels, sequence_length)`, *optional*):
            Mask to avoid computing scaling factors on padding token indices (can we avoid computing conv on these+).
            Mask values selected in `[0, 1]`:

            - 1 for tokens that are **not masked**,
            - 0 for tokens that are **masked**.

            <Tip warning={true}>

            `padding_mask` should always be passed, unless the input was truncated or not padded. This is because in
            order to process tensors effectively, the input audio should be padded so that `input_length % stride =
            step` with `step = chunk_length-stride`. This ensures that all chunks are of the same shape

            </Tip>
        bandwidth (`float`, *optional*):
            The target bandwidth. Must be one of `config.target_bandwidths`. If `None`, uses the smallest possible
            bandwidth. bandwidth is represented as a thousandth of what it is, e.g. 6kbps bandwidth is represented as
            `bandwidth == 6.0`
        audio_codes (`torch.LongTensor`  of shape `(nb_frames, batch_size, nb_quantizers, frame_len)`, *optional*):
            Discrete code embeddings computed using `model.encode`.
        audio_scales (list of length `nb_frames` of `torch.Tensor` of shape `(batch_size, 1)`, *optional*):
            Scaling factor for each `audio_codes` input.
        return_dict (`bool`, *optional*):
            Whether to return outputs as a dict.
        last_frame_pad_length (`int`, *optional*):
            The length of the padding in the last frame, if any. This is used to ensure that the encoded frames can be
            outputted as a tensor. This value should be passed during decoding to ensure padding is removed from the
            encoded frames.

        Examples:

        ```python
        >>> from datasets import load_dataset
        >>> from transformers import AutoProcessor, EncodecModel

        >>> dataset = load_dataset("hf-internal-testing/ashraq-esc50-1-dog-example")
        >>> audio_sample = dataset["train"]["audio"][0]["array"]

        >>> model_id = "facebook/encodec_24khz"
        >>> model = EncodecModel.from_pretrained(model_id)
        >>> processor = AutoProcessor.from_pretrained(model_id)

        >>> inputs = processor(raw_audio=audio_sample, return_tensors="pt")

        >>> outputs = model(**inputs)
        >>> audio_codes = outputs.audio_codes
        >>> audio_values = outputs.audio_values
        ```"""
        ...

__all__ = ["EncodecModel", "EncodecPreTrainedModel"]
