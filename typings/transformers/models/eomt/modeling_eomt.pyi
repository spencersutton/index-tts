"""
This type stub file was generated by pyright.
"""

import numpy as np
import torch
from dataclasses import dataclass
from typing import Optional, Union
from torch import Tensor, nn
from ...file_utils import ModelOutput, is_scipy_available
from ...modeling_layers import GradientCheckpointingLayer
from ...modeling_utils import PreTrainedModel
from ...utils import auto_docstring, can_return_tuple, is_accelerate_available
from .configuration_eomt import EomtConfig

if is_scipy_available(): ...
if is_accelerate_available(): ...

@dataclass
@auto_docstring(custom_intro=...)
class EomtForUniversalSegmentationOutput(ModelOutput):
    loss: Optional[torch.FloatTensor] = ...
    class_queries_logits: Optional[torch.FloatTensor] = ...
    masks_queries_logits: Optional[torch.FloatTensor] = ...
    last_hidden_state: Optional[torch.FloatTensor] = ...
    hidden_states: Optional[tuple[torch.FloatTensor]] = ...
    attentions: Optional[tuple[torch.FloatTensor]] = ...
    patch_offsets: Optional[list[torch.Tensor]] = ...

def sample_point(
    input_features: torch.Tensor, point_coordinates: torch.Tensor, add_dim=..., **kwargs
) -> torch.Tensor: ...
def pair_wise_dice_loss(inputs: Tensor, labels: Tensor) -> Tensor: ...
def pair_wise_sigmoid_cross_entropy_loss(inputs: torch.Tensor, labels: torch.Tensor) -> torch.Tensor: ...

class EomtHungarianMatcher(nn.Module):
    def __init__(
        self, cost_class: float = ..., cost_mask: float = ..., cost_dice: float = ..., num_points: int = ...
    ) -> None: ...
    @torch.no_grad()
    def forward(
        self,
        masks_queries_logits: torch.Tensor,
        class_queries_logits: torch.Tensor,
        mask_labels: torch.Tensor,
        class_labels: torch.Tensor,
    ) -> list[tuple[Tensor]]: ...

def dice_loss(inputs: Tensor, labels: Tensor, num_masks: int) -> Tensor: ...
def sigmoid_cross_entropy_loss(inputs: torch.Tensor, labels: torch.Tensor, num_masks: int) -> torch.Tensor: ...

class EomtLoss(nn.Module):
    def __init__(self, config: EomtConfig, weight_dict: dict[str, float]) -> None: ...
    def loss_labels(
        self, class_queries_logits: Tensor, class_labels: list[Tensor], indices: tuple[np.array]
    ) -> dict[str, Tensor]: ...
    def loss_masks(
        self,
        masks_queries_logits: torch.Tensor,
        mask_labels: list[torch.Tensor],
        indices: tuple[np.array],
        num_masks: int,
    ) -> dict[str, torch.Tensor]: ...
    def calculate_uncertainty(self, logits: torch.Tensor) -> torch.Tensor: ...
    def sample_points_using_uncertainty(
        self,
        logits: torch.Tensor,
        uncertainty_function,
        num_points: int,
        oversample_ratio: int,
        importance_sample_ratio: float,
    ) -> torch.Tensor: ...
    def forward(
        self,
        masks_queries_logits: torch.Tensor,
        class_queries_logits: torch.Tensor,
        mask_labels: list[torch.Tensor],
        class_labels: list[torch.Tensor],
        auxiliary_predictions: Optional[dict[str, torch.Tensor]] = ...,
    ) -> dict[str, torch.Tensor]: ...
    def get_num_masks(self, class_labels: torch.Tensor, device: torch.device) -> torch.Tensor: ...

class EomtPatchEmbeddings(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, pixel_values: torch.Tensor) -> torch.Tensor: ...

class EomtEmbeddings(nn.Module):
    def __init__(self, config: EomtConfig) -> None: ...
    def forward(self, pixel_values: torch.Tensor) -> torch.Tensor: ...

def eager_attention_forward(
    module: nn.Module,
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: Optional[torch.Tensor],
    scaling: float,
    dropout: float = ...,
    **kwargs,
): ...

class EomtAttention(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(
        self, hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor] = ..., **kwargs
    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]: ...

class EomtLayerScale(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_state: torch.Tensor) -> torch.Tensor: ...

def drop_path(input: torch.Tensor, drop_prob: float = ..., training: bool = ...) -> torch.Tensor: ...

class EomtDropPath(nn.Module):
    def __init__(self, drop_prob: Optional[float] = ...) -> None: ...
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor: ...
    def extra_repr(self) -> str: ...

class EomtMLP(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_state: torch.Tensor) -> torch.Tensor: ...

class EomtSwiGLUFFN(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_state: torch.Tensor) -> torch.Tensor: ...

class EomtLayer(GradientCheckpointingLayer):
    def __init__(self, config: EomtConfig) -> None: ...
    def forward(
        self, hidden_states: torch.Tensor, head_mask: Optional[torch.Tensor] = ..., output_attentions: bool = ...
    ) -> Union[tuple[torch.Tensor, torch.Tensor], tuple[torch.Tensor]]: ...

class EomtLayerNorm2d(nn.LayerNorm):
    def __init__(self, num_channels, eps=..., affine=...) -> None: ...
    def forward(self, hidden_state: torch.Tensor) -> torch.Tensor: ...

class EomtScaleLayer(nn.Module):
    def __init__(self, config: EomtConfig) -> None: ...
    def forward(self, hidden_states: torch.tensor) -> torch.Tensor: ...

class EomtScaleBlock(nn.Module):
    def __init__(self, config: EomtConfig) -> None: ...
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor: ...

class EomtMaskHead(nn.Module):
    def __init__(self, config: EomtConfig) -> None: ...
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor: ...

@auto_docstring
class EomtPreTrainedModel(PreTrainedModel):
    config: EomtConfig
    base_model_prefix = ...
    main_input_name = ...
    supports_gradient_checkpointing = ...
    _no_split_modules = ...
    _supports_sdpa = ...
    _supports_flash_attn = ...

@auto_docstring(custom_intro=...)
class EomtForUniversalSegmentation(EomtPreTrainedModel):
    main_input_name = ...
    def __init__(self, config: EomtConfig) -> None: ...
    def get_loss_dict(
        self,
        masks_queries_logits: Tensor,
        class_queries_logits: Tensor,
        mask_labels: Tensor,
        class_labels: Tensor,
        auxiliary_predictions: dict[str, Tensor],
    ) -> dict[str, Tensor]: ...
    def get_loss(self, loss_dict: dict[str, Tensor]) -> Tensor: ...
    @auto_docstring
    @can_return_tuple
    def forward(
        self,
        pixel_values: Tensor,
        mask_labels: Optional[list[Tensor]] = ...,
        class_labels: Optional[list[Tensor]] = ...,
        output_hidden_states: Optional[bool] = ...,
        output_attentions: Optional[bool] = ...,
        patch_offsets: Optional[list[Tensor]] = ...,
    ) -> EomtForUniversalSegmentationOutput: ...
    def get_input_embeddings(self): ...
    def predict(self, logits: torch.Tensor): ...

__all__ = ["EomtPreTrainedModel", "EomtForUniversalSegmentation"]
