"""
This type stub file was generated by pyright.
"""

import torch
import torch.nn as nn
from collections.abc import Sequence
from dataclasses import dataclass
from typing import Callable, Optional, Union
from ...modeling_outputs import ModelOutput
from ...utils import auto_docstring
from .modeling_esm import EsmPreTrainedModel
from .openfold_utils import Rigid

logger = ...

@dataclass
@auto_docstring(custom_intro=...)
class EsmForProteinFoldingOutput(ModelOutput):
    frames: Optional[torch.FloatTensor] = ...
    sidechain_frames: Optional[torch.FloatTensor] = ...
    unnormalized_angles: Optional[torch.FloatTensor] = ...
    angles: Optional[torch.FloatTensor] = ...
    positions: Optional[torch.FloatTensor] = ...
    states: Optional[torch.FloatTensor] = ...
    s_s: Optional[torch.FloatTensor] = ...
    s_z: Optional[torch.FloatTensor] = ...
    distogram_logits: Optional[torch.FloatTensor] = ...
    lm_logits: Optional[torch.FloatTensor] = ...
    aatype: Optional[torch.FloatTensor] = ...
    atom14_atom_exists: Optional[torch.FloatTensor] = ...
    residx_atom14_to_atom37: Optional[torch.FloatTensor] = ...
    residx_atom37_to_atom14: Optional[torch.FloatTensor] = ...
    atom37_atom_exists: Optional[torch.FloatTensor] = ...
    residue_index: Optional[torch.FloatTensor] = ...
    lddt_head: Optional[torch.FloatTensor] = ...
    plddt: Optional[torch.FloatTensor] = ...
    ptm_logits: Optional[torch.FloatTensor] = ...
    ptm: Optional[torch.FloatTensor] = ...
    aligned_confidence_probs: Optional[torch.FloatTensor] = ...
    predicted_aligned_error: Optional[torch.FloatTensor] = ...
    max_predicted_aligned_error: Optional[torch.FloatTensor] = ...

def is_fp16_enabled(device_type): ...
def is_deepspeed_initialized(): ...
def collate_dense_tensors(samples: list[torch.Tensor], pad_v: float = ...) -> torch.Tensor: ...
def flatten_final_dims(t: torch.Tensor, no_dims: int): ...
def permute_final_dims(tensor: torch.Tensor, inds: list[int]): ...
def dict_multimap(fn, dicts): ...
def trunc_normal_init_(weights, scale=..., fan=...): ...
def ipa_point_weights_init_(weights): ...

class EsmFoldLinear(nn.Linear):
    def __init__(
        self,
        in_dim: int,
        out_dim: int,
        bias: bool = ...,
        init: str = ...,
        init_fn: Optional[Callable[[torch.Tensor, torch.Tensor], None]] = ...,
    ) -> None: ...

class EsmFoldLayerNorm(nn.Module):
    def __init__(self, c_in, eps=...) -> None: ...
    def forward(self, x): ...

@torch.jit.ignore
def softmax_no_cast(t: torch.Tensor, dim: int = ...) -> torch.Tensor: ...

class EsmFoldAttention(nn.Module):
    def __init__(self, c_q: int, c_k: int, c_v: int, c_hidden: int, no_heads: int, gating: bool = ...) -> None: ...
    def forward(
        self,
        q_x: torch.Tensor,
        kv_x: torch.Tensor,
        biases: Optional[list[torch.Tensor]] = ...,
        use_memory_efficient_kernel: bool = ...,
        use_lma: bool = ...,
        lma_q_chunk_size: int = ...,
        lma_kv_chunk_size: int = ...,
        use_flash: bool = ...,
        flash_mask: Optional[torch.Tensor] = ...,
    ) -> torch.Tensor: ...

class EsmFoldTriangleAttention(nn.Module):
    def __init__(self, c_in, c_hidden, no_heads, starting=..., inf=...) -> None: ...
    def forward(
        self,
        x: torch.Tensor,
        mask: Optional[torch.Tensor] = ...,
        chunk_size: Optional[int] = ...,
        use_memory_efficient_kernel: bool = ...,
        use_lma: bool = ...,
        inplace_safe: bool = ...,
    ) -> torch.Tensor: ...

class EsmFoldTriangleMultiplicativeUpdate(nn.Module):
    def __init__(self, config, _outgoing=...) -> None: ...
    def forward(
        self,
        z: torch.Tensor,
        mask: Optional[torch.Tensor] = ...,
        inplace_safe: bool = ...,
        _add_with_inplace: bool = ...,
        _inplace_chunk_size: Optional[int] = ...,
    ) -> torch.Tensor: ...

class EsmFoldPreTrainedModel(EsmPreTrainedModel): ...

class EsmFoldSelfAttention(nn.Module):
    def __init__(self, embed_dim, num_heads, head_width, gated=...) -> None: ...
    def forward(self, x, mask=..., bias=..., indices=...): ...

class EsmFoldDropout(nn.Module):
    def __init__(self, r: float, batch_dim: Union[int, list[int]]) -> None: ...
    def forward(self, x: torch.Tensor) -> torch.Tensor: ...

class EsmFoldSequenceToPair(nn.Module):
    def __init__(self, sequence_state_dim, inner_dim, pairwise_state_dim) -> None: ...
    def forward(self, sequence_state): ...

class EsmFoldPairToSequence(nn.Module):
    def __init__(self, pairwise_state_dim, num_heads) -> None: ...
    def forward(self, pairwise_state): ...

class EsmFoldResidueMLP(nn.Module):
    def __init__(self, embed_dim, inner_dim, dropout=...) -> None: ...
    def forward(self, x): ...

class EsmFoldTriangularSelfAttentionBlock(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, sequence_state, pairwise_state, mask=..., chunk_size=..., **__kwargs): ...

class EsmCategoricalMixture:
    def __init__(self, param, bins=..., start=..., end=...) -> None: ...
    def log_prob(self, true): ...
    def mean(self): ...

def categorical_lddt(logits, bins=...): ...
def get_axial_mask(mask): ...

class EsmFoldRelativePosition(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, residue_index, mask=...): ...

class EsmFoldAngleResnetBlock(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, a: torch.Tensor) -> torch.Tensor: ...

class EsmFoldAngleResnet(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, s: torch.Tensor, s_initial: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]: ...

class EsmFoldInvariantPointAttention(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(
        self,
        s: torch.Tensor,
        z: Optional[torch.Tensor],
        r: Rigid,
        mask: torch.Tensor,
        _offload_inference: bool = ...,
        _z_reference_list: Optional[Sequence[torch.Tensor]] = ...,
    ) -> torch.Tensor: ...

class EsmFoldBackboneUpdate(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, s: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]: ...

class EsmFoldStructureModuleTransitionLayer(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, s): ...

class EsmFoldStructureModuleTransition(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, s): ...

class EsmFoldStructureModule(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, evoformer_output_dict, aatype, mask=..., _offload_inference=...): ...
    def torsion_angles_to_frames(self, r, alpha, f): ...
    def frames_and_literature_positions_to_atom14_pos(self, r, f): ...

class EsmFoldingTrunk(nn.Module):
    def __init__(self, config) -> None: ...
    def set_chunk_size(self, chunk_size): ...
    def forward(self, seq_feats, pair_feats, true_aa, residx, mask, no_recycles): ...
    @staticmethod
    def distogram(coords, min_bin, max_bin, num_bins): ...

@auto_docstring(custom_intro=...)
class EsmForProteinFolding(EsmPreTrainedModel):
    _no_split_modules = ...
    _supports_flash_attn = ...
    def __init__(self, config) -> None: ...
    @auto_docstring
    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = ...,
        position_ids: Optional[torch.Tensor] = ...,
        masking_pattern: Optional[torch.Tensor] = ...,
        num_recycles: Optional[int] = ...,
        output_hidden_states: Optional[bool] = ...,
    ) -> EsmForProteinFoldingOutput: ...
    def af2_idx_to_esm_idx(self, aa, mask): ...
    def compute_language_model_representations(self, esmaa: torch.Tensor) -> torch.Tensor: ...
    def bert_mask(self, aa, esmaa, mask, pattern): ...
    @torch.no_grad()
    def infer(self, seqs: Union[str, list[str]], position_ids=...): ...
    @staticmethod
    def output_to_pdb(output: dict) -> list[str]: ...
    def infer_pdb(self, seqs, *args, **kwargs) -> str: ...
    def infer_pdbs(self, seqs: list[str], *args, **kwargs) -> list[str]: ...

__all__ = ["EsmForProteinFolding", "EsmFoldPreTrainedModel"]
