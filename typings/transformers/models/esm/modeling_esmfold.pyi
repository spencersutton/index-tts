from collections.abc import Callable, Sequence
from dataclasses import dataclass

import torch
from torch import nn

from ...modeling_outputs import ModelOutput
from .modeling_esm import EsmPreTrainedModel
from .openfold_utils import Rigid

logger = ...

@dataclass
class EsmForProteinFoldingOutput(ModelOutput):
    frames: torch.FloatTensor | None = ...
    sidechain_frames: torch.FloatTensor | None = ...
    unnormalized_angles: torch.FloatTensor | None = ...
    angles: torch.FloatTensor | None = ...
    positions: torch.FloatTensor | None = ...
    states: torch.FloatTensor | None = ...
    s_s: torch.FloatTensor | None = ...
    s_z: torch.FloatTensor | None = ...
    distogram_logits: torch.FloatTensor | None = ...
    lm_logits: torch.FloatTensor | None = ...
    aatype: torch.FloatTensor | None = ...
    atom14_atom_exists: torch.FloatTensor | None = ...
    residx_atom14_to_atom37: torch.FloatTensor | None = ...
    residx_atom37_to_atom14: torch.FloatTensor | None = ...
    atom37_atom_exists: torch.FloatTensor | None = ...
    residue_index: torch.FloatTensor | None = ...
    lddt_head: torch.FloatTensor | None = ...
    plddt: torch.FloatTensor | None = ...
    ptm_logits: torch.FloatTensor | None = ...
    ptm: torch.FloatTensor | None = ...
    aligned_confidence_probs: torch.FloatTensor | None = ...
    predicted_aligned_error: torch.FloatTensor | None = ...
    max_predicted_aligned_error: torch.FloatTensor | None = ...

def is_fp16_enabled(device_type):  # -> bool:
    ...
def is_deepspeed_initialized():  # -> Literal[False]:
    ...
def collate_dense_tensors(samples: list[torch.Tensor], pad_v: float = ...) -> torch.Tensor: ...
def flatten_final_dims(t: torch.Tensor, no_dims: int):  # -> Tensor:
    ...
def permute_final_dims(tensor: torch.Tensor, inds: list[int]):  # -> Tensor:
    ...
def dict_multimap(fn, dicts):  # -> dict[Any, Any]:
    ...
def trunc_normal_init_(weights, scale=..., fan=...):  # -> None:
    ...
def ipa_point_weights_init_(weights):  # -> None:
    ...

class EsmFoldLinear(nn.Linear):
    def __init__(
        self,
        in_dim: int,
        out_dim: int,
        bias: bool = ...,
        init: str = ...,
        init_fn: Callable[[torch.Tensor, torch.Tensor], None] | None = ...,
    ) -> None: ...

class EsmFoldLayerNorm(nn.Module):
    def __init__(self, c_in, eps=...) -> None: ...
    def forward(self, x):  # -> Tensor:
        ...

@torch.jit.ignore
def softmax_no_cast(t: torch.Tensor, dim: int = ...) -> torch.Tensor: ...

class EsmFoldAttention(nn.Module):
    def __init__(self, c_q: int, c_k: int, c_v: int, c_hidden: int, no_heads: int, gating: bool = ...) -> None: ...
    def forward(
        self,
        q_x: torch.Tensor,
        kv_x: torch.Tensor,
        biases: list[torch.Tensor] | None = ...,
        use_memory_efficient_kernel: bool = ...,
        use_lma: bool = ...,
        lma_q_chunk_size: int = ...,
        lma_kv_chunk_size: int = ...,
        use_flash: bool = ...,
        flash_mask: torch.Tensor | None = ...,
    ) -> torch.Tensor: ...

class EsmFoldTriangleAttention(nn.Module):
    def __init__(self, c_in, c_hidden, no_heads, starting=..., inf=...) -> None: ...
    def forward(
        self,
        x: torch.Tensor,
        mask: torch.Tensor | None = ...,
        chunk_size: int | None = ...,
        use_memory_efficient_kernel: bool = ...,
        use_lma: bool = ...,
        inplace_safe: bool = ...,
    ) -> torch.Tensor: ...

class EsmFoldTriangleMultiplicativeUpdate(nn.Module):
    def __init__(self, config, _outgoing=...) -> None: ...
    def forward(
        self,
        z: torch.Tensor,
        mask: torch.Tensor | None = ...,
        inplace_safe: bool = ...,
        _add_with_inplace: bool = ...,
        _inplace_chunk_size: int | None = ...,
    ) -> torch.Tensor: ...

class EsmFoldPreTrainedModel(EsmPreTrainedModel): ...

class EsmFoldSelfAttention(nn.Module):
    def __init__(self, embed_dim, num_heads, head_width, gated=...) -> None: ...
    def forward(self, x, mask=..., bias=..., indices=...):  # -> tuple[Any, Tensor]:

        ...

class EsmFoldDropout(nn.Module):
    def __init__(self, r: float, batch_dim: int | list[int]) -> None: ...
    def forward(self, x: torch.Tensor) -> torch.Tensor: ...

class EsmFoldSequenceToPair(nn.Module):
    def __init__(self, sequence_state_dim, inner_dim, pairwise_state_dim) -> None: ...
    def forward(self, sequence_state):  # -> Any:

        ...

class EsmFoldPairToSequence(nn.Module):
    def __init__(self, pairwise_state_dim, num_heads) -> None: ...
    def forward(self, pairwise_state):  # -> Any:

        ...

class EsmFoldResidueMLP(nn.Module):
    def __init__(self, embed_dim, inner_dim, dropout=...) -> None: ...
    def forward(self, x): ...

class EsmFoldTriangularSelfAttentionBlock(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, sequence_state, pairwise_state, mask=..., chunk_size=..., **__kwargs):  # -> tuple[Any, Any]:

        ...

class EsmCategoricalMixture:
    def __init__(self, param, bins=..., start=..., end=...) -> None: ...
    def log_prob(self, true):  # -> Tensor:
        ...
    def mean(self): ...

def categorical_lddt(logits, bins=...): ...
def get_axial_mask(mask):  # -> None:

    ...

class EsmFoldRelativePosition(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, residue_index, mask=...):  # -> Any:

        ...

class EsmFoldAngleResnetBlock(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, a: torch.Tensor) -> torch.Tensor: ...

class EsmFoldAngleResnet(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, s: torch.Tensor, s_initial: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]: ...

class EsmFoldInvariantPointAttention(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(
        self,
        s: torch.Tensor,
        z: torch.Tensor | None,
        r: Rigid,
        mask: torch.Tensor,
        _offload_inference: bool = ...,
        _z_reference_list: Sequence[torch.Tensor] | None = ...,
    ) -> torch.Tensor: ...

class EsmFoldBackboneUpdate(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, s: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]: ...

class EsmFoldStructureModuleTransitionLayer(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, s): ...

class EsmFoldStructureModuleTransition(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, s):  # -> Any:
        ...

class EsmFoldStructureModule(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, evoformer_output_dict, aatype, mask=..., _offload_inference=...):  # -> dict[Any, Any]:

        ...
    def torsion_angles_to_frames(self, r, alpha, f):  # -> Rigid:
        ...
    def frames_and_literature_positions_to_atom14_pos(self, r, f):  # -> Tensor:
        ...

class EsmFoldingTrunk(nn.Module):
    def __init__(self, config) -> None: ...
    def set_chunk_size(self, chunk_size):  # -> None:
        ...
    def forward(self, seq_feats, pair_feats, true_aa, residx, mask, no_recycles):  # -> Any:

        ...
    @staticmethod
    def distogram(coords, min_bin, max_bin, num_bins):  # -> Tensor:
        ...

class EsmForProteinFolding(EsmPreTrainedModel):
    _no_split_modules = ...
    _supports_flash_attn = ...
    def __init__(self, config) -> None: ...
    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor | None = ...,
        position_ids: torch.Tensor | None = ...,
        masking_pattern: torch.Tensor | None = ...,
        num_recycles: int | None = ...,
        output_hidden_states: bool | None = ...,
    ) -> EsmForProteinFoldingOutput: ...
    def af2_idx_to_esm_idx(self, aa, mask): ...
    def compute_language_model_representations(self, esmaa: torch.Tensor) -> torch.Tensor: ...
    def bert_mask(self, aa, esmaa, mask, pattern):  # -> tuple[Any, Any, Any]:
        ...
    @torch.no_grad()
    def infer(self, seqs: str | list[str], position_ids=...):  # -> EsmForProteinFoldingOutput:
        ...
    @staticmethod
    def output_to_pdb(output: dict) -> list[str]: ...
    def infer_pdb(self, seqs, *args, **kwargs) -> str: ...
    def infer_pdbs(self, seqs: list[str], *args, **kwargs) -> list[str]: ...

__all__ = ["EsmFoldPreTrainedModel", "EsmForProteinFolding"]
