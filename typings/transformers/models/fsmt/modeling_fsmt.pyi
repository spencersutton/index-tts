"""
This type stub file was generated by pyright.
"""

import torch
from typing import Any, Optional, Union
from torch import Tensor, nn
from ...cache_utils import Cache
from ...generation import GenerationMixin
from ...modeling_outputs import Seq2SeqLMOutput, Seq2SeqModelOutput
from ...modeling_utils import PreTrainedModel
from ...utils import auto_docstring
from .configuration_fsmt import FSMTConfig

logger = ...

def invert_mask(attention_mask): ...
def triu_onnx(x, diagonal=...): ...

@auto_docstring
class PretrainedFSMTModel(PreTrainedModel):
    config: FSMTConfig
    base_model_prefix = ...
    @property
    def dummy_inputs(self): ...

def shift_tokens_right(input_ids, pad_token_id): ...
def make_padding_mask(input_ids, padding_idx=...): ...

class EncoderLayer(nn.Module):
    def __init__(self, config: FSMTConfig) -> None: ...
    def forward(self, x, encoder_padding_mask, layer_head_mask, output_attentions=...): ...

class FSMTEncoder(nn.Module):
    def __init__(self, config: FSMTConfig, embed_tokens) -> None: ...
    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = ...,
        inputs_embeds: Optional[torch.Tensor] = ...,
        head_mask: Optional[torch.Tensor] = ...,
        output_attentions: bool = ...,
        output_hidden_states: bool = ...,
        return_dict: bool = ...,
    ): ...

class DecoderLayer(nn.Module):
    def __init__(self, config: FSMTConfig, layer_idx=...) -> None: ...
    def forward(
        self,
        x,
        encoder_hidden_states,
        encoder_attn_mask=...,
        layer_state=...,
        causal_mask=...,
        layer_head_mask=...,
        cross_attn_layer_head_mask=...,
        decoder_padding_mask=...,
        output_attentions=...,
        cache_position=...,
    ): ...

class FSMTDecoder(nn.Module):
    def __init__(self, config: FSMTConfig, embed_tokens: nn.Embedding) -> None: ...
    def forward(
        self,
        input_ids: torch.Tensor,
        encoder_hidden_states: torch.Tensor,
        encoder_padding_mask: torch.Tensor,
        decoder_padding_mask: torch.Tensor,
        decoder_causal_mask: torch.Tensor,
        head_mask: Optional[torch.Tensor] = ...,
        inputs_embeds: Optional[torch.Tensor] = ...,
        cross_attn_head_mask: Optional[torch.Tensor] = ...,
        past_key_values: Optional[list[torch.FloatTensor]] = ...,
        use_cache: Optional[bool] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
        cache_position: Optional[torch.Tensor] = ...,
    ): ...

class Attention(nn.Module):
    def __init__(
        self, embed_dim, num_heads, dropout=..., bias=..., encoder_decoder_attention=..., layer_idx=...
    ) -> None: ...
    def forward(
        self,
        query,
        key: Optional[Tensor],
        key_padding_mask: Optional[Tensor] = ...,
        layer_state: Optional[Cache] = ...,
        attn_mask: Optional[Tensor] = ...,
        layer_head_mask: Optional[Tensor] = ...,
        output_attentions: Optional[bool] = ...,
        cache_position: Optional[torch.Tensor] = ...,
    ) -> tuple[Tensor, Optional[Tensor]]: ...

def fill_with_neg_inf(t): ...

@auto_docstring
class FSMTModel(PretrainedFSMTModel):
    _tied_weights_keys = ...
    def __init__(self, config: FSMTConfig) -> None: ...
    def get_encoder(self): ...
    def get_decoder(self): ...
    @auto_docstring
    def forward(
        self,
        input_ids: torch.LongTensor,
        attention_mask: Optional[torch.Tensor] = ...,
        decoder_input_ids: Optional[torch.LongTensor] = ...,
        decoder_attention_mask: Optional[torch.BoolTensor] = ...,
        head_mask: Optional[torch.Tensor] = ...,
        decoder_head_mask: Optional[torch.Tensor] = ...,
        cross_attn_head_mask: Optional[torch.Tensor] = ...,
        encoder_outputs: Optional[tuple[torch.FloatTensor]] = ...,
        past_key_values: Optional[tuple[torch.FloatTensor]] = ...,
        use_cache: Optional[bool] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        inputs_embeds: Optional[torch.FloatTensor] = ...,
        decoder_inputs_embeds: Optional[torch.FloatTensor] = ...,
        return_dict: Optional[bool] = ...,
        cache_position: Optional[torch.Tensor] = ...,
    ) -> Union[tuple[torch.Tensor], Seq2SeqModelOutput]: ...
    def get_input_embeddings(self): ...
    def set_input_embeddings(self, value): ...
    def get_output_embeddings(self): ...
    def set_output_embeddings(self, value): ...

@auto_docstring(custom_intro=...)
class FSMTForConditionalGeneration(PretrainedFSMTModel, GenerationMixin):
    base_model_prefix = ...
    _tied_weights_keys = ...
    def __init__(self, config: FSMTConfig) -> None: ...
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        decoder_input_ids: Optional[torch.LongTensor] = ...,
        decoder_attention_mask: Optional[torch.BoolTensor] = ...,
        head_mask: Optional[torch.Tensor] = ...,
        decoder_head_mask: Optional[torch.Tensor] = ...,
        cross_attn_head_mask: Optional[torch.Tensor] = ...,
        encoder_outputs: Optional[tuple[torch.FloatTensor]] = ...,
        past_key_values: Optional[tuple[torch.FloatTensor]] = ...,
        inputs_embeds: Optional[torch.Tensor] = ...,
        decoder_inputs_embeds: Optional[torch.Tensor] = ...,
        labels: Optional[torch.LongTensor] = ...,
        use_cache: Optional[bool] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
        cache_position: Optional[torch.Tensor] = ...,
    ) -> Union[tuple[torch.Tensor], Seq2SeqLMOutput]: ...
    def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor): ...
    def get_encoder(self): ...
    def get_decoder(self): ...
    def get_output_embeddings(self): ...
    def set_output_embeddings(self, value): ...

class SinusoidalPositionalEmbedding(nn.Embedding):
    def __init__(self, num_positions, embedding_dim, padding_idx) -> None: ...
    def make_weight(self, num_positions, embedding_dim, padding_idx): ...
    @staticmethod
    def get_embedding(num_embeddings, embedding_dim, padding_idx): ...
    @staticmethod
    def make_positions(tensor, padding_idx: int): ...
    def forward(self, input, incremental_state: Optional[Any] = ..., timestep: Optional[Tensor] = ...): ...

__all__ = ["FSMTForConditionalGeneration", "FSMTModel", "PretrainedFSMTModel"]
