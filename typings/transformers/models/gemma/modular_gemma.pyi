"""
This type stub file was generated by pyright.
"""

import torch
from typing import Any, Optional, TYPE_CHECKING
from torch import nn
from ...cache_utils import Cache
from ...configuration_utils import PretrainedConfig
from ...modeling_outputs import BaseModelOutputWithPast
from ...processing_utils import Unpack
from ...tokenization_utils import PreTrainedTokenizer
from ...utils import TransformersKwargs
from ..llama.modeling_llama import (
    LlamaForCausalLM,
    LlamaForSequenceClassification,
    LlamaForTokenClassification,
    LlamaMLP,
    LlamaModel,
)
from ..llama.tokenization_llama import LlamaTokenizer
from ...tokenization_utils_base import TextInput

if TYPE_CHECKING: ...
VOCAB_FILES_NAMES = ...
SPIECE_UNDERLINE = ...
logger = ...

class GemmaConfig(PretrainedConfig):
    model_type = ...
    keys_to_ignore_at_inference = ...
    base_model_tp_plan = ...
    base_model_pp_plan = ...
    def __init__(
        self,
        vocab_size=...,
        hidden_size=...,
        intermediate_size=...,
        num_hidden_layers=...,
        num_attention_heads=...,
        num_key_value_heads=...,
        head_dim=...,
        hidden_act=...,
        hidden_activation=...,
        max_position_embeddings=...,
        initializer_range=...,
        rms_norm_eps=...,
        use_cache=...,
        pad_token_id=...,
        eos_token_id=...,
        bos_token_id=...,
        tie_word_embeddings=...,
        rope_theta=...,
        attention_bias=...,
        attention_dropout=...,
        **kwargs,
    ) -> None: ...

class GemmaTokenizer(LlamaTokenizer, PreTrainedTokenizer):
    def __init__(
        self,
        vocab_file,
        unk_token=...,
        bos_token=...,
        eos_token=...,
        pad_token=...,
        sp_model_kwargs: Optional[dict[str, Any]] = ...,
        add_bos_token=...,
        add_eos_token=...,
        clean_up_tokenization_spaces=...,
        use_default_system_prompt=...,
        spaces_between_special_tokens=...,
        **kwargs,
    ) -> None: ...
    def get_spm_processor(self): ...
    def unk_token_length(self): ...
    def tokenize(self, text: TextInput, **kwargs) -> list[str]: ...
    def convert_tokens_to_string(self, tokens): ...

class GemmaRMSNorm(nn.Module):
    def __init__(self, dim: int, eps: float = ...) -> None: ...
    def forward(self, x): ...
    def extra_repr(self): ...

class GemmaMLP(LlamaMLP):
    def __init__(self, config) -> None: ...

class GemmaModel(LlamaModel):
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        position_ids: Optional[torch.LongTensor] = ...,
        past_key_values: Optional[Cache] = ...,
        inputs_embeds: Optional[torch.FloatTensor] = ...,
        use_cache: Optional[bool] = ...,
        cache_position: Optional[torch.LongTensor] = ...,
        **kwargs: Unpack[TransformersKwargs],
    ) -> BaseModelOutputWithPast: ...

class GemmaForCausalLM(LlamaForCausalLM):
    def forward(**super_kwargs): ...

class GemmaForSequenceClassification(LlamaForSequenceClassification): ...
class GemmaForTokenClassification(LlamaForTokenClassification): ...

__all__ = [
    "GemmaConfig",
    "GemmaTokenizer",
    "GemmaModel",
    "GemmaForCausalLM",
    "GemmaForSequenceClassification",
    "GemmaForTokenClassification",
    "GemmaPreTrainedModel",
]
