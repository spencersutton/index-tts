"""
This type stub file was generated by pyright.
"""

import torch
from typing import Optional
from torch import nn
from ...cache_utils import Cache
from ...integrations.hub_kernels import use_kernel_forward_from_hub
from ...modeling_outputs import MoeModelOutputWithPast
from ...modeling_rope_utils import dynamic_rope_update
from ...processing_utils import Unpack
from ...utils import TransformersKwargs, auto_docstring
from ...utils.generic import check_model_inputs
from ..llama.modeling_llama import LlamaDecoderLayer, LlamaPreTrainedModel, LlamaRMSNorm, LlamaRotaryEmbedding
from ..mixtral.modeling_mixtral import MixtralForCausalLM, MixtralModel
from ..qwen2.modeling_qwen2 import Qwen2Attention
from .configuration_gpt_oss import GptOssConfig

logger = ...

class GptOssRMSNorm(LlamaRMSNorm):
    def forward(self, hidden_states): ...

class GptOssExperts(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states: torch.Tensor, router_indices=..., routing_weights=...) -> torch.Tensor:
        """
        When training is is more efficient to just loop over the experts and compute the output for each expert
        as otherwise the memory would explode.

        For inference we can sacrifice some memory and compute the output for all experts at once. By repeating the inputs.

        Args:
            hidden_states (torch.Tensor): (batch_size, seq_len, hidden_size)
            selected_experts (torch.Tensor): (batch_size * token_num, top_k)
            routing_weights (torch.Tensor): (batch_size * token_num, num_experts)
        Returns:
            torch.Tensor
        """
        ...

class GptOssTopKRouter(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states):  # -> tuple[Tensor, Tensor]:
        ...

@use_kernel_forward_from_hub("MegaBlocksMoeMLP")
class GptOssMLP(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states):  # -> tuple[Any, Any]:
        ...

class GptOssRotaryEmbedding(LlamaRotaryEmbedding):
    @torch.no_grad()
    @dynamic_rope_update
    def forward(self, x, position_ids):  # -> tuple[Any, Any]:
        ...

def apply_rotary_pos_emb(q, k, cos, sin, position_ids=..., unsqueeze_dim=...):  # -> tuple[Tensor, Tensor]:
    ...
def eager_attention_forward(
    module: nn.Module,
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: Optional[torch.Tensor],
    scaling: float,
    dropout: float = ...,
    **kwargs,
):  # -> tuple[Tensor, Tensor]:
    ...

class GptOssAttention(Qwen2Attention):
    def __init__(self, config: GptOssConfig, layer_idx: int) -> None: ...
    def forward(
        self,
        hidden_states: torch.Tensor,
        position_embeddings: tuple[torch.Tensor, torch.Tensor],
        attention_mask: Optional[torch.Tensor],
        past_key_value: Optional[Cache] = ...,
        cache_position: Optional[torch.LongTensor] = ...,
        **kwargs: Unpack[TransformersKwargs],
    ) -> tuple[torch.Tensor, torch.Tensor]: ...

class GptOssDecoderLayer(LlamaDecoderLayer):
    def __init__(self, config: GptOssConfig, layer_idx: int) -> None: ...
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = ...,
        position_ids: Optional[torch.LongTensor] = ...,
        past_key_value: Optional[Cache] = ...,
        use_cache: Optional[bool] = ...,
        cache_position: Optional[torch.LongTensor] = ...,
        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = ...,
        **kwargs: Unpack[TransformersKwargs],
    ) -> tuple[torch.Tensor]: ...

class GptOssPreTrainedModel(LlamaPreTrainedModel):
    _keep_in_fp32_modules = ...
    _supports_sdpa = ...
    _supports_flash_attention = ...
    _supports_flex_attention = ...
    _can_record_outputs = ...

class GptOssModel(MixtralModel):
    _no_split_modules = ...
    @check_model_inputs
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        position_ids: Optional[torch.LongTensor] = ...,
        past_key_values: Optional[list[torch.FloatTensor]] = ...,
        inputs_embeds: Optional[torch.FloatTensor] = ...,
        use_cache: Optional[bool] = ...,
        cache_position: Optional[torch.LongTensor] = ...,
        **kwargs: Unpack[TransformersKwargs],
    ) -> MoeModelOutputWithPast: ...

class GptOssForCausalLM(MixtralForCausalLM): ...

__all__ = ["GptOssForCausalLM", "GptOssModel", "GptOssPreTrainedModel"]
