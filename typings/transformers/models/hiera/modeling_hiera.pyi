"""
This type stub file was generated by pyright.
"""

import torch
from dataclasses import dataclass
from typing import Optional, Union
from torch import nn
from ...modeling_layers import GradientCheckpointingLayer
from ...modeling_outputs import (
    BackboneOutput,
    BaseModelOutput,
    BaseModelOutputWithPooling,
    ImageClassifierOutput,
    ModelOutput,
)
from ...modeling_utils import PreTrainedModel
from ...utils import auto_docstring
from ...utils.backbone_utils import BackboneMixin
from .configuration_hiera import HieraConfig

logger = ...

@dataclass
@auto_docstring(custom_intro=...)
class HieraEncoderOutput(ModelOutput):
    last_hidden_state: Optional[torch.FloatTensor] = ...
    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = ...
    attentions: Optional[tuple[torch.FloatTensor, ...]] = ...
    reshaped_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = ...

@dataclass
@auto_docstring(custom_intro=...)
class HieraModelOutput(ModelOutput):
    last_hidden_state: Optional[torch.FloatTensor] = ...
    pooler_output: Optional[torch.FloatTensor] = ...
    bool_masked_pos: torch.BoolTensor = ...
    ids_restore: Optional[torch.LongTensor] = ...
    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = ...
    attentions: Optional[tuple[torch.FloatTensor, ...]] = ...
    reshaped_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = ...

@dataclass
@auto_docstring(
    custom_intro="""
    Hiera image classification outputs.
    """
)
class HieraForImageClassificationOutput(ImageClassifierOutput):
    loss: Optional[torch.FloatTensor] = ...
    logits: Optional[torch.FloatTensor] = ...
    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = ...
    attentions: Optional[tuple[torch.FloatTensor, ...]] = ...
    reshaped_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = ...

@dataclass
@auto_docstring(custom_intro=...)
class HieraForPreTrainingOutput(ModelOutput):
    loss: Optional[torch.FloatTensor] = ...
    logits: Optional[torch.FloatTensor] = ...
    bool_masked_pos: torch.BoolTensor = ...
    ids_restore: Optional[torch.LongTensor] = ...
    hidden_states: Optional[tuple[torch.FloatTensor]] = ...
    attentions: Optional[tuple[torch.FloatTensor]] = ...
    reshaped_hidden_states: Optional[tuple[torch.FloatTensor]] = ...

class HieraPatchEmbeddings(nn.Module):
    def __init__(self, config, is_mae: bool = ...) -> None: ...
    def masked_conv(
        self, pixel_values: torch.FloatTensor, bool_masked_pos: Optional[torch.BoolTensor] = ...
    ) -> torch.Tensor: ...
    def random_masking(
        self, pixel_values: torch.FloatTensor, noise: Optional[torch.FloatTensor] = ...
    ) -> tuple[torch.BoolTensor, torch.LongTensor]: ...
    def forward(
        self, pixel_values: torch.FloatTensor, noise: Optional[torch.FloatTensor] = ...
    ) -> tuple[torch.Tensor, Optional[torch.BoolTensor], Optional[torch.LongTensor]]: ...

class HieraEmbeddings(nn.Module):
    def __init__(self, config: HieraConfig, is_mae: bool = ...) -> None: ...
    def interpolate_pos_encoding(
        self, embeddings: torch.Tensor, pos_embeds: torch.Tensor, height: int, width: int
    ) -> torch.Tensor: ...
    def get_position_embedding(
        self, embeddings: torch.Tensor, height: int, width: int, interpolate_pos_encoding: bool
    ) -> torch.FloatTensor: ...
    def forward(
        self,
        pixel_values: torch.FloatTensor,
        noise: Optional[torch.FloatTensor] = ...,
        interpolate_pos_encoding: bool = ...,
    ) -> tuple[torch.Tensor, Optional[torch.BoolTensor], Optional[torch.LongTensor]]: ...

class HieraMaskUnitAttention(nn.Module):
    def __init__(
        self,
        hidden_size: int,
        hidden_size_output: int,
        num_heads: int,
        query_stride: int = ...,
        window_size: int = ...,
        use_mask_unit_attn: bool = ...,
    ) -> None: ...
    def forward(
        self, hidden_states: torch.Tensor, head_mask: Optional[torch.FloatTensor] = ..., output_attentions: bool = ...
    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]: ...

def drop_path(input: torch.Tensor, drop_prob: float = ..., training: bool = ...) -> torch.Tensor: ...

class HieraDropPath(nn.Module):
    def __init__(self, drop_prob: Optional[float] = ...) -> None: ...
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor: ...
    def extra_repr(self) -> str: ...

class HieraMlp(nn.Module):
    def __init__(self, config, dim: int) -> None: ...
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor: ...

class HieraLayer(nn.Module):
    def __init__(
        self,
        config,
        hidden_size: int,
        hidden_size_output: int,
        num_heads: int,
        drop_path: float = ...,
        query_stride: int = ...,
        window_size: int = ...,
        use_mask_unit_attn: bool = ...,
    ) -> None: ...
    def forward(
        self, hidden_states: torch.Tensor, head_mask: Optional[torch.FloatTensor] = ..., output_attentions: bool = ...
    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]: ...

class HieraStage(GradientCheckpointingLayer):
    def __init__(
        self,
        config,
        depth: int,
        hidden_size: int,
        hidden_size_output: int,
        num_heads: int,
        drop_path: list[float],
        query_stride: list[int],
        window_size: int,
        use_mask_unit_attn: bool,
        stage_num: Optional[int] = ...,
    ) -> None: ...
    def forward(
        self, hidden_states: torch.Tensor, head_mask: Optional[torch.FloatTensor], output_attentions: bool = ...
    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]: ...

def undo_windowing(hidden_states: torch.Tensor, shape: list[int], mask_unit_shape: list[int]) -> torch.Tensor: ...

class HieraEncoder(nn.Module):
    def __init__(self, config: HieraConfig) -> None: ...
    def reroll(
        self, hidden_states: torch.Tensor, stage_idx: int, bool_masked_pos: Optional[torch.BoolTensor] = ...
    ) -> torch.Tensor: ...
    def forward(
        self,
        hidden_states: torch.Tensor,
        bool_masked_pos: Optional[torch.BoolTensor] = ...,
        head_mask: Optional[torch.FloatTensor] = ...,
        output_attentions: bool = ...,
        output_hidden_states: bool = ...,
        return_dict: bool = ...,
    ) -> Union[tuple, BaseModelOutput]: ...

def unroll(
    hidden_states: torch.Tensor, image_shape: tuple[int, int], patch_stride: tuple[int, int], schedule: list[list[int]]
) -> torch.Tensor: ...

@auto_docstring
class HieraPreTrainedModel(PreTrainedModel):
    config: HieraConfig
    base_model_prefix = ...
    main_input_name = ...
    supports_gradient_checkpointing = ...

class HieraPooler(nn.Module):
    def __init__(self, config: HieraConfig) -> None: ...
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor: ...

@auto_docstring
class HieraModel(HieraPreTrainedModel):
    def __init__(self, config: HieraConfig, add_pooling_layer: bool = ..., is_mae: bool = ...) -> None: ...
    def get_input_embeddings(self) -> HieraPatchEmbeddings: ...
    @auto_docstring
    def forward(
        self,
        pixel_values: Optional[torch.Tensor] = ...,
        noise: Optional[torch.FloatTensor] = ...,
        head_mask: Optional[torch.Tensor] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        interpolate_pos_encoding: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[tuple, BaseModelOutputWithPooling]: ...

class HieraDecoder(nn.Module):
    def __init__(self, config: HieraConfig) -> None: ...
    def forward(
        self,
        encoder_hidden_states: torch.Tensor,
        bool_masked_pos: torch.BoolTensor,
        head_mask: Optional[torch.Tensor] = ...,
        output_attentions: bool = ...,
    ) -> tuple[torch.Tensor, torch.BoolTensor]: ...

class HieraMultiScaleHead(nn.Module):
    def __init__(self, config: HieraConfig) -> None: ...
    def apply_fusion_head(self, head: nn.Module, hidden_states: torch.Tensor) -> torch.Tensor: ...
    def forward(self, feature_maps: list[torch.Tensor]) -> torch.Tensor: ...

@auto_docstring(custom_intro=...)
class HieraForPreTraining(HieraPreTrainedModel):
    def __init__(self, config: HieraConfig) -> None: ...
    def get_pixel_label_2d(self, pixel_values: torch.Tensor, bool_masked_pos: torch.BoolTensor) -> torch.Tensor: ...
    def forward_loss(self, pixel_values: torch.Tensor, logits: torch.Tensor, bool_masked_pos: torch.BoolTensor): ...
    @auto_docstring
    def forward(
        self,
        pixel_values: Optional[torch.Tensor] = ...,
        noise: Optional[torch.FloatTensor] = ...,
        head_mask: Optional[torch.Tensor] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        interpolate_pos_encoding: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[tuple, HieraForPreTrainingOutput]: ...

@auto_docstring(custom_intro=...)
class HieraForImageClassification(HieraPreTrainedModel):
    def __init__(self, config: HieraConfig) -> None: ...
    @auto_docstring
    def forward(
        self,
        pixel_values,
        head_mask: Optional[torch.Tensor] = ...,
        labels: Optional[torch.Tensor] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        interpolate_pos_encoding: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[tuple, HieraForImageClassificationOutput]: ...

@auto_docstring(custom_intro=...)
class HieraBackbone(HieraPreTrainedModel, BackboneMixin):
    def __init__(self, config: HieraConfig) -> None: ...
    def get_input_embeddings(self): ...
    def forward(
        self,
        pixel_values: torch.Tensor,
        output_hidden_states: Optional[bool] = ...,
        output_attentions: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> BackboneOutput: ...

__all__ = ["HieraForImageClassification", "HieraForPreTraining", "HieraBackbone", "HieraModel", "HieraPreTrainedModel"]
