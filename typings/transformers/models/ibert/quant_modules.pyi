"""
This type stub file was generated by pyright.
"""

from torch import nn
from torch.autograd import Function

logger = ...

class QuantEmbedding(nn.Module):
    def __init__(
        self,
        num_embeddings,
        embedding_dim,
        padding_idx=...,
        max_norm=...,
        norm_type=...,
        scale_grad_by_freq=...,
        sparse=...,
        _weight=...,
        weight_bit=...,
        momentum=...,
        quant_mode=...,
    ) -> None: ...
    def forward(self, x, positions=..., incremental_state=...): ...

class QuantAct(nn.Module):
    def __init__(
        self, activation_bit, act_range_momentum=..., per_channel=..., channel_len=..., quant_mode=...
    ) -> None: ...
    def __repr__(self): ...
    def forward(
        self,
        x,
        pre_act_scaling_factor=...,
        identity=...,
        identity_scaling_factor=...,
        specified_min=...,
        specified_max=...,
    ): ...

class QuantLinear(nn.Module):
    def __init__(
        self, in_features, out_features, bias=..., weight_bit=..., bias_bit=..., per_channel=..., quant_mode=...
    ) -> None: ...
    def __repr__(self): ...
    def forward(self, x, prev_act_scaling_factor=...): ...

class IntGELU(nn.Module):
    def __init__(self, quant_mode=..., force_dequant=...) -> None: ...
    def int_erf(self, x_int, scaling_factor): ...
    def forward(self, x, scaling_factor=...): ...

class IntSoftmax(nn.Module):
    def __init__(self, output_bit, quant_mode=..., force_dequant=...) -> None: ...
    def int_polynomial(self, x_int, scaling_factor): ...
    def int_exp(self, x_int, scaling_factor): ...
    def forward(self, x, scaling_factor): ...

class IntLayerNorm(nn.Module):
    def __init__(self, normalized_shape, eps, output_bit=..., quant_mode=..., force_dequant=...) -> None: ...
    def set_shift(self, y_int): ...
    def overflow_fallback(self, y_int): ...
    def forward(self, x, scaling_factor=...): ...

def get_percentile_min_max(input, lower_percentile, upper_percentile, output_tensor=...): ...
def linear_quantize(input, scale, zero_point, inplace=...): ...
def symmetric_linear_quantization_params(num_bits, saturation_min, saturation_max, per_channel=...): ...

class SymmetricQuantFunction(Function):
    @staticmethod
    def forward(ctx, x, k, percentile_mode, scale): ...
    @staticmethod
    def backward(ctx, grad_output): ...

class floor_ste(Function):
    @staticmethod
    def forward(ctx, x): ...
    @staticmethod
    def backward(ctx, grad_output): ...

class round_ste(Function):
    @staticmethod
    def forward(ctx, x): ...
    @staticmethod
    def backward(ctx, grad_output): ...

def batch_frexp(inputs, max_bit=...): ...

class FixedPointMul(Function):
    @staticmethod
    def forward(
        ctx, pre_act, pre_act_scaling_factor, bit_num, z_scaling_factor, identity=..., identity_scaling_factor=...
    ): ...
    @staticmethod
    def backward(ctx, grad_output): ...
