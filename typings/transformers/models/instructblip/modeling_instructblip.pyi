"""
This type stub file was generated by pyright.
"""

import torch
from dataclasses import dataclass
from typing import Any, Optional, Union
from torch import nn
from ...generation import GenerationMixin
from ...modeling_flash_attention_utils import FlashAttentionKwargs
from ...modeling_layers import GradientCheckpointingLayer
from ...modeling_outputs import (
    BaseModelOutput,
    BaseModelOutputWithPooling,
    BaseModelOutputWithPoolingAndCrossAttentions,
)
from ...modeling_utils import PreTrainedModel
from ...processing_utils import Unpack
from ...utils import ModelOutput, TransformersKwargs, auto_docstring, can_return_tuple
from .configuration_instructblip import InstructBlipConfig, InstructBlipQFormerConfig, InstructBlipVisionConfig

logger = ...

@dataclass
@auto_docstring(custom_intro=...)
class InstructBlipForConditionalGenerationModelOutput(ModelOutput):
    loss: Optional[tuple[torch.FloatTensor]] = ...
    logits: Optional[tuple[torch.FloatTensor]] = ...
    vision_outputs: Optional[torch.FloatTensor] = ...
    qformer_outputs: Optional[tuple[torch.FloatTensor]] = ...
    language_model_outputs: Optional[tuple[torch.FloatTensor]] = ...
    def to_tuple(self) -> tuple[Any]: ...

class InstructBlipVisionEmbeddings(nn.Module):
    def __init__(self, config: InstructBlipVisionConfig) -> None: ...
    def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor: ...
    def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding: bool = ...) -> torch.Tensor: ...

def eager_attention_forward(
    module: nn.Module,
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: Optional[torch.Tensor],
    scaling: float,
    dropout: float = ...,
    **kwargs,
): ...

class InstructBlipAttention(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(
        self,
        hidden_states: torch.Tensor,
        head_mask: Optional[torch.Tensor] = ...,
        output_attentions: Optional[bool] = ...,
        **kwargs,
    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]: ...

class InstructBlipMLP(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor: ...

class InstructBlipEncoderLayer(GradientCheckpointingLayer):
    def __init__(self, config: InstructBlipConfig) -> None: ...
    def forward(
        self, hidden_states: torch.Tensor, attention_mask: torch.Tensor, output_attentions: Optional[bool] = ...
    ) -> tuple[torch.FloatTensor]: ...

@auto_docstring
class InstructBlipPreTrainedModel(PreTrainedModel):
    config: InstructBlipConfig
    base_model_prefix = ...
    supports_gradient_checkpointing = ...
    _supports_attention_backend = ...
    _supports_flash_attn = ...
    _supports_sdpa = ...
    _supports_flex_attn = ...
    _can_compile_fullgraph = ...
    _no_split_modules = ...

class InstructBlipEncoder(nn.Module):
    def __init__(self, config: InstructBlipConfig) -> None: ...
    def forward(
        self,
        inputs_embeds,
        attention_mask: Optional[torch.Tensor] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[tuple, BaseModelOutput]: ...

class InstructBlipVisionModel(InstructBlipPreTrainedModel):
    main_input_name = ...
    config: InstructBlipVisionConfig
    def __init__(self, config: InstructBlipVisionConfig) -> None: ...
    @auto_docstring
    def forward(
        self,
        pixel_values: Optional[torch.FloatTensor] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
        interpolate_pos_encoding: bool = ...,
    ) -> Union[tuple, BaseModelOutputWithPooling]: ...
    def get_input_embeddings(self): ...

class InstructBlipQFormerMultiHeadAttention(nn.Module):
    def __init__(self, config, is_cross_attention=...) -> None: ...
    def save_attn_gradients(self, attn_gradients): ...
    def get_attn_gradients(self): ...
    def save_attention_map(self, attention_map): ...
    def get_attention_map(self): ...
    def transpose_for_scores(self, x): ...
    def forward(
        self,
        hidden_states,
        attention_mask=...,
        head_mask=...,
        encoder_hidden_states=...,
        encoder_attention_mask=...,
        output_attentions=...,
    ): ...

class InstructBlipQFormerSelfOutput(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor: ...

class InstructBlipQFormerAttention(nn.Module):
    def __init__(self, config, is_cross_attention=...) -> None: ...
    def prune_heads(self, heads): ...
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.FloatTensor] = ...,
        head_mask: Optional[torch.FloatTensor] = ...,
        encoder_hidden_states: Optional[torch.FloatTensor] = ...,
        encoder_attention_mask: Optional[torch.FloatTensor] = ...,
        output_attentions: Optional[bool] = ...,
    ) -> tuple[torch.Tensor]: ...

class InstructBlipQFormerIntermediate(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor: ...

class InstructBlipQFormerOutput(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor: ...

class InstructBlipQFormerLayer(GradientCheckpointingLayer):
    def __init__(self, config, layer_idx) -> None: ...
    def forward(
        self,
        hidden_states,
        attention_mask=...,
        head_mask=...,
        encoder_hidden_states=...,
        encoder_attention_mask=...,
        output_attentions=...,
        query_length=...,
    ): ...
    def feed_forward_chunk(self, attention_output): ...
    def feed_forward_chunk_query(self, attention_output): ...

class InstructBlipQFormerEncoder(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(
        self,
        hidden_states,
        attention_mask=...,
        head_mask=...,
        encoder_hidden_states=...,
        encoder_attention_mask=...,
        output_attentions=...,
        output_hidden_states=...,
        return_dict=...,
        query_length=...,
    ): ...

class InstructBlipQFormerEmbeddings(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, input_ids=..., position_ids=..., query_embeds=..., past_key_values_length=...): ...

class InstructBlipQFormerModel(InstructBlipPreTrainedModel):
    _supports_attention_backend = ...
    _supports_flash_attn = ...
    _supports_sdpa = ...
    _supports_flex_attn = ...
    def __init__(self, config: InstructBlipQFormerConfig) -> None: ...
    def get_input_embeddings(self): ...
    def set_input_embeddings(self, value): ...
    def get_extended_attention_mask(
        self, attention_mask: torch.Tensor, input_shape: tuple[int], device: torch.device, has_query: bool = ...
    ) -> torch.Tensor: ...
    def forward(
        self,
        input_ids: torch.LongTensor,
        attention_mask: Optional[torch.FloatTensor] = ...,
        position_ids: Optional[torch.LongTensor] = ...,
        query_embeds: Optional[torch.Tensor] = ...,
        head_mask: Optional[torch.FloatTensor] = ...,
        encoder_hidden_states: Optional[torch.FloatTensor] = ...,
        encoder_attention_mask: Optional[torch.FloatTensor] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[tuple[torch.FloatTensor], BaseModelOutputWithPoolingAndCrossAttentions]: ...

@auto_docstring(custom_intro=...)
class InstructBlipModel(InstructBlipPreTrainedModel):
    main_input_name = ...
    _keep_in_fp32_modules = ...
    def __init__(self, config: InstructBlipConfig) -> None: ...
    def get_input_embeddings(self): ...
    def set_input_embeddings(self, value): ...
    def get_placeholder_mask(self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor): ...
    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        pixel_values: torch.FloatTensor,
        qformer_input_ids: torch.FloatTensor,
        qformer_attention_mask: Optional[torch.LongTensor] = ...,
        input_ids: Optional[torch.FloatTensor] = ...,
        attention_mask: Optional[torch.LongTensor] = ...,
        decoder_input_ids: Optional[torch.LongTensor] = ...,
        decoder_attention_mask: Optional[torch.LongTensor] = ...,
        inputs_embeds: Optional[torch.Tensor] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
        interpolate_pos_encoding: bool = ...,
        use_cache: Optional[bool] = ...,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> Union[tuple, InstructBlipForConditionalGenerationModelOutput]: ...

@auto_docstring(custom_intro=...)
class InstructBlipForConditionalGeneration(InstructBlipPreTrainedModel, GenerationMixin):
    config: InstructBlipConfig
    main_input_name = ...
    _can_compile_fullgraph = ...
    _keep_in_fp32_modules = ...
    def __init__(self, config: InstructBlipConfig) -> None: ...
    def get_input_embeddings(self): ...
    def set_input_embeddings(self, value): ...
    def set_output_embeddings(self, new_embeddings): ...
    def get_output_embeddings(self) -> nn.Module: ...
    def get_encoder(self): ...
    def get_decoder(self): ...
    def get_image_features(
        self,
        pixel_values: torch.FloatTensor,
        qformer_input_ids: torch.LongTensor,
        qformer_attention_mask: Optional[torch.LongTensor] = ...,
        interpolate_pos_encoding: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ): ...
    def get_placeholder_mask(self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor): ...
    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        pixel_values: torch.FloatTensor,
        qformer_input_ids: torch.FloatTensor,
        qformer_attention_mask: Optional[torch.LongTensor] = ...,
        input_ids: Optional[torch.FloatTensor] = ...,
        attention_mask: Optional[torch.LongTensor] = ...,
        decoder_input_ids: Optional[torch.LongTensor] = ...,
        decoder_attention_mask: Optional[torch.LongTensor] = ...,
        inputs_embeds: Optional[torch.FloatTensor] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        labels: Optional[torch.LongTensor] = ...,
        return_dict: Optional[bool] = ...,
        interpolate_pos_encoding: bool = ...,
        use_cache: Optional[bool] = ...,
        **kwargs: Unpack[TransformersKwargs],
    ) -> Union[tuple, InstructBlipForConditionalGenerationModelOutput]: ...
    @torch.no_grad()
    def generate(
        self,
        pixel_values: torch.FloatTensor,
        qformer_input_ids: Optional[torch.LongTensor] = ...,
        qformer_attention_mask: Optional[torch.LongTensor] = ...,
        input_ids: Optional[torch.LongTensor] = ...,
        attention_mask: Optional[torch.LongTensor] = ...,
        inputs_embeds: Optional[torch.FloatTensor] = ...,
        interpolate_pos_encoding: bool = ...,
        **generate_kwargs,
    ) -> torch.LongTensor: ...

__all__ = [
    "InstructBlipQFormerModel",
    "InstructBlipPreTrainedModel",
    "InstructBlipModel",
    "InstructBlipForConditionalGeneration",
    "InstructBlipVisionModel",
]
