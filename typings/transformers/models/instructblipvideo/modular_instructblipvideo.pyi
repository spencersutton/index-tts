"""
This type stub file was generated by pyright.
"""

import torch
from typing import Optional, Union
from transformers.models.instructblip.configuration_instructblip import (
    InstructBlipQFormerConfig,
    InstructBlipVisionConfig,
)
from transformers.models.instructblip.modeling_instructblip import (
    InstructBlipForConditionalGeneration,
    InstructBlipForConditionalGenerationModelOutput,
    InstructBlipModel,
    InstructBlipPreTrainedModel,
    InstructBlipQFormerModel,
    InstructBlipVisionModel,
    TransformersKwargs,
)
from ...configuration_utils import PretrainedConfig
from ...modeling_flash_attention_utils import FlashAttentionKwargs
from ...processing_utils import Unpack

logger = ...

class InstructBlipVideoVisionConfig(InstructBlipVisionConfig): ...
class InstructBlipVideoQFormerConfig(InstructBlipQFormerConfig): ...

class InstructBlipVideoConfig(PretrainedConfig):
    model_type = ...
    attribute_map = ...
    sub_configs = ...
    def __init__(
        self,
        vision_config=...,
        qformer_config=...,
        text_config=...,
        num_query_tokens=...,
        video_token_index=...,
        **kwargs,
    ) -> None: ...
    @classmethod
    def from_vision_qformer_text_configs(
        cls,
        vision_config: InstructBlipVideoVisionConfig,
        qformer_config: InstructBlipVideoQFormerConfig,
        text_config: PretrainedConfig,
        **kwargs,
    ): ...

class InstructBlipVideoPreTrainedModel(InstructBlipPreTrainedModel): ...
class InstructBlipVideoVisionModel(InstructBlipVisionModel): ...
class InstructBlipVideoQFormerModel(InstructBlipQFormerModel): ...
class InstructBlipVideoForConditionalGenerationModelOutput(InstructBlipForConditionalGenerationModelOutput): ...

class InstructBlipVideoModel(InstructBlipModel):
    def forward(
        self,
        pixel_values: torch.FloatTensor,
        qformer_input_ids: torch.FloatTensor,
        qformer_attention_mask: Optional[torch.LongTensor] = ...,
        input_ids: Optional[torch.FloatTensor] = ...,
        attention_mask: Optional[torch.LongTensor] = ...,
        decoder_input_ids: Optional[torch.LongTensor] = ...,
        decoder_attention_mask: Optional[torch.LongTensor] = ...,
        inputs_embeds: Optional[torch.Tensor] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
        interpolate_pos_encoding: bool = ...,
        use_cache: Optional[bool] = ...,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> Union[tuple, InstructBlipVideoForConditionalGenerationModelOutput]: ...

class InstructBlipVideoForConditionalGeneration(InstructBlipForConditionalGeneration):
    def get_video_features(
        self,
        pixel_values: torch.FloatTensor,
        qformer_input_ids: torch.LongTensor,
        qformer_attention_mask: Optional[torch.LongTensor] = ...,
        interpolate_pos_encoding: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ): ...
    def get_image_features(
        self,
        pixel_values: torch.FloatTensor,
        qformer_input_ids: torch.LongTensor,
        qformer_attention_mask: Optional[torch.LongTensor] = ...,
        interpolate_pos_encoding: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ): ...
    def get_placeholder_mask(self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor): ...
    def forward(
        self,
        pixel_values: torch.FloatTensor,
        qformer_input_ids: torch.FloatTensor,
        qformer_attention_mask: Optional[torch.LongTensor] = ...,
        input_ids: Optional[torch.FloatTensor] = ...,
        attention_mask: Optional[torch.LongTensor] = ...,
        decoder_input_ids: Optional[torch.LongTensor] = ...,
        decoder_attention_mask: Optional[torch.LongTensor] = ...,
        inputs_embeds: Optional[torch.FloatTensor] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        labels: Optional[torch.LongTensor] = ...,
        return_dict: Optional[bool] = ...,
        interpolate_pos_encoding: bool = ...,
        use_cache: Optional[bool] = ...,
        **kwargs: Unpack[TransformersKwargs],
    ) -> Union[tuple, InstructBlipVideoForConditionalGenerationModelOutput]: ...
    @torch.no_grad()
    def generate(
        self,
        pixel_values: torch.FloatTensor,
        qformer_input_ids: Optional[torch.LongTensor] = ...,
        qformer_attention_mask: Optional[torch.LongTensor] = ...,
        input_ids: Optional[torch.LongTensor] = ...,
        attention_mask: Optional[torch.LongTensor] = ...,
        inputs_embeds: Optional[torch.FloatTensor] = ...,
        interpolate_pos_encoding: bool = ...,
        **generate_kwargs,
    ) -> torch.LongTensor: ...

__all__ = [
    "InstructBlipVideoConfig",
    "InstructBlipVideoQFormerConfig",
    "InstructBlipVideoVisionConfig",
    "InstructBlipVideoVisionModel",
    "InstructBlipVideoPreTrainedModel",
    "InstructBlipVideoQFormerModel",
    "InstructBlipVideoModel",
    "InstructBlipVideoForConditionalGeneration",
]
