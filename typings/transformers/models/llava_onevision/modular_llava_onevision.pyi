"""
This type stub file was generated by pyright.
"""

import torch
from typing import Optional, Union
from transformers.models.llava_next.image_processing_llava_next_fast import LlavaNextImageProcessorFast
from transformers.models.llava_next_video.modeling_llava_next_video import (
    LlavaNextVideoCausalLMOutputWithPast,
    LlavaNextVideoForConditionalGeneration,
    LlavaNextVideoModel,
    LlavaNextVideoModelOutputWithPast,
    LlavaNextVideoPreTrainedModel,
    TransformersKwargs,
)
from ...cache_utils import Cache
from ...image_processing_utils import BatchFeature
from ...image_processing_utils_fast import DefaultFastImageProcessorKwargs
from ...image_utils import ImageInput
from ...modeling_flash_attention_utils import FlashAttentionKwargs
from ...processing_utils import Unpack
from ...utils import auto_docstring, can_return_tuple, is_torchvision_available

if is_torchvision_available(): ...
logger = ...

class LlavaOnevisionFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):
    image_grid_pinpoints: Optional[list[list[int]]]
    do_pad: Optional[bool]
    ...

class LlavaOnevisionImageProcessorFast(LlavaNextImageProcessorFast):
    resample = ...
    image_mean = ...
    image_std = ...
    size = ...
    crop_size = ...
    default_to_square = ...
    do_resize = ...
    do_center_crop = ...
    do_rescale = ...
    do_normalize = ...
    do_convert_rgb = ...
    do_pad = ...
    image_grid_pinpoints = ...
    model_input_names = ...
    def pad_to_square(
        self, images: torch.Tensor, background_color: Union[int, tuple[int, int, int]] = ...
    ) -> torch.Tensor: ...
    @auto_docstring
    def preprocess(
        self, images: ImageInput, **kwargs: Unpack[LlavaOnevisionFastImageProcessorKwargs]
    ) -> BatchFeature: ...

class LlavaOnevisionModelOutputWithPast(LlavaNextVideoModelOutputWithPast): ...
class LlavaOnevisionCausalLMOutputWithPast(LlavaNextVideoCausalLMOutputWithPast): ...
class LlavaOnevisionPreTrainedModel(LlavaNextVideoPreTrainedModel): ...

class LlavaOnevisionModel(LlavaNextVideoModel):
    def __init__(self, config) -> None: ...
    def pack_image_features(self, image_features, image_sizes, image_newline=..., vision_aspect_ratio=...): ...
    def apply_pooling(self, image_features): ...
    def get_image_features(
        self,
        pixel_values: torch.FloatTensor,
        image_sizes: torch.Tensor,
        vision_feature_layer: Optional[Union[int, list[int]]] = ...,
        vision_feature_select_strategy: Optional[str] = ...,
        vision_aspect_ratio: Optional[str] = ...,
        batch_num_images: Optional[torch.LongTensor] = ...,
    ): ...
    def get_video_features(
        self,
        pixel_values: torch.FloatTensor,
        vision_feature_layer: Union[int, list[int]],
        vision_feature_select_strategy: str,
    ): ...
    def forward(
        self,
        input_ids: torch.LongTensor = ...,
        pixel_values: torch.FloatTensor = ...,
        image_sizes: Optional[torch.LongTensor] = ...,
        pixel_values_videos: torch.FloatTensor = ...,
        image_sizes_videos: Optional[torch.LongTensor] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        position_ids: Optional[torch.LongTensor] = ...,
        past_key_values: Optional[Cache] = ...,
        inputs_embeds: Optional[torch.FloatTensor] = ...,
        vision_feature_layer: Optional[Union[int, list[int]]] = ...,
        vision_feature_select_strategy: Optional[str] = ...,
        vision_aspect_ratio: Optional[str] = ...,
        batch_num_images: Optional[torch.LongTensor] = ...,
        use_cache: Optional[bool] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
        cache_position: Optional[torch.LongTensor] = ...,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> Union[tuple, LlavaOnevisionModelOutputWithPast]: ...

class LlavaOnevisionForConditionalGeneration(LlavaNextVideoForConditionalGeneration):
    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: torch.LongTensor = ...,
        pixel_values: torch.FloatTensor = ...,
        image_sizes: Optional[torch.LongTensor] = ...,
        pixel_values_videos: torch.FloatTensor = ...,
        image_sizes_videos: Optional[torch.LongTensor] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        position_ids: Optional[torch.LongTensor] = ...,
        past_key_values: Optional[Cache] = ...,
        inputs_embeds: Optional[torch.FloatTensor] = ...,
        vision_feature_layer: Optional[Union[int, list[int]]] = ...,
        vision_feature_select_strategy: Optional[str] = ...,
        vision_aspect_ratio: Optional[str] = ...,
        batch_num_images: Optional[torch.LongTensor] = ...,
        labels: Optional[torch.LongTensor] = ...,
        use_cache: Optional[bool] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
        cache_position: Optional[torch.LongTensor] = ...,
        logits_to_keep: Union[int, torch.Tensor] = ...,
        **kwargs: Unpack[TransformersKwargs],
    ) -> Union[tuple, LlavaOnevisionCausalLMOutputWithPast]: ...
    def prepare_inputs_for_generation(
        self,
        input_ids,
        past_key_values=...,
        inputs_embeds=...,
        pixel_values=...,
        image_sizes=...,
        pixel_values_videos=...,
        image_sizes_videos=...,
        attention_mask=...,
        cache_position=...,
        logits_to_keep=...,
        **kwargs,
    ): ...

__all__ = [
    "LlavaOnevisionImageProcessorFast",
    "LlavaOnevisionModel",
    "LlavaOnevisionForConditionalGeneration",
    "LlavaOnevisionPreTrainedModel",
]
