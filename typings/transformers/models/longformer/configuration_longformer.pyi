"""
This type stub file was generated by pyright.
"""

from collections.abc import Mapping
from typing import Any, Optional, TYPE_CHECKING, Union
from ...configuration_utils import PretrainedConfig
from ...onnx import OnnxConfig
from ...utils import TensorType
from ...onnx.config import PatchingSpec
from ...tokenization_utils_base import PreTrainedTokenizerBase

if TYPE_CHECKING: ...
logger = ...

class LongformerConfig(PretrainedConfig):
    model_type = ...
    def __init__(
        self,
        attention_window: Union[list[int], int] = ...,
        sep_token_id: int = ...,
        pad_token_id: int = ...,
        bos_token_id: int = ...,
        eos_token_id: int = ...,
        vocab_size: int = ...,
        hidden_size: int = ...,
        num_hidden_layers: int = ...,
        num_attention_heads: int = ...,
        intermediate_size: int = ...,
        hidden_act: str = ...,
        hidden_dropout_prob: float = ...,
        attention_probs_dropout_prob: float = ...,
        max_position_embeddings: int = ...,
        type_vocab_size: int = ...,
        initializer_range: float = ...,
        layer_norm_eps: float = ...,
        onnx_export: bool = ...,
        **kwargs,
    ) -> None: ...

class LongformerOnnxConfig(OnnxConfig):
    def __init__(
        self, config: PretrainedConfig, task: str = ..., patching_specs: Optional[list[PatchingSpec]] = ...
    ) -> None: ...
    @property
    def inputs(self) -> Mapping[str, Mapping[int, str]]: ...
    @property
    def outputs(self) -> Mapping[str, Mapping[int, str]]: ...
    @property
    def atol_for_validation(self) -> float: ...
    @property
    def default_onnx_opset(self) -> int: ...
    def generate_dummy_inputs(
        self,
        tokenizer: PreTrainedTokenizerBase,
        batch_size: int = ...,
        seq_length: int = ...,
        is_pair: bool = ...,
        framework: Optional[TensorType] = ...,
    ) -> Mapping[str, Any]: ...

__all__ = ["LongformerConfig", "LongformerOnnxConfig"]
