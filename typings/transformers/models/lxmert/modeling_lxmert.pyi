"""
This type stub file was generated by pyright.
"""

import torch
from dataclasses import dataclass
from typing import Optional, Union
from torch import nn
from ...modeling_utils import PreTrainedModel
from ...utils import ModelOutput, auto_docstring
from .configuration_lxmert import LxmertConfig

logger = ...

class GeLU(nn.Module):
    def __init__(self) -> None: ...
    def forward(self, x): ...

@dataclass
@auto_docstring(custom_intro=...)
class LxmertModelOutput(ModelOutput):
    language_output: Optional[torch.FloatTensor] = ...
    vision_output: Optional[torch.FloatTensor] = ...
    pooled_output: Optional[torch.FloatTensor] = ...
    language_hidden_states: Optional[tuple[torch.FloatTensor]] = ...
    vision_hidden_states: Optional[tuple[torch.FloatTensor]] = ...
    language_attentions: Optional[tuple[torch.FloatTensor]] = ...
    vision_attentions: Optional[tuple[torch.FloatTensor]] = ...
    cross_encoder_attentions: Optional[tuple[torch.FloatTensor]] = ...

@dataclass
@auto_docstring(custom_intro=...)
class LxmertForQuestionAnsweringOutput(ModelOutput):
    loss: Optional[torch.FloatTensor] = ...
    question_answering_score: Optional[torch.FloatTensor] = ...
    language_hidden_states: Optional[tuple[torch.FloatTensor]] = ...
    vision_hidden_states: Optional[tuple[torch.FloatTensor]] = ...
    language_attentions: Optional[tuple[torch.FloatTensor]] = ...
    vision_attentions: Optional[tuple[torch.FloatTensor]] = ...
    cross_encoder_attentions: Optional[tuple[torch.FloatTensor]] = ...

@dataclass
@auto_docstring(
    custom_intro="""
    Output type of [`LxmertForPreTraining`].
    """
)
class LxmertForPreTrainingOutput(ModelOutput):
    loss: Optional[torch.FloatTensor] = ...
    prediction_logits: Optional[torch.FloatTensor] = ...
    cross_relationship_score: Optional[torch.FloatTensor] = ...
    question_answering_score: Optional[torch.FloatTensor] = ...
    language_hidden_states: Optional[tuple[torch.FloatTensor]] = ...
    vision_hidden_states: Optional[tuple[torch.FloatTensor]] = ...
    language_attentions: Optional[tuple[torch.FloatTensor]] = ...
    vision_attentions: Optional[tuple[torch.FloatTensor]] = ...
    cross_encoder_attentions: Optional[tuple[torch.FloatTensor]] = ...

def load_tf_weights_in_lxmert(model, config, tf_checkpoint_path): ...

class LxmertEmbeddings(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, input_ids, token_type_ids=..., inputs_embeds=...): ...

class LxmertAttention(nn.Module):
    def __init__(self, config, ctx_dim=...) -> None: ...
    def forward(self, hidden_states, context, attention_mask=..., output_attentions=...): ...

class LxmertAttentionOutput(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states, input_tensor): ...

class LxmertCrossAttentionLayer(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, input_tensor, ctx_tensor, ctx_att_mask=..., output_attentions=...): ...

class LxmertSelfAttentionLayer(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, input_tensor, attention_mask, output_attentions=...): ...

class LxmertIntermediate(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states): ...

class LxmertOutput(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states, input_tensor): ...

class LxmertLayer(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states, attention_mask=..., output_attentions=...): ...

class LxmertXLayer(nn.Module):
    def __init__(self, config) -> None: ...
    def cross_att(
        self, lang_input, lang_attention_mask, visual_input, visual_attention_mask, output_x_attentions=...
    ): ...
    def self_att(self, lang_input, lang_attention_mask, visual_input, visual_attention_mask): ...
    def output_fc(self, lang_input, visual_input): ...
    def forward(self, lang_feats, lang_attention_mask, visual_feats, visual_attention_mask, output_attentions=...): ...

class LxmertVisualFeatureEncoder(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, visual_feats, visual_pos): ...

class LxmertEncoder(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(
        self,
        lang_feats,
        lang_attention_mask,
        visual_feats,
        visual_pos,
        visual_attention_mask=...,
        output_attentions=...,
    ): ...

class LxmertPooler(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states): ...

class LxmertPredictionHeadTransform(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states): ...

class LxmertLMPredictionHead(nn.Module):
    def __init__(self, config, lxmert_model_embedding_weights) -> None: ...
    def forward(self, hidden_states): ...

class LxmertVisualAnswerHead(nn.Module):
    def __init__(self, config, num_labels) -> None: ...
    def forward(self, hidden_states): ...

class LxmertVisualObjHead(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states): ...

class LxmertPreTrainingHeads(nn.Module):
    def __init__(self, config, lxmert_model_embedding_weights) -> None: ...
    def forward(self, sequence_output, pooled_output): ...

@auto_docstring
class LxmertPreTrainedModel(PreTrainedModel):
    config: LxmertConfig
    load_tf_weights = ...
    base_model_prefix = ...
    _supports_param_buffer_assignment = ...

@auto_docstring
class LxmertModel(LxmertPreTrainedModel):
    def __init__(self, config) -> None: ...
    def get_input_embeddings(self): ...
    def set_input_embeddings(self, new_embeddings): ...
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = ...,
        visual_feats: Optional[torch.FloatTensor] = ...,
        visual_pos: Optional[torch.FloatTensor] = ...,
        attention_mask: Optional[torch.FloatTensor] = ...,
        visual_attention_mask: Optional[torch.FloatTensor] = ...,
        token_type_ids: Optional[torch.LongTensor] = ...,
        inputs_embeds: Optional[torch.FloatTensor] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[LxmertModelOutput, tuple[torch.FloatTensor]]: ...

@auto_docstring
class LxmertForPreTraining(LxmertPreTrainedModel):
    _tied_weights_keys = ...
    def __init__(self, config) -> None: ...
    def resize_token_embeddings(
        self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = ..., mean_resizing: bool = ...
    ) -> nn.Embedding: ...
    def resize_num_qa_labels(self, num_labels): ...
    def get_qa_logit_layer(self) -> nn.Module: ...
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = ...,
        visual_feats: Optional[torch.FloatTensor] = ...,
        visual_pos: Optional[torch.FloatTensor] = ...,
        attention_mask: Optional[torch.FloatTensor] = ...,
        visual_attention_mask: Optional[torch.FloatTensor] = ...,
        token_type_ids: Optional[torch.LongTensor] = ...,
        inputs_embeds: Optional[torch.FloatTensor] = ...,
        labels: Optional[torch.LongTensor] = ...,
        obj_labels: Optional[dict[str, tuple[torch.FloatTensor, torch.FloatTensor]]] = ...,
        matched_label: Optional[torch.LongTensor] = ...,
        ans: Optional[torch.Tensor] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
        **kwargs,
    ) -> Union[LxmertForPreTrainingOutput, tuple[torch.FloatTensor]]: ...

@auto_docstring(custom_intro=...)
class LxmertForQuestionAnswering(LxmertPreTrainedModel):
    def __init__(self, config) -> None: ...
    def resize_num_qa_labels(self, num_labels): ...
    def get_qa_logit_layer(self) -> nn.Module: ...
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = ...,
        visual_feats: Optional[torch.FloatTensor] = ...,
        visual_pos: Optional[torch.FloatTensor] = ...,
        attention_mask: Optional[torch.FloatTensor] = ...,
        visual_attention_mask: Optional[torch.FloatTensor] = ...,
        token_type_ids: Optional[torch.LongTensor] = ...,
        inputs_embeds: Optional[torch.FloatTensor] = ...,
        labels: Optional[torch.Tensor] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[LxmertForQuestionAnsweringOutput, tuple[torch.FloatTensor]]: ...

__all__ = [
    "LxmertEncoder",
    "LxmertForPreTraining",
    "LxmertForQuestionAnswering",
    "LxmertModel",
    "LxmertPreTrainedModel",
    "LxmertVisualFeatureEncoder",
    "LxmertXLayer",
]
