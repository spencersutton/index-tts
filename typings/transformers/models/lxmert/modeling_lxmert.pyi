from dataclasses import dataclass

import torch
from torch import nn

from ...modeling_utils import PreTrainedModel
from ...utils import ModelOutput
from .configuration_lxmert import LxmertConfig

"""PyTorch LXMERT model."""
logger = ...

class GeLU(nn.Module):
    def __init__(self) -> None: ...
    def forward(self, x): ...

@dataclass
class LxmertModelOutput(ModelOutput):
    language_output: torch.FloatTensor | None = ...
    vision_output: torch.FloatTensor | None = ...
    pooled_output: torch.FloatTensor | None = ...
    language_hidden_states: tuple[torch.FloatTensor] | None = ...
    vision_hidden_states: tuple[torch.FloatTensor] | None = ...
    language_attentions: tuple[torch.FloatTensor] | None = ...
    vision_attentions: tuple[torch.FloatTensor] | None = ...
    cross_encoder_attentions: tuple[torch.FloatTensor] | None = ...

@dataclass
class LxmertForQuestionAnsweringOutput(ModelOutput):
    loss: torch.FloatTensor | None = ...
    question_answering_score: torch.FloatTensor | None = ...
    language_hidden_states: tuple[torch.FloatTensor] | None = ...
    vision_hidden_states: tuple[torch.FloatTensor] | None = ...
    language_attentions: tuple[torch.FloatTensor] | None = ...
    vision_attentions: tuple[torch.FloatTensor] | None = ...
    cross_encoder_attentions: tuple[torch.FloatTensor] | None = ...

@dataclass
class LxmertForPreTrainingOutput(ModelOutput):
    loss: torch.FloatTensor | None = ...
    prediction_logits: torch.FloatTensor | None = ...
    cross_relationship_score: torch.FloatTensor | None = ...
    question_answering_score: torch.FloatTensor | None = ...
    language_hidden_states: tuple[torch.FloatTensor] | None = ...
    vision_hidden_states: tuple[torch.FloatTensor] | None = ...
    language_attentions: tuple[torch.FloatTensor] | None = ...
    vision_attentions: tuple[torch.FloatTensor] | None = ...
    cross_encoder_attentions: tuple[torch.FloatTensor] | None = ...

def load_tf_weights_in_lxmert(model, config, tf_checkpoint_path): ...

class LxmertEmbeddings(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, input_ids, token_type_ids=..., inputs_embeds=...):  # -> Any:
        ...

class LxmertAttention(nn.Module):
    def __init__(self, config, ctx_dim=...) -> None: ...
    def forward(
        self, hidden_states, context, attention_mask=..., output_attentions=...
    ):  # -> tuple[Tensor, Any] | tuple[Tensor]:
        ...

class LxmertAttentionOutput(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states, input_tensor):  # -> Any:
        ...

class LxmertCrossAttentionLayer(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(
        self, input_tensor, ctx_tensor, ctx_att_mask=..., output_attentions=...
    ):  # -> tuple[Any, Any] | tuple[Any]:
        ...

class LxmertSelfAttentionLayer(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, input_tensor, attention_mask, output_attentions=...):  # -> tuple[Any, Any] | tuple[Any]:
        ...

class LxmertIntermediate(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states): ...

class LxmertOutput(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states, input_tensor):  # -> Any:
        ...

class LxmertLayer(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states, attention_mask=..., output_attentions=...):  # -> Any:
        ...

class LxmertXLayer(nn.Module):
    def __init__(self, config) -> None: ...
    def cross_att(
        self, lang_input, lang_attention_mask, visual_input, visual_attention_mask, output_x_attentions=...
    ):  # -> tuple[Any, Any]:
        ...
    def self_att(self, lang_input, lang_attention_mask, visual_input, visual_attention_mask):  # -> tuple[Any, Any]:
        ...
    def output_fc(self, lang_input, visual_input):  # -> tuple[Any, Any]:
        ...
    def forward(
        self, lang_feats, lang_attention_mask, visual_feats, visual_attention_mask, output_attentions=...
    ):  # -> tuple[Any, Any, Any] | tuple[Any, Any]:
        ...

class LxmertVisualFeatureEncoder(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, visual_feats, visual_pos):  # -> Any:
        ...

class LxmertEncoder(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(
        self,
        lang_feats,
        lang_attention_mask,
        visual_feats,
        visual_pos,
        visual_attention_mask=...,
        output_attentions=...,
    ):  # -> tuple[tuple[tuple[()] | tuple[Any, ...], tuple[()] | tuple[Any] | None], tuple[tuple[()] | tuple[Any, ...], tuple[()] | tuple[Any] | None], tuple[()] | tuple[Any] | None]:
        ...

class LxmertPooler(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states):  # -> Any:
        ...

class LxmertPredictionHeadTransform(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states):  # -> Any:
        ...

class LxmertLMPredictionHead(nn.Module):
    def __init__(self, config, lxmert_model_embedding_weights) -> None: ...
    def forward(self, hidden_states):  # -> Any:
        ...

class LxmertVisualAnswerHead(nn.Module):
    def __init__(self, config, num_labels) -> None: ...
    def forward(self, hidden_states):  # -> Any:
        ...

class LxmertVisualObjHead(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states):  # -> dict[Any, Any]:
        ...

class LxmertPreTrainingHeads(nn.Module):
    def __init__(self, config, lxmert_model_embedding_weights) -> None: ...
    def forward(self, sequence_output, pooled_output):  # -> tuple[Any, Any]:
        ...

class LxmertPreTrainedModel(PreTrainedModel):
    config: LxmertConfig
    load_tf_weights = ...
    base_model_prefix = ...
    _supports_param_buffer_assignment = ...

class LxmertModel(LxmertPreTrainedModel):
    def __init__(self, config) -> None: ...
    def get_input_embeddings(self):  # -> Embedding:
        ...
    def set_input_embeddings(self, new_embeddings):  # -> None:
        ...
    def forward(
        self,
        input_ids: torch.LongTensor | None = ...,
        visual_feats: torch.FloatTensor | None = ...,
        visual_pos: torch.FloatTensor | None = ...,
        attention_mask: torch.FloatTensor | None = ...,
        visual_attention_mask: torch.FloatTensor | None = ...,
        token_type_ids: torch.LongTensor | None = ...,
        inputs_embeds: torch.FloatTensor | None = ...,
        output_attentions: bool | None = ...,
        output_hidden_states: bool | None = ...,
        return_dict: bool | None = ...,
    ) -> LxmertModelOutput | tuple[torch.FloatTensor]: ...

class LxmertForPreTraining(LxmertPreTrainedModel):
    _tied_weights_keys = ...
    def __init__(self, config) -> None: ...
    def resize_token_embeddings(
        self, new_num_tokens: int, pad_to_multiple_of: int | None = ..., mean_resizing: bool = ...
    ) -> nn.Embedding: ...
    def resize_num_qa_labels(self, num_labels):  # -> Module | None:

        ...
    def get_qa_logit_layer(self) -> nn.Module: ...
    def forward(
        self,
        input_ids: torch.LongTensor | None = ...,
        visual_feats: torch.FloatTensor | None = ...,
        visual_pos: torch.FloatTensor | None = ...,
        attention_mask: torch.FloatTensor | None = ...,
        visual_attention_mask: torch.FloatTensor | None = ...,
        token_type_ids: torch.LongTensor | None = ...,
        inputs_embeds: torch.FloatTensor | None = ...,
        labels: torch.LongTensor | None = ...,
        obj_labels: dict[str, tuple[torch.FloatTensor, torch.FloatTensor]] | None = ...,
        matched_label: torch.LongTensor | None = ...,
        ans: torch.Tensor | None = ...,
        output_attentions: bool | None = ...,
        output_hidden_states: bool | None = ...,
        return_dict: bool | None = ...,
        **kwargs,
    ) -> LxmertForPreTrainingOutput | tuple[torch.FloatTensor]: ...

class LxmertForQuestionAnswering(LxmertPreTrainedModel):
    def __init__(self, config) -> None: ...
    def resize_num_qa_labels(self, num_labels):  # -> Module | None:

        ...
    def get_qa_logit_layer(self) -> nn.Module: ...
    def forward(
        self,
        input_ids: torch.LongTensor | None = ...,
        visual_feats: torch.FloatTensor | None = ...,
        visual_pos: torch.FloatTensor | None = ...,
        attention_mask: torch.FloatTensor | None = ...,
        visual_attention_mask: torch.FloatTensor | None = ...,
        token_type_ids: torch.LongTensor | None = ...,
        inputs_embeds: torch.FloatTensor | None = ...,
        labels: torch.Tensor | None = ...,
        output_attentions: bool | None = ...,
        output_hidden_states: bool | None = ...,
        return_dict: bool | None = ...,
    ) -> LxmertForQuestionAnsweringOutput | tuple[torch.FloatTensor]: ...

__all__ = [
    "LxmertEncoder",
    "LxmertForPreTraining",
    "LxmertForQuestionAnswering",
    "LxmertModel",
    "LxmertPreTrainedModel",
    "LxmertVisualFeatureEncoder",
    "LxmertXLayer",
]
