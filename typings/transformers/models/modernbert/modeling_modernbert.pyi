"""
This type stub file was generated by pyright.
"""

import torch
from typing import Optional, Union
from torch import nn
from ...modeling_layers import GradientCheckpointingLayer
from ...modeling_outputs import (
    BaseModelOutput,
    MaskedLMOutput,
    MultipleChoiceModelOutput,
    QuestionAnsweringModelOutput,
    SequenceClassifierOutput,
    TokenClassifierOutput,
)
from ...modeling_rope_utils import dynamic_rope_update
from ...modeling_utils import PreTrainedModel
from ...utils import auto_docstring, is_flash_attn_2_available
from .configuration_modernbert import ModernBertConfig
from flash_attn.layers.rotary import RotaryEmbedding

if is_flash_attn_2_available(): ...
else:
    RotaryEmbedding = ...
logger = ...

class ApplyRotaryEmbUnpad(torch.autograd.Function):
    @staticmethod
    def forward(ctx, qkv, cos, sin, cu_seqlens: Optional[torch.Tensor] = ..., max_seqlen: Optional[int] = ...): ...
    @staticmethod
    def backward(ctx, do): ...

def apply_rotary_unpadded(qkv, cos, sin, cu_seqlens: Optional[torch.Tensor] = ..., max_seqlen: Optional[int] = ...): ...

class ModernBertUnpaddedRotaryEmbedding(RotaryEmbedding):
    def __init__(
        self,
        dim: int,
        base: float = ...,
        max_seqlen: Optional[int] = ...,
        device: Optional[torch.device] = ...,
        dtype: Optional[torch.dtype] = ...,
    ) -> None: ...
    def forward(
        self, qkv: torch.Tensor, cu_seqlens: torch.Tensor, max_seqlen: Optional[int] = ...
    ) -> Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]]: ...
    def extra_repr(self) -> str: ...

class ModernBertEmbeddings(nn.Module):
    def __init__(self, config: ModernBertConfig) -> None: ...
    @torch.compile(dynamic=True)
    def compiled_embeddings(self, input_ids: torch.LongTensor) -> torch.Tensor: ...
    def forward(
        self, input_ids: Optional[torch.LongTensor] = ..., inputs_embeds: Optional[torch.Tensor] = ...
    ) -> torch.Tensor: ...

class ModernBertMLP(nn.Module):
    def __init__(self, config: ModernBertConfig) -> None: ...
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor: ...

class ModernBertRotaryEmbedding(nn.Module):
    def __init__(self, config: ModernBertConfig, device=...) -> None: ...
    @torch.no_grad()
    @dynamic_rope_update
    def forward(self, x, position_ids): ...

def rotate_half(x): ...
def apply_rotary_pos_emb(q, k, cos, sin, position_ids=..., unsqueeze_dim=...): ...
def eager_attention_forward(
    module: ModernBertAttention,
    qkv: torch.Tensor,
    attention_mask: torch.Tensor,
    sliding_window_mask: torch.Tensor,
    position_ids: Optional[torch.LongTensor],
    local_attention: tuple[int, int],
    bs: int,
    dim: int,
    output_attentions: Optional[bool] = ...,
    **_kwargs,
) -> Union[tuple[torch.Tensor, torch.Tensor], tuple[torch.Tensor]]: ...
def flash_attention_forward(
    module: ModernBertAttention,
    qkv: torch.Tensor,
    rotary_emb: ModernBertUnpaddedRotaryEmbedding,
    cu_seqlens: torch.Tensor,
    max_seqlen: int,
    local_attention: tuple[int, int],
    bs: int,
    dim: int,
    target_dtype: torch.dtype = ...,
    **_kwargs,
) -> tuple[torch.Tensor]: ...
def sdpa_attention_forward(
    module: ModernBertAttention,
    qkv: torch.Tensor,
    attention_mask: torch.Tensor,
    sliding_window_mask: torch.Tensor,
    position_ids: Optional[torch.LongTensor],
    local_attention: tuple[int, int],
    bs: int,
    dim: int,
    **_kwargs,
) -> tuple[torch.Tensor]: ...

MODERNBERT_ATTENTION_FUNCTION = ...

class ModernBertAttention(nn.Module):
    def __init__(self, config: ModernBertConfig, layer_id: Optional[int] = ...) -> None: ...
    def forward(
        self, hidden_states: torch.Tensor, output_attentions: Optional[bool] = ..., **kwargs
    ) -> torch.Tensor: ...

class ModernBertEncoderLayer(GradientCheckpointingLayer):
    def __init__(self, config: ModernBertConfig, layer_id: Optional[int] = ...) -> None: ...
    @torch.compile(dynamic=True)
    def compiled_mlp(self, hidden_states: torch.Tensor) -> torch.Tensor: ...
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = ...,
        sliding_window_mask: Optional[torch.Tensor] = ...,
        position_ids: Optional[torch.LongTensor] = ...,
        cu_seqlens: Optional[torch.Tensor] = ...,
        max_seqlen: Optional[int] = ...,
        output_attentions: Optional[bool] = ...,
    ) -> torch.Tensor: ...

@auto_docstring
class ModernBertPreTrainedModel(PreTrainedModel):
    config: ModernBertConfig
    base_model_prefix = ...
    supports_gradient_checkpointing = ...
    _no_split_modules = ...
    _supports_flash_attn = ...
    _supports_sdpa = ...
    _supports_flex_attn = ...
    def resize_token_embeddings(self, *args, **kwargs): ...

@auto_docstring
class ModernBertModel(ModernBertPreTrainedModel):
    def __init__(self, config: ModernBertConfig) -> None: ...
    def get_input_embeddings(self): ...
    def set_input_embeddings(self, value): ...
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        sliding_window_mask: Optional[torch.Tensor] = ...,
        position_ids: Optional[torch.LongTensor] = ...,
        inputs_embeds: Optional[torch.Tensor] = ...,
        indices: Optional[torch.Tensor] = ...,
        cu_seqlens: Optional[torch.Tensor] = ...,
        max_seqlen: Optional[int] = ...,
        batch_size: Optional[int] = ...,
        seq_len: Optional[int] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[tuple[torch.Tensor, ...], BaseModelOutput]: ...

class ModernBertPredictionHead(nn.Module):
    def __init__(self, config: ModernBertConfig) -> None: ...
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor: ...

@auto_docstring(custom_intro=...)
class ModernBertForMaskedLM(ModernBertPreTrainedModel):
    _tied_weights_keys = ...
    def __init__(self, config: ModernBertConfig) -> None: ...
    def get_output_embeddings(self): ...
    def set_output_embeddings(self, new_embeddings: nn.Linear): ...
    @torch.compile(dynamic=True)
    def compiled_head(self, output: torch.Tensor) -> torch.Tensor: ...
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        sliding_window_mask: Optional[torch.Tensor] = ...,
        position_ids: Optional[torch.Tensor] = ...,
        inputs_embeds: Optional[torch.Tensor] = ...,
        labels: Optional[torch.Tensor] = ...,
        indices: Optional[torch.Tensor] = ...,
        cu_seqlens: Optional[torch.Tensor] = ...,
        max_seqlen: Optional[int] = ...,
        batch_size: Optional[int] = ...,
        seq_len: Optional[int] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
        **kwargs,
    ) -> Union[tuple[torch.Tensor], MaskedLMOutput]: ...

@auto_docstring(custom_intro=...)
class ModernBertForSequenceClassification(ModernBertPreTrainedModel):
    def __init__(self, config: ModernBertConfig) -> None: ...
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        sliding_window_mask: Optional[torch.Tensor] = ...,
        position_ids: Optional[torch.Tensor] = ...,
        inputs_embeds: Optional[torch.Tensor] = ...,
        labels: Optional[torch.Tensor] = ...,
        indices: Optional[torch.Tensor] = ...,
        cu_seqlens: Optional[torch.Tensor] = ...,
        max_seqlen: Optional[int] = ...,
        batch_size: Optional[int] = ...,
        seq_len: Optional[int] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
        **kwargs,
    ) -> Union[tuple[torch.Tensor], SequenceClassifierOutput]: ...

@auto_docstring(custom_intro=...)
class ModernBertForTokenClassification(ModernBertPreTrainedModel):
    def __init__(self, config: ModernBertConfig) -> None: ...
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        sliding_window_mask: Optional[torch.Tensor] = ...,
        position_ids: Optional[torch.Tensor] = ...,
        inputs_embeds: Optional[torch.Tensor] = ...,
        labels: Optional[torch.Tensor] = ...,
        indices: Optional[torch.Tensor] = ...,
        cu_seqlens: Optional[torch.Tensor] = ...,
        max_seqlen: Optional[int] = ...,
        batch_size: Optional[int] = ...,
        seq_len: Optional[int] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[tuple[torch.Tensor], TokenClassifierOutput]: ...

@auto_docstring
class ModernBertForQuestionAnswering(ModernBertPreTrainedModel):
    def __init__(self, config: ModernBertConfig) -> None: ...
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.Tensor],
        attention_mask: Optional[torch.Tensor] = ...,
        sliding_window_mask: Optional[torch.Tensor] = ...,
        position_ids: Optional[torch.Tensor] = ...,
        start_positions: Optional[torch.Tensor] = ...,
        end_positions: Optional[torch.Tensor] = ...,
        indices: Optional[torch.Tensor] = ...,
        cu_seqlens: Optional[torch.Tensor] = ...,
        max_seqlen: Optional[int] = ...,
        batch_size: Optional[int] = ...,
        seq_len: Optional[int] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
        **kwargs,
    ) -> Union[tuple[torch.Tensor], QuestionAnsweringModelOutput]: ...

@auto_docstring(custom_intro=...)
class ModernBertForMultipleChoice(ModernBertPreTrainedModel):
    def __init__(self, config: ModernBertConfig) -> None: ...
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        sliding_window_mask: Optional[torch.Tensor] = ...,
        position_ids: Optional[torch.Tensor] = ...,
        inputs_embeds: Optional[torch.Tensor] = ...,
        labels: Optional[torch.Tensor] = ...,
        indices: Optional[torch.Tensor] = ...,
        cu_seqlens: Optional[torch.Tensor] = ...,
        max_seqlen: Optional[int] = ...,
        batch_size: Optional[int] = ...,
        seq_len: Optional[int] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
        **kwargs,
    ) -> Union[tuple[torch.Tensor], MultipleChoiceModelOutput]: ...

__all__ = [
    "ModernBertModel",
    "ModernBertPreTrainedModel",
    "ModernBertForMaskedLM",
    "ModernBertForSequenceClassification",
    "ModernBertForTokenClassification",
    "ModernBertForQuestionAnswering",
    "ModernBertForMultipleChoice",
]
