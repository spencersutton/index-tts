"""
This type stub file was generated by pyright.
"""

from typing import Optional, Union
from transformers.tokenization_utils_base import INIT_TOKENIZER_DOCSTRING
from transformers.tokenization_utils_fast import PreTrainedTokenizerFast
from transformers.utils import add_end_docstrings
from ...utils import is_levenshtein_available, is_nltk_available

if is_levenshtein_available(): ...
if is_nltk_available(): ...
logger = ...
VOCAB_FILES_NAMES = ...

def markdown_compatible(text: str) -> str: ...
def normalize_list_like_lines(generation): ...
def find_next_punctuation(text: str, start_idx=...): ...
def truncate_repetitions(text: str, min_len: int = ...) -> str: ...
def remove_numbers(lines): ...
def get_slices(lines, clean_lines): ...
def remove_slice_from_lines(lines, clean_text, slice) -> str: ...

@add_end_docstrings(INIT_TOKENIZER_DOCSTRING)
class NougatTokenizerFast(PreTrainedTokenizerFast):
    vocab_files_names = ...
    model_input_names = ...
    slow_tokenizer_class = ...
    def __init__(
        self,
        vocab_file=...,
        tokenizer_file=...,
        clean_up_tokenization_spaces=...,
        unk_token=...,
        bos_token=...,
        eos_token=...,
        pad_token=...,
        **kwargs,
    ) -> None: ...
    def remove_hallucinated_references(self, text: str) -> str: ...
    def correct_tables(self, generation: str) -> str: ...
    def post_process_single(self, generation: str, fix_markdown: bool = ...) -> str: ...
    def post_process_generation(
        self, generation: Union[str, list[str]], fix_markdown: bool = ..., num_workers: Optional[int] = ...
    ) -> Union[str, list[str]]: ...

__all__ = ["NougatTokenizerFast"]
