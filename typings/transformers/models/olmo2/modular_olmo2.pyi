"""
This type stub file was generated by pyright.
"""

import torch
from typing import Optional
from transformers.utils.generic import TransformersKwargs
from ...cache_utils import Cache
from ...processing_utils import Unpack
from ..llama.modeling_llama import LlamaPreTrainedModel, LlamaRMSNorm
from ..olmo.configuration_olmo import OlmoConfig
from ..olmo.modeling_olmo import OlmoAttention, OlmoDecoderLayer, OlmoForCausalLM, OlmoModel, OlmoRotaryEmbedding

logger = ...

class Olmo2Config(OlmoConfig):
    model_type = ...
    base_model_tp_plan = ...
    base_model_pp_plan = ...
    def __init__(
        self,
        vocab_size=...,
        hidden_size=...,
        intermediate_size=...,
        num_hidden_layers=...,
        num_attention_heads=...,
        num_key_value_heads=...,
        hidden_act=...,
        max_position_embeddings=...,
        initializer_range=...,
        use_cache=...,
        pad_token_id=...,
        bos_token_id=...,
        eos_token_id=...,
        tie_word_embeddings=...,
        rope_theta=...,
        rope_scaling=...,
        attention_bias=...,
        attention_dropout=...,
        rms_norm_eps=...,
        **kwargs,
    ) -> None: ...

class Olmo2RMSNorm(LlamaRMSNorm):
    def forward(self, hidden_states): ...

def rotate_half(x): ...

class Olmo2Attention(OlmoAttention):
    def __init__(self, config: Olmo2Config, layer_idx: Optional[int] = ...) -> None: ...
    def forward(
        self,
        hidden_states: torch.Tensor,
        position_embeddings: tuple[torch.Tensor, torch.Tensor],
        attention_mask: Optional[torch.Tensor],
        past_key_value: Optional[Cache] = ...,
        cache_position: Optional[torch.LongTensor] = ...,
        **kwargs: Unpack[TransformersKwargs],
    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]: ...

class Olmo2DecoderLayer(OlmoDecoderLayer):
    def __init__(self, config: Olmo2Config, layer_idx: int) -> None: ...
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = ...,
        position_ids: Optional[torch.LongTensor] = ...,
        past_key_value: Optional[Cache] = ...,
        use_cache: Optional[bool] = ...,
        cache_position: Optional[torch.LongTensor] = ...,
        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = ...,
        **kwargs: Unpack[TransformersKwargs],
    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]: ...

class Olmo2RotaryEmbedding(OlmoRotaryEmbedding): ...
class Olmo2PreTrainedModel(LlamaPreTrainedModel): ...

class Olmo2Model(OlmoModel):
    def __init__(self, config: Olmo2Config) -> None: ...

class Olmo2ForCausalLM(OlmoForCausalLM): ...

__all__ = ["Olmo2Config", "Olmo2ForCausalLM", "Olmo2Model", "Olmo2PreTrainedModel"]
