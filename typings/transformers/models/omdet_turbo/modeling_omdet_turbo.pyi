"""
This type stub file was generated by pyright.
"""

import torch
from dataclasses import dataclass
from functools import lru_cache
from typing import Optional, Union
from torch import Tensor, nn
from ...file_utils import ModelOutput
from ...integrations import use_kernel_forward_from_hub
from ...modeling_layers import GradientCheckpointingLayer
from ...modeling_utils import PreTrainedModel
from ...utils import auto_docstring
from .configuration_omdet_turbo import OmDetTurboConfig

"""PyTorch OmDet-Turbo model."""
logger = ...

@dataclass
@auto_docstring(
    custom_intro="""
    Base class for outputs of the OmDetTurboHybridEncoder.
    """
)
class OmDetTurboEncoderOutput(ModelOutput):
    r"""
    last_hidden_state (`torch.FloatTensor`):
        Last hidden states of the encoder.
    extracted_states (`tuple[torch.FloatTensor]`):
        The extracted states from the Feature Pyramid Network (FPN) and Path Aggregation Network (PAN) of the encoder.
    """

    last_hidden_state: torch.FloatTensor | None = ...
    hidden_states: tuple[torch.FloatTensor] | None = ...
    attentions: tuple[torch.FloatTensor] | None = ...
    extracted_states: tuple[torch.FloatTensor] = ...

@dataclass
@auto_docstring(
    custom_intro="""
    Base class for outputs of the OmDetTurboDecoder.
    """
)
class OmDetTurboDecoderOutput(ModelOutput):
    r"""
    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):
        Sequence of hidden-states at the output of the last layer of the decoder.
    decoder_coords (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):
        The predicted coordinates of the objects.
    decoder_classes (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes)`):
        The predicted classes of the objects.
    encoder_coord_logits (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):
        The predicted coordinates of the objects from the encoder.
    encoder_class_logits (`tuple[torch.FloatTensor]` of shape `(batch_size, num_queries, num_classes)`):
        The predicted class of the objects from the encoder.
    init_reference_points (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):
        The initial reference points.
    intermediate_reference_points (`tuple[tuple[torch.FloatTensor]]`):
        The intermediate reference points.
    """

    last_hidden_state: torch.FloatTensor | None = ...
    hidden_states: tuple[torch.FloatTensor] | None = ...
    attentions: tuple[tuple[torch.FloatTensor]] | None = ...
    decoder_coords: torch.FloatTensor | None = ...
    decoder_classes: torch.FloatTensor | None = ...
    encoder_coord_logits: torch.FloatTensor | None = ...
    encoder_class_logits: tuple[torch.FloatTensor] = ...
    init_reference_points: torch.FloatTensor | None = ...
    intermediate_reference_points: tuple[tuple[torch.FloatTensor]] = ...

@dataclass
@auto_docstring(
    custom_intro="""
    Output type of [`OmDetTurboObjectDetectionOutput`].
    """
)
class OmDetTurboObjectDetectionOutput(ModelOutput):
    r"""
    loss (`torch.FloatTensor`):
        The loss value.
    decoder_coord_logits (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):
        The predicted coordinates logits of the objects.
    decoder_class_logits (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes)`):
        The predicted class of the objects.
    init_reference_points (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):
        The initial reference points.
    intermediate_reference_points (`tuple[tuple[torch.FloatTensor]]`):
        The intermediate reference points.
    encoder_coord_logits (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):
        The predicted coordinates of the objects from the encoder.
    encoder_class_logits (`tuple[torch.FloatTensor]`):
        The predicted class of the objects from the encoder.
    encoder_extracted_states (`torch.FloatTensor`):
        The extracted states from the Feature Pyramid Network (FPN) and Path Aggregation Network (PAN) of the encoder.
    decoder_hidden_states (`tuple[torch.FloatTensor]`, *optional*):
        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of shape
        `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer
        plus the initial embedding outputs.
    decoder_attentions (`tuple[tuple[torch.FloatTensor]]`, *optional*):
        Tuple of tuples of `torch.FloatTensor` (one for attention for each layer) of shape `(batch_size, num_heads,
        sequence_length, sequence_length)`. Attentions weights after the attention softmax, used to compute the
        weighted average in the self-attention, cross-attention and multi-scale deformable attention heads.
    encoder_hidden_states (`tuple[torch.FloatTensor]`, *optional*):
        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of shape
        `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer
        plus the initial embedding outputs.
    encoder_attentions (`tuple[tuple[torch.FloatTensor]]`, *optional*):
        Tuple of tuples of `torch.FloatTensor` (one for attention for each layer) of shape `(batch_size, num_heads,
        sequence_length, sequence_length)`. Attentions weights after the attention softmax, used to compute the
        weighted average in the self-attention, cross-attention and multi-scale deformable attention heads.
    classes_structure (`torch.LongTensor`, *optional*):
        The number of queried classes for each image.
    """

    loss: torch.FloatTensor | None = ...
    decoder_coord_logits: torch.FloatTensor | None = ...
    decoder_class_logits: torch.FloatTensor | None = ...
    init_reference_points: torch.FloatTensor | None = ...
    intermediate_reference_points: tuple[tuple[torch.FloatTensor]] | None = ...
    encoder_coord_logits: torch.FloatTensor | None = ...
    encoder_class_logits: tuple[torch.FloatTensor] = ...
    encoder_extracted_states: torch.FloatTensor | None = ...
    decoder_hidden_states: tuple[torch.FloatTensor] | None = ...
    decoder_attentions: tuple[tuple[torch.FloatTensor]] | None = ...
    encoder_hidden_states: tuple[torch.FloatTensor] | None = ...
    encoder_attentions: tuple[tuple[torch.FloatTensor]] | None = ...
    classes_structure: torch.LongTensor | None = ...

@use_kernel_forward_from_hub("MultiScaleDeformableAttention")
class MultiScaleDeformableAttention(nn.Module):
    def forward(
        self,
        value: Tensor,
        value_spatial_shapes: Tensor,
        value_spatial_shapes_list: list[tuple],
        level_start_index: Tensor,
        sampling_locations: Tensor,
        attention_weights: Tensor,
        im2col_step: int,
    ):  # -> Tensor:
        ...

class OmDetTurboLRUCache:
    def __init__(self, capacity: int) -> None: ...
    def has(self, key) -> bool: ...
    def get(self, key):  # -> None:
        """
        Get the value of the key if the key exists in the cache, otherwise return None.
        Move the key to the end of the cache to show that it was recently used.
        """
        ...

    def put(self, key, value) -> None:
        """
        Add the key-value pair to the cache.
        Move the key to the end of the cache to show that it was recently used.
        If the cache is full, remove the first key (least recently used).
        """
        ...

class OmDetTurboLanguageBackbone(nn.Module):
    def __init__(self, config: OmDetTurboConfig) -> None: ...
    def forward(self, hidden_states, mask=..., encode_type=...):  # -> tuple[Any, Any] | Any:
        ...

class OmDetTurboVisionBackbone(nn.Module):
    def __init__(self, config: OmDetTurboConfig) -> None: ...
    def forward(self, pixel_values):  # -> list[Any] | Any:
        ...

class OmDetTurboMultiscaleDeformableAttention(nn.Module):
    """
    Multiscale deformable attention as proposed in Deformable DETR.
    """
    def __init__(self, config: OmDetTurboConfig, num_heads: int, n_points: int) -> None: ...
    def with_pos_embed(self, tensor: torch.Tensor, position_embeddings: Tensor | None):  # -> Tensor:
        ...
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: torch.Tensor | None = ...,
        encoder_hidden_states=...,
        encoder_attention_mask=...,
        position_embeddings: torch.Tensor | None = ...,
        reference_points=...,
        spatial_shapes=...,
        spatial_shapes_list=...,
        level_start_index=...,
        output_attentions: bool = ...,
    ):  # -> tuple[Any, Tensor]:
        ...

class OmDetTurboConvNormLayer(nn.Module):
    def __init__(self, config, in_channels, out_channels, kernel_size, stride, padding=..., activation=...) -> None: ...
    def forward(self, hidden_state):  # -> Any:
        ...

class OmDetTurboRepVggBlock(nn.Module):
    """
    RepVGG architecture block introduced by the work "RepVGG: Making VGG-style ConvNets Great Again".
    """
    def __init__(self, config: OmDetTurboConfig) -> None: ...
    def forward(self, x):  # -> Any:
        ...

class OmDetTurboCSPRepLayer(nn.Module):
    """
    Cross Stage Partial (CSP) network layer with RepVGG blocks.
    """
    def __init__(self, config: OmDetTurboConfig) -> None: ...
    def forward(self, hidden_state):  # -> Any:
        ...

class OmDetTurboMultiheadAttention(nn.Module):
    """Equivalent implementation of nn.MultiheadAttention with `batch_first=True`."""
    def __init__(self, config, hidden_size, num_attention_heads, dropout) -> None: ...
    def forward(
        self,
        queries: torch.Tensor,
        keys: torch.Tensor,
        values: torch.Tensor,
        attention_mask: torch.FloatTensor | None = ...,
        output_attentions: bool | None = ...,
    ) -> tuple[torch.Tensor]: ...

class OmDetTurboEncoderLayer(nn.Module):
    def __init__(self, config: OmDetTurboConfig) -> None: ...
    @staticmethod
    def with_pos_embed(tensor, pos_embed): ...
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: torch.Tensor,
        position_embeddings: torch.Tensor | None = ...,
        output_attentions: bool = ...,
    ):  # -> tuple[Tensor, Any | None] | tuple[Tensor]:
        """
        Args:
            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`
            attention_mask (`torch.FloatTensor`): attention mask of size
                `(batch, 1, target_len, source_len)` where padding elements are indicated by very large negative
                values.
            position_embeddings (`torch.FloatTensor`, *optional*):
                Object queries (also called content embeddings), to be added to the hidden states.
            output_attentions (`bool`, *optional*, defaults to `False`):
                Whether or not to return the attentions tensors of all attention layers. See `attentions` under
                returned tensors for more detail.
        """
        ...

class OmDetTurboEncoder(nn.Module):
    def __init__(self, config: OmDetTurboConfig) -> None: ...
    def forward(
        self, src, src_mask=..., pos_embed=..., output_attentions: bool = ...
    ) -> tuple[torch.Tensor | tuple[torch.Tensor]]: ...

class OmDetTurboHybridEncoder(nn.Module):
    """
    Encoder consisting of channel projection layers, a set of `OmDetTurboEncoder`, a top-down Feature Pyramid Network
    (FPN) and a bottom-up Path Aggregation Network (PAN). More details on the paper: https://huggingface.co/papers/2304.08069

    Args:
        config: OmDetTurboConfig
    """
    def __init__(self, config: OmDetTurboConfig) -> None: ...
    @staticmethod
    def build_2d_sincos_position_embedding(
        width, height, embed_dim=..., temperature=..., device=..., dtype=...
    ):  # -> Tensor:
        ...
    def forward(
        self, inputs_embeddings=..., output_attentions=..., output_hidden_states=..., return_dict=...
    ):  # -> tuple[Any, tuple[Any, ...] | Any | tuple[()] | None, tuple[()] | tuple[Any, ...] | None, list[Any]] | OmDetTurboEncoderOutput:
        r"""
        Args:
            inputs_embeddings (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):
                Flattened feature map (output of the backbone + projection layers) that is passed to the encoder.
            output_attentions (`bool`, *optional*):
                Whether or not to return the attentions tensors of all attention layers. See `attentions` under
                returned tensors for more detail.
            output_hidden_states (`bool`, *optional*):
                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors
                for more detail.
            return_dict (`bool`, *optional*):
                Whether or not to return a [`~file_utils.ModelOutput`] instead of a plain tuple.
        """
        ...

class OmDetTurboMLPWithDropout(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, x):  # -> Any:
        ...

class OmDetTurboMLP(nn.Module):
    """Very simple multi-layer perceptron (also called FFN)"""
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers) -> None: ...
    def forward(self, x):  # -> Tensor | Any:
        ...

class OmDetTurboResidualLayer(nn.Module):
    """
    A residual connection followed by a layer norm.
    """
    def __init__(self, config) -> None: ...
    def forward(self, x, y):  # -> Any:
        ...

class OmDetTurboTaskEncoder(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, x):  # -> Any:
        ...

class OmDetTurboDeformableTransformerDecoderLayer(GradientCheckpointingLayer):
    """
    A single layer of the Deformable Transformer Decoder.
    """
    def __init__(self, config) -> None: ...
    @staticmethod
    def with_pos_embed(tensor, pos): ...
    def forward(
        self,
        decoder_embeddings,
        task_features,
        reference_points,
        vision_features,
        vision_shapes,
        vision_shapes_list,
        level_start_index=...,
        attention_mask=...,
        padding_mask=...,
        query_position=...,
        output_attentions=...,
        output_hidden_states=...,
    ):  # -> tuple[Any, Any, Any | None, Any | None]:
        ...

@auto_docstring
class OmDetTurboPreTrainedModel(PreTrainedModel):
    config: OmDetTurboConfig
    base_model_prefix = ...
    main_input_name = ...
    def get_cached_class_embeddings(self, classes_input_ids, classes_attention_mask):  # -> Tensor:
        ...
    def get_cached_task_embeddings(self, tasks_input_ids, tasks_attention_mask):  # -> tuple[Tensor, Tensor]:
        ...
    def get_language_embedding(
        self, classes_input_ids, classes_attention_mask, tasks_input_ids, tasks_attention_mask, classes_structure
    ):  # -> tuple[Tensor, Tensor, Tensor]:
        ...

def get_class_similarity(class_distance_type, cls_feature, class_proj):  # -> Tensor:
    ...

class OmDetTurboDecoder(OmDetTurboPreTrainedModel):
    def __init__(self, config: OmDetTurboConfig) -> None: ...
    @lru_cache(maxsize=32)
    def generate_anchors(self, spatial_shapes=..., grid_size=..., device=..., dtype=...):  # -> tuple[Tensor, Tensor]:
        ...
    def forward(
        self,
        vision_features,
        class_features,
        task_features,
        task_mask,
        output_attentions=...,
        output_hidden_states=...,
        return_dict=...,
    ):  # -> tuple[Any | Tensor, tuple[Tensor | Any, ...] | Any | tuple[Any, ...] | tuple[()] | None, tuple[tuple[Any, ...] | Any | tuple[()] | None, ...] | Any | tuple[()] | None, Tensor, Tensor, Any, Tensor, Tensor, Tensor | Any] | OmDetTurboDecoderOutput:
        """
        Args:
            vision_features (`torch.FloatTensor`): The sequence of vision features. shape depends on the vision
                backbone.
            class_features (`torch.FloatTensor`): The sequence of class features of shape
                `(class_sequence_length, batch_size, class_embed_dim)`.
            task_features (`torch.FloatTensor`): The sequence of task features of shape
                `(task_sequence_length, batch_size, decoder_hidden_dim)`.
            task_mask (`torch.LongTensor`): The mask for the task features of shape `(batch_size, task_sequence_length)`.
            output_attentions (`bool`, *optional*): Whether or not to return the attentions tensors of all attention
                layers. See `attentions` under returned tensors for more detail.
            output_hidden_states (`bool`, *optional*): Whether or not to return the hidden states of all layers. See
                `hidden_states` under returned tensors for more detail.
            return_dict (`bool`, *optional*): Whether or not to return a [`~file_utils.ModelOutput`] instead of a plain
                tuple.
        """
        ...

@auto_docstring(
    custom_intro="""
    OmDetTurbo Model (consisting of a vision and a text backbone, and encoder-decoder architecture) outputting
    bounding boxes and classes scores for tasks such as COCO detection.
    """
)
class OmDetTurboForObjectDetection(OmDetTurboPreTrainedModel):
    def __init__(self, config: OmDetTurboConfig) -> None: ...
    def get_input_embeddings(self):  # -> Any:
        ...
    def set_input_embeddings(self, value):  # -> None:
        ...
    def resize_token_embeddings(
        self, new_num_tokens: int | None = ..., pad_to_multiple_of=..., mean_resizing: bool = ...
    ) -> nn.Embedding: ...
    @auto_docstring
    def forward(
        self,
        pixel_values: torch.FloatTensor,
        classes_input_ids: torch.LongTensor,
        classes_attention_mask: torch.LongTensor,
        tasks_input_ids: torch.LongTensor,
        tasks_attention_mask: torch.LongTensor,
        classes_structure: torch.LongTensor,
        labels: torch.LongTensor | None = ...,
        output_attentions: bool | None = ...,
        output_hidden_states: bool | None = ...,
        return_dict: bool | None = ...,
    ) -> tuple[torch.FloatTensor] | OmDetTurboObjectDetectionOutput:
        r"""
        classes_input_ids (`torch.LongTensor` of shape `(total_classes (>= batch_size), sequence_length)`):
            Indices of input classes sequence tokens in the vocabulary of the language model.
            Several classes can be provided for each tasks, thus the tokenized classes are flattened
            and the structure of the classes is provided in the `classes_structure` argument.

            Indices can be obtained using [`OmDetTurboProcessor`]. See [`OmDetTurboProcessor.__call__`] for
            details.

            [What are input IDs?](../glossary#input-ids)
        classes_attention_mask (`torch.BoolTensor` of shape `(total_classes (>= batch_size), num_classes, sequence_length)`):
            Attention mask for the classes. This is a binary mask that indicates which tokens should be attended to,
            and which should not.
        tasks_input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
            Indices of input tasks sequence tokens in the vocabulary of the language model.

            Indices can be obtained using [`OmDetTurboProcessor`]. See [`OmDetTurboProcessor.__call__`] for
            details.

            [What are input IDs?](../glossary#input-ids)
        tasks_attention_mask (`torch.BoolTensor` of shape `(batch_size, sequence_length)`):
            Attention mask for the tasks. This is a binary mask that indicates which tokens should be attended to,
            and which should not.
        classes_structure (torch.LongTensor of shape `(batch_size)`):
            Structure of the classes. This tensor indicates the number of classes for each task.

        Examples:

        ```python
        >>> import requests
        >>> from PIL import Image

        >>> from transformers import AutoProcessor, OmDetTurboForObjectDetection

        >>> processor = AutoProcessor.from_pretrained("omlab/omdet-turbo-swin-tiny-hf")
        >>> model = OmDetTurboForObjectDetection.from_pretrained("omlab/omdet-turbo-swin-tiny-hf")

        >>> url = "http://images.cocodataset.org/val2017/000000039769.jpg"
        >>> image = Image.open(requests.get(url, stream=True).raw)
        >>> classes = ["cat", "remote"]
        >>> task = "Detect {}.".format(", ".join(classes))
        >>> inputs = processor(image, text=classes, task=task, return_tensors="pt")

        >>> outputs = model(**inputs)

        >>> # convert outputs (bounding boxes and class logits)
        >>> results = processor.post_process_grounded_object_detection(
        ...     outputs,
        ...     classes=classes,
        ...     target_sizes=[image.size[::-1]],
        ...     score_threshold=0.3,
        ...     nms_threshold=0.3,
        >>> )[0]
        >>> for score, class_name, box in zip(results["scores"], results["classes"], results["boxes"]):
        ...     box = [round(i, 1) for i in box.tolist()]
        ...     print(
        ...         f"Detected {class_name} with confidence "
        ...         f"{round(score.item(), 2)} at location {box}"
        ...     )
        Detected remote with confidence 0.76 at location [39.9, 71.3, 176.5, 117.9]
        Detected cat with confidence 0.72 at location [345.1, 22.5, 639.7, 371.9]
        Detected cat with confidence 0.65 at location [12.7, 53.8, 315.5, 475.3]
        Detected remote with confidence 0.57 at location [333.4, 75.6, 370.7, 187.0]
        ```"""
        ...

__all__ = ["OmDetTurboForObjectDetection", "OmDetTurboPreTrainedModel"]
