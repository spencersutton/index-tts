"""
This type stub file was generated by pyright.
"""

import torch
import torch.nn as nn
from dataclasses import dataclass
from typing import Optional, Union
from transformers.modeling_utils import PreTrainedModel
from transformers.utils import ModelOutput
from ...modeling_flash_attention_utils import FlashAttentionKwargs
from ...processing_utils import Unpack
from ...utils import auto_docstring
from .configuration_patchtsmixer import PatchTSMixerConfig

logger = ...

class PatchTSMixerGatedAttention(nn.Module):
    def __init__(self, in_size: int, out_size: int) -> None: ...
    def forward(self, inputs): ...

class PatchTSMixerBatchNorm(nn.Module):
    def __init__(self, config: PatchTSMixerConfig) -> None: ...
    def forward(self, inputs: torch.Tensor): ...

class PatchTSMixerPositionalEncoding(nn.Module):
    def __init__(self, config: PatchTSMixerConfig) -> None: ...
    def forward(self, patch_input: torch.Tensor): ...

class PatchTSMixerNormLayer(nn.Module):
    def __init__(self, config: PatchTSMixerConfig) -> None: ...
    def forward(self, inputs: torch.Tensor): ...

class PatchTSMixerMLP(nn.Module):
    def __init__(self, in_features, out_features, config) -> None: ...
    def forward(self, inputs: torch.Tensor): ...

class PatchTSMixerChannelFeatureMixerBlock(nn.Module):
    def __init__(self, config: PatchTSMixerConfig) -> None: ...
    def forward(self, inputs: torch.Tensor): ...

def eager_attention_forward(
    module: nn.Module,
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: Optional[torch.Tensor],
    scaling: Optional[float] = ...,
    dropout: float = ...,
    head_mask: Optional[torch.Tensor] = ...,
    **kwargs,
): ...

class PatchTSMixerAttention(nn.Module):
    def __init__(
        self,
        embed_dim: int,
        num_heads: int,
        dropout: float = ...,
        is_decoder: bool = ...,
        bias: bool = ...,
        is_causal: bool = ...,
        config: Optional[PatchTSMixerConfig] = ...,
    ) -> None: ...
    def forward(
        self,
        hidden_states: torch.Tensor,
        key_value_states: Optional[torch.Tensor] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        layer_head_mask: Optional[torch.Tensor] = ...,
        output_attentions: Optional[bool] = ...,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]: ...

class PatchMixerBlock(nn.Module):
    def __init__(self, config: PatchTSMixerConfig) -> None: ...
    def forward(self, hidden_state): ...

class FeatureMixerBlock(nn.Module):
    def __init__(self, config: PatchTSMixerConfig) -> None: ...
    def forward(self, hidden: torch.Tensor): ...

class PatchTSMixerLayer(nn.Module):
    def __init__(self, config: PatchTSMixerConfig) -> None: ...
    def forward(self, hidden: torch.Tensor): ...

class PatchTSMixerBlock(nn.Module):
    def __init__(self, config: PatchTSMixerConfig) -> None: ...
    def forward(self, hidden_state, output_hidden_states: bool = ...): ...

class PatchTSMixerForPredictionHead(nn.Module):
    def __init__(self, config: PatchTSMixerConfig, distribution_output=...) -> None: ...
    def forward(self, hidden_features): ...

class PatchTSMixerLinearHead(nn.Module):
    def __init__(self, config: PatchTSMixerConfig, distribution_output=...) -> None: ...
    def forward(self, hidden_features): ...

@auto_docstring
class PatchTSMixerPreTrainedModel(PreTrainedModel):
    config: PatchTSMixerConfig
    base_model_prefix = ...
    main_input_name = ...
    supports_gradient_checkpointing = ...

class PatchTSMixerPretrainHead(nn.Module):
    def __init__(self, config: PatchTSMixerConfig) -> None: ...
    def forward(self, hidden_features): ...

def random_masking(
    inputs: torch.Tensor,
    mask_ratio: float,
    unmasked_channel_indices: Optional[list] = ...,
    channel_consistent_masking: bool = ...,
    mask_value: int = ...,
): ...
def forecast_masking(
    inputs: torch.Tensor,
    num_forecast_mask_patches: Union[list, int],
    unmasked_channel_indices: Optional[list] = ...,
    mask_value: int = ...,
): ...

class PatchTSMixerPatchify(nn.Module):
    def __init__(self, config: PatchTSMixerConfig) -> None: ...
    def forward(self, past_values: torch.Tensor): ...

class PatchTSMixerMasking(nn.Module):
    def __init__(self, config: PatchTSMixerConfig) -> None: ...
    def forward(self, patch_input: torch.Tensor): ...

class PatchTSMixerStdScaler(nn.Module):
    def __init__(self, config: PatchTSMixerConfig) -> None: ...
    def forward(
        self, data: torch.Tensor, observed_indicator: torch.Tensor
    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]: ...

class PatchTSMixerMeanScaler(nn.Module):
    def __init__(self, config: PatchTSMixerConfig) -> None: ...
    def forward(
        self, data: torch.Tensor, observed_indicator: torch.Tensor
    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]: ...

class PatchTSMixerNOPScaler(nn.Module):
    def __init__(self, config: PatchTSMixerConfig) -> None: ...
    def forward(
        self, data: torch.Tensor, observed_indicator: Optional[torch.Tensor] = ...
    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]: ...

@dataclass
@auto_docstring(custom_intro=...)
class PatchTSMixerEncoderOutput(ModelOutput):
    last_hidden_state: Optional[torch.FloatTensor] = ...
    hidden_states: Optional[tuple[torch.FloatTensor]] = ...

class PatchTSMixerEncoder(PatchTSMixerPreTrainedModel):
    def __init__(self, config: PatchTSMixerConfig) -> None: ...
    @auto_docstring
    def forward(
        self, past_values: torch.Tensor, output_hidden_states: Optional[bool] = ..., return_dict: Optional[bool] = ...
    ) -> Union[tuple, PatchTSMixerEncoderOutput]: ...

@dataclass
@auto_docstring(custom_intro=...)
class PatchTSMixerModelOutput(ModelOutput):
    last_hidden_state: Optional[torch.FloatTensor] = ...
    hidden_states: Optional[tuple[torch.FloatTensor]] = ...
    patch_input: Optional[torch.FloatTensor] = ...
    mask: Optional[torch.FloatTensor] = ...
    loc: Optional[torch.FloatTensor] = ...
    scale: Optional[torch.FloatTensor] = ...

@auto_docstring(custom_intro=...)
class PatchTSMixerModel(PatchTSMixerPreTrainedModel):
    def __init__(self, config: PatchTSMixerConfig, mask_input: bool = ...) -> None: ...
    @auto_docstring
    def forward(
        self,
        past_values: torch.Tensor,
        observed_mask: Optional[torch.Tensor] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> PatchTSMixerModelOutput: ...

@dataclass
@auto_docstring(custom_intro=...)
class PatchTSMixerForPreTrainingOutput(ModelOutput):
    loss: Optional[torch.FloatTensor] = ...
    prediction_outputs: Optional[torch.FloatTensor] = ...
    last_hidden_state: Optional[torch.FloatTensor] = ...
    hidden_states: Optional[tuple[torch.FloatTensor]] = ...

@auto_docstring(
    custom_intro="""
    `PatchTSMixer` for mask pretraining.
    """
)
class PatchTSMixerForPretraining(PatchTSMixerPreTrainedModel):
    def __init__(self, config: PatchTSMixerConfig) -> None: ...
    @auto_docstring
    def forward(
        self,
        past_values: torch.Tensor,
        observed_mask: Optional[torch.Tensor] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_loss: bool = ...,
        return_dict: Optional[bool] = ...,
    ) -> PatchTSMixerForPreTrainingOutput: ...

@dataclass
@auto_docstring(custom_intro=...)
class PatchTSMixerForPredictionOutput(ModelOutput):
    loss: Optional[torch.FloatTensor] = ...
    prediction_outputs: Optional[torch.FloatTensor] = ...
    last_hidden_state: Optional[torch.FloatTensor] = ...
    hidden_states: Optional[tuple[torch.FloatTensor]] = ...
    loc: Optional[torch.FloatTensor] = ...
    scale: Optional[torch.FloatTensor] = ...

@dataclass
@auto_docstring(custom_intro=...)
class SamplePatchTSMixerPredictionOutput(ModelOutput):
    sequences: Optional[torch.FloatTensor] = ...

@dataclass
@auto_docstring(custom_intro=...)
class SamplePatchTSMixerRegressionOutput(ModelOutput):
    sequences: Optional[torch.FloatTensor] = ...

def nll(input: torch.distributions.Distribution, target: torch.Tensor) -> torch.Tensor: ...
def weighted_average(input_tensor: torch.Tensor, weights: Optional[torch.Tensor] = ..., dim=...) -> torch.Tensor: ...

class PatchTSMixerForPrediction(PatchTSMixerPreTrainedModel):
    def __init__(self, config: PatchTSMixerConfig) -> None: ...
    @auto_docstring
    def forward(
        self,
        past_values: torch.Tensor,
        observed_mask: Optional[torch.Tensor] = ...,
        future_values: Optional[torch.Tensor] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_loss: bool = ...,
        return_dict: Optional[bool] = ...,
    ) -> PatchTSMixerForPredictionOutput: ...
    @torch.no_grad()
    def generate(
        self, past_values: torch.Tensor, observed_mask: Optional[torch.Tensor] = ...
    ) -> SamplePatchTSMixerPredictionOutput: ...

@dataclass
@auto_docstring(custom_intro=...)
class PatchTSMixerForTimeSeriesClassificationOutput(ModelOutput):
    loss: Optional[torch.FloatTensor] = ...
    prediction_outputs: Optional[torch.FloatTensor] = ...
    last_hidden_state: Optional[torch.FloatTensor] = ...
    hidden_states: Optional[tuple[torch.FloatTensor]] = ...

class PatchTSMixerForTimeSeriesClassification(PatchTSMixerPreTrainedModel):
    def __init__(self, config: PatchTSMixerConfig) -> None: ...
    @auto_docstring
    def forward(
        self,
        past_values: torch.Tensor,
        target_values: Optional[torch.Tensor] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_loss: bool = ...,
        return_dict: Optional[bool] = ...,
    ) -> PatchTSMixerForTimeSeriesClassificationOutput: ...

@dataclass
@auto_docstring(custom_intro=...)
class PatchTSMixerForRegressionOutput(ModelOutput):
    loss: Optional[torch.FloatTensor] = ...
    regression_outputs: Optional[torch.FloatTensor] = ...
    last_hidden_state: Optional[torch.FloatTensor] = ...
    hidden_states: Optional[tuple[torch.FloatTensor]] = ...

class InjectScalerStatistics4D(nn.Module):
    def __init__(self, d_model: int, num_patches: int, expansion: int = ...) -> None: ...
    def forward(self, inputs: torch.Tensor, loc: torch.Tensor, scale: torch.Tensor): ...

@auto_docstring(custom_intro=...)
class PatchTSMixerForRegression(PatchTSMixerPreTrainedModel):
    def __init__(self, config: PatchTSMixerConfig) -> None: ...
    @auto_docstring
    def forward(
        self,
        past_values: torch.Tensor,
        target_values: Optional[torch.Tensor] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_loss: bool = ...,
        return_dict: Optional[bool] = ...,
    ) -> PatchTSMixerForRegressionOutput: ...
    @torch.no_grad()
    def generate(self, past_values: torch.Tensor) -> SamplePatchTSMixerRegressionOutput: ...

__all__ = [
    "PatchTSMixerPreTrainedModel",
    "PatchTSMixerModel",
    "PatchTSMixerForPretraining",
    "PatchTSMixerForPrediction",
    "PatchTSMixerForTimeSeriesClassification",
    "PatchTSMixerForRegression",
]
