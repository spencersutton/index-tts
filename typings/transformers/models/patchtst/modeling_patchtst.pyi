"""
This type stub file was generated by pyright.
"""

import torch
from dataclasses import dataclass
from typing import Optional, Union
from torch import nn
from ...modeling_flash_attention_utils import FlashAttentionKwargs
from ...modeling_outputs import BaseModelOutput
from ...modeling_utils import PreTrainedModel
from ...processing_utils import Unpack
from ...utils import ModelOutput, auto_docstring
from .configuration_patchtst import PatchTSTConfig

logger = ...

def eager_attention_forward(
    module: nn.Module,
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: Optional[torch.Tensor],
    scaling: Optional[float] = ...,
    dropout: float = ...,
    head_mask: Optional[torch.Tensor] = ...,
    **kwargs,
): ...

class PatchTSTAttention(nn.Module):
    def __init__(
        self,
        embed_dim: int,
        num_heads: int,
        dropout: float = ...,
        is_decoder: bool = ...,
        bias: bool = ...,
        is_causal: bool = ...,
        config: Optional[PatchTSTConfig] = ...,
    ) -> None: ...
    def forward(
        self,
        hidden_states: torch.Tensor,
        key_value_states: Optional[torch.Tensor] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        layer_head_mask: Optional[torch.Tensor] = ...,
        output_attentions: Optional[bool] = ...,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]: ...

class PatchTSTBatchNorm(nn.Module):
    def __init__(self, config: PatchTSTConfig) -> None: ...
    def forward(self, inputs: torch.Tensor): ...

def random_masking(
    inputs: torch.Tensor,
    mask_ratio: float,
    unmasked_channel_indices: Optional[list] = ...,
    channel_consistent_masking: bool = ...,
    mask_value: int = ...,
): ...
def forecast_masking(
    inputs: torch.Tensor,
    num_forecast_mask_patches: Union[list, int],
    unmasked_channel_indices: Optional[list] = ...,
    mask_value: int = ...,
): ...

class PatchTSTPatchify(nn.Module):
    def __init__(self, config: PatchTSTConfig) -> None: ...
    def forward(self, past_values: torch.Tensor): ...

class PatchTSTMasking(nn.Module):
    def __init__(self, config: PatchTSTConfig) -> None: ...
    def forward(self, patch_input: torch.Tensor): ...

class PatchTSTEncoderLayer(nn.Module):
    def __init__(self, config: PatchTSTConfig) -> None: ...
    def forward(self, hidden_state: torch.Tensor, output_attentions: Optional[bool] = ...): ...

@auto_docstring
class PatchTSTPreTrainedModel(PreTrainedModel):
    config: PatchTSTConfig
    base_model_prefix = ...
    main_input_name = ...
    supports_gradient_checkpointing = ...

class PatchTSTEmbedding(nn.Module):
    def __init__(self, config: PatchTSTConfig) -> None: ...
    def forward(self, patch_input: torch.Tensor): ...

class PatchTSTPositionalEncoding(nn.Module):
    def __init__(self, config: PatchTSTConfig, num_patches: int) -> None: ...
    def forward(self, patch_input: torch.Tensor): ...

class PatchTSTEncoder(PatchTSTPreTrainedModel):
    def __init__(self, config: PatchTSTConfig, num_patches: int) -> None: ...
    def forward(
        self,
        patch_input: torch.Tensor,
        output_hidden_states: Optional[bool] = ...,
        output_attentions: Optional[bool] = ...,
    ) -> BaseModelOutput: ...

@dataclass
@auto_docstring(custom_intro=...)
class PatchTSTModelOutput(ModelOutput):
    last_hidden_state: Optional[torch.FloatTensor] = ...
    hidden_states: Optional[tuple[torch.FloatTensor]] = ...
    attentions: Optional[tuple[torch.FloatTensor]] = ...
    mask: Optional[torch.FloatTensor] = ...
    loc: Optional[torch.FloatTensor] = ...
    scale: Optional[torch.FloatTensor] = ...
    patch_input: Optional[torch.FloatTensor] = ...

@dataclass
@auto_docstring(custom_intro=...)
class PatchTSTForPretrainingOutput(ModelOutput):
    loss: Optional[torch.FloatTensor] = ...
    prediction_output: Optional[torch.FloatTensor] = ...
    hidden_states: Optional[tuple[torch.FloatTensor]] = ...
    attentions: Optional[tuple[torch.FloatTensor]] = ...

@dataclass
@auto_docstring(custom_intro=...)
class PatchTSTForRegressionOutput(ModelOutput):
    loss: Optional[torch.FloatTensor] = ...
    regression_outputs: Optional[torch.FloatTensor] = ...
    hidden_states: Optional[tuple[torch.FloatTensor]] = ...
    attentions: Optional[tuple[torch.FloatTensor]] = ...

@dataclass
@auto_docstring(custom_intro=...)
class PatchTSTForPredictionOutput(ModelOutput):
    loss: Optional[torch.FloatTensor] = ...
    prediction_outputs: Optional[torch.FloatTensor] = ...
    hidden_states: Optional[tuple[torch.FloatTensor]] = ...
    attentions: Optional[tuple[torch.FloatTensor]] = ...
    loc: Optional[torch.FloatTensor] = ...
    scale: Optional[torch.FloatTensor] = ...

@dataclass
@auto_docstring(custom_intro=...)
class PatchTSTForClassificationOutput(ModelOutput):
    loss: Optional[torch.FloatTensor] = ...
    prediction_logits: Optional[torch.FloatTensor] = ...
    hidden_states: Optional[tuple[torch.FloatTensor]] = ...
    attentions: Optional[tuple[torch.FloatTensor]] = ...

@dataclass
@auto_docstring(custom_intro=...)
class SamplePatchTSTOutput(ModelOutput):
    sequences: Optional[torch.FloatTensor] = ...

def nll(input: torch.distributions.Distribution, target: torch.Tensor) -> torch.Tensor: ...
def weighted_average(input_tensor: torch.Tensor, weights: Optional[torch.Tensor] = ..., dim=...) -> torch.Tensor: ...

class PatchTSTStdScaler(nn.Module):
    def __init__(self, config: PatchTSTConfig) -> None: ...
    def forward(
        self, data: torch.Tensor, observed_indicator: torch.Tensor
    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]: ...

class PatchTSTMeanScaler(nn.Module):
    def __init__(self, config: PatchTSTConfig) -> None: ...
    def forward(
        self, data: torch.Tensor, observed_indicator: torch.Tensor
    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]: ...

class PatchTSTNOPScaler(nn.Module):
    def __init__(self, config: PatchTSTConfig) -> None: ...
    def forward(
        self, data: torch.Tensor, observed_indicator: Optional[torch.Tensor] = ...
    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]: ...

class PatchTSTScaler(nn.Module):
    def __init__(self, config: PatchTSTConfig) -> None: ...
    def forward(
        self, data: torch.Tensor, observed_indicator: torch.Tensor
    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]: ...

@auto_docstring
class PatchTSTModel(PatchTSTPreTrainedModel):
    def __init__(self, config: PatchTSTConfig) -> None: ...
    def forward(
        self,
        past_values: torch.Tensor,
        past_observed_mask: Optional[torch.Tensor] = ...,
        future_values: Optional[torch.Tensor] = ...,
        output_hidden_states: Optional[bool] = ...,
        output_attentions: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[tuple, PatchTSTModelOutput]: ...

class PatchTSTMaskPretrainHead(nn.Module):
    def __init__(self, config: PatchTSTConfig) -> None: ...
    def forward(self, embedding: torch.Tensor) -> torch.Tensor: ...

@auto_docstring(
    custom_intro="""
    The PatchTST for pretrain model.
    """
)
class PatchTSTForPretraining(PatchTSTPreTrainedModel):
    def __init__(self, config: PatchTSTConfig) -> None: ...
    def forward(
        self,
        past_values: torch.Tensor,
        past_observed_mask: Optional[torch.Tensor] = ...,
        output_hidden_states: Optional[bool] = ...,
        output_attentions: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[tuple, PatchTSTForPretrainingOutput]: ...

class PatchTSTClassificationHead(nn.Module):
    def __init__(self, config: PatchTSTConfig) -> None: ...
    def forward(self, embedding: torch.Tensor): ...

@auto_docstring(
    custom_intro="""
    The PatchTST for classification model.
    """
)
class PatchTSTForClassification(PatchTSTPreTrainedModel):
    def __init__(self, config: PatchTSTConfig) -> None: ...
    @auto_docstring
    def forward(
        self,
        past_values: torch.Tensor,
        target_values: Optional[torch.Tensor] = ...,
        past_observed_mask: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        output_attentions: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[tuple, PatchTSTForClassificationOutput]: ...

@auto_docstring(
    custom_intro="""
    The PatchTST for regression Model.
    """
)
class PatchTSTPredictionHead(nn.Module):
    def __init__(self, config: PatchTSTConfig, num_patches: int, distribution_output=...) -> None: ...
    def forward(self, embedding: torch.Tensor): ...

@auto_docstring(
    custom_intro="""
    The PatchTST for prediction model.
    """
)
class PatchTSTForPrediction(PatchTSTPreTrainedModel):
    def __init__(self, config: PatchTSTConfig) -> None: ...
    def forward(
        self,
        past_values: torch.Tensor,
        past_observed_mask: Optional[torch.Tensor] = ...,
        future_values: Optional[torch.Tensor] = ...,
        output_hidden_states: Optional[bool] = ...,
        output_attentions: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[tuple, PatchTSTForPredictionOutput]: ...
    @torch.no_grad()
    def generate(
        self, past_values: torch.Tensor, past_observed_mask: Optional[torch.Tensor] = ...
    ) -> SamplePatchTSTOutput: ...

class PatchTSTRegressionHead(nn.Module):
    def __init__(self, config: PatchTSTConfig, distribution_output=...) -> None: ...
    def forward(self, embedding: torch.Tensor): ...

@auto_docstring(
    custom_intro="""
    The PatchTST for regression model.
    """
)
class PatchTSTForRegression(PatchTSTPreTrainedModel):
    def __init__(self, config: PatchTSTConfig) -> None: ...
    @auto_docstring
    def forward(
        self,
        past_values: torch.Tensor,
        target_values: Optional[torch.Tensor] = ...,
        past_observed_mask: Optional[torch.Tensor] = ...,
        output_hidden_states: Optional[bool] = ...,
        output_attentions: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[tuple, PatchTSTForRegressionOutput]: ...
    @torch.no_grad()
    def generate(
        self, past_values: torch.Tensor, past_observed_mask: Optional[torch.Tensor] = ...
    ) -> SamplePatchTSTOutput: ...

__all__ = [
    "PatchTSTModel",
    "PatchTSTPreTrainedModel",
    "PatchTSTForPrediction",
    "PatchTSTForPretraining",
    "PatchTSTForRegression",
    "PatchTSTForClassification",
]
