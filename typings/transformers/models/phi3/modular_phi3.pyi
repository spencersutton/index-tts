"""
This type stub file was generated by pyright.
"""

import torch
from typing import Optional
from torch import nn
from ...cache_utils import Cache
from ...modeling_flash_attention_utils import FlashAttentionKwargs
from ...processing_utils import Unpack
from ..mistral.modeling_mistral import (
    MistralDecoderLayer,
    MistralForCausalLM,
    MistralForSequenceClassification,
    MistralForTokenClassification,
    MistralPreTrainedModel,
)
from .configuration_phi3 import Phi3Config

logger = ...
_CHECKPOINT_FOR_DOC = ...
_CONFIG_FOR_DOC = ...

class Phi3MLP(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states: torch.FloatTensor) -> torch.FloatTensor: ...

def apply_rotary_pos_emb(q, k, cos, sin, position_ids=..., unsqueeze_dim=...): ...

class Phi3Attention(nn.Module):
    def __init__(self, config: Phi3Config, layer_idx: Optional[int] = ...) -> None: ...
    def forward(
        self,
        hidden_states: torch.Tensor,
        position_embeddings: tuple[torch.Tensor, torch.Tensor],
        attention_mask: Optional[torch.Tensor],
        past_key_value: Optional[Cache] = ...,
        cache_position: Optional[torch.LongTensor] = ...,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]: ...

class Phi3DecoderLayer(MistralDecoderLayer):
    def __init__(self, config: Phi3Config, layer_idx: int) -> None: ...
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = ...,
        position_ids: Optional[torch.LongTensor] = ...,
        past_key_value: Optional[Cache] = ...,
        use_cache: Optional[bool] = ...,
        cache_position: Optional[torch.LongTensor] = ...,
        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = ...,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]: ...

class Phi3PreTrainedModel(MistralPreTrainedModel):
    _version = ...

class Phi3ForCausalLM(MistralForCausalLM, Phi3PreTrainedModel):
    def prepare_inputs_for_generation(
        self,
        input_ids,
        past_key_values=...,
        attention_mask=...,
        inputs_embeds=...,
        cache_position=...,
        position_ids=...,
        use_cache=...,
        logits_to_keep=...,
        **kwargs,
    ): ...

class Phi3ForSequenceClassification(MistralForSequenceClassification): ...
class Phi3ForTokenClassification(MistralForTokenClassification): ...

__all__ = [
    "Phi3PreTrainedModel",
    "Phi3Model",
    "Phi3ForCausalLM",
    "Phi3ForSequenceClassification",
    "Phi3ForTokenClassification",
]
