"""
This type stub file was generated by pyright.
"""

import torch
from dataclasses import dataclass
from typing import Optional, Union
from torch import Tensor, nn
from ...cache_utils import Cache
from ...generation import GenerationMixin
from ...modeling_layers import GradientCheckpointingLayer
from ...modeling_outputs import BaseModelOutput
from ...modeling_utils import PreTrainedModel
from ...utils import ModelOutput, auto_docstring
from .configuration_prophetnet import ProphetNetConfig

logger = ...

def softmax(hidden_state, dim, onnx_trace=...): ...
def ngram_attention_bias(sequence_length, ngram, device, dtype): ...
def compute_relative_buckets(num_buckets, max_distance, relative_positions, is_bidirectional=...): ...
def compute_all_stream_relative_buckets(num_buckets, max_distance, position_ids): ...

@dataclass
@auto_docstring(custom_intro=...)
class ProphetNetSeq2SeqLMOutput(ModelOutput):
    loss: Optional[torch.FloatTensor] = ...
    logits: Optional[torch.FloatTensor] = ...
    logits_ngram: Optional[torch.FloatTensor] = ...
    past_key_values: Optional[tuple[torch.FloatTensor]] = ...
    decoder_hidden_states: Optional[tuple[torch.FloatTensor]] = ...
    decoder_ngram_hidden_states: Optional[tuple[torch.FloatTensor]] = ...
    decoder_attentions: Optional[tuple[torch.FloatTensor]] = ...
    decoder_ngram_attentions: Optional[tuple[torch.FloatTensor]] = ...
    cross_attentions: Optional[tuple[torch.FloatTensor]] = ...
    encoder_last_hidden_state: Optional[torch.FloatTensor] = ...
    encoder_hidden_states: Optional[tuple[torch.FloatTensor]] = ...
    encoder_attentions: Optional[tuple[torch.FloatTensor]] = ...
    @property
    def decoder_cross_attentions(self): ...

@dataclass
@auto_docstring(custom_intro=...)
class ProphetNetSeq2SeqModelOutput(ModelOutput):
    last_hidden_state: torch.FloatTensor
    last_hidden_state_ngram: Optional[torch.FloatTensor] = ...
    past_key_values: Optional[tuple[torch.FloatTensor]] = ...
    decoder_hidden_states: Optional[tuple[torch.FloatTensor]] = ...
    decoder_ngram_hidden_states: Optional[tuple[torch.FloatTensor]] = ...
    decoder_attentions: Optional[tuple[torch.FloatTensor]] = ...
    decoder_ngram_attentions: Optional[tuple[torch.FloatTensor]] = ...
    cross_attentions: Optional[tuple[torch.FloatTensor]] = ...
    encoder_last_hidden_state: Optional[torch.FloatTensor] = ...
    encoder_hidden_states: Optional[tuple[torch.FloatTensor]] = ...
    encoder_attentions: Optional[tuple[torch.FloatTensor]] = ...
    @property
    def decoder_cross_attentions(self): ...

@dataclass
@auto_docstring(custom_intro=...)
class ProphetNetDecoderModelOutput(ModelOutput):
    last_hidden_state: torch.FloatTensor
    last_hidden_state_ngram: Optional[torch.FloatTensor] = ...
    past_key_values: Optional[tuple[torch.FloatTensor]] = ...
    hidden_states: Optional[tuple[torch.FloatTensor]] = ...
    hidden_states_ngram: Optional[tuple[torch.FloatTensor]] = ...
    attentions: Optional[tuple[torch.FloatTensor]] = ...
    ngram_attentions: Optional[tuple[torch.FloatTensor]] = ...
    cross_attentions: Optional[tuple[torch.FloatTensor]] = ...

@dataclass
@auto_docstring(custom_intro=...)
class ProphetNetDecoderLMOutput(ModelOutput):
    loss: Optional[torch.FloatTensor] = ...
    logits: Optional[torch.FloatTensor] = ...
    logits_ngram: Optional[torch.FloatTensor] = ...
    past_key_values: Optional[tuple[torch.FloatTensor]] = ...
    hidden_states: Optional[tuple[torch.FloatTensor]] = ...
    hidden_states_ngram: Optional[tuple[torch.FloatTensor]] = ...
    attentions: Optional[tuple[torch.FloatTensor]] = ...
    ngram_attentions: Optional[tuple[torch.FloatTensor]] = ...
    cross_attentions: Optional[tuple[torch.FloatTensor]] = ...

@auto_docstring
class ProphetNetPreTrainedModel(PreTrainedModel):
    config: ProphetNetConfig
    base_model_prefix = ...
    supports_gradient_checkpointing = ...

class ProphetNetPositionalEmbeddings(nn.Embedding):
    def __init__(self, config: ProphetNetConfig) -> None: ...
    def forward(self, inputs_shape, device, attention_mask=..., past_key_values=..., position_ids=...): ...

class ProphetNetAttention(nn.Module):
    def __init__(self, config: ProphetNetConfig, num_attn_heads: int, layer_idx: Optional[int] = ...) -> None: ...
    def forward(
        self,
        hidden_states,
        key_value_states: Optional[Tensor] = ...,
        attention_mask: Optional[Tensor] = ...,
        layer_head_mask: Optional[Tensor] = ...,
        past_key_value: Optional[Cache] = ...,
        output_attentions: Optional[bool] = ...,
        cache_position: Optional[torch.Tensor] = ...,
    ) -> tuple[Tensor, Optional[Tensor]]: ...

class ProphetNetFeedForward(nn.Module):
    def __init__(self, config: ProphetNetConfig, ffn_dim: int) -> None: ...
    def forward(self, hidden_states): ...

class ProphetNetNgramSelfAttention(nn.Module):
    def __init__(self, config: ProphetNetConfig, layer_idx=...) -> None: ...
    def prepare_for_onnx_export_(self): ...
    def forward(
        self,
        hidden_states,
        past_key_value: Optional[tuple[Tensor]] = ...,
        attention_mask=...,
        layer_head_mask=...,
        extended_predict_attention_mask=...,
        main_relative_position_buckets=...,
        predict_relative_position_buckets=...,
        position_ids=...,
        cache_position=...,
    ): ...
    def get_main_relative_pos_embeddings(
        self, hidden_states, attn_weights, position_ids, main_relative_position_buckets
    ): ...
    def get_predict_relative_pos_embeddings(
        self, hidden_states, attn_weights, position_ids, predict_relative_position_buckets
    ): ...

class ProphetNetEncoderLayer(GradientCheckpointingLayer):
    def __init__(self, config: ProphetNetConfig) -> None: ...
    def forward(self, hidden_states, attention_mask, layer_head_mask, output_attentions: bool = ...): ...

class ProphetNetDecoderLayer(GradientCheckpointingLayer):
    def __init__(self, config: ProphetNetConfig, layer_idx=...) -> None: ...
    def forward(
        self,
        hidden_states,
        attention_mask=...,
        encoder_hidden_states=...,
        encoder_attn_mask=...,
        layer_head_mask=...,
        cross_attn_layer_head_mask=...,
        extended_predict_attention_mask=...,
        main_relative_position_buckets=...,
        predict_relative_position_buckets=...,
        position_ids=...,
        past_key_value=...,
        use_cache: Optional[bool] = ...,
        output_attentions: Optional[bool] = ...,
        cache_position: Optional[torch.Tensor] = ...,
    ): ...

@auto_docstring(custom_intro=...)
class ProphetNetEncoder(ProphetNetPreTrainedModel):
    def __init__(self, config: ProphetNetConfig, word_embeddings: nn.Embedding = ...) -> None: ...
    def get_input_embeddings(self): ...
    def set_input_embeddings(self, value): ...
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.Tensor] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        head_mask: Optional[torch.Tensor] = ...,
        inputs_embeds: Optional[torch.Tensor] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[tuple, BaseModelOutput]: ...

@auto_docstring(custom_intro=...)
class ProphetNetDecoder(ProphetNetPreTrainedModel):
    def __init__(self, config: ProphetNetConfig, word_embeddings: Optional[nn.Embedding] = ...) -> None: ...
    def get_input_embeddings(self): ...
    def set_input_embeddings(self, value): ...
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.Tensor] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        encoder_hidden_states: Optional[torch.Tensor] = ...,
        encoder_attention_mask: Optional[torch.Tensor] = ...,
        head_mask: Optional[torch.Tensor] = ...,
        cross_attn_head_mask: Optional[torch.Tensor] = ...,
        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = ...,
        inputs_embeds: Optional[torch.Tensor] = ...,
        use_cache: Optional[bool] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
        cache_position: Optional[torch.Tensor] = ...,
    ) -> Union[tuple, ProphetNetDecoderModelOutput]: ...
    def compute_buffered_relative_buckets(self, position_ids): ...
    def prepare_attention_mask(self, hidden_states, attention_mask): ...
    def prepare_predict_attention_mask(self, hidden_states, attention_mask): ...

@auto_docstring
class ProphetNetModel(ProphetNetPreTrainedModel):
    _tied_weights_keys = ...
    def __init__(self, config: ProphetNetConfig) -> None: ...
    def get_input_embeddings(self): ...
    def set_input_embeddings(self, value): ...
    def get_encoder(self): ...
    def get_decoder(self): ...
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.Tensor] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        decoder_input_ids: Optional[torch.Tensor] = ...,
        decoder_attention_mask: Optional[torch.BoolTensor] = ...,
        head_mask: Optional[torch.Tensor] = ...,
        decoder_head_mask: Optional[torch.Tensor] = ...,
        cross_attn_head_mask: Optional[torch.Tensor] = ...,
        encoder_outputs: Optional[tuple] = ...,
        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = ...,
        inputs_embeds: Optional[torch.Tensor] = ...,
        decoder_inputs_embeds: Optional[torch.Tensor] = ...,
        use_cache: Optional[bool] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
        cache_position: Optional[torch.Tensor] = ...,
    ) -> Union[tuple, ProphetNetSeq2SeqModelOutput]: ...

@auto_docstring(custom_intro=...)
class ProphetNetForConditionalGeneration(ProphetNetPreTrainedModel, GenerationMixin):
    _tied_weights_keys = ...
    def __init__(self, config: ProphetNetConfig) -> None: ...
    def get_input_embeddings(self): ...
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.Tensor] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        decoder_input_ids: Optional[torch.Tensor] = ...,
        decoder_attention_mask: Optional[torch.BoolTensor] = ...,
        head_mask: Optional[torch.Tensor] = ...,
        decoder_head_mask: Optional[torch.Tensor] = ...,
        cross_attn_head_mask: Optional[torch.Tensor] = ...,
        encoder_outputs: Optional[torch.Tensor] = ...,
        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = ...,
        inputs_embeds: Optional[torch.Tensor] = ...,
        decoder_inputs_embeds: Optional[torch.Tensor] = ...,
        labels: Optional[torch.Tensor] = ...,
        use_cache: Optional[bool] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
        cache_position: Optional[torch.Tensor] = ...,
    ) -> Union[tuple, ProphetNetSeq2SeqLMOutput]: ...
    def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor): ...
    def get_encoder(self): ...
    def get_decoder(self): ...

@auto_docstring(custom_intro=...)
class ProphetNetForCausalLM(ProphetNetPreTrainedModel, GenerationMixin):
    _tied_weights_keys = ...
    def __init__(self, config: ProphetNetConfig) -> None: ...
    def get_input_embeddings(self): ...
    def set_input_embeddings(self, value): ...
    def set_decoder(self, decoder): ...
    def get_decoder(self): ...
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.Tensor] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        encoder_hidden_states: Optional[torch.Tensor] = ...,
        encoder_attention_mask: Optional[torch.Tensor] = ...,
        head_mask: Optional[torch.Tensor] = ...,
        cross_attn_head_mask: Optional[torch.Tensor] = ...,
        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = ...,
        inputs_embeds: Optional[torch.Tensor] = ...,
        labels: Optional[torch.Tensor] = ...,
        use_cache: Optional[bool] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[tuple, ProphetNetDecoderLMOutput]: ...
    def prepare_inputs_for_generation(
        self, input_ids, past_key_values=..., attention_mask=..., head_mask=..., use_cache=..., **kwargs
    ): ...

class ProphetNetDecoderWrapper(ProphetNetPreTrainedModel):
    def __init__(self, config: ProphetNetConfig) -> None: ...
    def forward(self, *args, **kwargs): ...

__all__ = [
    "ProphetNetDecoder",
    "ProphetNetEncoder",
    "ProphetNetForCausalLM",
    "ProphetNetForConditionalGeneration",
    "ProphetNetModel",
    "ProphetNetPreTrainedModel",
]
