"""
This type stub file was generated by pyright.
"""

import torch
from dataclasses import dataclass
from typing import Optional, Union
from torch import nn
from ...cache_utils import Cache
from ...generation import GenerationMixin
from ...modeling_flash_attention_utils import FlashAttentionKwargs
from ...modeling_layers import GradientCheckpointingLayer
from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput
from ...modeling_rope_utils import dynamic_rope_update
from ...modeling_utils import PreTrainedModel
from ...processing_utils import Unpack
from ...utils import TransformersKwargs, auto_docstring
from .configuration_qwen2_5_omni import (
    Qwen2_5OmniAudioEncoderConfig,
    Qwen2_5OmniBigVGANConfig,
    Qwen2_5OmniConfig,
    Qwen2_5OmniDiTConfig,
    Qwen2_5OmniTalkerConfig,
    Qwen2_5OmniTextConfig,
    Qwen2_5OmniThinkerConfig,
    Qwen2_5OmniToken2WavConfig,
    Qwen2_5OmniVisionEncoderConfig,
)

logger = ...

@auto_docstring
class Qwen2_5OmniPreTrainedModel(PreTrainedModel):
    config: Qwen2_5OmniConfig
    base_model_prefix = ...
    supports_gradient_checkpointing = ...
    _no_split_modules = ...
    _skip_keys_device_placement = ...
    _supports_flash_attn = ...
    _supports_sdpa = ...
    _can_compile_fullgraph = ...
    _supports_attention_backend = ...

class Qwen2_5OmniPreTrainedModelForConditionalGeneration(Qwen2_5OmniPreTrainedModel):
    def get_llm_pos_ids_for_vision(
        self,
        start_idx: int,
        vision_idx: int,
        spatial_merge_size: int,
        t_index: list[int],
        grid_hs: list[int],
        grid_ws: list[int],
    ): ...
    def get_chunked_index(
        self, token_indices: torch.Tensor, tokens_per_chunk: int, remove_index: int
    ) -> list[tuple[int, int]]: ...
    def get_rope_index(
        self,
        input_ids: Optional[torch.LongTensor] = ...,
        image_grid_thw: Optional[torch.LongTensor] = ...,
        video_grid_thw: Optional[torch.LongTensor] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        use_audio_in_video: bool = ...,
        audio_seqlens: Optional[torch.LongTensor] = ...,
        second_per_grids: Optional[torch.Tensor] = ...,
    ) -> tuple[torch.Tensor, torch.Tensor]: ...

@dataclass
@auto_docstring(custom_intro=...)
class Qwen2_5OmniThinkerCausalLMOutputWithPast(ModelOutput):
    loss: Optional[torch.FloatTensor] = ...
    logits: Optional[torch.FloatTensor] = ...
    past_key_values: Optional[list[torch.FloatTensor]] = ...
    hidden_states: Optional[tuple[torch.FloatTensor]] = ...
    attentions: Optional[tuple[torch.FloatTensor]] = ...
    rope_deltas: Optional[torch.LongTensor] = ...

def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor: ...
def eager_attention_forward(
    module: nn.Module,
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: Optional[torch.Tensor],
    scaling: float,
    dropout: float = ...,
    **kwargs,
): ...

class Qwen2_5OmniAudioAttention(nn.Module):
    def __init__(self, config: Qwen2_5OmniAudioEncoderConfig) -> None: ...
    def forward(
        self,
        hidden_states: torch.Tensor,
        cu_seqlens: Optional[torch.Tensor] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        **kwargs,
    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]: ...

class Qwen2_5OmniAudioEncoderLayer(GradientCheckpointingLayer):
    def __init__(self, config: Qwen2_5OmniAudioEncoderConfig) -> None: ...
    def forward(
        self,
        hidden_states: torch.Tensor,
        cu_seqlens: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = ...,
        **kwargs,
    ) -> torch.Tensor: ...

class SinusoidsPositionEmbedding(nn.Module):
    def __init__(self, length, channels, max_timescale=...) -> None: ...
    def forward(self, seqlen: int): ...

@auto_docstring(custom_intro=...)
class Qwen2_5OmniAudioEncoder(Qwen2_5OmniPreTrainedModel):
    config: Qwen2_5OmniAudioEncoderConfig
    main_input_name = ...
    _no_split_modules = ...
    _supports_sdpa = ...
    def __init__(self, config: Qwen2_5OmniAudioEncoderConfig) -> None: ...
    def get_input_embeddings(self) -> nn.Module: ...
    def set_input_embeddings(self, value: nn.Module): ...
    @auto_docstring
    def forward(self, input_features, feature_lens=..., aftercnn_lens=..., **kwargs): ...
    def padded_and_mask_function(self, tensor_list, tensor_len, padding_value=..., padding_side=...): ...

def rotate_half(x): ...
def apply_rotary_pos_emb_vision(tensor: torch.Tensor, freqs: torch.Tensor) -> torch.Tensor: ...

class Qwen2_5OmniVisionAttention(nn.Module):
    def __init__(self, config: Qwen2_5OmniVisionEncoderConfig = ...) -> None: ...
    def forward(
        self,
        hidden_states: torch.Tensor,
        cu_seqlens: torch.Tensor,
        rotary_pos_emb: Optional[torch.Tensor] = ...,
        **kwargs,
    ) -> torch.Tensor: ...

class Qwen2_5OmniMLP(nn.Module):
    def __init__(self, config, bias: bool = ...) -> None: ...
    def forward(self, hidden_state): ...

class Qwen2RMSNorm(nn.Module):
    def __init__(self, hidden_size, eps=...) -> None: ...
    def forward(self, hidden_states): ...
    def extra_repr(self): ...

class Qwen2_5OmniVisionBlock(GradientCheckpointingLayer):
    def __init__(self, config: Qwen2_5OmniVisionEncoderConfig) -> None: ...
    def forward(
        self,
        hidden_states: torch.Tensor,
        cu_seqlens: torch.Tensor,
        rotary_pos_emb: Optional[torch.Tensor] = ...,
        **kwargs,
    ) -> torch.Tensor: ...

class Qwen2_5_VisionPatchEmbed(nn.Module):
    def __init__(
        self, patch_size: int = ..., temporal_patch_size: int = ..., in_channels: int = ..., embed_dim: int = ...
    ) -> None: ...
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor: ...

class Qwen2_5_VisionRotaryEmbedding(nn.Module):
    def __init__(self, dim: int, theta: float = ...) -> None: ...
    def forward(self, seqlen: int) -> torch.Tensor: ...

class Qwen2_5OmniPatchMerger(nn.Module):
    def __init__(self, dim: int, context_dim: int, spatial_merge_size: int = ...) -> None: ...
    def forward(self, x: torch.Tensor) -> torch.Tensor: ...

class Qwen2_5OmniVisionEncoder(Qwen2_5OmniPreTrainedModel):
    config: Qwen2_5OmniVisionEncoderConfig
    _no_split_modules = ...
    def __init__(self, config: Qwen2_5OmniVisionEncoderConfig, *inputs, **kwargs) -> None: ...
    def rot_pos_emb(self, grid_thw): ...
    def get_window_index(self, grid_thw): ...
    def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor, **kwargs) -> torch.Tensor: ...

class Qwen2_5OmniRotaryEmbedding(nn.Module):
    def __init__(self, config: Qwen2_5OmniThinkerConfig, device=...) -> None: ...
    @torch.no_grad()
    @dynamic_rope_update
    def forward(self, x, position_ids): ...

def apply_multimodal_rotary_pos_emb(q, k, cos, sin, mrope_section, unsqueeze_dim=...): ...

class Qwen2_5OmniAttention(nn.Module):
    def __init__(self, config: Qwen2_5OmniConfig, layer_idx: Optional[int] = ...) -> None: ...
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = ...,
        position_ids: Optional[torch.LongTensor] = ...,
        past_key_value: Optional[Cache] = ...,
        output_attentions: bool = ...,
        use_cache: bool = ...,
        cache_position: Optional[torch.LongTensor] = ...,
        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = ...,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]: ...

class Qwen2MLP(nn.Module):
    def __init__(self, config, bias: bool = ...) -> None: ...
    def forward(self, hidden_state): ...

class Qwen2_5OmniDecoderLayer(GradientCheckpointingLayer):
    def __init__(self, config: Qwen2_5OmniTextConfig, layer_idx: int) -> None: ...
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = ...,
        position_ids: Optional[torch.LongTensor] = ...,
        past_key_value: Optional[tuple[torch.Tensor]] = ...,
        output_attentions: Optional[bool] = ...,
        use_cache: Optional[bool] = ...,
        cache_position: Optional[torch.LongTensor] = ...,
        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = ...,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]: ...

@auto_docstring
class Qwen2_5OmniThinkerTextModel(Qwen2_5OmniPreTrainedModel):
    config: Qwen2_5OmniTextConfig
    _no_split_modules = ...
    def __init__(self, config: Qwen2_5OmniTextConfig) -> None: ...
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        position_ids: Optional[torch.LongTensor] = ...,
        past_key_values: Optional[Cache] = ...,
        inputs_embeds: Optional[torch.FloatTensor] = ...,
        use_cache: Optional[bool] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
        cache_position: Optional[torch.LongTensor] = ...,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> Union[tuple, BaseModelOutputWithPast]: ...

@auto_docstring(custom_intro=...)
class Qwen2_5OmniThinkerForConditionalGeneration(Qwen2_5OmniPreTrainedModelForConditionalGeneration, GenerationMixin):
    config: Qwen2_5OmniThinkerConfig
    base_model_prefix = ...
    _tied_weights_keys = ...
    _no_split_modules = ...
    def __init__(self, config: Qwen2_5OmniThinkerConfig) -> None: ...
    def get_input_embeddings(self): ...
    def set_input_embeddings(self, value): ...
    def set_decoder(self, decoder): ...
    def get_decoder(self): ...
    def get_video_features(
        self, pixel_values_videos: torch.FloatTensor, video_grid_thw: Optional[torch.LongTensor] = ...
    ): ...
    def get_image_features(self, pixel_values: torch.FloatTensor, image_grid_thw: Optional[torch.LongTensor] = ...): ...
    def get_audio_features(
        self,
        input_features: torch.FloatTensor,
        feature_attention_mask: Optional[torch.LongTensor] = ...,
        audio_feature_lengths: Optional[torch.LongTensor] = ...,
    ): ...
    def get_placeholder_mask(
        self,
        input_ids: torch.LongTensor,
        inputs_embeds: torch.FloatTensor,
        image_features: torch.FloatTensor = ...,
        video_features: torch.FloatTensor = ...,
    ): ...
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = ...,
        input_features: Optional[torch.FloatTensor] = ...,
        pixel_values: Optional[torch.FloatTensor] = ...,
        pixel_values_videos: Optional[torch.FloatTensor] = ...,
        image_grid_thw: Optional[torch.LongTensor] = ...,
        video_grid_thw: Optional[torch.LongTensor] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        feature_attention_mask: Optional[torch.Tensor] = ...,
        audio_feature_lengths: Optional[torch.LongTensor] = ...,
        position_ids: Optional[torch.LongTensor] = ...,
        past_key_values: Optional[Cache] = ...,
        inputs_embeds: Optional[torch.FloatTensor] = ...,
        rope_deltas: Optional[torch.LongTensor] = ...,
        labels: Optional[torch.LongTensor] = ...,
        use_cache: Optional[bool] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
        use_audio_in_video: Optional[bool] = ...,
        cache_position: Optional[torch.LongTensor] = ...,
        video_second_per_grid: Optional[torch.LongTensor] = ...,
        **kwargs: Unpack[TransformersKwargs],
    ) -> Union[tuple, Qwen2_5OmniThinkerCausalLMOutputWithPast]: ...
    def prepare_inputs_for_generation(
        self,
        input_ids,
        past_key_values=...,
        attention_mask=...,
        inputs_embeds=...,
        cache_position=...,
        position_ids=...,
        use_cache=...,
        pixel_values=...,
        pixel_values_videos=...,
        image_grid_thw=...,
        video_grid_thw=...,
        input_features=...,
        feature_attention_mask=...,
        use_audio_in_video=...,
        video_second_per_grid=...,
        **kwargs,
    ): ...

@dataclass
@auto_docstring(custom_intro=...)
class Qwen2_5OmniTalkerCausalLMOutputWithPast(ModelOutput):
    loss: Optional[torch.FloatTensor] = ...
    logits: torch.FloatTensor = ...
    past_key_values: Optional[list[torch.FloatTensor]] = ...
    hidden_states: Optional[tuple[torch.FloatTensor]] = ...
    attentions: Optional[tuple[torch.FloatTensor]] = ...
    rope_deltas: Optional[torch.LongTensor] = ...
    thinker_reply_part: torch.FloatTensor = ...

@auto_docstring
class Qwen2_5OmniTalkerModel(Qwen2_5OmniPreTrainedModel):
    config: Qwen2_5OmniTalkerConfig
    _no_split_modules = ...
    def __init__(self, config: Qwen2_5OmniTalkerConfig) -> None: ...
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        position_ids: Optional[torch.LongTensor] = ...,
        past_key_values: Optional[Cache] = ...,
        inputs_embeds: Optional[torch.FloatTensor] = ...,
        use_cache: Optional[bool] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
        cache_position: Optional[torch.LongTensor] = ...,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> Union[tuple, BaseModelOutputWithPast]: ...

class Qwen2_5OmniTalkerForConditionalGeneration(Qwen2_5OmniPreTrainedModelForConditionalGeneration, GenerationMixin):
    config: Qwen2_5OmniTalkerConfig
    base_model_prefix = ...
    def __init__(self, config: Qwen2_5OmniTalkerConfig) -> None: ...
    def get_input_embeddings(self): ...
    def set_input_embeddings(self, value): ...
    @auto_docstring
    def forward(
        self,
        input_ids: torch.LongTensor = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        position_ids: Optional[torch.LongTensor] = ...,
        past_key_values: Optional[Cache] = ...,
        thinker_reply_part: Optional[torch.FloatTensor] = ...,
        inputs_embeds: Optional[torch.FloatTensor] = ...,
        rope_deltas: Optional[torch.LongTensor] = ...,
        use_cache: Optional[bool] = ...,
        cache_position: Optional[torch.LongTensor] = ...,
        input_text_ids: Optional[torch.LongTensor] = ...,
        image_grid_thw: Optional[torch.LongTensor] = ...,
        video_grid_thw: Optional[torch.LongTensor] = ...,
        use_audio_in_video: Optional[bool] = ...,
        audio_feature_lengths: Optional[torch.LongTensor] = ...,
        video_second_per_grid: Optional[torch.LongTensor] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[tuple, Qwen2_5OmniTalkerCausalLMOutputWithPast]: ...
    def prepare_inputs_for_generation(
        self,
        input_ids,
        input_text_ids,
        past_key_values=...,
        attention_mask=...,
        inputs_embeds=...,
        thinker_reply_part=...,
        cache_position=...,
        position_ids=...,
        use_cache=...,
        pixel_values=...,
        pixel_values_videos=...,
        image_grid_thw=...,
        video_grid_thw=...,
        input_audio_features=...,
        audio_feature_attention_mask=...,
        audio_feature_lengths=...,
        use_audio_in_video=...,
        video_second_per_grid=...,
        **kwargs,
    ): ...

class Qwen2_5OmniDiTRotaryEmbedding(nn.Module):
    def __init__(self, dim, base=...) -> None: ...
    def forward(self, x): ...

class TimeDelayNetBlock(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, dilation) -> None: ...
    def forward(self, hidden_states: torch.Tensor): ...

class Res2NetBlock(torch.nn.Module):
    def __init__(self, in_channels, out_channels, scale=..., kernel_size=..., dilation=...) -> None: ...
    def forward(self, hidden_states): ...

class SqueezeExcitationBlock(nn.Module):
    def __init__(self, in_channels, se_channels, out_channels) -> None: ...
    def forward(self, hidden_states): ...

class AttentiveStatisticsPooling(nn.Module):
    def __init__(self, channels, attention_channels=...) -> None: ...
    def forward(self, hidden_states): ...

class SqueezeExcitationRes2NetBlock(nn.Module):
    def __init__(
        self, in_channels, out_channels, res2net_scale=..., se_channels=..., kernel_size=..., dilation=...
    ) -> None: ...
    def forward(self, hidden_state): ...

class ECAPA_TimeDelayNet(torch.nn.Module):
    def __init__(self, config: Qwen2_5OmniDiTConfig) -> None: ...
    def forward(self, hidden_states): ...

class DiTInputEmbedding(nn.Module):
    def __init__(self, config: Qwen2_5OmniDiTConfig) -> None: ...
    def forward(
        self,
        hidden_states: torch.Tensor,
        speaker_embedding: torch.Tensor,
        condition_vector: torch.Tensor,
        code_embed: torch.Tensor,
        drop_audio_cond: Optional[bool] = ...,
        code_embed_uncond: Optional[bool] = ...,
        apply_cfg: Optional[bool] = ...,
    ): ...

class DiTCodecEmbedding(nn.Module):
    def __init__(self, codec_num_embeds, codec_dim, repeats) -> None: ...
    def forward(self, code, drop_code=...): ...

class Qwen2_5_OmniAdaLayerNormZero(nn.Module):
    def __init__(self, dim) -> None: ...
    def forward(self, hidden_states, emb=...): ...

class Qwen2_5_OmniAdaLayerNormZero_Final(nn.Module):
    def __init__(self, dim) -> None: ...
    def forward(self, hidden_states, emb): ...

class DiTMLP(nn.Module):
    def __init__(self, dim, mult=..., dropout=...) -> None: ...
    def forward(self, hidden_states): ...

def apply_rotary_pos_emb(q, k, cos, sin, position_ids=..., unsqueeze_dim=...): ...

class DiTAttention(nn.Module):
    def __init__(self, config: Qwen2_5OmniDiTConfig) -> None: ...
    def forward(self, hidden_states, position_embeddings=..., attention_mask=...) -> torch.Tensor: ...

class SinusPositionEmbedding(nn.Module):
    def __init__(self, dim) -> None: ...
    def forward(self, hidden_states, scale=...): ...

class DiTTimestepEmbedding(nn.Module):
    def __init__(self, dim, freq_embed_dim=...) -> None: ...
    def forward(self, timestep): ...

class DiTDecoderLayer(nn.Module):
    def __init__(self, config: Qwen2_5OmniDiTConfig, look_ahead_block=..., look_backward_block=...) -> None: ...
    def forward(self, hidden_states, timestep, position_embeddings=..., block_diff=...): ...

class SnakeBeta(nn.Module):
    def __init__(self, in_features, alpha=...) -> None: ...
    def forward(self, hidden_states): ...

def kaiser_sinc_filter1d(cutoff, half_width, kernel_size): ...

class UpSample1d(nn.Module):
    def __init__(self, ratio=..., kernel_size=...) -> None: ...
    def forward(self, hidden_states): ...

class DownSample1d(nn.Module):
    def __init__(self, ratio=..., kernel_size=...) -> None: ...
    def forward(self, hidden_states): ...

class TorchActivation1d(nn.Module):
    def __init__(
        self,
        activation,
        up_ratio: int = ...,
        down_ratio: int = ...,
        up_kernel_size: int = ...,
        down_kernel_size: int = ...,
    ) -> None: ...
    def forward(self, hidden_states): ...

class AMPBlock(torch.nn.Module):
    def __init__(self, channels, kernel_size=..., dilation=...) -> None: ...
    def forward(self, hidden_states): ...

@auto_docstring(custom_intro=...)
class Qwen2_5OmniToken2WavBigVGANModel(Qwen2_5OmniPreTrainedModel):
    config: Qwen2_5OmniBigVGANConfig
    def __init__(self, config: Qwen2_5OmniBigVGANConfig) -> None: ...
    def normalize_spectrogram(self, spectrogram, max_value, min_db): ...
    def amplitude_to_db(self, amplitude, min_db_level): ...
    def process_mel_spectrogram(self, mel_spectrogram): ...
    def forward(self, mel_spectrogram): ...

class RungeKutta4ODESolver:
    def __init__(self, function, initial_value) -> None: ...
    def integrate(self, time_points): ...

@auto_docstring(custom_intro=...)
class Qwen2_5OmniToken2WavDiTModel(Qwen2_5OmniPreTrainedModel):
    config: Qwen2_5OmniDiTConfig
    _no_split_modules = ...
    def __init__(self, config: Qwen2_5OmniDiTConfig) -> None: ...
    def forward(
        self,
        hidden_states,
        condition_vector,
        speaker_embedding,
        quantized_code,
        time_step,
        drop_audio_conditioning=...,
        drop_code=...,
        apply_cfg=...,
    ): ...
    @torch.no_grad()
    def sample(
        self,
        conditioning_vector,
        reference_mel_spectrogram,
        quantized_code,
        num_steps=...,
        guidance_scale=...,
        sway_coefficient=...,
    ): ...

@auto_docstring(custom_intro=...)
class Qwen2_5OmniToken2WavModel(Qwen2_5OmniPreTrainedModel):
    config: Qwen2_5OmniToken2WavConfig
    base_model_prefix = ...
    _no_split_modules = ...
    def __init__(self, config: Qwen2_5OmniToken2WavConfig) -> None: ...
    def forward(
        self, code, conditioning, reference_mel, num_steps=..., guidance_scale=..., sway_coefficient=..., **kwargs
    ): ...

@auto_docstring(custom_intro=...)
class Qwen2_5OmniForConditionalGeneration(Qwen2_5OmniPreTrainedModel, GenerationMixin):
    config: Qwen2_5OmniConfig
    _no_split_modules = ...
    def __init__(self, config) -> None: ...
    def enable_talker(self): ...
    def load_speakers(self, path): ...
    def disable_talker(self): ...
    @classmethod
    def from_pretrained(
        cls,
        pretrained_model_name_or_path,
        *model_args,
        config=...,
        cache_dir=...,
        ignore_mismatched_sizes=...,
        force_download=...,
        local_files_only=...,
        token=...,
        revision=...,
        use_safetensors=...,
        weights_only=...,
        **kwargs,
    ): ...
    @torch.no_grad()
    def generate(
        self,
        input_ids: Optional[torch.Tensor] = ...,
        speaker: str = ...,
        use_audio_in_video: bool = ...,
        return_audio: Optional[bool] = ...,
        thinker_max_new_tokens: int = ...,
        talker_max_new_tokens: int = ...,
        talker_do_sample: bool = ...,
        talker_top_k: int = ...,
        talker_top_p: float = ...,
        talker_temperature: float = ...,
        talker_eos_token_id: list[int] = ...,
        talker_repetition_penalty: float = ...,
        **kwargs,
    ): ...

__all__ = [
    "Qwen2_5OmniForConditionalGeneration",
    "Qwen2_5OmniThinkerTextModel",
    "Qwen2_5OmniThinkerForConditionalGeneration",
    "Qwen2_5OmniTalkerModel",
    "Qwen2_5OmniTalkerForConditionalGeneration",
    "Qwen2_5OmniToken2WavDiTModel",
    "Qwen2_5OmniToken2WavBigVGANModel",
    "Qwen2_5OmniToken2WavModel",
    "Qwen2_5OmniPreTrainedModel",
    "Qwen2_5OmniPreTrainedModelForConditionalGeneration",
]
