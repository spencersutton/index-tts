"""
This type stub file was generated by pyright.
"""

import torch
import torch.nn as nn
from typing import Optional, Union
from transformers.models.qwen2_vl.configuration_qwen2_vl import Qwen2VLConfig, Qwen2VLTextConfig
from transformers.models.qwen2_vl.modeling_qwen2_vl import (
    PatchEmbed,
    PatchMerger,
    Qwen2VLCausalLMOutputWithPast,
    Qwen2VLForConditionalGeneration,
    Qwen2VLModel,
    Qwen2VLModelOutputWithPast,
    Qwen2VLPreTrainedModel,
    TransformersKwargs,
    VisionAttention,
    VisionRotaryEmbedding,
)
from transformers.models.qwen2_vl.processing_qwen2_vl import Qwen2VLImagesKwargs, Qwen2VLProcessor
from ...cache_utils import Cache
from ...configuration_utils import PretrainedConfig
from ...feature_extraction_utils import BatchFeature
from ...image_utils import ImageInput
from ...modeling_flash_attention_utils import is_flash_attn_available
from ...modeling_layers import GradientCheckpointingLayer
from ...processing_utils import ProcessingKwargs, Unpack, VideosKwargs
from ...tokenization_utils_base import PreTokenizedInput, TextInput
from ...video_utils import VideoInput

if is_flash_attn_available(): ...
logger = ...

class Qwen2_5_VLVisionConfig(PretrainedConfig):
    model_type = ...
    base_config_key = ...
    def __init__(
        self,
        depth=...,
        hidden_size=...,
        hidden_act=...,
        intermediate_size=...,
        num_heads=...,
        in_channels=...,
        patch_size=...,
        spatial_merge_size=...,
        temporal_patch_size=...,
        tokens_per_second=...,
        window_size=...,
        out_hidden_size=...,
        fullatt_block_indexes=...,
        initializer_range=...,
        **kwargs,
    ) -> None: ...

class Qwen2_5_VLTextConfig(Qwen2VLTextConfig):
    model_type = ...

class Qwen2_5_VLConfig(Qwen2VLConfig):
    model_type = ...
    sub_configs = ...

class Qwen2_5_VLMLP(nn.Module):
    def __init__(self, config, bias: bool = ...) -> None: ...
    def forward(self, hidden_state): ...

class Qwen2_5_VisionPatchEmbed(PatchEmbed): ...
class Qwen2_5_VisionRotaryEmbedding(VisionRotaryEmbedding): ...

class Qwen2_5_VLPatchMerger(PatchMerger):
    def __init__(self, dim: int, context_dim: int, spatial_merge_size: int = ...) -> None: ...

class Qwen2_5_VLVisionAttention(VisionAttention):
    def __init__(self, config: Qwen2_5_VLVisionConfig) -> None: ...

class Qwen2_5_VLVisionBlock(GradientCheckpointingLayer):
    def __init__(self, config, attn_implementation: str = ...) -> None: ...
    def forward(
        self,
        hidden_states: torch.Tensor,
        cu_seqlens: torch.Tensor,
        rotary_pos_emb: Optional[torch.Tensor] = ...,
        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = ...,
        **kwargs,
    ) -> torch.Tensor: ...

class Qwen2_5_VLPreTrainedModel(Qwen2VLPreTrainedModel): ...

class Qwen2_5_VisionTransformerPretrainedModel(Qwen2_5_VLPreTrainedModel):
    config: Qwen2_5_VLVisionConfig
    _no_split_modules = ...
    def __init__(self, config, *inputs, **kwargs) -> None: ...
    def rot_pos_emb(self, grid_thw): ...
    def get_window_index(self, grid_thw): ...
    def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor, **kwargs) -> torch.Tensor: ...

class Qwen2_5_VLModelOutputWithPast(Qwen2VLModelOutputWithPast): ...

class Qwen2_5_VLModel(Qwen2VLModel):
    config: Qwen2_5_VLConfig
    base_model_prefix = ...
    _no_split_modules = ...
    def __init__(self, config) -> None: ...
    def get_rope_index(
        self,
        input_ids: Optional[torch.LongTensor] = ...,
        image_grid_thw: Optional[torch.LongTensor] = ...,
        video_grid_thw: Optional[torch.LongTensor] = ...,
        second_per_grid_ts: Optional[torch.Tensor] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
    ) -> tuple[torch.Tensor, torch.Tensor]: ...
    def forward(
        self,
        input_ids: torch.LongTensor = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        position_ids: Optional[torch.LongTensor] = ...,
        past_key_values: Optional[Cache] = ...,
        inputs_embeds: Optional[torch.FloatTensor] = ...,
        use_cache: Optional[bool] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
        pixel_values: Optional[torch.Tensor] = ...,
        pixel_values_videos: Optional[torch.FloatTensor] = ...,
        image_grid_thw: Optional[torch.LongTensor] = ...,
        video_grid_thw: Optional[torch.LongTensor] = ...,
        rope_deltas: Optional[torch.LongTensor] = ...,
        cache_position: Optional[torch.LongTensor] = ...,
        second_per_grid_ts: Optional[torch.Tensor] = ...,
        **kwargs: Unpack[TransformersKwargs],
    ) -> Union[tuple, Qwen2_5_VLModelOutputWithPast]: ...

class Qwen2_5_VLCausalLMOutputWithPast(Qwen2VLCausalLMOutputWithPast): ...

class Qwen2_5_VLForConditionalGeneration(Qwen2VLForConditionalGeneration):
    def forward(
        self,
        input_ids: torch.LongTensor = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        position_ids: Optional[torch.LongTensor] = ...,
        past_key_values: Optional[Cache] = ...,
        inputs_embeds: Optional[torch.FloatTensor] = ...,
        labels: Optional[torch.LongTensor] = ...,
        use_cache: Optional[bool] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        pixel_values: Optional[torch.Tensor] = ...,
        pixel_values_videos: Optional[torch.FloatTensor] = ...,
        image_grid_thw: Optional[torch.LongTensor] = ...,
        video_grid_thw: Optional[torch.LongTensor] = ...,
        rope_deltas: Optional[torch.LongTensor] = ...,
        cache_position: Optional[torch.LongTensor] = ...,
        second_per_grid_ts: Optional[torch.Tensor] = ...,
        logits_to_keep: Union[int, torch.Tensor] = ...,
        **kwargs: Unpack[TransformersKwargs],
    ) -> Union[tuple, Qwen2_5_VLCausalLMOutputWithPast]: ...
    def prepare_inputs_for_generation(
        self,
        input_ids,
        past_key_values=...,
        attention_mask=...,
        inputs_embeds=...,
        cache_position=...,
        position_ids=...,
        use_cache=...,
        pixel_values=...,
        pixel_values_videos=...,
        image_grid_thw=...,
        video_grid_thw=...,
        second_per_grid_ts=...,
        **kwargs,
    ): ...

class Qwen2_5_VLVideosProcessorKwargs(VideosKwargs, total=False):
    fps: Union[list[float], float]
    ...

class Qwen2_5_VLImagesKwargs(Qwen2VLImagesKwargs): ...

class Qwen2_5_VLProcessorKwargs(ProcessingKwargs, total=False):
    images_kwargs: Qwen2_5_VLImagesKwargs
    videos_kwargs: Qwen2_5_VLVideosProcessorKwargs
    _defaults = ...

class Qwen2_5_VLProcessor(Qwen2VLProcessor):
    image_processor_class = ...
    @property
    def model_input_names(self): ...
    def __call__(
        self,
        images: ImageInput = ...,
        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = ...,
        videos: VideoInput = ...,
        **kwargs: Unpack[Qwen2_5_VLProcessorKwargs],
    ) -> BatchFeature: ...

__all__ = [
    "Qwen2_5_VLConfig",
    "Qwen2_5_VLTextConfig",
    "Qwen2_5_VLForConditionalGeneration",
    "Qwen2_5_VLModel",
    "Qwen2_5_VLPreTrainedModel",
    "Qwen2_5_VLProcessor",
    "Qwen2_5_VLTextModel",
]
