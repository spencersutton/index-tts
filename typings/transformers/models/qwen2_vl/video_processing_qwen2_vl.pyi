"""
This type stub file was generated by pyright.
"""

from typing import Optional, Union

import torch

from ...processing_utils import Unpack, VideosKwargs
from ...utils import add_start_docstrings, is_torch_available, is_torchvision_available, is_vision_available
from ...utils.import_utils import requires
from ...video_processing_utils import BASE_VIDEO_PROCESSOR_DOCSTRING, BaseVideoProcessor
from ...video_utils import VideoMetadata

"""video processor class for Qwen2-VL."""
if is_vision_available(): ...
if is_torchvision_available(): ...
if is_torch_available(): ...

class Qwen2VLVideoProcessorInitKwargs(VideosKwargs):
    min_pixels: int | None
    max_pixels: int | None
    patch_size: int | None
    temporal_patch_size: int | None
    merge_size: int | None
    min_frames: int | None
    max_frames: int | None
    ...

@add_start_docstrings(
    "Constructs a fast Qwen2-VL image processor that dynamically resizes videos based on the original videos.",
    BASE_VIDEO_PROCESSOR_DOCSTRING,
    """
        min_pixels (`int`, *optional*, defaults to `56 * 56`):
            The min pixels of the image to resize the image.
        max_pixels (`int`, *optional*, defaults to `28 * 28 * 1280`):
            The max pixels of the image to resize the image.
        patch_size (`int`, *optional*, defaults to 14):
            The spacial patch size of the vision encoder.
        temporal_patch_size (`int`, *optional*, defaults to 2):
            The temporal patch size of the vision encoder.
        merge_size (`int`, *optional*, defaults to 2):
            The merge size of the vision encoder to llm encoder.
        min_frames (`int`, *optional*, defaults to 4):
            The minimum number of frames that can be sampled.
        max_frames (`int`, *optional*, defaults to 768):
            The maximum number of frames that can be sampled.
    """,
)
@requires(backends=("torchvision",))
class Qwen2VLVideoProcessor(BaseVideoProcessor):
    resample = ...
    size = ...
    image_mean = ...
    image_std = ...
    do_resize = ...
    do_rescale = ...
    do_normalize = ...
    do_convert_rgb = ...
    min_pixels = ...
    max_pixels = ...
    patch_size = ...
    temporal_patch_size = ...
    merge_size = ...
    min_frames = ...
    max_frames = ...
    do_sample_frames = ...
    valid_kwargs = Qwen2VLVideoProcessorInitKwargs
    model_input_names = ...
    def __init__(self, **kwargs: Unpack[Qwen2VLVideoProcessorInitKwargs]) -> None: ...
    def sample_frames(
        self,
        video: torch.Tensor,
        frame_factor: int,
        min_frames: int,
        max_frames: int,
        metadata: VideoMetadata | dict | None = ...,
        num_frames: int | None = ...,
        fps: int | float | None = ...,
    ):  # -> Tensor:
        """
        Default sampling function which uniformly samples the desired number of frames between 0 and total number of frames.
        If `fps` is passed along with metadata, `fps` frames per second are sampled uniformty. Arguments `num_frames`
        and `fps` are mutually exclusive.

        Args:
            video (`torch.Tensor`):
                Video that need to be sampled.
            frame_factor (`int`):
                The temporal patch size of the vision encoder. Number of sampled frames will be rounded to be divisible by frame factor.
            min_frames (`int`):
                The minimum number of frames that can be sampled.
            max_frames (`int`):
                The maximum number of frames that can be sampled.
            metadata (`VideoMetadata`, *optional*):
                Metadata of the video containing information about total duration, fps and total number of frames.
            num_frames (`int`, *optional*):
                Maximum number of frames to sample. Defaults to `self.num_frames`.
            fps (`int` or `float`, *optional*):
                Target frames to sample per second. Defaults to `self.fps`.

        Returns:
            torch.Tensor:
                Sampled video frames.
        """
        ...

    def get_num_of_video_patches(self, num_frames: int, height: int, width: int, videos_kwargs=...):  # -> int:
        """
        A utility that returns number of video patches a given video size.

        Args:
            num_frames (`int`):
                Number of frames in the input video.
            height (`int`):
                Height of the input video.
            width (`int`):
                Width of the input video.
            videos_kwargs (`dict`, *optional*)
                Any kwargs to override defaults of the video processor.
        Returns:
            `Tuple(int, int)`: Number of placeholder tokens required and number of patches per image.
        """
        ...

__all__ = ["Qwen2VLVideoProcessor"]
