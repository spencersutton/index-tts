import torch

from ...processing_utils import Unpack, VideosKwargs
from ...utils import add_start_docstrings, is_torch_available, is_torchvision_available, is_vision_available
from ...utils.import_utils import requires
from ...video_processing_utils import BASE_VIDEO_PROCESSOR_DOCSTRING, BaseVideoProcessor
from ...video_utils import VideoMetadata

"""video processor class for Qwen2-VL."""
if is_vision_available(): ...
if is_torchvision_available(): ...
if is_torch_available(): ...

class Qwen2VLVideoProcessorInitKwargs(VideosKwargs):
    min_pixels: int | None
    max_pixels: int | None
    patch_size: int | None
    temporal_patch_size: int | None
    merge_size: int | None
    min_frames: int | None
    max_frames: int | None

@add_start_docstrings(
    ...,
    BASE_VIDEO_PROCESSOR_DOCSTRING,
    ...,
)
@requires(backends=("torchvision",))
class Qwen2VLVideoProcessor(BaseVideoProcessor):
    resample = ...
    size = ...
    image_mean = ...
    image_std = ...
    do_resize = ...
    do_rescale = ...
    do_normalize = ...
    do_convert_rgb = ...
    min_pixels = ...
    max_pixels = ...
    patch_size = ...
    temporal_patch_size = ...
    merge_size = ...
    min_frames = ...
    max_frames = ...
    do_sample_frames = ...
    valid_kwargs = Qwen2VLVideoProcessorInitKwargs
    model_input_names = ...
    def __init__(self, **kwargs: Unpack[Qwen2VLVideoProcessorInitKwargs]) -> None: ...
    def sample_frames(
        self,
        video: torch.Tensor,
        frame_factor: int,
        min_frames: int,
        max_frames: int,
        metadata: VideoMetadata | dict | None = ...,
        num_frames: int | None = ...,
        fps: float | None = ...,
    ):  # -> Tensor:

        ...
    def get_num_of_video_patches(self, num_frames: int, height: int, width: int, videos_kwargs=...):  # -> int:

        ...

__all__ = ["Qwen2VLVideoProcessor"]
