import numpy as np

from ...tokenization_utils import PreTrainedTokenizer
from ...tokenization_utils_base import BatchEncoding
from ...utils import is_datasets_available, is_faiss_available

"""RAG Retriever model implementation."""
if is_datasets_available(): ...
if is_faiss_available(): ...
logger = ...
LEGACY_INDEX_PATH = ...

class Index:
    def get_doc_dicts(self, doc_ids: np.ndarray) -> list[dict]: ...
    def get_top_docs(self, question_hidden_states: np.ndarray, n_docs=...) -> tuple[np.ndarray, np.ndarray]: ...
    def is_initialized(self): ...
    def init_index(self): ...

class LegacyIndex(Index):
    INDEX_FILENAME = ...
    PASSAGE_FILENAME = ...
    def __init__(self, vector_size, index_path) -> None: ...
    def is_initialized(self):  # -> bool:
        ...
    def init_index(self):  # -> None:
        ...
    def get_doc_dicts(self, doc_ids: np.array):  # -> list[Any]:
        ...
    def get_top_docs(self, question_hidden_states: np.ndarray, n_docs=...) -> tuple[np.ndarray, np.ndarray]: ...

class HFIndexBase(Index):
    def __init__(self, vector_size, dataset, index_initialized=...) -> None: ...
    def init_index(self): ...
    def is_initialized(self):  # -> bool:
        ...
    def get_doc_dicts(self, doc_ids: np.ndarray) -> list[dict]: ...
    def get_top_docs(self, question_hidden_states: np.ndarray, n_docs=...) -> tuple[np.ndarray, np.ndarray]: ...

class CanonicalHFIndex(HFIndexBase):
    def __init__(
        self,
        vector_size: int,
        dataset_name: str = ...,
        dataset_split: str = ...,
        index_name: str | None = ...,
        index_path: str | None = ...,
        use_dummy_dataset=...,
        dataset_revision=...,
    ) -> None: ...
    def init_index(self):  # -> None:
        ...

class CustomHFIndex(HFIndexBase):
    def __init__(self, vector_size: int, dataset, index_path=...) -> None: ...
    @classmethod
    def load_from_disk(cls, vector_size, dataset_path, index_path):  # -> Self:
        ...
    def init_index(self):  # -> None:
        ...

class RagRetriever:
    def __init__(
        self, config, question_encoder_tokenizer, generator_tokenizer, index=..., init_retrieval=...
    ) -> None: ...
    @classmethod
    def from_pretrained(cls, retriever_name_or_path, indexed_dataset=..., **kwargs):  # -> Self:
        ...
    def save_pretrained(self, save_directory):  # -> None:
        ...
    def init_retrieval(self):  # -> None:

        ...
    def postprocess_docs(self, docs, input_strings, prefix, n_docs, return_tensors=...):  # -> tuple[Any, Any]:

        ...
    def retrieve(
        self, question_hidden_states: np.ndarray, n_docs: int
    ) -> tuple[np.ndarray, np.ndarray, list[dict]]: ...
    def set_ctx_encoder_tokenizer(self, ctx_encoder_tokenizer: PreTrainedTokenizer):  # -> None:
        ...
    def __call__(
        self,
        question_input_ids: list[list[int]],
        question_hidden_states: np.ndarray,
        prefix=...,
        n_docs=...,
        return_tensors=...,
    ) -> BatchEncoding: ...

__all__ = ["RagRetriever"]
