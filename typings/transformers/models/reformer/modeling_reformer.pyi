"""
This type stub file was generated by pyright.
"""

import torch
from dataclasses import dataclass
from typing import Any, Iterable, Optional, Union
from torch import nn
from torch.autograd.function import Function
from ...cache_utils import DynamicCache
from ...generation import GenerationMixin
from ...modeling_outputs import CausalLMOutput, MaskedLMOutput, QuestionAnsweringModelOutput, SequenceClassifierOutput
from ...modeling_utils import PreTrainedModel
from ...utils import ModelOutput, auto_docstring
from .configuration_reformer import ReformerConfig

logger = ...
LSHSelfAttentionOutput = ...
LocalSelfAttentionOutput = ...
AttentionOutput = ...
ReformerOutput = ...
ReformerBackwardOutput = ...
ReformerEncoderOutput = ...

class ReformerDynamicCache(DynamicCache):
    def __init__(self, _distributed_cache_data: Optional[Iterable] = ...) -> None: ...
    def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor]: ...
    def __iter__(self): ...
    def __len__(self): ...
    def update(
        self, buckets: torch.Tensor, states: torch.Tensor, layer_idx: int, cache_kwargs: Optional[dict[str, Any]] = ...
    ) -> tuple[torch.Tensor, torch.Tensor]: ...
    def get_seq_length(self, layer_idx: Optional[int] = ...) -> int: ...
    def to_legacy_cache(self) -> tuple[tuple[torch.Tensor, torch.Tensor]]: ...
    @classmethod
    def from_legacy_cache(
        cls, past_buckets_states: Optional[tuple[tuple[torch.FloatTensor, torch.FloatTensor]]] = ...
    ) -> ReformerDynamicCache: ...

class AxialPositionEmbeddings(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, position_ids): ...

class PositionEmbeddings(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, position_ids): ...

class ReformerEmbeddings(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, input_ids=..., position_ids=..., inputs_embeds=..., start_idx_pos_encodings=...): ...

class EfficientAttentionMixin: ...

class LSHSelfAttention(nn.Module, EfficientAttentionMixin):
    def __init__(self, config, layer_idx=...) -> None: ...
    def forward(
        self,
        hidden_states,
        attention_mask=...,
        head_mask=...,
        num_hashes=...,
        buckets=...,
        past_buckets_states=...,
        use_cache=...,
        output_attentions=...,
        cache_position=...,
        **kwargs,
    ): ...

class ReverseSort(Function):
    @staticmethod
    def forward(ctx, out_vectors, logits, sorted_bucket_idx, undo_sorted_bucket_idx): ...
    @staticmethod
    def backward(ctx, grad_out_vectors, grad_logits): ...

class LocalSelfAttention(nn.Module, EfficientAttentionMixin):
    def __init__(self, config, layer_idx=...) -> None: ...
    def forward(
        self,
        hidden_states,
        attention_mask=...,
        head_mask=...,
        past_buckets_states=...,
        use_cache=...,
        output_attentions=...,
        **kwargs,
    ): ...

class ReformerSelfOutput(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states): ...

class ReformerAttention(nn.Module):
    def __init__(self, config, layer_id=...) -> None: ...
    def forward(
        self,
        hidden_states,
        attention_mask=...,
        head_mask=...,
        num_hashes=...,
        past_buckets_states=...,
        use_cache=...,
        orig_sequence_length=...,
        output_attentions=...,
        buckets=...,
        cache_position=...,
    ): ...

class ReformerFeedForwardDense(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states): ...

class ReformerFeedForwardOutput(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states): ...

class ChunkReformerFeedForward(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, attention_output): ...
    def forward_chunk(self, hidden_states): ...

class ReformerLayer(nn.Module):
    def __init__(self, config, layer_id=...) -> None: ...
    def forward(
        self,
        prev_attn_output,
        hidden_states,
        attention_mask=...,
        head_mask=...,
        num_hashes=...,
        past_buckets_states=...,
        use_cache=...,
        orig_sequence_length=...,
        output_attentions=...,
    ): ...
    def backward_pass(
        self,
        next_attn_output,
        hidden_states,
        grad_attn_output,
        grad_hidden_states,
        attention_mask=...,
        head_mask=...,
        buckets=...,
    ): ...

class _ReversibleFunction(Function):
    @staticmethod
    def forward(
        ctx,
        hidden_states,
        layers,
        attention_mask,
        head_mask,
        num_hashes,
        all_hidden_states,
        all_attentions,
        past_buckets_states,
        use_cache,
        orig_sequence_length,
        output_hidden_states,
        output_attentions,
    ): ...
    @staticmethod
    def backward(ctx, grad_hidden_states): ...

class ReformerEncoder(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(
        self,
        hidden_states,
        attention_mask=...,
        head_mask=...,
        num_hashes=...,
        past_buckets_states=...,
        use_cache=...,
        orig_sequence_length=...,
        output_hidden_states=...,
        output_attentions=...,
    ): ...

class ReformerOnlyLMHead(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states): ...
    def forward_chunk(self, hidden_states): ...

@auto_docstring
class ReformerPreTrainedModel(PreTrainedModel):
    config: ReformerConfig
    base_model_prefix = ...
    @property
    def dummy_inputs(self): ...

@dataclass
@auto_docstring(
    custom_intro="""
    Output type of [`ReformerModel`].
    """
)
class ReformerModelOutput(ModelOutput):
    last_hidden_state: torch.FloatTensor
    past_buckets_states: Optional[list[tuple[torch.LongTensor, torch.FloatTensor]]] = ...
    hidden_states: Optional[tuple[torch.FloatTensor]] = ...
    attentions: Optional[tuple[torch.FloatTensor]] = ...

@dataclass
@auto_docstring(custom_intro=...)
class ReformerModelWithLMHeadOutput(ModelOutput):
    loss: Optional[torch.FloatTensor] = ...
    logits: Optional[torch.FloatTensor] = ...
    past_buckets_states: Optional[list[tuple[torch.LongTensor, torch.FloatTensor]]] = ...
    hidden_states: Optional[tuple[torch.FloatTensor]] = ...
    attentions: Optional[tuple[torch.FloatTensor]] = ...

@auto_docstring
class ReformerModel(ReformerPreTrainedModel):
    def __init__(self, config) -> None: ...
    def get_input_embeddings(self): ...
    def set_input_embeddings(self, value): ...
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.Tensor] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        position_ids: Optional[torch.Tensor] = ...,
        head_mask: Optional[torch.Tensor] = ...,
        inputs_embeds: Optional[torch.Tensor] = ...,
        num_hashes: Optional[int] = ...,
        past_buckets_states: Optional[list[tuple[torch.Tensor]]] = ...,
        use_cache: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        output_attentions: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[tuple, ReformerModelOutput]: ...

@auto_docstring(custom_intro=...)
class ReformerModelWithLMHead(ReformerPreTrainedModel, GenerationMixin):
    _tied_weights_keys = ...
    def __init__(self, config) -> None: ...
    def get_output_embeddings(self): ...
    def set_output_embeddings(self, new_embeddings): ...
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.Tensor] = ...,
        position_ids: Optional[torch.Tensor] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        head_mask: Optional[torch.Tensor] = ...,
        inputs_embeds: Optional[torch.Tensor] = ...,
        num_hashes: Optional[int] = ...,
        past_buckets_states: Optional[list[tuple[torch.Tensor]]] = ...,
        use_cache: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        output_attentions: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
        labels: Optional[torch.Tensor] = ...,
        **kwargs,
    ) -> Union[tuple, CausalLMOutput]: ...
    def prepare_inputs_for_generation(
        self, input_ids, past_key_values=..., use_cache=..., num_hashes=..., **kwargs
    ): ...

@auto_docstring
class ReformerForMaskedLM(ReformerPreTrainedModel):
    _tied_weights_keys = ...
    def __init__(self, config) -> None: ...
    def get_output_embeddings(self): ...
    def set_output_embeddings(self, new_embeddings): ...
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.Tensor] = ...,
        position_ids: Optional[torch.Tensor] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        head_mask: Optional[torch.Tensor] = ...,
        inputs_embeds: Optional[torch.Tensor] = ...,
        num_hashes: Optional[int] = ...,
        labels: Optional[torch.Tensor] = ...,
        output_hidden_states: Optional[bool] = ...,
        output_attentions: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[tuple, MaskedLMOutput]: ...

@auto_docstring(custom_intro=...)
class ReformerForSequenceClassification(ReformerPreTrainedModel):
    def __init__(self, config) -> None: ...
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.Tensor] = ...,
        position_ids: Optional[torch.Tensor] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        head_mask: Optional[torch.Tensor] = ...,
        inputs_embeds: Optional[torch.Tensor] = ...,
        num_hashes: Optional[int] = ...,
        labels: Optional[torch.Tensor] = ...,
        output_hidden_states: Optional[bool] = ...,
        output_attentions: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[tuple, SequenceClassifierOutput]: ...

class ReformerClassificationHead(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states, **kwargs): ...

@auto_docstring
class ReformerForQuestionAnswering(ReformerPreTrainedModel):
    def __init__(self, config) -> None: ...
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.Tensor] = ...,
        position_ids: Optional[torch.Tensor] = ...,
        attention_mask: Optional[torch.Tensor] = ...,
        head_mask: Optional[torch.Tensor] = ...,
        inputs_embeds: Optional[torch.Tensor] = ...,
        num_hashes: Optional[int] = ...,
        start_positions: Optional[torch.Tensor] = ...,
        end_positions: Optional[torch.Tensor] = ...,
        output_hidden_states: Optional[bool] = ...,
        output_attentions: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[tuple, QuestionAnsweringModelOutput]: ...

__all__ = [
    "ReformerAttention",
    "ReformerForMaskedLM",
    "ReformerForQuestionAnswering",
    "ReformerForSequenceClassification",
    "ReformerLayer",
    "ReformerModel",
    "ReformerModelWithLMHead",
    "ReformerPreTrainedModel",
]
