"""
This type stub file was generated by pyright.
"""

from tokenizers import NormalizedString, PreTokenizedString

class JiebaPreTokenizer:
    def __init__(self, vocab) -> None: ...
    def jieba_split(self, i: int, normalized_string: NormalizedString) -> list[NormalizedString]: ...
    def pre_tokenize(self, pretok: PreTokenizedString): ...
