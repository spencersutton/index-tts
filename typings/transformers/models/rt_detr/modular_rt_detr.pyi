"""
This type stub file was generated by pyright.
"""

import pathlib
from typing import Optional, Union

import torch
from transformers.models.detr.image_processing_detr_fast import DetrFastImageProcessorKwargs, DetrImageProcessorFast

from ...image_processing_utils import BatchFeature
from ...image_processing_utils_fast import BaseImageProcessorFast
from ...image_utils import AnnotationFormat, AnnotationType, ChannelDimension, ImageInput
from ...processing_utils import Unpack
from ...utils import TensorType, is_torch_available, is_torchvision_v2_available

if is_torch_available(): ...
if is_torchvision_v2_available(): ...
else: ...
logger = ...
SUPPORTED_ANNOTATION_FORMATS = ...

def prepare_coco_detection_annotation(
    image,
    target,
    return_segmentation_masks: bool = ...,
    input_data_format: ChannelDimension | str | None = ...,
):  # -> dict[str, Tensor]:
    """
    Convert the target in COCO format into the format expected by RT-DETR.
    """
    ...

class RTDetrFastImageProcessorKwargs(DetrFastImageProcessorKwargs): ...

class RTDetrImageProcessorFast(DetrImageProcessorFast, BaseImageProcessorFast):
    resample = ...
    image_mean = ...
    image_std = ...
    format = ...
    do_convert_annotations = ...
    do_resize = ...
    do_rescale = ...
    do_normalize = ...
    do_pad = ...
    size = ...
    default_to_square = ...
    model_input_names = ...
    valid_kwargs = RTDetrFastImageProcessorKwargs
    def __init__(self, **kwargs: Unpack[RTDetrFastImageProcessorKwargs]) -> None: ...
    def preprocess(
        self,
        images: ImageInput,
        annotations: AnnotationType | list[AnnotationType] | None = ...,
        masks_path: str | pathlib.Path | None = ...,
        **kwargs: Unpack[RTDetrFastImageProcessorKwargs],
    ) -> BatchFeature: ...
    def prepare_annotation(
        self,
        image: torch.Tensor,
        target: dict,
        format: AnnotationFormat | None = ...,
        return_segmentation_masks: bool | None = ...,
        masks_path: str | pathlib.Path | None = ...,
        input_data_format: str | ChannelDimension | None = ...,
    ) -> dict: ...
    def post_process_object_detection(
        self,
        outputs,
        threshold: float = ...,
        target_sizes: TensorType | list[tuple] = ...,
        use_focal_loss: bool = ...,
    ):  # -> list[Any]:
        """
        Converts the raw output of [`DetrForObjectDetection`] into final bounding boxes in (top_left_x, top_left_y,
        bottom_right_x, bottom_right_y) format. Only supports PyTorch.

        Args:
            outputs ([`DetrObjectDetectionOutput`]):
                Raw outputs of the model.
            threshold (`float`, *optional*, defaults to 0.5):
                Score threshold to keep object detection predictions.
            target_sizes (`torch.Tensor` or `list[tuple[int, int]]`, *optional*):
                Tensor of shape `(batch_size, 2)` or list of tuples (`tuple[int, int]`) containing the target size
                `(height, width)` of each image in the batch. If unset, predictions will not be resized.
            use_focal_loss (`bool` defaults to `True`):
                Variable informing if the focal loss was used to predict the outputs. If `True`, a sigmoid is applied
                to compute the scores of each detection, otherwise, a softmax function is used.

        Returns:
            `list[Dict]`: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image
            in the batch as predicted by the model.
        """
        ...

    def from_dict(): ...
    def post_process(): ...
    def post_process_segmentation(): ...
    def post_process_instance(): ...
    def post_process_panoptic(): ...
    def post_process_instance_segmentation(): ...
    def post_process_semantic_segmentation(): ...
    def post_process_panoptic_segmentation(): ...

__all__ = ["RTDetrImageProcessorFast"]
