from dataclasses import dataclass

import torch
from torch import nn

from ...generation import GenerationMixin
from ...modeling_layers import GradientCheckpointingLayer
from ...modeling_utils import PreTrainedModel
from ...utils import ModelOutput
from .configuration_rwkv import RwkvConfig

"""PyTorch RWKV model."""
logger = ...
rwkv_cuda_kernel = ...

def load_wkv_cuda_kernel(context_length):  # -> None:
    ...

class RwkvLinearAttention(torch.autograd.Function):
    @staticmethod
    def forward(
        ctx, time_decay, time_first, key, value, state=..., return_state=...
    ):  # -> tuple[Tensor, list[Tensor] | None]:
        ...
    @staticmethod
    def backward(ctx, g_output, g_state=...):  # -> tuple[Tensor, Tensor, Tensor, Tensor, None, None]:
        ...

def rwkv_linear_attention_cpu(
    time_decay, time_first, key, value, state=..., return_state=...
):  # -> tuple[Tensor, list[Tensor | Any] | None]:
    ...
def rwkv_linear_attention(
    time_decay, time_first, key, value, state=..., return_state=...
):  # -> tuple[Tensor, list[Tensor | Any] | None] | Any | None:
    ...

class RwkvSelfAttention(nn.Module):
    def __init__(self, config, layer_id=...) -> None: ...
    def extract_key_value(self, hidden, state=...):  # -> tuple[Tensor, Any, Any, Any | None]:
        ...
    def forward(self, hidden, state=..., use_cache=...):  # -> tuple[Any, Any | None]:
        ...

class RwkvFeedForward(nn.Module):
    def __init__(self, config, layer_id=...) -> None: ...
    def forward(self, hidden, state=...):  # -> tuple[Any, Any | None]:
        ...

class RwkvBlock(GradientCheckpointingLayer):
    def __init__(self, config, layer_id) -> None: ...
    def forward(
        self, hidden, state=..., use_cache=..., output_attentions=...
    ):  # -> tuple[Any, Any, Any] | tuple[Any, Any, None]:
        ...

class RwkvPreTrainedModel(PreTrainedModel):
    config: RwkvConfig
    base_model_prefix = ...
    _no_split_modules = ...
    _keep_in_fp32_modules = ...
    supports_gradient_checkpointing = ...
    _is_stateful = ...

@dataclass
class RwkvOutput(ModelOutput):
    last_hidden_state: torch.FloatTensor | None = ...
    state: list[torch.FloatTensor] | None = ...
    hidden_states: tuple[torch.FloatTensor, ...] | None = ...
    attentions: tuple[torch.FloatTensor, ...] | None = ...

@dataclass
class RwkvCausalLMOutput(ModelOutput):
    loss: torch.FloatTensor | None = ...
    logits: torch.FloatTensor | None = ...
    state: list[torch.FloatTensor] | None = ...
    hidden_states: tuple[torch.FloatTensor, ...] | None = ...
    attentions: tuple[torch.FloatTensor, ...] | None = ...

class RwkvModel(RwkvPreTrainedModel):
    def __init__(self, config) -> None: ...
    def get_input_embeddings(self):  # -> Embedding:
        ...
    def set_input_embeddings(self, new_embeddings):  # -> None:
        ...
    def forward(
        self,
        input_ids: torch.LongTensor | None = ...,
        attention_mask: torch.LongTensor | None = ...,
        inputs_embeds: torch.FloatTensor | None = ...,
        state: list[torch.FloatTensor] | None = ...,
        use_cache: bool | None = ...,
        output_attentions: bool | None = ...,
        output_hidden_states: bool | None = ...,
        return_dict: bool | None = ...,
    ) -> tuple | RwkvOutput: ...

class RwkvForCausalLM(RwkvPreTrainedModel, GenerationMixin):
    _tied_weights_keys = ...
    def __init__(self, config) -> None: ...
    def get_output_embeddings(self):  # -> Linear:
        ...
    def set_output_embeddings(self, new_embeddings):  # -> None:
        ...
    def prepare_inputs_for_generation(
        self, input_ids, state=..., inputs_embeds=..., use_cache=..., **kwargs
    ):  # -> dict[str, Any]:
        ...
    def forward(
        self,
        input_ids: torch.LongTensor | None = ...,
        attention_mask: torch.LongTensor | None = ...,
        inputs_embeds: torch.FloatTensor | None = ...,
        state: list[torch.FloatTensor] | None = ...,
        labels: torch.LongTensor | None = ...,
        use_cache: bool | None = ...,
        output_attentions: bool | None = ...,
        output_hidden_states: bool | None = ...,
        return_dict: bool | None = ...,
        **kwargs,
    ) -> tuple | RwkvCausalLMOutput: ...

__all__ = ["RwkvForCausalLM", "RwkvModel", "RwkvPreTrainedModel"]
