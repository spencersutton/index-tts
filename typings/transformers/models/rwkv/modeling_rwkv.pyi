"""
This type stub file was generated by pyright.
"""

import torch
from dataclasses import dataclass
from typing import Optional, Union
from torch import nn
from ...generation import GenerationMixin
from ...modeling_layers import GradientCheckpointingLayer
from ...modeling_utils import PreTrainedModel
from ...utils import ModelOutput, auto_docstring
from .configuration_rwkv import RwkvConfig

"""PyTorch RWKV model."""
logger = ...
rwkv_cuda_kernel = ...

def load_wkv_cuda_kernel(context_length):  # -> None:
    ...

class RwkvLinearAttention(torch.autograd.Function):
    @staticmethod
    def forward(
        ctx, time_decay, time_first, key, value, state=..., return_state=...
    ):  # -> tuple[Tensor, list[Tensor] | None]:
        ...
    @staticmethod
    def backward(ctx, g_output, g_state=...):  # -> tuple[Tensor, Tensor, Tensor, Tensor, None, None]:
        ...

def rwkv_linear_attention_cpu(
    time_decay, time_first, key, value, state=..., return_state=...
):  # -> tuple[Tensor, list[Tensor | Any] | None]:
    ...
def rwkv_linear_attention(
    time_decay, time_first, key, value, state=..., return_state=...
):  # -> tuple[Tensor, list[Tensor | Any] | None] | Any | None:
    ...

class RwkvSelfAttention(nn.Module):
    def __init__(self, config, layer_id=...) -> None: ...
    def extract_key_value(self, hidden, state=...):  # -> tuple[Tensor, Any, Any, Any | None]:
        ...
    def forward(self, hidden, state=..., use_cache=...):  # -> tuple[Any, Any | None]:
        ...

class RwkvFeedForward(nn.Module):
    def __init__(self, config, layer_id=...) -> None: ...
    def forward(self, hidden, state=...):  # -> tuple[Any, Any | None]:
        ...

class RwkvBlock(GradientCheckpointingLayer):
    def __init__(self, config, layer_id) -> None: ...
    def forward(
        self, hidden, state=..., use_cache=..., output_attentions=...
    ):  # -> tuple[Any, Any, Any] | tuple[Any, Any, None]:
        ...

@auto_docstring
class RwkvPreTrainedModel(PreTrainedModel):
    config: RwkvConfig
    base_model_prefix = ...
    _no_split_modules = ...
    _keep_in_fp32_modules = ...
    supports_gradient_checkpointing = ...
    _is_stateful = ...

@dataclass
@auto_docstring(
    custom_intro="""
    Class for the RWKV model outputs.
    """
)
class RwkvOutput(ModelOutput):
    r"""
    state (list of five `torch.FloatTensor` of shape `(batch_size, hidden_size, num_hidden_layers)`):
        The state of the model at the last time step. Can be used in a forward method with the next `input_ids` to
        avoid providing the old `input_ids`.
    """

    last_hidden_state: torch.FloatTensor | None = ...
    state: list[torch.FloatTensor] | None = ...
    hidden_states: tuple[torch.FloatTensor, ...] | None = ...
    attentions: tuple[torch.FloatTensor, ...] | None = ...

@dataclass
@auto_docstring(
    custom_intro="""
    Base class for causal language model (or autoregressive) outputs.
    """
)
class RwkvCausalLMOutput(ModelOutput):
    r"""
    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):
        Language modeling loss (for next-token prediction).
    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):
        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).
    state (list of five `torch.FloatTensor` of shape `(batch_size, hidden_size, num_hidden_layers)`):
        The state of the model at the last time step. Can be used in a forward method with the next `input_ids` to
        avoid providing the old `input_ids`.
    """

    loss: torch.FloatTensor | None = ...
    logits: torch.FloatTensor | None = ...
    state: list[torch.FloatTensor] | None = ...
    hidden_states: tuple[torch.FloatTensor, ...] | None = ...
    attentions: tuple[torch.FloatTensor, ...] | None = ...

@auto_docstring
class RwkvModel(RwkvPreTrainedModel):
    def __init__(self, config) -> None: ...
    def get_input_embeddings(self):  # -> Embedding:
        ...
    def set_input_embeddings(self, new_embeddings):  # -> None:
        ...
    @auto_docstring
    def forward(
        self,
        input_ids: torch.LongTensor | None = ...,
        attention_mask: torch.LongTensor | None = ...,
        inputs_embeds: torch.FloatTensor | None = ...,
        state: list[torch.FloatTensor] | None = ...,
        use_cache: bool | None = ...,
        output_attentions: bool | None = ...,
        output_hidden_states: bool | None = ...,
        return_dict: bool | None = ...,
    ) -> tuple | RwkvOutput:
        r"""
        input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):
            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else
            `past_key_values.get_seq_length()` (`sequence_length` of input past key value states). Indices of input
            sequence tokens in the vocabulary.

            If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as
            `input_ids`.

            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
            [`PreTrainedTokenizer.__call__`] for details.

            [What are input IDs?](../glossary#input-ids)
        state (tuple of five `torch.FloatTensor` of shape `(batch_size, hidden_size, num_hidden_layers)`, *optional*):
            If passed along, the model uses the previous state in all the blocks (which will give the output for the
            `input_ids` provided as if the model add `state_input_ids + input_ids` as context).
        use_cache (`bool`, *optional*):
            If set to `True`, the last state is returned and can be used to quickly generate the next logits.
        """
        ...

@auto_docstring(
    custom_intro="""
    The RWKV Model transformer with a language modeling head on top (linear layer with weights tied to the input
    embeddings).
    """
)
class RwkvForCausalLM(RwkvPreTrainedModel, GenerationMixin):
    _tied_weights_keys = ...
    def __init__(self, config) -> None: ...
    def get_output_embeddings(self):  # -> Linear:
        ...
    def set_output_embeddings(self, new_embeddings):  # -> None:
        ...
    def prepare_inputs_for_generation(
        self, input_ids, state=..., inputs_embeds=..., use_cache=..., **kwargs
    ):  # -> dict[str, Any]:
        ...
    @auto_docstring
    def forward(
        self,
        input_ids: torch.LongTensor | None = ...,
        attention_mask: torch.LongTensor | None = ...,
        inputs_embeds: torch.FloatTensor | None = ...,
        state: list[torch.FloatTensor] | None = ...,
        labels: torch.LongTensor | None = ...,
        use_cache: bool | None = ...,
        output_attentions: bool | None = ...,
        output_hidden_states: bool | None = ...,
        return_dict: bool | None = ...,
        **kwargs,
    ) -> tuple | RwkvCausalLMOutput:
        r"""
        input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):
            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else
            `past_key_values.get_seq_length()` (`sequence_length` of input past key value states). Indices of input
            sequence tokens in the vocabulary.

            If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as
            `input_ids`.

            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
            [`PreTrainedTokenizer.__call__`] for details.

            [What are input IDs?](../glossary#input-ids)
        state (tuple of five `torch.FloatTensor` of shape `(batch_size, hidden_size, num_hidden_layers)`, *optional*):
            If passed along, the model uses the previous state in all the blocks (which will give the output for the
            `input_ids` provided as if the model add `state_input_ids + input_ids` as context).
        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set
            `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`
            are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`
        use_cache (`bool`, *optional*):
            If set to `True`, the last state is returned and can be used to quickly generate the next logits.
        """
        ...

__all__ = ["RwkvForCausalLM", "RwkvModel", "RwkvPreTrainedModel"]
