"""
This type stub file was generated by pyright.
"""

import torch
from typing import Optional, Union
from torch import nn
from ...modeling_layers import GradientCheckpointingLayer
from ...modeling_outputs import BaseModelOutput, CausalLMOutput, SequenceClassifierOutput
from ...modeling_utils import PreTrainedModel
from ...utils import auto_docstring
from .configuration_sew_d import SEWDConfig

logger = ...
_HIDDEN_STATES_START_POSITION = ...

def make_log_bucket_position(relative_pos, bucket_size, max_position): ...
def build_relative_position(query_size, key_size, bucket_size=..., max_position=..., device=...): ...
@torch.jit.script
def c2p_dynamic_expand(c2p_pos, query_layer, relative_pos): ...
@torch.jit.script
def p2c_dynamic_expand(c2p_pos, query_layer, key_layer): ...
@torch.jit.script
def pos_dynamic_expand(pos_index, p2c_att, key_layer): ...
def get_mask(input, local_context): ...

class SEWDNoLayerNormConvLayer(GradientCheckpointingLayer):
    def __init__(self, config, layer_id=...) -> None: ...
    def forward(self, hidden_states): ...

class SEWDLayerNormConvLayer(GradientCheckpointingLayer):
    def __init__(self, config, layer_id=...) -> None: ...
    def forward(self, hidden_states): ...

class SEWDGroupNormConvLayer(GradientCheckpointingLayer):
    def __init__(self, config, layer_id=...) -> None: ...
    def forward(self, hidden_states): ...

class SEWDPositionalConvEmbedding(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states): ...

class SEWDSamePadLayer(nn.Module):
    def __init__(self, num_conv_pos_embeddings) -> None: ...
    def forward(self, hidden_states): ...

class SEWDUpsampling(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states): ...

class SEWDFeatureEncoder(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, input_values): ...

class SEWDFeatureExtractor(SEWDFeatureEncoder):
    def __init__(self, config) -> None: ...

class ContextPooler(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states): ...
    @property
    def output_dim(self): ...

class XSoftmax(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input, mask, dim): ...
    @staticmethod
    def backward(ctx, grad_output): ...
    @staticmethod
    def symbolic(g, self, mask, dim): ...

class DropoutContext:
    def __init__(self) -> None: ...

class XDropout(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input, local_ctx): ...
    @staticmethod
    def backward(ctx, grad_output): ...
    @staticmethod
    def symbolic(
        g: torch._C.Graph, input: torch._C.Value, local_ctx: Union[float, DropoutContext]
    ) -> torch._C.Value: ...

class StableDropout(nn.Module):
    def __init__(self, drop_prob) -> None: ...
    def forward(self, x): ...
    def clear_context(self): ...
    def init_context(self, reuse_mask=..., scale=...): ...
    def get_context(self): ...

class SEWDSelfOutput(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states, input_tensor): ...

class DisentangledSelfAttention(nn.Module):
    def __init__(self, config) -> None: ...
    def transpose_for_scores(self, x, attention_heads): ...
    def forward(
        self,
        hidden_states,
        attention_mask,
        output_attentions=...,
        query_states=...,
        relative_pos=...,
        rel_embeddings=...,
    ): ...
    def disentangled_attention_bias(self, query_layer, key_layer, relative_pos, rel_embeddings, scale_factor): ...

class SEWDAttention(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(
        self,
        hidden_states,
        attention_mask,
        output_attentions=...,
        query_states=...,
        relative_pos=...,
        rel_embeddings=...,
    ): ...

class SEWDIntermediate(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor: ...

class SEWDOutput(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states, input_tensor): ...

class SEWDLayer(GradientCheckpointingLayer):
    def __init__(self, config) -> None: ...
    def forward(
        self,
        hidden_states,
        attention_mask,
        query_states=...,
        relative_pos=...,
        rel_embeddings=...,
        output_attentions=...,
    ): ...

class ConvLayer(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states, residual_states, input_mask): ...

class SEWDTransformerEncoder(nn.Module):
    def __init__(self, config) -> None: ...
    def get_rel_embedding(self): ...
    def get_attention_mask(self, attention_mask): ...
    def get_rel_pos(self, hidden_states, query_states=..., relative_pos=...): ...
    def forward(
        self,
        hidden_states,
        attention_mask,
        output_hidden_states=...,
        output_attentions=...,
        query_states=...,
        relative_pos=...,
        return_dict=...,
    ): ...

class SEWDEncoder(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(
        self,
        hidden_states: torch.tensor,
        attention_mask: Optional[torch.Tensor] = ...,
        output_attentions: bool = ...,
        output_hidden_states: bool = ...,
        return_dict: bool = ...,
    ): ...

@auto_docstring
class SEWDPreTrainedModel(PreTrainedModel):
    config: SEWDConfig
    base_model_prefix = ...
    main_input_name = ...
    supports_gradient_checkpointing = ...

@auto_docstring
class SEWDModel(SEWDPreTrainedModel):
    def __init__(self, config: SEWDConfig) -> None: ...
    @auto_docstring
    def forward(
        self,
        input_values: Optional[torch.Tensor],
        attention_mask: Optional[torch.Tensor] = ...,
        mask_time_indices: Optional[torch.FloatTensor] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[tuple, BaseModelOutput]: ...

@auto_docstring(custom_intro=...)
class SEWDForCTC(SEWDPreTrainedModel):
    def __init__(self, config, target_lang: Optional[str] = ...) -> None: ...
    def tie_weights(self): ...
    def freeze_feature_extractor(self): ...
    def freeze_feature_encoder(self): ...
    def freeze_base_model(self): ...
    @auto_docstring
    def forward(
        self,
        input_values: Optional[torch.Tensor],
        attention_mask: Optional[torch.Tensor] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
        labels: Optional[torch.Tensor] = ...,
    ) -> Union[tuple, CausalLMOutput]: ...

@auto_docstring(custom_intro=...)
class SEWDForSequenceClassification(SEWDPreTrainedModel):
    def __init__(self, config) -> None: ...
    def freeze_feature_extractor(self): ...
    def freeze_feature_encoder(self): ...
    def freeze_base_model(self): ...
    @auto_docstring
    def forward(
        self,
        input_values: Optional[torch.Tensor],
        attention_mask: Optional[torch.Tensor] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
        labels: Optional[torch.Tensor] = ...,
    ) -> Union[tuple, SequenceClassifierOutput]: ...

__all__ = ["SEWDForCTC", "SEWDForSequenceClassification", "SEWDModel", "SEWDPreTrainedModel"]
