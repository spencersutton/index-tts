"""
This type stub file was generated by pyright.
"""

import enum
import pandas as pd
from dataclasses import dataclass
from typing import Callable, Optional, TypeAlias, Union
from ...tokenization_utils import PreTrainedTokenizer
from ...tokenization_utils_base import (
    BatchEncoding,
    ENCODE_KWARGS_DOCSTRING,
    EncodedInput,
    PreTokenizedInput,
    TextInput,
)
from ...utils import ExplicitEnum, PaddingStrategy, TensorType, add_end_docstrings, is_pandas_available

if is_pandas_available(): ...
logger = ...
VOCAB_FILES_NAMES = ...

class TapasTruncationStrategy(ExplicitEnum):
    DROP_ROWS_TO_FIT = ...
    DO_NOT_TRUNCATE = ...

TableValue = ...

@dataclass(frozen=True)
class TokenCoordinates:
    column_index: int
    row_index: int
    token_index: int
    ...

@dataclass
class TokenizedTable:
    rows: list[list[list[str]]]
    selected_tokens: list[TokenCoordinates]
    ...

@dataclass(frozen=True)
class SerializedExample:
    tokens: list[str]
    column_ids: list[int]
    row_ids: list[int]
    segment_ids: list[int]
    ...

def load_vocab(vocab_file): ...
def whitespace_tokenize(text): ...

TAPAS_ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING = ...

class TapasTokenizer(PreTrainedTokenizer):
    vocab_files_names = ...
    def __init__(
        self,
        vocab_file,
        do_lower_case=...,
        do_basic_tokenize=...,
        never_split=...,
        unk_token=...,
        sep_token=...,
        pad_token=...,
        cls_token=...,
        mask_token=...,
        empty_token=...,
        tokenize_chinese_chars=...,
        strip_accents=...,
        cell_trim_length: int = ...,
        max_column_id: Optional[int] = ...,
        max_row_id: Optional[int] = ...,
        strip_column_names: bool = ...,
        update_answer_coordinates: bool = ...,
        min_question_length=...,
        max_question_length=...,
        model_max_length: int = ...,
        additional_special_tokens: Optional[list[str]] = ...,
        clean_up_tokenization_spaces=...,
        **kwargs,
    ) -> None: ...
    @property
    def do_lower_case(self): ...
    @property
    def vocab_size(self): ...
    def get_vocab(self): ...
    def convert_tokens_to_string(self, tokens): ...
    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = ...) -> tuple[str]: ...
    def create_attention_mask_from_sequences(
        self, query_ids: list[int], table_values: list[TableValue]
    ) -> list[int]: ...
    def create_segment_token_type_ids_from_sequences(
        self, query_ids: list[int], table_values: list[TableValue]
    ) -> list[int]: ...
    def create_column_token_type_ids_from_sequences(
        self, query_ids: list[int], table_values: list[TableValue]
    ) -> list[int]: ...
    def create_row_token_type_ids_from_sequences(
        self, query_ids: list[int], table_values: list[TableValue]
    ) -> list[int]: ...
    def build_inputs_with_special_tokens(
        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = ...
    ) -> list[int]: ...
    def get_special_tokens_mask(
        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = ..., already_has_special_tokens: bool = ...
    ) -> list[int]: ...
    @add_end_docstrings(TAPAS_ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING)
    def __call__(
        self,
        table: pd.DataFrame,
        queries: Optional[
            Union[
                TextInput,
                PreTokenizedInput,
                EncodedInput,
                list[TextInput],
                list[PreTokenizedInput],
                list[EncodedInput],
            ]
        ] = ...,
        answer_coordinates: Optional[Union[list[tuple], list[list[tuple]]]] = ...,
        answer_text: Optional[Union[list[TextInput], list[list[TextInput]]]] = ...,
        add_special_tokens: bool = ...,
        padding: Union[bool, str, PaddingStrategy] = ...,
        truncation: Union[bool, str, TapasTruncationStrategy] = ...,
        max_length: Optional[int] = ...,
        pad_to_multiple_of: Optional[int] = ...,
        padding_side: Optional[str] = ...,
        return_tensors: Optional[Union[str, TensorType]] = ...,
        return_token_type_ids: Optional[bool] = ...,
        return_attention_mask: Optional[bool] = ...,
        return_overflowing_tokens: bool = ...,
        return_special_tokens_mask: bool = ...,
        return_offsets_mapping: bool = ...,
        return_length: bool = ...,
        verbose: bool = ...,
        **kwargs,
    ) -> BatchEncoding: ...
    @add_end_docstrings(ENCODE_KWARGS_DOCSTRING, TAPAS_ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING)
    def batch_encode_plus(
        self,
        table: pd.DataFrame,
        queries: Optional[
            Union[
                list[TextInput],
                list[PreTokenizedInput],
                list[EncodedInput],
            ]
        ] = ...,
        answer_coordinates: Optional[list[list[tuple]]] = ...,
        answer_text: Optional[list[list[TextInput]]] = ...,
        add_special_tokens: bool = ...,
        padding: Union[bool, str, PaddingStrategy] = ...,
        truncation: Union[bool, str, TapasTruncationStrategy] = ...,
        max_length: Optional[int] = ...,
        pad_to_multiple_of: Optional[int] = ...,
        padding_side: Optional[str] = ...,
        return_tensors: Optional[Union[str, TensorType]] = ...,
        return_token_type_ids: Optional[bool] = ...,
        return_attention_mask: Optional[bool] = ...,
        return_overflowing_tokens: bool = ...,
        return_special_tokens_mask: bool = ...,
        return_offsets_mapping: bool = ...,
        return_length: bool = ...,
        verbose: bool = ...,
        **kwargs,
    ) -> BatchEncoding: ...
    @add_end_docstrings(ENCODE_KWARGS_DOCSTRING)
    def encode(
        self,
        table: pd.DataFrame,
        query: Optional[
            Union[
                TextInput,
                PreTokenizedInput,
                EncodedInput,
            ]
        ] = ...,
        add_special_tokens: bool = ...,
        padding: Union[bool, str, PaddingStrategy] = ...,
        truncation: Union[bool, str, TapasTruncationStrategy] = ...,
        max_length: Optional[int] = ...,
        return_tensors: Optional[Union[str, TensorType]] = ...,
        **kwargs,
    ) -> list[int]: ...
    @add_end_docstrings(ENCODE_KWARGS_DOCSTRING, TAPAS_ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING)
    def encode_plus(
        self,
        table: pd.DataFrame,
        query: Optional[
            Union[
                TextInput,
                PreTokenizedInput,
                EncodedInput,
            ]
        ] = ...,
        answer_coordinates: Optional[list[tuple]] = ...,
        answer_text: Optional[list[TextInput]] = ...,
        add_special_tokens: bool = ...,
        padding: Union[bool, str, PaddingStrategy] = ...,
        truncation: Union[bool, str, TapasTruncationStrategy] = ...,
        max_length: Optional[int] = ...,
        pad_to_multiple_of: Optional[int] = ...,
        padding_side: Optional[str] = ...,
        return_tensors: Optional[Union[str, TensorType]] = ...,
        return_token_type_ids: Optional[bool] = ...,
        return_attention_mask: Optional[bool] = ...,
        return_special_tokens_mask: bool = ...,
        return_offsets_mapping: bool = ...,
        return_length: bool = ...,
        verbose: bool = ...,
        **kwargs,
    ) -> BatchEncoding: ...
    @add_end_docstrings(ENCODE_KWARGS_DOCSTRING, TAPAS_ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING)
    def prepare_for_model(
        self,
        raw_table: pd.DataFrame,
        raw_query: Union[
            TextInput,
            PreTokenizedInput,
            EncodedInput,
        ],
        tokenized_table: Optional[TokenizedTable] = ...,
        query_tokens: Optional[TokenizedTable] = ...,
        answer_coordinates: Optional[list[tuple]] = ...,
        answer_text: Optional[list[TextInput]] = ...,
        add_special_tokens: bool = ...,
        padding: Union[bool, str, PaddingStrategy] = ...,
        truncation: Union[bool, str, TapasTruncationStrategy] = ...,
        max_length: Optional[int] = ...,
        pad_to_multiple_of: Optional[int] = ...,
        padding_side: Optional[str] = ...,
        return_tensors: Optional[Union[str, TensorType]] = ...,
        return_token_type_ids: Optional[bool] = ...,
        return_attention_mask: Optional[bool] = ...,
        return_special_tokens_mask: bool = ...,
        return_offsets_mapping: bool = ...,
        return_length: bool = ...,
        verbose: bool = ...,
        prepend_batch_axis: bool = ...,
        **kwargs,
    ) -> BatchEncoding: ...
    def get_answer_ids(
        self, column_ids, row_ids, tokenized_table, answer_texts_question, answer_coordinates_question
    ): ...
    def convert_logits_to_predictions(self, data, logits, logits_agg=..., cell_classification_threshold=...): ...

class BasicTokenizer:
    def __init__(
        self, do_lower_case=..., never_split=..., tokenize_chinese_chars=..., strip_accents=..., do_split_on_punc=...
    ) -> None: ...
    def tokenize(self, text, never_split=...): ...

class WordpieceTokenizer:
    def __init__(self, vocab, unk_token, max_input_chars_per_word=...) -> None: ...
    def tokenize(self, text): ...

class Relation(enum.Enum):
    HEADER_TO_CELL = ...
    CELL_TO_HEADER = ...
    QUERY_TO_HEADER = ...
    QUERY_TO_CELL = ...
    ROW_TO_CELL = ...
    CELL_TO_ROW = ...
    EQ = ...
    LT = ...
    GT = ...

@dataclass
class Date:
    year: Optional[int] = ...
    month: Optional[int] = ...
    day: Optional[int] = ...

@dataclass
class NumericValue:
    float_value: Optional[float] = ...
    date: Optional[Date] = ...

@dataclass
class NumericValueSpan:
    begin_index: Optional[int] = ...
    end_index: Optional[int] = ...
    values: list[NumericValue] = ...

@dataclass
class Cell:
    text: str
    numeric_value: Optional[NumericValue] = ...

@dataclass
class Question:
    original_text: str
    text: str
    numeric_spans: Optional[list[NumericValueSpan]] = ...

_DateMask = ...
_YEAR = ...
_YEAR_MONTH = ...
_YEAR_MONTH_DAY = ...
_MONTH = ...
_MONTH_DAY = ...
_DATE_PATTERNS = ...
_FIELD_TO_REGEX = ...
_PROCESSED_DATE_PATTERNS = ...
_MAX_DATE_NGRAM_SIZE = ...
_NUMBER_WORDS = ...
_ORDINAL_WORDS = ...
_ORDINAL_SUFFIXES = ...
_NUMBER_PATTERN = ...
_MIN_YEAR = ...
_MAX_YEAR = ...
_INF = ...

def get_all_spans(text, max_ngram_length): ...
def normalize_for_match(text): ...
def format_text(text): ...
def parse_text(text): ...

_PrimitiveNumericValue: TypeAlias = Union[float, tuple[Optional[float], Optional[float], Optional[float]]]
_SortKeyFn: TypeAlias = Callable[[NumericValue], tuple[float, Ellipsis]]
_DATE_TUPLE_SIZE = ...
EMPTY_TEXT = ...
NUMBER_TYPE = ...
DATE_TYPE = ...

def get_numeric_sort_key_fn(numeric_values): ...
def get_numeric_relation(value, other_value, sort_key_fn): ...
def add_numeric_values_to_question(question): ...
def filter_invalid_unicode(text): ...
def filter_invalid_unicode_from_table(table): ...
def add_numeric_table_values(table, min_consolidation_fraction=..., debug_info=...): ...

__all__ = ["TapasTokenizer"]
