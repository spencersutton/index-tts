"""
This type stub file was generated by pyright.
"""

import torch
import torch.nn as nn
from collections.abc import Sequence
from dataclasses import dataclass
from typing import Optional, Union
from ...modeling_flash_attention_utils import FlashAttentionKwargs
from ...modeling_outputs import BaseModelOutput
from ...modeling_utils import PreTrainedModel
from ...processing_utils import Unpack
from ...utils import auto_docstring, can_return_tuple
from ..llama.modeling_llama import LlamaRMSNorm
from .configuration_timesfm import TimesFmConfig

logger = ...

@dataclass
@auto_docstring
class TimesFmOutput(BaseModelOutput):
    loc: Optional[torch.Tensor] = ...
    scale: Optional[torch.Tensor] = ...

@dataclass
@auto_docstring
class TimesFmOutputForPrediction(BaseModelOutput):
    mean_predictions: Optional[torch.Tensor] = ...
    full_predictions: Optional[torch.Tensor] = ...
    loss: Optional[Union[torch.Tensor, float]] = ...

class TimesFmMLP(nn.Module):
    def __init__(self, config: TimesFmConfig) -> None: ...
    def forward(self, x, paddings=...): ...

class TimesFmResidualBlock(nn.Module):
    def __init__(self, input_dims, hidden_dims, output_dims) -> None: ...
    def forward(self, x): ...

class TimesFmRMSNorm(LlamaRMSNorm): ...

class TimesFmPositionalEmbedding(nn.Module):
    def __init__(self, config: TimesFmConfig) -> None: ...
    def forward(self, seq_length=..., position=...): ...

class TimesFmAttention(nn.Module):
    def __init__(self, config: TimesFmConfig, layer_idx: int) -> None: ...
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = ...,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]: ...

class TimesFmDecoderLayer(nn.Module):
    def __init__(self, config: TimesFmConfig, layer_idx: int) -> None: ...
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: torch.Tensor,
        paddings: torch.Tensor,
        output_attentions: bool = ...,
    ) -> tuple[Optional[torch.Tensor], torch.Tensor]: ...

@auto_docstring
class TimesFmPreTrainedModel(PreTrainedModel):
    config: TimesFmConfig
    base_model_prefix = ...
    _no_split_modules = ...
    main_input_name = ...
    _supports_sdpa = ...

@auto_docstring
class TimesFmModel(TimesFmPreTrainedModel):
    def __init__(self, config: TimesFmConfig) -> None: ...
    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        past_values: torch.Tensor,
        past_values_padding: torch.LongTensor,
        freq: torch.Tensor,
        output_attentions: bool = ...,
        output_hidden_states: bool = ...,
    ) -> TimesFmOutput: ...

class TimesFmModelForPrediction(TimesFmPreTrainedModel):
    def __init__(self, config: TimesFmConfig) -> None: ...
    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        past_values: Sequence[torch.Tensor],
        freq: Optional[Sequence[Union[torch.Tensor, int]]] = ...,
        window_size: Optional[int] = ...,
        future_values: Optional[torch.Tensor] = ...,
        forecast_context_len: Optional[int] = ...,
        return_forecast_on_context: bool = ...,
        truncate_negative: bool = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
    ) -> TimesFmOutputForPrediction: ...

__all__ = ["TimesFmModelForPrediction", "TimesFmPreTrainedModel", "TimesFmModel"]
