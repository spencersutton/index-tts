"""
This type stub file was generated by pyright.
"""

import torch
from abc import ABC, abstractmethod
from collections.abc import Sequence
from dataclasses import dataclass
from typing import Any, Optional, Union
from torch import Tensor, nn
from transformers import UdopConfig
from ...cache_utils import Cache
from ...generation import GenerationMixin
from ...modeling_layers import GradientCheckpointingLayer
from ...modeling_utils import PreTrainedModel
from ...utils import ModelOutput, auto_docstring, is_torch_flex_attn_available

if is_torch_flex_attn_available(): ...
logger = ...

@dataclass
@auto_docstring(custom_intro=...)
class BaseModelOutputWithAttentionMask(ModelOutput):
    last_hidden_state: Optional[torch.FloatTensor] = ...
    attention_mask: Optional[torch.FloatTensor] = ...
    past_key_values: Optional[Cache] = ...
    hidden_states: Optional[tuple[torch.FloatTensor]] = ...
    attentions: Optional[tuple[torch.FloatTensor]] = ...
    cross_attentions: Optional[tuple[torch.FloatTensor]] = ...

def get_visual_bbox(image_size=..., patch_size=...): ...
def pad_sequence(seq, target_len, pad_value=...): ...
def combine_image_text_embeddings(
    image_embeddings,
    inputs_embeds,
    bbox,
    visual_bbox,
    attention_mask=...,
    num_patches=...,
    max_len=...,
    image_size=...,
    patch_size=...,
): ...

class UdopPatchEmbeddings(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, pixel_values): ...

@auto_docstring
class UdopPreTrainedModel(PreTrainedModel):
    config: UdopConfig
    base_model_prefix = ...
    supports_gradient_checkpointing = ...
    _can_compile_fullgraph = ...
    _keep_in_fp32_modules = ...

class UdopLayerNorm(nn.Module):
    def __init__(self, hidden_size, eps=...) -> None: ...
    def forward(self, hidden_states): ...

class UdopDenseActDense(nn.Module):
    def __init__(self, config: UdopConfig) -> None: ...
    def forward(self, hidden_states): ...

class UdopDenseGatedActDense(nn.Module):
    def __init__(self, config: UdopConfig) -> None: ...
    def forward(self, hidden_states): ...

class UdopLayerFF(nn.Module):
    def __init__(self, config: UdopConfig) -> None: ...
    def forward(self, hidden_states): ...

class UdopAttention(nn.Module):
    def __init__(self, config: UdopConfig, has_relative_attention_bias=..., layer_idx: Optional[int] = ...) -> None: ...
    def prune_heads(self, heads): ...
    def compute_bias(self, query_length, key_length, device=..., cache_position=...): ...
    def forward(
        self,
        hidden_states,
        mask=...,
        key_value_states=...,
        position_bias=...,
        past_key_value=...,
        layer_head_mask=...,
        query_length=...,
        use_cache=...,
        output_attentions=...,
        cache_position=...,
    ): ...

class UdopLayerSelfAttention(nn.Module):
    def __init__(self, config, has_relative_attention_bias=..., layer_idx: Optional[int] = ...) -> None: ...
    def forward(
        self,
        hidden_states,
        attention_mask=...,
        position_bias=...,
        layer_head_mask=...,
        past_key_value=...,
        use_cache=...,
        output_attentions=...,
        cache_position=...,
    ): ...

class UdopLayerCrossAttention(nn.Module):
    def __init__(self, config, layer_idx: Optional[int] = ...) -> None: ...
    def forward(
        self,
        hidden_states,
        key_value_states,
        attention_mask=...,
        position_bias=...,
        layer_head_mask=...,
        past_key_value=...,
        use_cache=...,
        query_length=...,
        output_attentions=...,
        cache_position=...,
    ): ...

class UdopBlock(GradientCheckpointingLayer):
    def __init__(self, config, has_relative_attention_bias=..., layer_idx: Optional[int] = ...) -> None: ...
    def forward(
        self,
        hidden_states,
        attention_mask=...,
        position_bias=...,
        encoder_hidden_states=...,
        encoder_attention_mask=...,
        encoder_decoder_position_bias=...,
        layer_head_mask=...,
        cross_attn_layer_head_mask=...,
        past_key_value=...,
        use_cache=...,
        output_attentions=...,
        return_dict=...,
        cache_position=...,
    ): ...

class UdopCellEmbeddings(nn.Module):
    def __init__(self, max_2d_position_embeddings=..., hidden_size=...) -> None: ...
    def forward(self, bbox): ...

get_relative_position_bucket = ...
AUGMENTATION_RANGE = ...

class RelativePositionBiasBase(nn.Module, ABC):
    def __init__(
        self,
        num_heads=...,
        relative_attention_num_buckets=...,
        bidirectional=...,
        scaling_factor=...,
        max_distance=...,
        level=...,
        augmentation=...,
        prefix_bucket=...,
        expand=...,
    ) -> None: ...
    @abstractmethod
    def prepare_input(self, attention_mask: Optional[Tensor] = ..., bbox: Optional[dict[str, Any]] = ...) -> Tensor: ...
    def get_bucket(self, attention_mask: Optional[Tensor] = ..., bbox: Optional[dict[str, Any]] = ...) -> Tensor: ...
    def get_relative_position(self, positions): ...
    def forward(self, attention_mask: Optional[Tensor] = ..., bbox: Optional[dict[str, Any]] = ...) -> Tensor: ...

class RelativePositionBias1D(RelativePositionBiasBase):
    def __init__(self, scaling_factor=..., max_distance=..., **kwargs) -> None: ...
    def prepare_input(self, attention_mask: Optional[Tensor] = ..., bbox: Optional[dict[str, Any]] = ...) -> Tensor: ...

class RelativePositionBiasHorizontal(RelativePositionBiasBase):
    def __init__(self, scaling_factor=..., max_distance=..., **kwargs) -> None: ...
    def prepare_input(self, attention_mask: Optional[Tensor] = ..., bbox: Optional[dict[str, Any]] = ...) -> Tensor: ...

class RelativePositionBiasVertical(RelativePositionBiasBase):
    def __init__(self, scaling_factor=..., max_distance=..., **kwargs) -> None: ...
    def prepare_input(self, attention_mask: Optional[Tensor] = ..., bbox: Optional[dict[str, Any]] = ...) -> Tensor: ...

class RelativePositionBiasAggregated(nn.Module):
    def __init__(self, modules: Sequence[RelativePositionBiasBase]) -> None: ...
    def forward(
        self, attention_mask: Optional[Tensor] = ..., bbox: Optional[dict[str, Any]] = ...
    ) -> Union[float, Tensor]: ...

BIAS_CLASSES = ...

def create_relative_bias(config: UdopConfig) -> Sequence[RelativePositionBiasBase]: ...

class UdopStack(UdopPreTrainedModel):
    def __init__(self, config, embed_tokens=..., embed_patches=...) -> None: ...
    def get_output_embeddings(self): ...
    def set_input_embeddings(self, new_embeddings): ...
    def forward(
        self,
        input_ids=...,
        attention_mask=...,
        bbox=...,
        encoder_hidden_states=...,
        encoder_attention_mask=...,
        inputs_embeds=...,
        pixel_values=...,
        visual_bbox=...,
        image_embeddings=...,
        position_bias=...,
        head_mask=...,
        cross_attn_head_mask=...,
        past_key_values=...,
        use_cache=...,
        output_attentions=...,
        output_hidden_states=...,
        return_dict=...,
        cache_position=...,
    ): ...

@auto_docstring
class UdopModel(UdopPreTrainedModel):
    _tied_weights_keys = ...
    def __init__(self, config) -> None: ...
    def get_input_embeddings(self): ...
    def set_input_embeddings(self, new_embeddings): ...
    def get_encoder(self): ...
    def get_decoder(self): ...
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[Tensor] = ...,
        attention_mask: Optional[Tensor] = ...,
        bbox: Optional[dict[str, Any]] = ...,
        pixel_values: Optional[Tensor] = ...,
        visual_bbox: Optional[dict[str, Any]] = ...,
        decoder_input_ids: Optional[Tensor] = ...,
        decoder_attention_mask: Optional[Tensor] = ...,
        inputs_embeds: Optional[Tensor] = ...,
        encoder_outputs: Optional[Tensor] = ...,
        past_key_values: Optional[Cache] = ...,
        head_mask: Optional[Tensor] = ...,
        decoder_inputs_embeds: Optional[Tensor] = ...,
        decoder_head_mask: Optional[Tensor] = ...,
        cross_attn_head_mask: Optional[Tensor] = ...,
        use_cache=...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
        cache_position: Optional[torch.LongTensor] = ...,
    ) -> tuple[Tensor, ...]: ...

@auto_docstring(custom_intro=...)
class UdopForConditionalGeneration(UdopPreTrainedModel, GenerationMixin):
    _tied_weights_keys = ...
    def __init__(self, config) -> None: ...
    def get_input_embeddings(self): ...
    def set_input_embeddings(self, new_embeddings): ...
    def get_encoder(self): ...
    def get_decoder(self): ...
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[Tensor] = ...,
        attention_mask: Optional[Tensor] = ...,
        bbox: Optional[dict[str, Any]] = ...,
        pixel_values: Optional[Tensor] = ...,
        visual_bbox: Optional[dict[str, Any]] = ...,
        decoder_input_ids: Optional[Tensor] = ...,
        decoder_attention_mask: Optional[Tensor] = ...,
        inputs_embeds: Optional[Tensor] = ...,
        encoder_outputs: Optional[Tensor] = ...,
        past_key_values: Optional[Cache] = ...,
        head_mask: Optional[Tensor] = ...,
        decoder_inputs_embeds: Optional[Tensor] = ...,
        decoder_head_mask: Optional[Tensor] = ...,
        cross_attn_head_mask: Optional[Tensor] = ...,
        use_cache=...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
        labels: Optional[Tensor] = ...,
        cache_position: Optional[torch.LongTensor] = ...,
    ) -> tuple[Tensor, ...]: ...

@auto_docstring
class UdopEncoderModel(UdopPreTrainedModel):
    _tied_weights_keys = ...
    def __init__(self, config: UdopConfig) -> None: ...
    def get_input_embeddings(self): ...
    def set_input_embeddings(self, new_embeddings): ...
    def get_encoder(self): ...
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[Tensor] = ...,
        bbox: Optional[dict[str, Any]] = ...,
        attention_mask: Optional[Tensor] = ...,
        pixel_values: Optional[Tensor] = ...,
        visual_bbox: Optional[dict[str, Any]] = ...,
        head_mask: Optional[Tensor] = ...,
        inputs_embeds: Optional[Tensor] = ...,
        output_attentions: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        return_dict: Optional[bool] = ...,
    ) -> Union[tuple[torch.FloatTensor], BaseModelOutputWithAttentionMask]: ...

__all__ = ["UdopForConditionalGeneration", "UdopPreTrainedModel", "UdopModel", "UdopEncoderModel"]
