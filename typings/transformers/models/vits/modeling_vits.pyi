"""
This type stub file was generated by pyright.
"""

import torch
from dataclasses import dataclass
from typing import Any, Optional, Union
from torch import nn
from ...modeling_layers import GradientCheckpointingLayer
from ...modeling_outputs import BaseModelOutput, ModelOutput
from ...modeling_utils import PreTrainedModel
from ...utils import auto_docstring
from .configuration_vits import VitsConfig

"""PyTorch VITS model."""
logger = ...

@dataclass
@auto_docstring(
    custom_intro="""
    Describes the outputs for the VITS model, with potential hidden states and attentions.
    """
)
class VitsModelOutput(ModelOutput):
    r"""
    waveform (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):
        The final audio waveform predicted by the model.
    sequence_lengths (`torch.FloatTensor` of shape `(batch_size,)`):
        The length in samples of each element in the `waveform` batch.
    spectrogram (`torch.FloatTensor` of shape `(batch_size, sequence_length, num_bins)`):
        The log-mel spectrogram predicted at the output of the flow model. This spectrogram is passed to the Hi-Fi
        GAN decoder model to obtain the final audio waveform.
    """

    waveform: torch.FloatTensor | None = ...
    sequence_lengths: torch.FloatTensor | None = ...
    spectrogram: tuple[torch.FloatTensor] | None = ...
    hidden_states: tuple[torch.FloatTensor] | None = ...
    attentions: tuple[torch.FloatTensor] | None = ...

@dataclass
@auto_docstring(
    custom_intro="""
    Describes the outputs for the VITS text encoder model, with potential hidden states and attentions.
    """
)
class VitsTextEncoderOutput(ModelOutput):
    r"""
    prior_means (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):
        The predicted mean values of the prior distribution for the latent text variables.
    prior_log_variances (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):
        The predicted log-variance values of the prior distribution for the latent text variables.
    """

    last_hidden_state: torch.FloatTensor | None = ...
    prior_means: torch.FloatTensor | None = ...
    prior_log_variances: torch.FloatTensor | None = ...
    hidden_states: tuple[torch.FloatTensor] | None = ...
    attentions: tuple[torch.FloatTensor] | None = ...

@torch.jit.script
def fused_add_tanh_sigmoid_multiply(input_a, input_b, num_channels):  # -> Tensor:
    ...

class VitsWaveNet(torch.nn.Module):
    def __init__(self, config: VitsConfig, num_layers: int) -> None: ...
    def forward(self, inputs, padding_mask, global_conditioning=...): ...
    def remove_weight_norm(self):  # -> None:
        ...

class VitsPosteriorEncoder(nn.Module):
    def __init__(self, config: VitsConfig) -> None: ...
    def forward(self, inputs, padding_mask, global_conditioning=...):  # -> tuple[Any, Tensor, Tensor]:
        ...

class HifiGanResidualBlock(nn.Module):
    def __init__(self, channels, kernel_size=..., dilation=..., leaky_relu_slope=...) -> None: ...
    def get_padding(self, kernel_size, dilation=...): ...
    def apply_weight_norm(self):  # -> None:
        ...
    def remove_weight_norm(self):  # -> None:
        ...
    def forward(self, hidden_states): ...

class VitsHifiGan(nn.Module):
    def __init__(self, config: VitsConfig) -> None: ...
    def apply_weight_norm(self):  # -> None:
        ...
    def remove_weight_norm(self):  # -> None:
        ...
    def forward(
        self, spectrogram: torch.FloatTensor, global_conditioning: torch.FloatTensor | None = ...
    ) -> torch.FloatTensor:
        r"""
        Converts a spectrogram into a speech waveform.

        Args:
            spectrogram (`torch.FloatTensor` of shape `(batch_size, config.spectrogram_bins, sequence_length)`):
                Tensor containing the spectrograms.
            global_conditioning (`torch.FloatTensor` of shape `(batch_size, config.speaker_embedding_size, 1)`, *optional*):
                Tensor containing speaker embeddings, for multispeaker models.

        Returns:
            `torch.FloatTensor`: Tensor of shape shape `(batch_size, 1, num_frames)` containing the speech waveform.
        """
        ...

class VitsResidualCouplingLayer(nn.Module):
    def __init__(self, config: VitsConfig) -> None: ...
    def forward(
        self, inputs, padding_mask, global_conditioning=..., reverse=...
    ):  # -> tuple[Tensor, Tensor] | tuple[Tensor, None]:
        ...

class VitsResidualCouplingBlock(nn.Module):
    def __init__(self, config: VitsConfig) -> None: ...
    def forward(self, inputs, padding_mask, global_conditioning=..., reverse=...):  # -> Tensor | Any:
        ...

class VitsDilatedDepthSeparableConv(nn.Module):
    def __init__(self, config: VitsConfig, dropout_rate=...) -> None: ...
    def forward(self, inputs, padding_mask, global_conditioning=...): ...

class VitsConvFlow(nn.Module):
    def __init__(self, config: VitsConfig) -> None: ...
    def forward(
        self, inputs, padding_mask, global_conditioning=..., reverse=...
    ):  # -> tuple[Any, Tensor] | tuple[Any, None]:
        ...

class VitsElementwiseAffine(nn.Module):
    def __init__(self, config: VitsConfig) -> None: ...
    def forward(
        self, inputs, padding_mask, global_conditioning=..., reverse=...
    ):  # -> tuple[Any, Tensor] | tuple[Any, None]:
        ...

class VitsStochasticDurationPredictor(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(
        self, inputs, padding_mask, global_conditioning=..., durations=..., reverse=..., noise_scale=...
    ):  # -> Any | Tensor:
        ...

class VitsDurationPredictor(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, inputs, padding_mask, global_conditioning=...): ...

class VitsAttention(nn.Module):
    """Multi-headed attention with relative positional representation."""
    def __init__(self, config: VitsConfig) -> None: ...
    def forward(
        self,
        hidden_states: torch.Tensor,
        key_value_states: torch.Tensor | None = ...,
        attention_mask: torch.Tensor | None = ...,
        layer_head_mask: torch.Tensor | None = ...,
        output_attentions: bool = ...,
    ) -> tuple[torch.Tensor, torch.Tensor | None]:
        """Input shape: Batch x Time x Channel"""
        ...

class VitsFeedForward(nn.Module):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states, padding_mask): ...

class VitsEncoderLayer(GradientCheckpointingLayer):
    def __init__(self, config: VitsConfig) -> None: ...
    def forward(
        self,
        hidden_states: torch.Tensor,
        padding_mask: torch.FloatTensor,
        attention_mask: torch.Tensor | None = ...,
        output_attentions: bool = ...,
    ):  # -> tuple[Tensor, Any] | tuple[Tensor]:
        ...

class VitsEncoder(nn.Module):
    def __init__(self, config: VitsConfig) -> None: ...
    def forward(
        self,
        hidden_states: torch.FloatTensor,
        padding_mask: torch.FloatTensor,
        attention_mask: torch.Tensor | None = ...,
        output_attentions: bool | None = ...,
        output_hidden_states: bool | None = ...,
        return_dict: bool | None = ...,
    ) -> tuple | BaseModelOutput: ...

class VitsTextEncoder(nn.Module):
    """
    Transformer encoder that uses relative positional representation instead of absolute positional encoding.
    """
    def __init__(self, config: VitsConfig) -> None: ...
    def forward(
        self,
        input_ids: torch.Tensor,
        padding_mask: torch.FloatTensor,
        attention_mask: torch.Tensor | None = ...,
        output_attentions: bool | None = ...,
        output_hidden_states: bool | None = ...,
        return_dict: bool | None = ...,
    ) -> tuple[torch.Tensor] | VitsTextEncoderOutput: ...

@auto_docstring
class VitsPreTrainedModel(PreTrainedModel):
    config: VitsConfig
    base_model_prefix = ...
    main_input_name = ...
    supports_gradient_checkpointing = ...

@auto_docstring(
    custom_intro="""
    The complete VITS model, for text-to-speech synthesis.
    """
)
class VitsModel(VitsPreTrainedModel):
    def __init__(self, config: VitsConfig) -> None: ...
    def get_encoder(self):  # -> VitsTextEncoder:
        ...
    @auto_docstring
    def forward(
        self,
        input_ids: torch.Tensor | None = ...,
        attention_mask: torch.Tensor | None = ...,
        speaker_id: int | None = ...,
        output_attentions: bool | None = ...,
        output_hidden_states: bool | None = ...,
        return_dict: bool | None = ...,
        labels: torch.FloatTensor | None = ...,
    ) -> tuple[Any] | VitsModelOutput:
        r"""
        speaker_id (`int`, *optional*):
            Which speaker embedding to use. Only used for multispeaker models.
        labels (`torch.FloatTensor` of shape `(batch_size, config.spectrogram_bins, sequence_length)`, *optional*):
            Float values of target spectrogram. Timesteps set to `-100.0` are ignored (masked) for the loss
            computation.

        Example:

        ```python
        >>> from transformers import VitsTokenizer, VitsModel, set_seed
        >>> import torch

        >>> tokenizer = VitsTokenizer.from_pretrained("facebook/mms-tts-eng")
        >>> model = VitsModel.from_pretrained("facebook/mms-tts-eng")

        >>> inputs = tokenizer(text="Hello - my dog is cute", return_tensors="pt")

        >>> set_seed(555)  # make deterministic

        >>> with torch.no_grad():
        ...     outputs = model(inputs["input_ids"])
        >>> outputs.waveform.shape
        torch.Size([1, 45824])
        ```
        """
        ...

__all__ = ["VitsModel", "VitsPreTrainedModel"]
