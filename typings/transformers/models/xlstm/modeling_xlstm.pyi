"""
This type stub file was generated by pyright.
"""

import torch
from dataclasses import dataclass
from typing import Callable, Literal, Optional, TypeAlias, Union
from torch import nn
from ...generation import GenerationMixin
from ...modeling_utils import PreTrainedModel
from ...utils import ModelOutput, auto_docstring, can_return_tuple, is_xlstm_available
from .configuration_xlstm import xLSTMConfig
from xlstm.xlstm_large.model import mLSTMStateType

if is_xlstm_available():
    external_xlstm = ...
else:
    mLSTMLayerStateType: TypeAlias = tuple[torch.Tensor, torch.Tensor, torch.Tensor]
    mLSTMStateType = ...
    external_xlstm = ...
    def soft_cap(values: torch.Tensor, cap_value: Optional[Union[float, torch.Tensor]] = ...) -> torch.Tensor: ...
    def mlstm_chunkwise_recurrent_fw_C(
        matK: torch.Tensor,
        matV: torch.Tensor,
        vecB: torch.Tensor,
        vecI: torch.Tensor,
        matC_states: torch.Tensor = ...,
        vecN_states: torch.Tensor = ...,
        scaMinter_states: torch.Tensor = ...,
        matC_initial: torch.Tensor = ...,
        vecN_initial: torch.Tensor = ...,
        scaMinter_initial: torch.Tensor = ...,
        qk_scale: Optional[float] = ...,
        chunk_size: int = ...,
        num_chunks: int = ...,
    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]: ...
    def mlstm_chunkwise_parallel_fw_H(
        matQ: torch.Tensor,
        matK: torch.Tensor,
        matV: torch.Tensor,
        matC_states: torch.Tensor,
        vecN_states: torch.Tensor,
        scaMinter_states: torch.Tensor,
        vecI: torch.Tensor,
        vecB: torch.Tensor,
        qk_scale: float,
        chunk_size: int = ...,
        num_chunks: int = ...,
        eps: float = ...,
    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]: ...
    def mlstm_chunkwise_fw(
        query: torch.Tensor,
        key: torch.Tensor,
        value: torch.Tensor,
        igate: torch.Tensor,
        fgate: torch.Tensor,
        cstate: torch.Tensor = ...,
        nstate: torch.Tensor = ...,
        mstate: torch.Tensor = ...,
        qk_scale: Optional[float] = ...,
        return_last_states: bool = ...,
        return_all_states: bool = ...,
        chunk_size: int = ...,
        eps: float = ...,
    ) -> tuple[
        torch.Tensor,
        torch.Tensor,
        torch.Tensor,
        Optional[tuple[torch.Tensor, torch.Tensor, torch.Tensor]],
        Optional[tuple[torch.Tensor, torch.Tensor, torch.Tensor]],
    ]: ...
    def mlstm_chunkwise_native_autograd(
        query: torch.Tensor,
        key: torch.Tensor,
        value: torch.Tensor,
        igate: torch.Tensor,
        fgate: torch.Tensor,
        c_initial: torch.Tensor = ...,
        n_initial: torch.Tensor = ...,
        m_initial: torch.Tensor = ...,
        return_last_states: bool = ...,
        eps: float = ...,
        chunk_size: int = ...,
        **kwargs,
    ) -> Union[torch.Tensor, tuple[torch.Tensor, tuple[torch.Tensor, torch.Tensor, torch.Tensor]]]: ...
    def mlstm_recurrent_step_native(
        query: torch.Tensor,
        key: torch.Tensor,
        value: torch.Tensor,
        igate: torch.Tensor,
        fgate: torch.Tensor,
        cstate: torch.Tensor,
        nstate: torch.Tensor,
        mstate: torch.Tensor,
        eps: float = ...,
        dtype_state: torch.dtype = ...,
        **kwargs,
    ) -> tuple[torch.Tensor, tuple[torch.Tensor, torch.Tensor, torch.Tensor]]: ...
    def mlstm_recurrent_sequence_native(
        query: torch.Tensor,
        key: torch.Tensor,
        value: torch.Tensor,
        igate: torch.Tensor,
        fgate: torch.Tensor,
        c_initial: torch.Tensor = ...,
        n_initial: torch.Tensor = ...,
        m_initial: torch.Tensor = ...,
        return_last_states: bool = ...,
        eps: float = ...,
        dtype_state: torch.dtype = ...,
        **kwargs,
    ) -> tuple[
        torch.Tensor,
        torch.Tensor,
        torch.Tensor,
        Optional[tuple[torch.Tensor, torch.Tensor, torch.Tensor]],
        Optional[tuple[torch.Tensor, torch.Tensor, torch.Tensor]],
    ]: ...
    def wrap_chunkwise_pad_zeros(
        mlstm_chunkwise_kernel: Callable,
        query: torch.Tensor,
        key: torch.Tensor,
        value: torch.Tensor,
        fgate: torch.Tensor,
        igate: torch.Tensor,
        c_initial: torch.Tensor = ...,
        n_initial: torch.Tensor = ...,
        m_initial: torch.Tensor = ...,
        return_last_states: bool = ...,
        eps: float = ...,
        autocast_kernel_dtype: torch.dtype = ...,
        chunk_size: int = ...,
        **kwargs,
    ) -> Union[torch.Tensor, tuple[torch.Tensor, tuple[torch.Tensor, torch.Tensor, torch.Tensor]]]: ...
    def wrap_chunkwise_arbitrary_sequence_length(
        mlstm_chunkwise_kernel: Callable,
        mlstm_sequence_kernel: Callable,
        mlstm_step_kernel: Callable,
        query: torch.Tensor,
        key: torch.Tensor,
        value: torch.Tensor,
        fgate: torch.Tensor,
        igate: torch.Tensor,
        c_initial: torch.Tensor = ...,
        n_initial: torch.Tensor = ...,
        m_initial: torch.Tensor = ...,
        return_last_states: bool = ...,
        eps: float = ...,
        autocast_kernel_dtype: torch.dtype = ...,
        chunk_size: int = ...,
        enable_logging: bool = ...,
    ) -> Union[torch.Tensor, tuple[torch.Tensor, tuple[torch.Tensor, torch.Tensor, torch.Tensor]]]: ...

    class xLSTMBackend(nn.Module):
        config_class = xLSTMConfig
        def __init__(self, config: xLSTMConfig) -> None: ...
        def forward(
            self,
            query: torch.Tensor,
            key: torch.Tensor,
            value: torch.Tensor,
            igate: torch.Tensor,
            fgate: torch.Tensor,
            c_initial: torch.Tensor = ...,
            n_initial: torch.Tensor = ...,
            m_initial: torch.Tensor = ...,
            return_last_states: bool = ...,
            mode: Optional[Literal["train", "inference"]] = ...,
        ) -> Union[torch.Tensor, tuple[torch.Tensor, tuple[torch.Tensor, torch.Tensor, torch.Tensor]]]: ...
        def extra_repr(self) -> str: ...

    class xLSTMRMSNorm(nn.Module):
        def __init__(
            self,
            num_features: int,
            eps: float = ...,
            use_weight: bool = ...,
            use_bias: bool = ...,
            force_float32_reductions: bool = ...,
        ) -> None: ...
        def forward(self, x: torch.Tensor) -> torch.Tensor: ...

    class xLSTMMultiHeadLayerNorm(nn.Module):
        def __init__(
            self,
            num_heads: int,
            head_dim: int,
            eps: float = ...,
            use_weight: bool = ...,
            use_bias: bool = ...,
            force_float32_reductions: bool = ...,
        ) -> None: ...
        def forward(self, x: torch.Tensor) -> torch.Tensor: ...

    class xLSTMFeedForward(nn.Module):
        def __init__(self, config: xLSTMConfig) -> None: ...
        def forward(self, x: torch.Tensor) -> torch.Tensor: ...

    class xLSTMLayer(nn.Module):
        def __init__(self, config: xLSTMConfig) -> None: ...
        def forward(
            self, x: torch.Tensor, state: Optional[mLSTMLayerStateType] = ...
        ) -> tuple[torch.Tensor, Optional[mLSTMLayerStateType]]: ...

    class xLSTMBlock(nn.Module):
        def __init__(self, config: xLSTMConfig) -> None: ...
        def forward(
            self, x: torch.Tensor, state: Optional[mLSTMStateType] = ...
        ) -> tuple[torch.Tensor, mLSTMStateType]: ...

def small_init_method(dim): ...
def wang_init_method(n_layers, dim): ...

class xLSTMPreTrainedModel(PreTrainedModel):
    config_class = xLSTMConfig
    base_model_prefix = ...
    _no_split_modules = ...
    supports_gradient_checkpointing = ...
    _is_stateful = ...

class xLSTMCache:
    def __init__(
        self, config: xLSTMConfig, max_batch_size: int, dtype: torch.dtype = ..., device: Optional[str] = ..., **kwargs
    ) -> None: ...
    def reset(self): ...

@dataclass
@auto_docstring
class xLSTMOutput(ModelOutput):
    last_hidden_state: Optional[torch.FloatTensor]
    cache_params: Optional[xLSTMCache] = ...
    hidden_states: Optional[tuple[torch.FloatTensor]] = ...

@auto_docstring
class xLSTMModel(xLSTMPreTrainedModel):
    def __init__(self, config) -> None: ...
    def get_input_embeddings(self): ...
    def set_input_embeddings(self, new_embedding): ...
    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = ...,
        inputs_embeds: Optional[torch.LongTensor] = ...,
        cache_params: Optional[xLSTMCache] = ...,
        use_cache: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        **kwargs,
    ) -> Union[tuple, xLSTMOutput]: ...

@dataclass
@auto_docstring
class xLSTMCausalLMOutput(ModelOutput):
    loss: Optional[torch.FloatTensor] = ...
    logits: Optional[torch.FloatTensor] = ...
    cache_params: Optional[xLSTMCache] = ...
    hidden_states: Optional[tuple[torch.FloatTensor]] = ...

@auto_docstring
class xLSTMForCausalLM(xLSTMPreTrainedModel, GenerationMixin):
    def __init__(self, config) -> None: ...
    def get_output_embeddings(self): ...
    def set_output_embeddings(self, new_embeddings): ...
    def get_input_embeddings(self): ...
    def set_input_embeddings(self, new_embeddings): ...
    def prepare_inputs_for_generation(
        self,
        input_ids,
        attention_mask=...,
        inputs_embeds=...,
        use_cache=...,
        cache_params: Optional[xLSTMCache] = ...,
        **kwargs,
    ): ...
    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = ...,
        inputs_embeds: Optional[torch.FloatTensor] = ...,
        cache_params: Optional[xLSTMCache] = ...,
        labels: Optional[torch.LongTensor] = ...,
        use_cache: Optional[bool] = ...,
        output_hidden_states: Optional[bool] = ...,
        **kwargs,
    ) -> Union[tuple, xLSTMCausalLMOutput]: ...

__all__ = ["xLSTMForCausalLM", "xLSTMModel", "xLSTMPreTrainedModel"]
