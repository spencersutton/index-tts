"""
This type stub file was generated by pyright.
"""

import pathlib
import torch
from typing import Any, Optional, Union
from ...image_processing_utils import BatchFeature
from ...image_processing_utils_fast import BaseImageProcessorFast, DefaultFastImageProcessorKwargs, SizeDict
from ...image_utils import AnnotationFormat, AnnotationType, ChannelDimension, ImageInput
from ...processing_utils import Unpack
from ...utils import TensorType, auto_docstring, is_torch_available, is_torchvision_v2_available
from ...utils.import_utils import requires
from torchvision.transforms.v2 import functional as F
from torchvision.transforms import functional as F

if is_torch_available(): ...
if is_torchvision_v2_available(): ...
else: ...
logger = ...

class YolosFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):
    format: Optional[Union[str, AnnotationFormat]]
    do_convert_annotations: Optional[bool]
    do_pad: Optional[bool]
    pad_size: Optional[dict[str, int]]
    return_segmentation_masks: Optional[bool]
    ...

SUPPORTED_ANNOTATION_FORMATS = ...

def convert_coco_poly_to_mask(segmentations, height: int, width: int, device: torch.device) -> torch.Tensor: ...
def prepare_coco_detection_annotation(
    image,
    target,
    return_segmentation_masks: bool = ...,
    input_data_format: Optional[Union[ChannelDimension, str]] = ...,
): ...
def masks_to_boxes(masks: torch.Tensor) -> torch.Tensor: ...
def rgb_to_id(color): ...
def prepare_coco_panoptic_annotation(
    image: torch.Tensor,
    target: dict,
    masks_path: Union[str, pathlib.Path],
    return_masks: bool = ...,
    input_data_format: Union[ChannelDimension, str] = ...,
) -> dict: ...
def get_size_with_aspect_ratio(
    image_size: tuple[int, int], size: int, max_size: Optional[int] = ..., mod_size: int = ...
) -> tuple[int, int]: ...

@auto_docstring
@requires(backends=("torchvision", "torch"))
class YolosImageProcessorFast(BaseImageProcessorFast):
    resample = ...
    image_mean = ...
    image_std = ...
    format = ...
    do_resize = ...
    do_rescale = ...
    do_normalize = ...
    do_pad = ...
    size = ...
    default_to_square = ...
    model_input_names = ...
    valid_kwargs = YolosFastImageProcessorKwargs
    def __init__(self, **kwargs: Unpack[YolosFastImageProcessorKwargs]) -> None: ...
    @classmethod
    def from_dict(cls, image_processor_dict: dict[str, Any], **kwargs): ...
    def prepare_annotation(
        self,
        image: torch.Tensor,
        target: dict,
        format: Optional[AnnotationFormat] = ...,
        return_segmentation_masks: Optional[bool] = ...,
        masks_path: Optional[Union[str, pathlib.Path]] = ...,
        input_data_format: Optional[Union[str, ChannelDimension]] = ...,
    ) -> dict: ...
    def resize(
        self, image: torch.Tensor, size: SizeDict, interpolation: F.InterpolationMode = ..., **kwargs
    ) -> torch.Tensor: ...
    def resize_annotation(
        self,
        annotation: dict[str, Any],
        orig_size: tuple[int, int],
        target_size: tuple[int, int],
        threshold: float = ...,
        interpolation: F.InterpolationMode = ...,
    ): ...
    def normalize_annotation(self, annotation: dict, image_size: tuple[int, int]) -> dict: ...
    def pad(
        self,
        image: torch.Tensor,
        padded_size: tuple[int, int],
        annotation: Optional[dict[str, Any]] = ...,
        update_bboxes: bool = ...,
        fill: int = ...,
    ): ...
    @auto_docstring
    def preprocess(
        self,
        images: ImageInput,
        annotations: Optional[Union[AnnotationType, list[AnnotationType]]] = ...,
        masks_path: Optional[Union[str, pathlib.Path]] = ...,
        **kwargs: Unpack[YolosFastImageProcessorKwargs],
    ) -> BatchFeature: ...
    def post_process(self, outputs, target_sizes): ...
    def post_process_object_detection(
        self, outputs, threshold: float = ..., target_sizes: Union[TensorType, list[tuple]] = ..., top_k: int = ...
    ): ...

__all__ = ["YolosImageProcessorFast"]
