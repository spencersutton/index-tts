"""
This type stub file was generated by pyright.
"""

from collections.abc import Iterable
from pathlib import Path
from typing import Optional, TYPE_CHECKING, Union
from packaging.version import Version
from ..utils import is_tf_available, is_torch_available
from .config import OnnxConfig
from ..modeling_utils import PreTrainedModel
from ..modeling_tf_utils import TFPreTrainedModel
from ..feature_extraction_utils import FeatureExtractionMixin
from ..processing_utils import ProcessorMixin
from ..tokenization_utils import PreTrainedTokenizer

if is_torch_available(): ...
if is_tf_available(): ...
if TYPE_CHECKING: ...
logger = ...
ORT_QUANTIZE_MINIMUM_VERSION = ...

def check_onnxruntime_requirements(minimum_version: Version):  # -> None:
    """
    Check onnxruntime is installed and if the installed version match is recent enough

    Raises:
        ImportError: If onnxruntime is not installed or too old version is found
    """
    ...

def export_pytorch(
    preprocessor: PreTrainedTokenizer | FeatureExtractionMixin | ProcessorMixin,
    model: PreTrainedModel,
    config: OnnxConfig,
    opset: int,
    output: Path,
    tokenizer: PreTrainedTokenizer | None = ...,
    device: str = ...,
) -> tuple[list[str], list[str]]:
    """
    Export a PyTorch model to an ONNX Intermediate Representation (IR)

    Args:
        preprocessor: ([`PreTrainedTokenizer`], [`FeatureExtractionMixin`] or [`ProcessorMixin`]):
            The preprocessor used for encoding the data.
        model ([`PreTrainedModel`]):
            The model to export.
        config ([`~onnx.config.OnnxConfig`]):
            The ONNX configuration associated with the exported model.
        opset (`int`):
            The version of the ONNX operator set to use.
        output (`Path`):
            Directory to store the exported ONNX model.
        device (`str`, *optional*, defaults to `cpu`):
            The device on which the ONNX model will be exported. Either `cpu` or `cuda`.

    Returns:
        `tuple[list[str], list[str]]`: A tuple with an ordered list of the model's inputs, and the named inputs from
        the ONNX configuration.
    """
    ...

def export_tensorflow(
    preprocessor: PreTrainedTokenizer | FeatureExtractionMixin,
    model: TFPreTrainedModel,
    config: OnnxConfig,
    opset: int,
    output: Path,
    tokenizer: PreTrainedTokenizer | None = ...,
) -> tuple[list[str], list[str]]:
    """
    Export a TensorFlow model to an ONNX Intermediate Representation (IR)

    Args:
        preprocessor: ([`PreTrainedTokenizer`] or [`FeatureExtractionMixin`]):
            The preprocessor used for encoding the data.
        model ([`TFPreTrainedModel`]):
            The model to export.
        config ([`~onnx.config.OnnxConfig`]):
            The ONNX configuration associated with the exported model.
        opset (`int`):
            The version of the ONNX operator set to use.
        output (`Path`):
            Directory to store the exported ONNX model.

    Returns:
        `tuple[list[str], list[str]]`: A tuple with an ordered list of the model's inputs, and the named inputs from
        the ONNX configuration.
    """
    ...

def export(
    preprocessor: PreTrainedTokenizer | FeatureExtractionMixin | ProcessorMixin,
    model: PreTrainedModel | TFPreTrainedModel,
    config: OnnxConfig,
    opset: int,
    output: Path,
    tokenizer: PreTrainedTokenizer | None = ...,
    device: str = ...,
) -> tuple[list[str], list[str]]:
    """
    Export a Pytorch or TensorFlow model to an ONNX Intermediate Representation (IR)

    Args:
        preprocessor: ([`PreTrainedTokenizer`], [`FeatureExtractionMixin`] or [`ProcessorMixin`]):
            The preprocessor used for encoding the data.
        model ([`PreTrainedModel`] or [`TFPreTrainedModel`]):
            The model to export.
        config ([`~onnx.config.OnnxConfig`]):
            The ONNX configuration associated with the exported model.
        opset (`int`):
            The version of the ONNX operator set to use.
        output (`Path`):
            Directory to store the exported ONNX model.
        device (`str`, *optional*, defaults to `cpu`):
            The device on which the ONNX model will be exported. Either `cpu` or `cuda`. Only PyTorch is supported for
            export on CUDA devices.

    Returns:
        `tuple[list[str], list[str]]`: A tuple with an ordered list of the model's inputs, and the named inputs from
        the ONNX configuration.
    """
    ...

def validate_model_outputs(
    config: OnnxConfig,
    preprocessor: PreTrainedTokenizer | FeatureExtractionMixin | ProcessorMixin,
    reference_model: PreTrainedModel | TFPreTrainedModel,
    onnx_model: Path,
    onnx_named_outputs: list[str],
    atol: float,
    tokenizer: PreTrainedTokenizer | None = ...,
):  # -> None:
    ...
def ensure_model_and_config_inputs_match(
    model: PreTrainedModel | TFPreTrainedModel, model_inputs: Iterable[str]
) -> tuple[bool, list[str]]:
    """

    :param model_inputs: :param config_inputs: :return:
    """
    ...
