"""
This type stub file was generated by pyright.
"""

import os
import tensorflow as tf
import torch
from abc import ABC, abstractmethod
from contextlib import contextmanager
from typing import Any, Optional, TYPE_CHECKING, TypeAlias, Union
from ..feature_extraction_utils import PreTrainedFeatureExtractor
from ..image_processing_utils import BaseImageProcessor
from ..modelcard import ModelCard
from ..models.auto import AutoConfig
from ..processing_utils import ProcessorMixin
from ..tokenization_utils import PreTrainedTokenizer
from ..utils import ModelOutput, PushToHubMixin, add_end_docstrings, is_tf_available, is_torch_available
from torch.utils.data import Dataset
from ..modeling_utils import PreTrainedModel
from .pt_utils import KeyDataset
from ..modeling_tf_utils import TFPreTrainedModel

GenericTensor: TypeAlias = Union[list[GenericTensor], torch.Tensor, tf.Tensor]
if is_tf_available(): ...
if is_torch_available(): ...
else:
    Dataset = ...
    KeyDataset = ...
if TYPE_CHECKING: ...
logger = ...

def no_collate_fn(items): ...
def pad_collate_fn(tokenizer, feature_extractor): ...
def infer_framework_load_model(
    model,
    config: AutoConfig,
    model_classes: Optional[dict[str, tuple[type]]] = ...,
    task: Optional[str] = ...,
    framework: Optional[str] = ...,
    **model_kwargs,
): ...
def infer_framework_from_model(
    model,
    model_classes: Optional[dict[str, tuple[type]]] = ...,
    task: Optional[str] = ...,
    framework: Optional[str] = ...,
    **model_kwargs,
): ...
def get_framework(model, revision: Optional[str] = ...): ...
def get_default_model_and_revision(
    targeted_task: dict, framework: Optional[str], task_options: Optional[Any]
) -> tuple[str, str]: ...
def load_assistant_model(
    model: PreTrainedModel,
    assistant_model: Optional[Union[str, PreTrainedModel]],
    assistant_tokenizer: Optional[PreTrainedTokenizer],
) -> tuple[Optional[PreTrainedModel], Optional[PreTrainedTokenizer]]: ...

class PipelineException(Exception):
    def __init__(self, task: str, model: str, reason: str) -> None: ...

class ArgumentHandler(ABC):
    @abstractmethod
    def __call__(self, *args, **kwargs): ...

class PipelineDataFormat:
    SUPPORTED_FORMATS = ...
    def __init__(
        self, output_path: Optional[str], input_path: Optional[str], column: Optional[str], overwrite: bool = ...
    ) -> None: ...
    @abstractmethod
    def __iter__(self): ...
    @abstractmethod
    def save(self, data: Union[dict, list[dict]]): ...
    def save_binary(self, data: Union[dict, list[dict]]) -> str: ...
    @staticmethod
    def from_str(
        format: str, output_path: Optional[str], input_path: Optional[str], column: Optional[str], overwrite=...
    ) -> PipelineDataFormat: ...

class CsvPipelineDataFormat(PipelineDataFormat):
    def __init__(
        self, output_path: Optional[str], input_path: Optional[str], column: Optional[str], overwrite=...
    ) -> None: ...
    def __iter__(self): ...
    def save(self, data: list[dict]): ...

class JsonPipelineDataFormat(PipelineDataFormat):
    def __init__(
        self, output_path: Optional[str], input_path: Optional[str], column: Optional[str], overwrite=...
    ) -> None: ...
    def __iter__(self): ...
    def save(self, data: dict): ...

class PipedPipelineDataFormat(PipelineDataFormat):
    def __iter__(self): ...
    def save(self, data: dict): ...
    def save_binary(self, data: Union[dict, list[dict]]) -> str: ...

class _ScikitCompat(ABC):
    @abstractmethod
    def transform(self, X): ...
    @abstractmethod
    def predict(self, X): ...

def build_pipeline_init_args(
    has_tokenizer: bool = ...,
    has_feature_extractor: bool = ...,
    has_image_processor: bool = ...,
    has_processor: bool = ...,
    supports_binary_output: bool = ...,
) -> str: ...

PIPELINE_INIT_ARGS = ...
SUPPORTED_PEFT_TASKS = ...
if is_torch_available(): ...

@add_end_docstrings(
    build_pipeline_init_args(
        has_tokenizer=True, has_feature_extractor=True, has_image_processor=True, has_processor=True
    )
)
class Pipeline(_ScikitCompat, PushToHubMixin):
    _load_processor = ...
    _load_image_processor = ...
    _load_feature_extractor = ...
    _load_tokenizer = ...
    _pipeline_calls_generate = ...
    default_input_names = ...
    def __init__(
        self,
        model: Union[PreTrainedModel, TFPreTrainedModel],
        tokenizer: Optional[PreTrainedTokenizer] = ...,
        feature_extractor: Optional[PreTrainedFeatureExtractor] = ...,
        image_processor: Optional[BaseImageProcessor] = ...,
        processor: Optional[ProcessorMixin] = ...,
        modelcard: Optional[ModelCard] = ...,
        framework: Optional[str] = ...,
        task: str = ...,
        args_parser: ArgumentHandler = ...,
        device: Union[int, torch.device] = ...,
        torch_dtype: Optional[Union[str, torch.dtype]] = ...,
        binary_output: bool = ...,
        **kwargs,
    ) -> None: ...
    def save_pretrained(self, save_directory: Union[str, os.PathLike], safe_serialization: bool = ..., **kwargs): ...
    def transform(self, X): ...
    def predict(self, X): ...
    @property
    def torch_dtype(self) -> Optional[torch.dtype]: ...
    @contextmanager
    def device_placement(self): ...
    def ensure_tensor_on_device(self, **inputs): ...
    def check_model_type(self, supported_models: Union[list[str], dict]): ...
    @abstractmethod
    def preprocess(self, input_: Any, **preprocess_parameters: dict) -> dict[str, GenericTensor]: ...
    @abstractmethod
    def postprocess(self, model_outputs: ModelOutput, **postprocess_parameters: dict) -> Any: ...
    def get_inference_context(self): ...
    def forward(self, model_inputs, **forward_params): ...
    def get_iterator(
        self, inputs, num_workers: int, batch_size: int, preprocess_params, forward_params, postprocess_params
    ): ...
    def __call__(self, inputs, *args, num_workers=..., batch_size=..., **kwargs): ...
    def run_multi(self, inputs, preprocess_params, forward_params, postprocess_params): ...
    def run_single(self, inputs, preprocess_params, forward_params, postprocess_params): ...
    def iterate(self, inputs, preprocess_params, forward_params, postprocess_params): ...

if Pipeline.push_to_hub.__doc__ is not None: ...

class ChunkPipeline(Pipeline):
    def run_single(self, inputs, preprocess_params, forward_params, postprocess_params): ...
    def get_iterator(
        self, inputs, num_workers: int, batch_size: int, preprocess_params, forward_params, postprocess_params
    ): ...

class PipelineRegistry:
    def __init__(self, supported_tasks: dict[str, Any], task_aliases: dict[str, str]) -> None: ...
    def get_supported_tasks(self) -> list[str]: ...
    def check_task(self, task: str) -> tuple[str, dict, Any]: ...
    def register_pipeline(
        self,
        task: str,
        pipeline_class: type,
        pt_model: Optional[Union[type, tuple[type]]] = ...,
        tf_model: Optional[Union[type, tuple[type]]] = ...,
        default: Optional[dict] = ...,
        type: Optional[str] = ...,
    ) -> None: ...
    def to_dict(self): ...
