"""
This type stub file was generated by pyright.
"""

import enum
from typing import Any, Optional, Union, overload
from ..utils import add_end_docstrings, is_torch_available, is_vision_available
from .base import Pipeline, build_pipeline_init_args
from PIL import Image

if is_vision_available(): ...
if is_torch_available(): ...
logger = ...
IMAGE_TOKEN = ...

class ReturnType(enum.Enum):
    TENSORS = ...
    NEW_TEXT = ...
    FULL_TEXT = ...

class Chat:
    """This class is intended to just be used internally in this pipeline and not exposed to users. We convert chats
    to this format because the rest of the pipeline code tends to assume that lists of messages are
    actually a batch of samples rather than messages in the same conversation."""
    def __init__(
        self, messages: dict, images: Optional[Union[str, list[str], Image.Image, list[Image.Image]]] = ...
    ) -> None: ...

def add_images_to_messages(
    messages: dict, images: Optional[Union[str, list[str], Image.Image, list[Image.Image]]]
):  # -> dict[Any, Any]:
    """
    Retrieve and combine images from the chat and the images passed as input.
    """
    ...

@add_end_docstrings(build_pipeline_init_args(has_processor=True))
class ImageTextToTextPipeline(Pipeline):
    """
    Image-text-to-text pipeline using an `AutoModelForImageTextToText`. This pipeline generates text given an image and text.
    When the underlying model is a conversational model, it can also accept one or more chats,
    in which case the pipeline will operate in chat mode and will continue the chat(s) by adding its response(s).
    Each chat takes the form of a list of dicts, where each dict contains "role" and "content" keys.

    Unless the model you're using explicitly sets these generation parameters in its configuration files
    (`generation_config.json`), the following default values will be used:
    - max_new_tokens: 256

    Example:

    ```python
    >>> from transformers import pipeline

    >>> pipe = pipeline(task="image-text-to-text", model="Salesforce/blip-image-captioning-base")
    >>> pipe("https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png", text="A photo of")
    [{'generated_text': 'a photo of two birds'}]
    ```

    ```python
    >>> from transformers import pipeline

    >>> pipe = pipeline("image-text-to-text", model="llava-hf/llava-interleave-qwen-0.5b-hf")
    >>> messages = [
    >>>     {
    >>>         "role": "user",
    >>>         "content": [
    >>>             {
    >>>                 "type": "image",
    >>>                 "url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg",
    >>>             },
    >>>             {"type": "text", "text": "Describe this image."},
    >>>         ],
    >>>     },
    >>>     {
    >>>         "role": "assistant",
    >>>         "content": [
    >>>             {"type": "text", "text": "There is a dog and"},
    >>>         ],
    >>>     },
    >>> ]
    >>> pipe(text=messages, max_new_tokens=20, return_full_text=False)
    [{'input_text': [{'role': 'user',
        'content': [{'type': 'image',
        'url': 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'},
        {'type': 'text', 'text': 'Describe this image.'}]},
    {'role': 'assistant',
        'content': [{'type': 'text', 'text': 'There is a dog and'}]}],
    'generated_text': ' a person in the image. The dog is sitting on the sand, and the person is sitting on'}]
    ```

    Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)

    This image-text to text pipeline can currently be loaded from pipeline() using the following task identifier:
    "image-text-to-text".

    See the list of available models on
    [huggingface.co/models](https://huggingface.co/models?pipeline_tag=image-text-to-text).
    """

    _load_processor = ...
    _load_image_processor = ...
    _load_feature_extractor = ...
    _load_tokenizer = ...
    _pipeline_calls_generate = ...
    _default_generation_config = ...
    def __init__(self, *args, **kwargs) -> None: ...
    @overload
    def __call__(
        self, image: Optional[Union[str, Image.Image]] = ..., text: Optional[str] = ..., **kwargs: Any
    ) -> list[dict[str, Any]]: ...
    @overload
    def __call__(
        self, image: Optional[Union[list[str], list[Image.Image]]] = ..., text: Optional[list[str]] = ..., **kwargs: Any
    ) -> list[list[dict[str, Any]]]: ...
    def __call__(
        self,
        images: Optional[
            Union[
                str,
                list[str],
                list[list[str]],
                Image.Image,
                list[Image.Image],
                list[list[Image.Image]],
                list[dict],
            ]
        ] = ...,
        text: Optional[Union[str, list[str], list[dict]]] = ...,
        **kwargs,
    ) -> Union[list[dict[str, Any]], list[list[dict[str, Any]]]]:
        """
        Generate a text given text and the image(s) passed as inputs.

        Args:
            images (`str`, `list[str]`, `PIL.Image, `list[PIL.Image]`, `list[dict[str, Union[str, PIL.Image]]]`):
                The pipeline handles three types of images:

                - A string containing a HTTP(s) link pointing to an image
                - A string containing a local path to an image
                - An image loaded in PIL directly

                The pipeline accepts either a single image or a batch of images. Finally, this pipeline also supports
                the chat format (see `text`) containing images and text in this argument.
            text (str, list[str], `list[dict[str, Union[str, PIL.Image]]]`):
                The text to be used for generation. If a list of strings is passed, the length of the list should be
                the same as the number of images. Text can also follow the chat format: a list of dictionaries where
                each dictionary represents a message in a conversation. Each dictionary should have two keys: 'role'
                and 'content'. 'role' should be one of 'user', 'system' or 'assistant'. 'content' should be a list of
                dictionary containing the text of the message and the type of the message. The type of the message
                can be either 'text' or 'image'. If the type is 'image', no text is needed.
            return_tensors (`bool`, *optional*, defaults to `False`):
                Returns the tensors of predictions (as token indices) in the outputs. If set to
                `True`, the decoded text is not returned.
            return_text (`bool`, *optional*):
                Returns the decoded texts in the outputs.
            return_full_text (`bool`, *optional*, defaults to `True`):
                If set to `False` only added text is returned, otherwise the full text is returned. Cannot be
                specified at the same time as `return_text`.
            clean_up_tokenization_spaces (`bool`, *optional*, defaults to `True`):
                Whether or not to clean up the potential extra spaces in the text output.
            continue_final_message( `bool`, *optional*): This indicates that you want the model to continue the
                last message in the input chat rather than starting a new one, allowing you to "prefill" its response.
                By default this is `True` when the final message in the input chat has the `assistant` role and
                `False` otherwise, but you can manually override that behaviour by setting this flag.

        Return:
            A list or a list of list of `dict`: Each result comes as a dictionary with the following key (cannot
            return a combination of both `generated_text` and `generated_token_ids`):

            - **generated_text** (`str`, present when `return_text=True`) -- The generated text.
            - **generated_token_ids** (`torch.Tensor`, present when `return_tensors=True`) -- The token
                ids of the generated text.
            - **input_text** (`str`) -- The input text.
        """
        ...

    def preprocess(self, inputs=..., timeout=..., continue_final_message=..., **processing_kwargs): ...
    def postprocess(
        self, model_outputs, return_type=..., continue_final_message=..., **postprocess_kwargs
    ):  # -> list[dict[str, str | Chat | Any]] | list[dict[str, dict[Any, Any] | str | Any]]:
        ...
