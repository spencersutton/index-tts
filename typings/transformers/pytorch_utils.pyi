from collections.abc import Callable

import torch
from indextts.util import patch_call
from torch import Tensor, nn

ALL_LAYERNORM_LAYERS = ...
logger = ...
is_torch_greater_or_equal_than_2_8 = ...
is_torch_greater_or_equal_than_2_6 = ...
is_torch_greater_or_equal_than_2_4 = ...
is_torch_greater_or_equal_than_2_3 = ...
is_torch_greater_or_equal_than_2_2 = ...
is_torch_greater_or_equal_than_2_1 = ...
is_torch_greater_or_equal_than_2_0 = ...
is_torch_greater_or_equal_than_1_13 = ...
is_torch_greater_or_equal_than_1_12 = ...
_torch_distributed_available = ...

def softmax_backward_data(parent, grad_output, output, dim, self): ...
def prune_linear_layer(layer: nn.Linear, index: torch.LongTensor, dim: int = ...) -> nn.Linear: ...

class Conv1D(nn.Module):
    def forward(self, x: Tensor) -> Tensor: ...
    @patch_call(forward)
    def __call__(self) -> None: ...

def prune_conv1d_layer(layer: Conv1D, index: torch.LongTensor, dim: int = ...) -> Conv1D: ...
def prune_layer(layer: nn.Linear | Conv1D, index: torch.LongTensor, dim: int | None = ...) -> nn.Linear | Conv1D: ...
def apply_chunking_to_forward(
    forward_fn: Callable[..., torch.Tensor], chunk_size: int, chunk_dim: int, *input_tensors
) -> torch.Tensor: ...
def find_pruneable_heads_and_indices(
    heads: list[int], n_heads: int, head_size: int, already_pruned_heads: set[int]
) -> tuple[set[int], torch.LongTensor]: ...
def meshgrid(*tensors: torch.Tensor | list[torch.Tensor], indexing: str | None = ...) -> tuple[torch.Tensor, ...]: ...
def id_tensor_storage(tensor: torch.Tensor) -> tuple[torch.device, int, int]: ...
def isin_mps_friendly(elements: torch.Tensor, test_elements: torch.Tensor | int) -> torch.Tensor: ...
def compile_compatible_method_lru_cache(*lru_args, **lru_kwargs):  # -> Callable[..., _Wrapped[..., Any, ..., Any]]:

    ...
