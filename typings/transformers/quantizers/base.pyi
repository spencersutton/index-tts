from abc import ABC, abstractmethod
from typing import Any

import torch
from torch.nn import ModuleList

from ..modeling_utils import PreTrainedModel
from ..utils import is_torch_available
from ..utils.quantization_config import QuantizationConfigMixin

if is_torch_available(): ...
else:
    ModuleList = ...

class HfQuantizer(ABC):
    requires_calibration = ...
    required_packages = ...
    requires_parameters_quantization = ...
    def __init__(self, quantization_config: QuantizationConfigMixin, **kwargs) -> None: ...
    def update_torch_dtype(self, torch_dtype: torch.dtype) -> torch.dtype: ...
    def update_device_map(self, device_map: dict[str, Any] | None) -> dict[str, Any] | None: ...
    def adjust_target_dtype(self, torch_dtype: torch.dtype) -> torch.dtype: ...
    def update_missing_keys(self, model, missing_keys: list[str], prefix: str) -> list[str]: ...
    def update_unexpected_keys(self, model, unexpected_keys: list[str], prefix: str) -> list[str]: ...
    def update_missing_keys_after_loading(self, model, missing_keys: list[str], prefix: str) -> list[str]: ...
    def update_expected_keys(self, model, expected_keys: list[str], loaded_keys: list[str]) -> list[str]: ...
    def get_special_dtypes_update(self, model, torch_dtype: torch.dtype) -> dict[str, torch.dtype]: ...
    def adjust_max_memory(self, max_memory: dict[str, int | str]) -> dict[str, int | str]: ...
    def check_quantized_param(
        self, model: PreTrainedModel, param_value: torch.Tensor, param_name: str, state_dict: dict[str, Any], **kwargs
    ) -> bool: ...
    def create_quantized_param(self, *args, **kwargs) -> torch.nn.Parameter: ...
    def validate_environment(self, *args, **kwargs):  # -> None:

        ...
    def update_tp_plan(self, config): ...
    def preprocess_model(self, model: PreTrainedModel, **kwargs):  # -> None:

        ...
    def postprocess_model(self, model: PreTrainedModel, **kwargs):  # -> None:

        ...
    def remove_quantization_config(self, model):  # -> None:

        ...
    def dequantize(self, model): ...
    def get_cuda_warm_up_factor(self):  # -> Literal[4]:

        ...
    def update_param_name(self, param_name: str) -> str: ...
    @staticmethod
    def get_modules_to_not_convert(
        model: PreTrainedModel,
        skip_modules: list[str] | None = ...,
        keep_in_fp32_modules: list[str] | None = ...,
        add_default_skips: bool = ...,
    ):  # -> list[Any]:
        ...
    @property
    def is_qat_trainable(self) -> bool: ...
    @property
    def is_compileable(self) -> bool: ...
    @abstractmethod
    def is_serializable(self, safe_serialization=...):  # -> None:
        ...
    @property
    @abstractmethod
    def is_trainable(self):  # -> None:
        ...

class SequentialLlama4TextExperts(ModuleList):
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor: ...

MODULES_TO_PATCH_FOR_QUANTIZATION = ...
