"""
This type stub file was generated by pyright.
"""

import torch
from abc import ABC, abstractmethod
from typing import Any, Optional, TYPE_CHECKING, Union
from ..utils import is_torch_available
from ..utils.quantization_config import QuantizationConfigMixin
from ..modeling_utils import PreTrainedModel
from torch.nn import ModuleList

if TYPE_CHECKING: ...
if is_torch_available(): ...
else:
    ModuleList = ...

class HfQuantizer(ABC):
    """
    Abstract class of the HuggingFace quantizer. Supports for now quantizing HF transformers models for inference and/or quantization.
    This class is used only for transformers.PreTrainedModel.from_pretrained and cannot be easily used outside the scope of that method
    yet.

    Attributes
        quantization_config (`transformers.utils.quantization_config.QuantizationConfigMixin`):
            The quantization config that defines the quantization parameters of your model that you want to quantize.
        modules_to_not_convert (`list[str]`, *optional*):
            The list of module names to not convert when quantizing the model.
        required_packages (`list[str]`, *optional*):
            The list of required pip packages to install prior to using the quantizer
        requires_calibration (`bool`):
            Whether the quantization method requires to calibrate the model before using it.
        requires_parameters_quantization (`bool`):
            Whether the quantization method requires to create a new Parameter. For example, for bitsandbytes, it is
            required to create a new xxxParameter in order to properly quantize the model.
    """

    requires_calibration = ...
    required_packages = ...
    requires_parameters_quantization = ...
    def __init__(self, quantization_config: QuantizationConfigMixin, **kwargs) -> None: ...
    def update_torch_dtype(self, torch_dtype: torch.dtype) -> torch.dtype:
        """
        Some quantization methods require to explicitly set the dtype of the model to a
        target dtype. You need to override this method in case you want to make sure that behavior is
        preserved

        Args:
            torch_dtype (`torch.dtype`):
                The input dtype that is passed in `from_pretrained`
        """
        ...

    def update_device_map(self, device_map: dict[str, Any] | None) -> dict[str, Any] | None:
        """
        Override this method if you want to pass a override the existing device map with a new
        one. E.g. for bitsandbytes, since `accelerate` is a hard requirement, if no device_map is
        passed, the device_map is set to `"auto"``

        Args:
            device_map (`Union[dict, str]`, *optional*):
                The device_map that is passed through the `from_pretrained` method.
        """
        ...

    def adjust_target_dtype(self, torch_dtype: torch.dtype) -> torch.dtype:
        """
        Override this method if you want to adjust the `target_dtype` variable used in `from_pretrained`
        to compute the device_map in case the device_map is a `str`. E.g. for bitsandbytes we force-set `target_dtype`
        to `torch.int8` and for 4-bit we pass a custom enum `accelerate.CustomDtype.int4`.

        Args:
            torch_dtype (`torch.dtype`, *optional*):
                The torch_dtype that is used to compute the device_map.
        """
        ...

    def update_missing_keys(self, model, missing_keys: list[str], prefix: str) -> list[str]:
        """
        Override this method if you want to adjust the `missing_keys`.

        Args:
            missing_keys (`list[str]`, *optional*):
                The list of missing keys in the checkpoint compared to the state dict of the model
        """
        ...

    def update_unexpected_keys(self, model, unexpected_keys: list[str], prefix: str) -> list[str]:
        """
        Override this method if you want to adjust the `unexpected_keys`.

        Args:
            unexpected_keys (`list[str]`, *optional*):
                The list of unexpected keys in the checkpoint compared to the state dict of the model
        """
        ...

    def update_missing_keys_after_loading(self, model, missing_keys: list[str], prefix: str) -> list[str]:
        """
        Override this method if you want to adjust the `missing_keys` after loading the model params,
        but before the model is post-processed.

        Args:
            missing_keys (`list[str]`, *optional*):
                The list of missing keys in the checkpoint compared to the state dict of the model
        """
        ...

    def update_expected_keys(self, model, expected_keys: list[str], loaded_keys: list[str]) -> list[str]:
        """
        Override this method if you want to adjust the `update_expected_keys`.

        Args:
            expected_keys (`list[str]`, *optional*):
                The list of the expected keys in the initialized model.
            loaded_keys (`list[str]`, *optional*):
                The list of the loaded keys in the checkpoint.
        """
        ...

    def get_special_dtypes_update(self, model, torch_dtype: torch.dtype) -> dict[str, torch.dtype]:
        """
        returns dtypes for modules that are not quantized - used for the computation of the device_map in case
        one passes a str as a device_map. The method will use the `modules_to_not_convert` that is modified
        in `_process_model_before_weight_loading`.

        Args:
            model (`~transformers.PreTrainedModel`):
                The model to quantize
            torch_dtype (`torch.dtype`):
                The dtype passed in `from_pretrained` method.
        """
        ...

    def adjust_max_memory(self, max_memory: dict[str, int | str]) -> dict[str, int | str]:
        """adjust max_memory argument for infer_auto_device_map() if extra memory is needed for quantization"""
        ...

    def check_quantized_param(
        self, model: PreTrainedModel, param_value: torch.Tensor, param_name: str, state_dict: dict[str, Any], **kwargs
    ) -> bool:
        """
        checks if a loaded state_dict component is part of quantized param + some validation; only defined if
        requires_parameters_quantization == True for quantization methods that require to create a new parameters
        for quantization.
        """
        ...

    def create_quantized_param(self, *args, **kwargs) -> torch.nn.Parameter:
        """
        takes needed components from state_dict and creates quantized param; only applicable if
        requires_parameters_quantization == True
        """
        ...

    def validate_environment(self, *args, **kwargs):  # -> None:
        """
        This method is used to potentially check for potential conflicts with arguments that are
        passed in `from_pretrained`. You need to define it for all future quantizers that are integrated with transformers.
        If no explicit check are needed, simply return nothing.
        """
        ...

    def update_tp_plan(self, config):
        "updates the tp plan for the scales"
        ...

    def preprocess_model(self, model: PreTrainedModel, **kwargs):  # -> None:
        """
        Setting model attributes and/or converting model before weights loading. At this point
        the model should be initialized on the meta device so you can freely manipulate the skeleton
        of the model in order to replace modules in-place. Make sure to override the abstract method `_process_model_before_weight_loading`.

        Args:
            model (`~transformers.PreTrainedModel`):
                The model to quantize
            kwargs (`dict`, *optional*):
                The keyword arguments that are passed along `_process_model_before_weight_loading`.
        """
        ...

    def postprocess_model(self, model: PreTrainedModel, **kwargs):  # -> None:
        """
        Post-process the model post weights loading.
        Make sure to override the abstract method `_process_model_after_weight_loading`.

        Args:
            model (`~transformers.PreTrainedModel`):
                The model to quantize
            kwargs (`dict`, *optional*):
                The keyword arguments that are passed along `_process_model_after_weight_loading`.
        """
        ...

    def remove_quantization_config(self, model):  # -> None:
        """
        Remove the quantization config from the model.
        """
        ...

    def dequantize(self, model):
        """
        Potentially dequantize the model to retrieve the original model, with some loss in accuracy / performance.
        Note not all quantization schemes support this.
        """
        ...

    def get_cuda_warm_up_factor(self):  # -> Literal[4]:
        """
        The factor to be used in `caching_allocator_warmup` to get the number of bytes to pre-allocate to warm up cuda.
        A factor of 2 means we allocate all bytes in the empty model (since we allocate in fp16), a factor of 4 means
        we allocate half the memory of the weights residing in the empty model, etc...
        """
        ...

    def update_param_name(self, param_name: str) -> str:
        """
        Override this method if you want to adjust the `param_name`.
        """
        ...

    @staticmethod
    def get_modules_to_not_convert(
        model: PreTrainedModel,
        skip_modules: list[str] | None = ...,
        keep_in_fp32_modules: list[str] | None = ...,
        add_default_skips: bool = ...,
    ):  # -> list[Any]:
        ...
    @property
    def is_qat_trainable(self) -> bool:
        """Flag indicating whether the quantized model can carry out quantization aware training"""
        ...

    @property
    def is_compileable(self) -> bool:
        """Flag indicating whether the quantized model can be compiled"""
        ...

    @abstractmethod
    def is_serializable(self, safe_serialization=...):  # -> None:
        ...
    @property
    @abstractmethod
    def is_trainable(self):  # -> None:
        ...

class SequentialLlama4TextExperts(ModuleList):
    """
    A module that implements a compressed version of a list of expert modules.
    This is specifically designed to work with Llama4TextExperts in MoE layers.
    """
    def __init__(self, config) -> None: ...
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor: ...

MODULES_TO_PATCH_FOR_QUANTIZATION = ...
