"""
This type stub file was generated by pyright.
"""

from typing import TYPE_CHECKING, Any, Union

import torch

from ..modeling_utils import PreTrainedModel
from ..utils import is_torch_available
from ..utils.quantization_config import QuantoConfig
from .base import HfQuantizer

if TYPE_CHECKING: ...
if is_torch_available(): ...
logger = ...

class QuantoHfQuantizer(HfQuantizer):
    """
    Quantizer for the quanto library
    """

    required_packages = ...
    requires_parameters_quantization = ...
    requires_calibration = ...
    def __init__(self, quantization_config: QuantoConfig, **kwargs) -> None: ...
    def post_init(self):  # -> None:
        r"""
        Safety checker
        """
        ...

    def validate_environment(self, *args, **kwargs):  # -> None:
        ...
    def update_device_map(self, device_map):  # -> dict[str, str] | dict[str, Any]:
        ...
    def update_torch_dtype(self, torch_dtype: torch.dtype) -> torch.dtype: ...
    def update_missing_keys(self, model, missing_keys: list[str], prefix: str) -> list[str]: ...
    def check_quantized_param(
        self, model: PreTrainedModel, param_value: torch.Tensor, param_name: str, state_dict: dict[str, Any], **kwargs
    ) -> bool:
        """
        Check if a parameter needs to be quantized.
        """
        ...

    def adjust_max_memory(self, max_memory: dict[str, int | str]) -> dict[str, int | str]: ...
    def create_quantized_param(
        self,
        model: PreTrainedModel,
        param_value: torch.Tensor,
        param_name: str,
        target_device: torch.device,
        *args,
        **kwargs,
    ):  # -> None:
        """
        Create the quantized parameter by calling .freeze() after setting it to the module.
        """
        ...

    def adjust_target_dtype(self, target_dtype: torch.dtype) -> torch.dtype: ...
    @property
    def is_trainable(self) -> bool: ...
    def is_serializable(self, safe_serialization=...):  # -> Literal[False]:
        ...
