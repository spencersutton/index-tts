"""
This type stub file was generated by pyright.
"""

from typing import TYPE_CHECKING, Any, Optional, Union

import torch

from ..modeling_utils import PreTrainedModel
from ..utils import is_torch_available
from .base import HfQuantizer

if TYPE_CHECKING: ...
if is_torch_available(): ...
logger = ...

def fuzzy_match_size(config_name: str) -> str | None:
    """
    Extract the size digit from strings like "4weight", "8weight".
    Returns the digit as an integer if found, otherwise None.
    """
    ...

def find_parent(model, name): ...

class TorchAoHfQuantizer(HfQuantizer):
    """
    Quantizer for torchao: https://github.com/pytorch/ao/
    """

    requires_parameters_quantization = ...
    requires_calibration = ...
    required_packages = ...
    def __init__(self, quantization_config, **kwargs) -> None: ...
    def validate_environment(self, *args, **kwargs):  # -> None:
        ...
    def update_torch_dtype(self, torch_dtype):  # -> dtype:
        ...
    def adjust_target_dtype(self, torch_dtype: torch.dtype) -> torch.dtype: ...
    def adjust_max_memory(self, max_memory: dict[str, int | str]) -> dict[str, int | str]: ...
    def check_quantized_param(
        self, model: PreTrainedModel, param_value: torch.Tensor, param_name: str, state_dict: dict[str, Any], **kwargs
    ) -> bool: ...
    def create_quantized_param(
        self,
        model: PreTrainedModel,
        param_value: torch.Tensor,
        param_name: str,
        target_device: torch.device,
        state_dict: dict[str, Any],
        unexpected_keys: list[str],
    ):  # -> None:
        """
        Each nn.Linear layer that needs to be quantized is processed here.
        First, we set the value the weight tensor, then we move it to the target device. Finally, we quantize the module.
        """
        ...

    def is_serializable(self, safe_serialization=...) -> bool: ...
    def get_cuda_warm_up_factor(self):  # -> int:
        """
        This factor is used in caching_allocator_warmup to determine how many bytes to pre-allocate for CUDA warmup.
        - A factor of 2 means we pre-allocate the full memory footprint of the model.
        - A factor of 4 means we pre-allocate half of that, and so on

        However, when using TorchAO, calculating memory usage with param.numel() * param.element_size() doesn't give the correct size for quantized weights (like int4 or int8)
        That's because TorchAO internally represents quantized tensors using subtensors and metadata, and the reported element_size() still corresponds to the torch_dtype
        not the actual bit-width of the quantized data.

        To correct for this:
        - Use a division factor of 8 for int4 weights
        - Use a division factor of 4 for int8 weights
        """
        ...

    @property
    def is_trainable(self) -> bool: ...
    @property
    def is_compileable(self) -> bool: ...
