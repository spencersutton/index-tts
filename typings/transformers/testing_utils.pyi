import contextlib
import doctest
import os
import unittest
from collections import UserDict
from collections.abc import Generator, Iterable, Iterator
from functools import cache
from typing import Any

from _pytest.doctest import Module
from pytest import DoctestItem
from transformers import Trainer

from .utils import (
    is_accelerate_available,
    is_flax_available,
    is_pytest_available,
    is_tf_available,
    is_torch_available,
    is_torch_hpu_available,
    is_torch_mlu_available,
    is_torch_npu_available,
    is_torch_xla_available,
    is_torch_xpu_available,
)

if is_accelerate_available(): ...
if is_pytest_available(): ...
else:
    Module = ...
    DoctestItem = ...
SMALL_MODEL_IDENTIFIER = ...
DUMMY_UNKNOWN_IDENTIFIER = ...
DUMMY_DIFF_TOKENIZER_IDENTIFIER = ...
USER = ...
ENDPOINT_STAGING = ...
TOKEN = ...
if is_torch_available():
    IS_ROCM_SYSTEM = ...
    IS_CUDA_SYSTEM = ...
    IS_XPU_SYSTEM = ...
else:
    IS_ROCM_SYSTEM = ...
    IS_CUDA_SYSTEM = ...
    IS_XPU_SYSTEM = ...
logger = ...

def parse_flag_from_env(key, default=...):  # -> bool | Literal[1, 0]:
    ...
def parse_int_from_env(key, default=...):  # -> int | None:
    ...

_run_slow_tests = ...
_run_flaky_tests = ...
_run_custom_tokenizers = ...
_run_staging = ...
_run_pipeline_tests = ...
_run_agent_tests = ...

def is_staging_test(test_case): ...
def is_pipeline_test(test_case): ...
def is_agent_test(test_case): ...
def slow(test_case): ...
def tooslow(test_case): ...
def skip_if_not_implemented(test_func):  # -> _Wrapped[..., Any, ..., Any]:
    ...
def apply_skip_if_not_implemented(cls): ...
def custom_tokenizers(test_case): ...
def require_bs4(test_case): ...
def require_galore_torch(test_case): ...
def require_apollo_torch(test_case): ...
def require_torch_optimi(test_case): ...
def require_lomo(test_case): ...
def require_grokadamw(test_case): ...
def require_schedulefree(test_case): ...
def require_cv2(test_case): ...
def require_levenshtein(test_case): ...
def require_nltk(test_case): ...
def require_accelerate(test_case, min_version: str = ...): ...
def require_triton(min_version: str = ...):  # -> Callable[..., Any]:

    ...
def require_gguf(test_case, min_version: str = ...): ...
def require_fsdp(test_case, min_version: str = ...): ...
def require_g2p_en(test_case): ...
def require_safetensors(test_case): ...
def require_rjieba(test_case): ...
def require_jieba(test_case): ...
def require_jinja(test_case): ...
def require_tf2onnx(test_case): ...
def require_onnx(test_case): ...
def require_timm(test_case): ...
def require_natten(test_case): ...
def require_torch(test_case): ...
def require_torch_greater_or_equal(version: str):  # -> Callable[..., Any]:

    ...
def require_huggingface_hub_greater_or_equal(version: str):  # -> Callable[..., Any]:

    ...
def require_flash_attn(test_case): ...
def require_kernels(test_case): ...
def require_flash_attn_3(test_case): ...
def require_torch_sdpa(test_case): ...
def require_read_token(test_case):  # -> type | _Wrapped[..., Any, ..., Any]:

    ...
def require_peft(test_case): ...
def require_torchvision(test_case): ...
def require_torchcodec(test_case): ...
def require_torch_or_tf(test_case): ...
def require_intel_extension_for_pytorch(test_case): ...
def require_tensorflow_probability(test_case): ...
def require_torchaudio(test_case): ...
def require_tf(test_case): ...
def require_flax(test_case): ...
def require_sentencepiece(test_case): ...
def require_sacremoses(test_case): ...
def require_seqio(test_case): ...
def require_scipy(test_case): ...
def require_tokenizers(test_case): ...
def require_tensorflow_text(test_case): ...
def require_keras_nlp(test_case): ...
def require_pandas(test_case): ...
def require_pytesseract(test_case): ...
def require_pytorch_quantization(test_case): ...
def require_vision(test_case): ...
def require_ftfy(test_case): ...
def require_spacy(test_case): ...
def require_torch_multi_gpu(test_case): ...
def require_torch_multi_accelerator(test_case): ...
def require_torch_non_multi_gpu(test_case): ...
def require_torch_non_multi_accelerator(test_case): ...
def require_torch_up_to_2_gpus(test_case): ...
def require_torch_up_to_2_accelerators(test_case): ...
def require_torch_xla(test_case): ...
def require_torch_neuroncore(test_case): ...
def require_torch_npu(test_case): ...
def require_torch_multi_npu(test_case): ...
def require_non_hpu(test_case): ...
def require_torch_xpu(test_case): ...
def require_non_xpu(test_case): ...
def require_torch_multi_xpu(test_case): ...
def require_torch_multi_hpu(test_case): ...

if is_torch_available(): ...
else:
    torch_device = ...
if is_tf_available(): ...
if is_flax_available():
    jax_device = ...
else:
    jax_device = ...

def require_torchdynamo(test_case): ...
def require_torchao(test_case): ...
def require_torchao_version_greater_or_equal(torchao_version):  # -> Callable[..., Any]:
    ...
def require_torch_tensorrt_fx(test_case): ...
def require_torch_gpu(test_case): ...
def require_torch_mps(test_case): ...
def require_large_cpu_ram(test_case, memory: float = ...): ...
def require_torch_large_gpu(test_case, memory: float = ...): ...
def require_torch_large_accelerator(test_case, memory: float = ...): ...
def require_torch_gpu_if_bnb_not_multi_backend_enabled(test_case): ...
def require_torch_accelerator(test_case): ...
def require_torch_fp16(test_case): ...
def require_fp8(test_case): ...
def require_torch_bf16(test_case): ...
def require_torch_bf16_gpu(test_case): ...
def require_deterministic_for_xpu(test_case):  # -> _Wrapped[..., Any, ..., Any]:
    ...
def require_torch_tf32(test_case): ...
def require_detectron2(test_case): ...
def require_faiss(test_case): ...
def require_optuna(test_case): ...
def require_ray(test_case): ...
def require_sigopt(test_case): ...
def require_swanlab(test_case): ...
def require_trackio(test_case): ...
def require_wandb(test_case): ...
def require_clearml(test_case): ...
def require_deepspeed(test_case): ...
def require_apex(test_case): ...
def require_aqlm(test_case): ...
def require_vptq(test_case): ...
def require_spqr(test_case): ...
def require_eetq(test_case): ...
def require_av(test_case): ...
def require_decord(test_case): ...
def require_bitsandbytes(test_case): ...
def require_optimum(test_case): ...
def require_tensorboard(test_case):  # -> Callable[[_FT], _FT]:

    ...
def require_gptq(test_case): ...
def require_hqq(test_case): ...
def require_auto_awq(test_case): ...
def require_auto_round(test_case): ...
def require_optimum_quanto(test_case): ...
def require_compressed_tensors(test_case): ...
def require_fbgemm_gpu(test_case): ...
def require_quark(test_case): ...
def require_flute_hadamard(test_case): ...
def require_fp_quant(test_case): ...
def require_qutlass(test_case): ...
def require_phonemizer(test_case): ...
def require_pyctcdecode(test_case): ...
def require_librosa(test_case): ...
def require_liger_kernel(test_case): ...
def require_essentia(test_case): ...
def require_pretty_midi(test_case): ...
def cmd_exists(cmd):  # -> bool:
    ...
def require_usr_bin_time(test_case): ...
def require_sudachi(test_case): ...
def require_sudachi_projection(test_case): ...
def require_jumanpp(test_case): ...
def require_cython(test_case): ...
def require_tiktoken(test_case): ...
def require_speech(test_case): ...
def require_openai(test_case): ...
def require_mistral_common(test_case): ...
def get_gpu_count():  # -> int:

    ...
def get_tests_dir(append_path=...):  # -> str:

    ...
def get_steps_per_epoch(trainer: Trainer) -> int: ...
def evaluate_side_effect_factory(side_effect_values: list[dict[str, float]]) -> Generator[dict[str, float]]: ...
def apply_print_resets(buf):  # -> str:
    ...
def assert_screenout(out, what):  # -> None:
    ...
def set_model_tester_for_less_flaky_test(test_case):  # -> None:
    ...
def set_config_for_less_flaky_test(config):  # -> None:
    ...
def set_model_for_less_flaky_test(model):  # -> None:
    ...

class CaptureStd:
    def __init__(self, out=..., err=..., replay=...) -> None: ...
    def __enter__(self):  # -> Self:
        ...
    def __exit__(self, *exc):  # -> None:
        ...

class CaptureStdout(CaptureStd):
    def __init__(self, replay=...) -> None: ...

class CaptureStderr(CaptureStd):
    def __init__(self, replay=...) -> None: ...

class CaptureLogger:
    def __init__(self, logger) -> None: ...
    def __enter__(self):  # -> Self:
        ...
    def __exit__(self, *exc):  # -> None:
        ...

@contextlib.contextmanager
def LoggingLevel(level):  # -> Generator[None, Any, None]:

    ...

class TemporaryHubRepo:
    def __init__(self, namespace: str | None = ..., token: str | None = ...) -> None: ...
    def __enter__(self):  # -> RepoUrl:
        ...
    def __exit__(self, exc, value, tb):  # -> None:
        ...

@contextlib.contextmanager
def ExtendSysPath(path: str | os.PathLike) -> Iterator[None]: ...

class TestCasePlus(unittest.TestCase):
    def setUp(self):  # -> None:
        ...
    @property
    def test_file_path(self):  # -> str:
        ...
    @property
    def test_file_path_str(self):  # -> str:
        ...
    @property
    def test_file_dir(self):  # -> Path:
        ...
    @property
    def test_file_dir_str(self):  # -> str:
        ...
    @property
    def tests_dir(self):  # -> Path:
        ...
    @property
    def tests_dir_str(self):  # -> str:
        ...
    @property
    def examples_dir(self):  # -> Path:
        ...
    @property
    def examples_dir_str(self):  # -> str:
        ...
    @property
    def repo_root_dir(self):  # -> Path:
        ...
    @property
    def repo_root_dir_str(self):  # -> str:
        ...
    @property
    def src_dir(self):  # -> Path:
        ...
    @property
    def src_dir_str(self):  # -> str:
        ...
    def get_env(self):  # -> dict[str, str]:

        ...
    def get_auto_remove_tmp_dir(self, tmp_dir=..., before=..., after=...):  # -> str:

        ...
    def python_one_liner_max_rss(self, one_liner_str):  # -> int:

        ...
    def tearDown(self):  # -> None:
        ...

def mockenv(**kwargs):  # -> _patch_dict:

    ...
@contextlib.contextmanager
def mockenv_context(*remove, **update):  # -> Generator[None, Any, None]:

    ...

pytest_opt_registered = ...

def pytest_addoption_shared(parser):  # -> None:

    ...
def pytest_terminal_summary_main(tr, id):  # -> None:

    ...

class _RunOutput:
    def __init__(self, returncode, stdout, stderr) -> None: ...

def execute_subprocess_async(cmd, env=..., stdin=..., timeout=..., quiet=..., echo=...) -> _RunOutput: ...
def pytest_xdist_worker_id():  # -> int:

    ...
def get_torch_dist_unique_port():  # -> int:

    ...
def nested_simplify(
    obj, decimals=...
):  # -> list[list[Any] | tuple[list[Any] | tuple[Any, ...] | Any | dict[Any, Any] | str | int | int64 | float, ...] | Any | dict[Any, Any] | str | int | int64 | float] | tuple[list[Any] | tuple[Any, ...] | Any | dict[Any, Any] | str | int | int64 | float, ...] | dict[Any, Any] | str | int | int64 | float:

    ...
def check_json_file_has_correct_format(file_path):  # -> None:
    ...
def to_2tuple(x):  # -> tuple[Any, Any]:
    ...

class SubprocessCallException(Exception): ...

def run_command(command: list[str], return_stdout=...):  # -> str | bytes | None:

    ...

class RequestCounter:
    def __enter__(self):  # -> Self:
        ...
    def __exit__(self, *args, **kwargs) -> None: ...
    def __getitem__(self, key: str) -> int: ...
    @property
    def total_calls(self) -> int: ...

def is_flaky(
    max_attempts: int = ..., wait_before_retry: float | None = ..., description: str | None = ...
):  # -> Callable[..., _Wrapped[..., Any, ..., Any]]:

    ...
def hub_retry(
    max_attempts: int = ..., wait_before_retry: float | None = ...
):  # -> Callable[..., _Wrapped[..., Any, ..., Any]]:

    ...
def run_first(test_case): ...
def run_test_in_subprocess(test_case, target_func, inputs=..., timeout=...):  # -> None:

    ...
def run_test_using_subprocess(func):  # -> _Wrapped[..., Any, ..., None]:

    ...
def preprocess_string(string, skip_cuda_tests):  # -> str:

    ...

class HfDocTestParser(doctest.DocTestParser):
    _EXAMPLE_RE = ...
    skip_cuda_tests: bool = ...
    def parse(self, string, name=...):  # -> list[str | Example]:

        ...

class HfDoctestModule(Module):
    def collect(self) -> Iterable[DoctestItem]: ...

if is_torch_available():
    BACKEND_MANUAL_SEED = ...
    BACKEND_EMPTY_CACHE = ...
    BACKEND_DEVICE_COUNT = ...
    BACKEND_RESET_MAX_MEMORY_ALLOCATED = ...
    BACKEND_MAX_MEMORY_ALLOCATED = ...
    BACKEND_RESET_PEAK_MEMORY_STATS = ...
    BACKEND_MEMORY_ALLOCATED = ...
    BACKEND_SYNCHRONIZE = ...
    BACKEND_TORCH_ACCELERATOR_MODULE = ...
else:
    BACKEND_MANUAL_SEED = ...
    BACKEND_EMPTY_CACHE = ...
    BACKEND_DEVICE_COUNT = ...
    BACKEND_RESET_MAX_MEMORY_ALLOCATED = ...
    BACKEND_RESET_PEAK_MEMORY_STATS = ...
    BACKEND_MAX_MEMORY_ALLOCATED = ...
    BACKEND_MEMORY_ALLOCATED = ...
    BACKEND_SYNCHRONIZE = ...
    BACKEND_TORCH_ACCELERATOR_MODULE = ...
if is_torch_hpu_available(): ...
if is_torch_mlu_available(): ...
if is_torch_npu_available(): ...
if is_torch_xpu_available(): ...
if is_torch_xla_available(): ...

def backend_manual_seed(device: str, seed: int): ...
def backend_empty_cache(device: str): ...
def backend_device_count(device: str): ...
def backend_reset_max_memory_allocated(device: str): ...
def backend_reset_peak_memory_stats(device: str): ...
def backend_max_memory_allocated(device: str): ...
def backend_memory_allocated(device: str): ...
def backend_synchronize(device: str): ...
def backend_torch_accelerator_module(device: str): ...

if is_torch_available(): ...

def compare_pipeline_output_to_hub_spec(output, hub_spec):  # -> None:
    ...
@require_torch
def cleanup(device: str, gc_collect=...):  # -> None:
    ...

type DeviceProperties = tuple[str | None, int | None, int | None]
type PackedDeviceProperties = tuple[str | None, None | int | tuple[int, int]]

@cache
def get_device_properties() -> DeviceProperties: ...
def unpack_device_properties(properties: PackedDeviceProperties | None = ...) -> DeviceProperties: ...

class Expectations(UserDict[PackedDeviceProperties, Any]):
    def get_expectation(self) -> Any: ...
    def unpacked(self) -> list[tuple[DeviceProperties, Any]]: ...
    @staticmethod
    def is_default(expectation_key: PackedDeviceProperties) -> bool: ...
    @staticmethod
    def score(properties: DeviceProperties, other: DeviceProperties) -> float: ...
    def find_expectation(self, properties: DeviceProperties = ...) -> Any: ...

def torchrun(script: str, nproc_per_node: int, is_torchrun: bool = ..., env: dict | None = ...):  # -> None:

    ...
