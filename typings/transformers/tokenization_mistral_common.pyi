"""
This type stub file was generated by pyright.
"""

import os
import numpy as np
import torch
from enum import Enum
from pathlib import Path
from typing import Callable, Optional, Union, overload
from transformers.tokenization_utils_base import BatchEncoding, EncodedInput, TextInput, TruncationStrategy
from transformers.utils import PaddingStrategy, TensorType, add_end_docstrings
from transformers.utils.hub import PushToHubMixin
from transformers.utils.import_utils import is_mistral_common_available, is_torch_available, requires
from mistral_common.protocol.instruct.validator import ValidationMode

if is_mistral_common_available(): ...
if is_torch_available(): ...
logger = ...
ENCODE_KWARGS_DOCSTRING = ...
ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING = ...

class MistralTokenizerType(str, Enum):
    """Enum for the different type of tokenizer."""

    spm = ...
    tekken = ...

@requires(backends=("mistral-common",))
class MistralCommonTokenizer(PushToHubMixin):
    """
    Class to wrap `mistral-common` tokenizers.

    `mistral-common` is the official tokenizer library for Mistral AI models. To use it, you need to install it with:

    ```bash
    pip install transformers[mistral-common]
    ```

    Otherwise the tokenizer falls back to the Transformers implementation of the tokenizer.

    For more info on `mistral-common`, see [mistral-common](https://github.com/mistralai/mistral-common).

    This class is a wrapper around a `mistral_common.tokens.tokenizers.mistral.MistralTokenizer`.
    It provides a Hugging Face compatible interface to tokenize using the official mistral-common tokenizer.

    Supports the following methods from the `PreTrainedTokenizerBase` class:

    - [`~MistralCommonTokenizer.get_vocab`]: Returns the vocabulary as a dictionary of token to index.
    - [`~MistralCommonTokenizer.encode`]: Encode a string to a list of integers.
    - [`~MistralCommonTokenizer.decode`]: Decode a list of integers to a string.
    - [`~MistralCommonTokenizer.batch_decode`]: Decode a batch of list of integers to a list of strings.
    - [`~MistralCommonTokenizer.convert_tokens_to_ids`]: Convert a list of tokens to a list of integers.
    - [`~MistralCommonTokenizer.convert_ids_to_tokens`]: Convert a list of integers to a list of tokens.
    - [`~MistralCommonTokenizer.tokenize`]: Tokenize a string.
    - [`~MistralCommonTokenizer.get_special_tokens_mask`]: Get the special tokens mask for a list of tokens.
    - [`~MistralCommonTokenizer.prepare_for_model`]: Prepare a list of inputs for the model.
    - [`~MistralCommonTokenizer.pad`]: Pad a list of inputs to the same length.
    - [`~MistralCommonTokenizer.truncate_sequences`]: Truncate a list of sequences to the same length.
    - [`~MistralCommonTokenizer.apply_chat_template`]: Apply a chat template to a list of messages.
    - [`~MistralCommonTokenizer.__call__`]: Tokenize a string or a list of strings.
    - [`~MistralCommonTokenizer.from_pretrained`]: Download and cache a pretrained tokenizer from the Hugging Face model hub or local directory.
    - [`~MistralCommonTokenizer.save_pretrained`]: Save a tokenizer to a directory, so it can be reloaded using the `from_pretrained` class method.
    - [`~MistralCommonTokenizer.push_to_hub`]: Upload tokenizer to the Hugging Face model hub.

    Here are the key differences with the `PreTrainedTokenizerBase` class:

    - Pair of sequences are not supported. The signature have been kept for compatibility but all arguments related to pair of sequences are ignored. The return values of pairs are returned as `None`.
    - The `is_split_into_words` argument is not supported.
    - The `return_token_type_ids` argument is not supported.
    - It is not possible to add new tokens to the tokenizer. Also the special tokens are handled differently from Transformers. In `mistral-common`, special tokens are never encoded directly. This means that: `tokenizer.encode("<s>")` will not return the ID of the `<s>` token. Instead, it will return a list of IDs corresponding to the tokenization of the string `"<s>"`. For more information, see the [mistral-common documentation](https://mistralai.github.io/mistral-common/usage/tokenizers/#special-tokens).

    If you have suggestions to improve this class, please open an issue on the [mistral-common GitHub repository](https://github.com/mistralai/mistral-common/issues) if it is related to the tokenizer or on the [Transformers GitHub repository](https://github.com/huggingface/transformers/issues) if it is related to the Hugging Face interface.
    """

    model_input_names: list[str] = ...
    padding_side: str = ...
    truncation_side: str = ...
    def __init__(
        self,
        tokenizer_path: Union[str, os.PathLike, Path],
        mode: ValidationMode = ...,
        model_max_length: int = ...,
        padding_side: str = ...,
        truncation_side: str = ...,
        model_input_names: Optional[list[str]] = ...,
        clean_up_tokenization_spaces: bool = ...,
        **kwargs,
    ) -> None:
        """
        Constructs a `MistralCommonTokenizer`.

        - **model_input_names** (`List[str]`) -- A list of inputs expected in the forward pass of the model.
        - **padding_side** (`str`) -- The default value for the side on which the model should have padding applied.
            Should be `'right'` or `'left'`.
        - **truncation_side** (`str`) -- The default value for the side on which the model should have truncation
            applied. Should be `'right'` or `'left'`.

        Args:
            tokenizer_path (`str` or `os.PathLike` or `Path`):
                Path to the tokenizer file to load the `MistralTokenizer`.
            mode (`ValidationMode`, *optional*, defaults to `ValidationMode.test`):
                The mode to use for the tokenizer. This will be passed to the `MistralTokenizer` constructor.
            model_max_length (`int`, *optional*):
                The maximum length (in number of tokens) for the inputs to the transformer model. When the tokenizer is
                loaded with [`~tokenization_utils_base.PreTrainedTokenizerBase.from_pretrained`], this will be set to the
                value stored for the associated model in `max_model_input_sizes` (see above). If no value is provided, will
                default to VERY_LARGE_INTEGER (`int(1e30)`).
            padding_side (`str`, *optional*):
                The side on which the model should have padding applied. Should be selected between ['right', 'left'].
                Default value is picked from the class attribute of the same name.
            truncation_side (`str`, *optional*):
                The side on which the model should have truncation applied. Should be selected between ['right', 'left'].
                Default value is picked from the class attribute of the same name.
            model_input_names (`List[string]`, *optional*):
                The list of inputs accepted by the forward pass of the model (like `"token_type_ids"` or
                `"attention_mask"`). Default value is picked from the class attribute of the same name.
            clean_up_tokenization_spaces (`bool`, *optional*, defaults to `False`):
                Whether or not the model should cleanup the spaces that were added when splitting the input text during the
                tokenization process.
        """
        ...

    @property
    def bos_token_id(self) -> int:
        """
        Id of the beginning of sentence token in the vocabulary.
        """
        ...

    @property
    def eos_token_id(self) -> int:
        """
        Id of the end of sentence token in the vocabulary.
        """
        ...

    @property
    def unk_token_id(self) -> int:
        """
        Id of the unknown token in the vocabulary.
        """
        ...

    @property
    def pad_token_id(self) -> int:
        """
        Id of the padding token in the vocabulary.
        """
        ...

    @property
    def bos_token(self) -> str:
        """
        String associated to the beginning of sentence token in the vocabulary.
        """
        ...

    @property
    def eos_token(self) -> str:
        """
        String associated to the end of sentence token in the vocabulary.
        """
        ...

    @property
    def unk_token(self) -> str:
        """
        String associated to the unknown token in the vocabulary.
        """
        ...

    @property
    def pad_token(self) -> str:
        """
        String associated to the padding token in the vocabulary.
        """
        ...

    @property
    def vocab_size(self) -> int:
        """
        Returns the size of the vocabulary.

        `int`: Size of the vocabulary.
        """
        ...

    def get_vocab(self) -> dict[str, int]:
        """
        Returns the vocabulary as a dictionary of token to index.

        This is a lossy conversion. There may be multiple token ids that decode to the same
        string due to partial UTF-8 byte sequences being converted to ï¿½.

        Returns:
            `Dict[str, int]`: The vocabulary.
        """
        ...

    def __len__(self):  # -> int:
        """
        Size of the full vocabulary with the added tokens.
        """
        ...

    @add_end_docstrings(
        ENCODE_KWARGS_DOCSTRING,
        """
            **kwargs: Not supported by `MistralCommonTokenizer.encode`.
                Will raise an error if used.
        """,
        """
        Returns:
            `List[int]`, `torch.Tensor`: The tokenized ids of the text.
        """,
    )
    def encode(
        self,
        text: Union[TextInput, EncodedInput],
        text_pair: None = ...,
        add_special_tokens: bool = ...,
        padding: Union[bool, str, PaddingStrategy] = ...,
        truncation: Union[bool, str, TruncationStrategy, None] = ...,
        max_length: Optional[int] = ...,
        stride: int = ...,
        pad_to_multiple_of: Optional[int] = ...,
        padding_side: Optional[str] = ...,
        return_tensors: Optional[Union[str, TensorType]] = ...,
        verbose: bool = ...,
        **kwargs,
    ) -> list[int]:
        """
        Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.

        Args:
            text (`str` or `List[int]`):
                The first sequence to be encoded. This can be a string or a list of integers (tokenized string ids).
            text_pair (`None`, *optional*):
                Not supported by `MistralCommonTokenizer.encode`. Kept to match `PreTrainedTokenizerBase.encode` signature.
        """
        ...

    def decode(
        self,
        token_ids: Union[int, list[int], np.ndarray, torch.Tensor],
        skip_special_tokens: bool = ...,
        clean_up_tokenization_spaces: Optional[bool] = ...,
        **kwargs,
    ) -> str:
        """
        Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special
        tokens and clean up tokenization spaces.

        Args:
            token_ids (`Union[int, List[int], np.ndarray, torch.Tensor]`):
                List of tokenized input ids. Can be obtained using the `__call__` method.
            skip_special_tokens (`bool`, *optional*, defaults to `False`):
                Whether or not to remove special tokens in the decoding.
            clean_up_tokenization_spaces (`bool`, *optional*):
                Whether or not to clean up the tokenization spaces. If `None`, will default to
                `self.clean_up_tokenization_spaces`.
            kwargs (additional keyword arguments, *optional*):
                Not supported by `MistralCommonTokenizer.decode`.
                Will raise an error if used.

        Returns:
            `str`: The decoded sentence.
        """
        ...

    def batch_decode(
        self,
        sequences: Union[list[int], list[list[int]], np.ndarray, torch.Tensor],
        skip_special_tokens: bool = ...,
        clean_up_tokenization_spaces: Optional[bool] = ...,
        **kwargs,
    ) -> list[str]:
        """
        Convert a list of lists of token ids into a list of strings by calling decode.

        Args:
            sequences (`Union[List[int], List[List[int]], np.ndarray, torch.Tensor]`):
                List of tokenized input ids. Can be obtained using the `__call__` method.
            skip_special_tokens (`bool`, *optional*, defaults to `False`):
                Whether or not to remove special tokens in the decoding.
            clean_up_tokenization_spaces (`bool`, *optional*):
                Whether or not to clean up the tokenization spaces. If `None`, will default to
                `self.clean_up_tokenization_spaces`.
            kwargs (additional keyword arguments, *optional*):
                Not supported by `MistralCommonTokenizer.batch_decode`.
                Will raise an error if used.

        Returns:
            `List[str]`: The list of decoded sentences.
        """
        ...

    @overload
    def convert_ids_to_tokens(self, ids: int, skip_special_tokens: bool = ...) -> str: ...
    @overload
    def convert_ids_to_tokens(self, ids: list[int], skip_special_tokens: bool = ...) -> list[str]: ...
    def convert_ids_to_tokens(
        self, ids: Union[int, list[int]], skip_special_tokens: bool = ...
    ) -> Union[str, list[str]]:
        """
        Converts a single index or a sequence of indices in a token or a sequence of tokens, using the vocabulary and
        added tokens.

        Args:
            ids (`int` or `List[int]`):
                The token id (or token ids) to convert to tokens.
            skip_special_tokens (`bool`, *optional*, defaults to `False`):
                Whether or not to remove special tokens in the decoding.

        Returns:
            `str` or `List[str]`: The decoded token(s).
        """
        ...

    def convert_tokens_to_ids(self, tokens: Union[str, list[str]]) -> Union[int, list[int]]:
        """
        Converts a token string (or a sequence of tokens) in a single integer id (or a sequence of ids), using the
        vocabulary.

        Args:
            tokens (`str` or `List[str]`): One or several token(s) to convert to token id(s).

        Returns:
            `int` or `List[int]`: The token id or list of token ids.
        """
        ...

    def tokenize(self, text: TextInput, **kwargs) -> list[str]:
        """
        Converts a string into a sequence of tokens, using the tokenizer.

        Split in words for word-based vocabulary or sub-words for sub-word-based vocabularies.

        Args:
            text (`str`):
                The sequence to be encoded.
            **kwargs (additional keyword arguments):
                Not supported by `MistralCommonTokenizer.tokenize`.
                Will raise an error if used.

        Returns:
            `List[str]`: The list of tokens.
        """
        ...

    def get_special_tokens_mask(
        self, token_ids_0: list, token_ids_1: None = ..., already_has_special_tokens: bool = ...
    ) -> list[int]:
        """
        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding
        special tokens using the tokenizer `prepare_for_model` or `encode_plus` methods.

        Args:
            token_ids_0 (`List[int]`):
                List of ids of the sequence.
            token_ids_1 (`List[int]`, *optional*):
                Not supported by `MistralCommonTokenizer`. Kept to match the interface of `PreTrainedTokenizerBase`.
            already_has_special_tokens (`bool`, *optional*, defaults to `False`):
                Whether or not the token list is already formatted with special tokens for the model.

        Returns:
            A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.
        """
        ...

    @add_end_docstrings(ENCODE_KWARGS_DOCSTRING, ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING)
    def prepare_for_model(
        self,
        ids: list[int],
        pair_ids: None = ...,
        add_special_tokens: bool = ...,
        padding: Union[bool, str, PaddingStrategy] = ...,
        truncation: Union[bool, str, TruncationStrategy, None] = ...,
        max_length: Optional[int] = ...,
        stride: int = ...,
        pad_to_multiple_of: Optional[int] = ...,
        padding_side: Optional[str] = ...,
        return_tensors: Optional[Union[str, TensorType]] = ...,
        return_attention_mask: Optional[bool] = ...,
        return_overflowing_tokens: bool = ...,
        return_special_tokens_mask: bool = ...,
        return_length: bool = ...,
        verbose: bool = ...,
        prepend_batch_axis: bool = ...,
        **kwargs,
    ) -> BatchEncoding:
        """
        Prepares a sequence of input id so that it can be used by the model. It
        adds special tokens, truncates sequences if overflowing while taking into account the special tokens and
        manages a moving window (with user defined stride) for overflowing tokens.

        Args:
            ids (`List[int]`):
                Tokenized input ids of the first sequence.
            pair_ids (`None`, *optional*):
                Not supported by `MistralCommonTokenizer`. Kept to match the interface of `PreTrainedTokenizerBase`.
        """
        ...

    def pad(
        self,
        encoded_inputs: Union[
            BatchEncoding,
            list[BatchEncoding],
            dict[str, EncodedInput],
            dict[str, list[EncodedInput]],
            list[dict[str, EncodedInput]],
        ],
        padding: Union[bool, str, PaddingStrategy] = ...,
        max_length: Optional[int] = ...,
        pad_to_multiple_of: Optional[int] = ...,
        padding_side: Optional[str] = ...,
        return_attention_mask: Optional[bool] = ...,
        return_tensors: Optional[Union[str, TensorType]] = ...,
        verbose: bool = ...,
    ) -> BatchEncoding:
        """
        Pad a single encoded input or a batch of encoded inputs up to predefined length or to the max sequence length
        in the batch.

        Padding side (left/right) padding token ids are defined at the tokenizer level (with `self.padding_side`,
        `self.pad_token_id`).
        <Tip>

        If the `encoded_inputs` passed are dictionary of numpy arrays, PyTorch tensors, the
        result will use the same type unless you provide a different tensor type with `return_tensors`. In the case of
        PyTorch tensors, you will lose the specific device of your tensors however.

        </Tip>

        Args:
            encoded_inputs ([`BatchEncoding`], list of [`BatchEncoding`], `Dict[str, List[int]]`, `Dict[str, List[List[int]]` or `List[Dict[str, List[int]]]`):
                Tokenized inputs. Can represent one input ([`BatchEncoding`] or `Dict[str, List[int]]`) or a batch of
                tokenized inputs (list of [`BatchEncoding`], *Dict[str, List[List[int]]]* or *List[Dict[str,
                List[int]]]*) so you can use this method during preprocessing as well as in a PyTorch Dataloader
                collate function.

                Instead of `List[int]` you can have tensors (numpy arrays, PyTorch tensors), see
                the note above for the return type.
            padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):
                 Select a strategy to pad the returned sequences (according to the model's padding side and padding
                 index) among:

                - `True` or `'longest'` (default): Pad to the longest sequence in the batch (or no padding if only a single
                  sequence if provided).
                - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum
                  acceptable input length for the model if that argument is not provided.
                - `False` or `'do_not_pad'`: No padding (i.e., can output a batch with sequences of different
                  lengths).
            max_length (`int`, *optional*):
                Maximum length of the returned list and optionally padding length (see above).
            pad_to_multiple_of (`int`, *optional*):
                If set will pad the sequence to a multiple of the provided value.

                This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability
                `>= 7.5` (Volta).
            padding_side (`str`, *optional*):
                The side on which the model should have padding applied. Should be selected between ['right', 'left'].
                Default value is picked from the class attribute of the same name.
            return_attention_mask (`bool`, *optional*):
                Whether to return the attention mask. If left to the default, will return the attention mask according
                to the specific tokenizer's default, defined by the `return_outputs` attribute.

                [What are attention masks?](../glossary#attention-mask)
            return_tensors (`str` or [`~utils.TensorType`], *optional*):
                If set, will return tensors instead of list of python integers. Acceptable values are:

                - `'pt'`: Return PyTorch `torch.Tensor` objects.
                - `'np'`: Return Numpy `np.ndarray` objects.
            verbose (`bool`, *optional*, defaults to `True`):
                Whether or not to print more information and warnings.
        """
        ...

    def truncate_sequences(
        self,
        ids: list[int],
        pair_ids: None = ...,
        num_tokens_to_remove: int = ...,
        truncation_strategy: Union[str, TruncationStrategy] = ...,
        stride: int = ...,
        **kwargs,
    ) -> tuple[list[int], None, list[int]]:
        """
        Truncates a sequence pair in-place following the strategy.

        Args:
            ids (`List[int]`):
                Tokenized input ids. Can be obtained from a string by chaining the `tokenize` and
                `convert_tokens_to_ids` methods.
            pair_ids (`None`, *optional*):
                Not supported by `MistralCommonTokenizer`. Kept to match the signature of `PreTrainedTokenizerBase.truncate_sequences`.
            num_tokens_to_remove (`int`, *optional*, defaults to 0):
                Number of tokens to remove using the truncation strategy.
            truncation_strategy (`str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `'longest_first'`):
                The strategy to follow for truncation. Can be:

                - `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or to the
                  maximum acceptable input length for the model if that argument is not provided.
                - `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths greater
                  than the model maximum admissible input size).
            stride (`int`, *optional*, defaults to 0):
                If set to a positive number, the overflowing tokens returned will contain some tokens from the main
                sequence returned. The value of this argument defines the number of additional tokens.

        Returns:
            `Tuple[List[int], None, List[int]]`: The truncated `ids` and the list of
            overflowing tokens. `None` is returned to match Transformers signature.
        """
        ...

    def apply_chat_template(
        self,
        conversation: Union[list[dict[str, str]], list[list[dict[str, str]]]],
        tools: Optional[list[Union[dict, Callable]]] = ...,
        continue_final_message: bool = ...,
        tokenize: bool = ...,
        padding: Union[bool, str, PaddingStrategy] = ...,
        truncation: bool = ...,
        max_length: Optional[int] = ...,
        return_tensors: Optional[Union[str, TensorType]] = ...,
        return_dict: bool = ...,
        **kwargs,
    ) -> Union[str, list[int], list[str], list[list[int]], BatchEncoding]:
        """
        Converts a list of dictionaries with `"role"` and `"content"` keys to a list of token
        ids.

        Args:
            conversation (Union[List[Dict[str, str]], List[List[Dict[str, str]]]]): A list of dicts
                with "role" and "content" keys, representing the chat history so far.
            tools (`List[Union[Dict, Callable]]`, *optional*):
                A list of tools (callable functions) that will be accessible to the model. If the template does not
                support function calling, this argument will have no effect. Each tool should be passed as a JSON Schema,
                giving the name, description and argument types for the tool. See our
                [chat templating guide](https://huggingface.co/docs/transformers/main/en/chat_templating#automated-function-conversion-for-tool-use)
                for more information.
            continue_final_message (bool, *optional*):
                If this is set, the chat will be formatted so that the final
                message in the chat is open-ended, without any EOS tokens. The model will continue this message
                rather than starting a new one. This allows you to "prefill" part of
                the model's response for it. Cannot be used at the same time as `add_generation_prompt`.
            tokenize (`bool`, defaults to `True`):
                Whether to tokenize the output. If `False`, the output will be a string.
            padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):
                 Select a strategy to pad the returned sequences (according to the model's padding side and padding
                 index) among:

                - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single
                  sequence if provided).
                - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum
                  acceptable input length for the model if that argument is not provided.
                - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different
                  lengths).
            truncation (`bool`, defaults to `False`):
                Whether to truncate sequences at the maximum length. Has no effect if tokenize is `False`.
            max_length (`int`, *optional*):
                Maximum length (in tokens) to use for padding or truncation. Has no effect if tokenize is `False`. If
                not specified, the tokenizer's `max_length` attribute will be used as a default.
            return_tensors (`str` or [`~utils.TensorType`], *optional*):
                If set, will return tensors of a particular framework. Has no effect if tokenize is `False`. Acceptable
                values are:
                - `'pt'`: Return PyTorch `torch.Tensor` objects.
            return_dict (`bool`, defaults to `False`):
                Whether to return a dictionary with named outputs. Has no effect if tokenize is `False`.
                If at least one conversation contains an image, its pixel values will be returned in the `pixel_values` key.
            kwargs (additional keyword arguments, *optional*):
                Not supported by `MistralCommonTokenizer.apply_chat_template`.
                Will raise an error if used.

        Returns:
            `Union[str, List[int], List[str], List[List[int]], BatchEncoding]`: A list of token ids representing the tokenized chat so far, including control
            tokens. This output is ready to pass to the model, either directly or via methods like `generate()`.
        """
        ...

    @add_end_docstrings(ENCODE_KWARGS_DOCSTRING, ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING)
    def __call__(
        self,
        text: Union[TextInput, EncodedInput, list[TextInput], list[EncodedInput], None] = ...,
        text_pair: None = ...,
        text_target: None = ...,
        text_pair_target: None = ...,
        add_special_tokens: bool = ...,
        padding: Union[bool, str, PaddingStrategy] = ...,
        truncation: Union[bool, str, TruncationStrategy, None] = ...,
        max_length: Optional[int] = ...,
        stride: int = ...,
        pad_to_multiple_of: Optional[int] = ...,
        padding_side: Optional[str] = ...,
        return_tensors: Optional[Union[str, TensorType]] = ...,
        return_attention_mask: Optional[bool] = ...,
        return_overflowing_tokens: bool = ...,
        return_special_tokens_mask: bool = ...,
        return_length: bool = ...,
        verbose: bool = ...,
        **kwargs,
    ) -> BatchEncoding:
        """
        Main method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of
        sequences.

        Args:
            text (`str`, `List[str]`, `List[List[str]]`, *optional*):
                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of int
                (encoded strings).
            text_pair (`None`, *optional*):
                Not supported by `MistralCommonTokenizer`. Kept to match the signature of `PreTrainedTokenizerBase.__call__`.
            text_target (`None`, *optional*):
                Not supported by `MistralCommonTokenizer`. Kept to match the signature of `PreTrainedTokenizerBase.__call__`.
            text_pair_target (`None`, *optional*):
                Not supported by `MistralCommonTokenizer`. Kept to match the signature of `PreTrainedTokenizerBase.__call__`.
        """
        ...

    @classmethod
    def from_pretrained(
        cls,
        pretrained_model_name_or_path: Union[str, os.PathLike],
        *init_inputs,
        mode: ValidationMode = ...,
        cache_dir: Optional[Union[str, os.PathLike]] = ...,
        force_download: bool = ...,
        local_files_only: bool = ...,
        token: Optional[Union[str, bool]] = ...,
        revision: str = ...,
        model_max_length: int = ...,
        padding_side: str = ...,
        truncation_side: str = ...,
        model_input_names: Optional[list[str]] = ...,
        clean_up_tokenization_spaces: bool = ...,
        **kwargs,
    ):  # -> Self:
        r"""
        Instantiate a `MistralCommonTokenizer` from a predefined
        tokenizer.

        Args:
            pretrained_model_name_or_path (`str` or `os.PathLike`):
                Can be either:

                - A string, the *model id* of a predefined tokenizer hosted inside a model repo on huggingface.co.
                - A path to a *directory* containing the tokenizer config, for instance saved
                  using the [`MistralCommonTokenizer.tokenization_mistral_common.save_pretrained`] method, e.g.,
                  `./my_model_directory/`.
            mode (`ValidationMode`, *optional*, defaults to `ValidationMode.test`):
                Validation mode for the `MistralTokenizer` tokenizer.
            cache_dir (`str` or `os.PathLike`, *optional*):
                Path to a directory in which a downloaded predefined tokenizer vocabulary files should be cached if the
                standard cache should not be used.
            force_download (`bool`, *optional*, defaults to `False`):
                Whether or not to force the (re-)download the vocabulary files and override the cached versions if they
                exist.
            token (`str` or *bool*, *optional*):
                The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated
                when running `hf auth login` (stored in `~/.huggingface`).
            local_files_only (`bool`, *optional*, defaults to `False`):
                Whether or not to only rely on local files and not to attempt to download any files.
            revision (`str`, *optional*, defaults to `"main"`):
                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any
                identifier allowed by git.
            max_length (`int`, *optional*):
                Controls the maximum length to use by one of the truncation/padding parameters.

                If left unset or set to `None`, this will use the predefined model maximum length if a maximum length
                is required by one of the truncation/padding parameters. If the model has no specific maximum input
                length (like XLNet) truncation/padding to a maximum length will be deactivated.
            padding_side (`str`, *optional*, defaults to `"left"`):
                The side on which the model should have padding applied. Should be selected between ['right', 'left'].
                Default value is picked from the class attribute of the same name.
            truncation_side (`str`, *optional*, defaults to `"right"`):
                The side on which the model should have truncation applied. Should be selected between ['right', 'left'].
            model_input_names (`List[string]`, *optional*):
                The list of inputs accepted by the forward pass of the model (like `"token_type_ids"` or
                `"attention_mask"`). Default value is picked from the class attribute of the same name.
            clean_up_tokenization_spaces (`bool`, *optional*, defaults to `False`):
                Whether or not the model should cleanup the spaces that were added when splitting the input text during the
                tokenization process.
            kwargs (additional keyword arguments, *optional*):
                Not supported by `MistralCommonTokenizer.from_pretrained`.
                Will raise an error if used.
        """
        ...

    def save_pretrained(
        self,
        save_directory: Union[str, os.PathLike, Path],
        push_to_hub: bool = ...,
        token: Optional[Union[str, bool]] = ...,
        commit_message: Optional[str] = ...,
        repo_id: Optional[str] = ...,
        private: Optional[bool] = ...,
        repo_url: Optional[str] = ...,
        organization: Optional[str] = ...,
        **kwargs,
    ) -> tuple[str]:
        """
        Save the full tokenizer state.


        This method make sure the full tokenizer can then be re-loaded using the
        [`~MistralCommonTokenizer.tokenization_mistral_common.from_pretrained`] class method.

        Args:
            save_directory (`str` or `os.PathLike`): The path to a directory where the tokenizer will be saved.
            push_to_hub (`bool`, *optional*, defaults to `False`):
                Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the
                repository you want to push to with `repo_id` (will default to the name of `save_directory` in your
                namespace).
            token (`str` or *bool*, *optional*, defaults to `None`):
                The token to use to push to the model hub. If `True`, will use the token in the `HF_TOKEN` environment
                variable.
            commit_message (`str`, *optional*): The commit message to use when pushing to the hub.
            repo_id (`str`, *optional*): The name of the repository to which push to the Hub.
            private (`bool`, *optional*): Whether the model repository is private or not.
            repo_url (`str`, *optional*): The URL to the Git repository to which push to the Hub.
            organization (`str`, *optional*): The name of the organization in which you would like to push your model.
            kwargs (`Dict[str, Any]`, *optional*):
                Not supported by `MistralCommonTokenizer.save_pretrained`.
                Will raise an error if used.

        Returns:
            A tuple of `str`: The files saved.
        """
        ...
