"""
This type stub file was generated by pyright.
"""

import os
import numpy as np
import torch
from enum import Enum
from pathlib import Path
from typing import Callable, Optional, Union, overload
from transformers.tokenization_utils_base import BatchEncoding, EncodedInput, TextInput, TruncationStrategy
from transformers.utils import PaddingStrategy, TensorType, add_end_docstrings
from transformers.utils.hub import PushToHubMixin
from transformers.utils.import_utils import is_mistral_common_available, is_torch_available, requires
from mistral_common.protocol.instruct.validator import ValidationMode

if is_mistral_common_available(): ...
if is_torch_available(): ...
logger = ...
ENCODE_KWARGS_DOCSTRING = ...
ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING = ...

class MistralTokenizerType(str, Enum):
    spm = ...
    tekken = ...

@requires(backends=("mistral-common",))
class MistralCommonTokenizer(PushToHubMixin):
    model_input_names: list[str] = ...
    padding_side: str = ...
    truncation_side: str = ...
    def __init__(
        self,
        tokenizer_path: Union[str, os.PathLike, Path],
        mode: ValidationMode = ...,
        model_max_length: int = ...,
        padding_side: str = ...,
        truncation_side: str = ...,
        model_input_names: Optional[list[str]] = ...,
        clean_up_tokenization_spaces: bool = ...,
        **kwargs,
    ) -> None: ...
    @property
    def bos_token_id(self) -> int: ...
    @property
    def eos_token_id(self) -> int: ...
    @property
    def unk_token_id(self) -> int: ...
    @property
    def pad_token_id(self) -> int: ...
    @property
    def bos_token(self) -> str: ...
    @property
    def eos_token(self) -> str: ...
    @property
    def unk_token(self) -> str: ...
    @property
    def pad_token(self) -> str: ...
    @property
    def vocab_size(self) -> int: ...
    def get_vocab(self) -> dict[str, int]: ...
    def __len__(self): ...
    @add_end_docstrings(ENCODE_KWARGS_DOCSTRING, ..., ...)
    def encode(
        self,
        text: Union[TextInput, EncodedInput],
        text_pair: None = ...,
        add_special_tokens: bool = ...,
        padding: Union[bool, str, PaddingStrategy] = ...,
        truncation: Union[bool, str, TruncationStrategy, None] = ...,
        max_length: Optional[int] = ...,
        stride: int = ...,
        pad_to_multiple_of: Optional[int] = ...,
        padding_side: Optional[str] = ...,
        return_tensors: Optional[Union[str, TensorType]] = ...,
        verbose: bool = ...,
        **kwargs,
    ) -> list[int]: ...
    def decode(
        self,
        token_ids: Union[int, list[int], np.ndarray, torch.Tensor],
        skip_special_tokens: bool = ...,
        clean_up_tokenization_spaces: Optional[bool] = ...,
        **kwargs,
    ) -> str: ...
    def batch_decode(
        self,
        sequences: Union[list[int], list[list[int]], np.ndarray, torch.Tensor],
        skip_special_tokens: bool = ...,
        clean_up_tokenization_spaces: Optional[bool] = ...,
        **kwargs,
    ) -> list[str]: ...
    @overload
    def convert_ids_to_tokens(self, ids: int, skip_special_tokens: bool = ...) -> str: ...
    @overload
    def convert_ids_to_tokens(self, ids: list[int], skip_special_tokens: bool = ...) -> list[str]: ...
    def convert_ids_to_tokens(
        self, ids: Union[int, list[int]], skip_special_tokens: bool = ...
    ) -> Union[str, list[str]]: ...
    def convert_tokens_to_ids(self, tokens: Union[str, list[str]]) -> Union[int, list[int]]: ...
    def tokenize(self, text: TextInput, **kwargs) -> list[str]: ...
    def get_special_tokens_mask(
        self, token_ids_0: list, token_ids_1: None = ..., already_has_special_tokens: bool = ...
    ) -> list[int]: ...
    @add_end_docstrings(ENCODE_KWARGS_DOCSTRING, ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING)
    def prepare_for_model(
        self,
        ids: list[int],
        pair_ids: None = ...,
        add_special_tokens: bool = ...,
        padding: Union[bool, str, PaddingStrategy] = ...,
        truncation: Union[bool, str, TruncationStrategy, None] = ...,
        max_length: Optional[int] = ...,
        stride: int = ...,
        pad_to_multiple_of: Optional[int] = ...,
        padding_side: Optional[str] = ...,
        return_tensors: Optional[Union[str, TensorType]] = ...,
        return_attention_mask: Optional[bool] = ...,
        return_overflowing_tokens: bool = ...,
        return_special_tokens_mask: bool = ...,
        return_length: bool = ...,
        verbose: bool = ...,
        prepend_batch_axis: bool = ...,
        **kwargs,
    ) -> BatchEncoding: ...
    def pad(
        self,
        encoded_inputs: Union[
            BatchEncoding,
            list[BatchEncoding],
            dict[str, EncodedInput],
            dict[str, list[EncodedInput]],
            list[dict[str, EncodedInput]],
        ],
        padding: Union[bool, str, PaddingStrategy] = ...,
        max_length: Optional[int] = ...,
        pad_to_multiple_of: Optional[int] = ...,
        padding_side: Optional[str] = ...,
        return_attention_mask: Optional[bool] = ...,
        return_tensors: Optional[Union[str, TensorType]] = ...,
        verbose: bool = ...,
    ) -> BatchEncoding: ...
    def truncate_sequences(
        self,
        ids: list[int],
        pair_ids: None = ...,
        num_tokens_to_remove: int = ...,
        truncation_strategy: Union[str, TruncationStrategy] = ...,
        stride: int = ...,
        **kwargs,
    ) -> tuple[list[int], None, list[int]]: ...
    def apply_chat_template(
        self,
        conversation: Union[list[dict[str, str]], list[list[dict[str, str]]]],
        tools: Optional[list[Union[dict, Callable]]] = ...,
        continue_final_message: bool = ...,
        tokenize: bool = ...,
        padding: Union[bool, str, PaddingStrategy] = ...,
        truncation: bool = ...,
        max_length: Optional[int] = ...,
        return_tensors: Optional[Union[str, TensorType]] = ...,
        return_dict: bool = ...,
        **kwargs,
    ) -> Union[str, list[int], list[str], list[list[int]], BatchEncoding]: ...
    @add_end_docstrings(ENCODE_KWARGS_DOCSTRING, ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING)
    def __call__(
        self,
        text: Union[TextInput, EncodedInput, list[TextInput], list[EncodedInput], None] = ...,
        text_pair: None = ...,
        text_target: None = ...,
        text_pair_target: None = ...,
        add_special_tokens: bool = ...,
        padding: Union[bool, str, PaddingStrategy] = ...,
        truncation: Union[bool, str, TruncationStrategy, None] = ...,
        max_length: Optional[int] = ...,
        stride: int = ...,
        pad_to_multiple_of: Optional[int] = ...,
        padding_side: Optional[str] = ...,
        return_tensors: Optional[Union[str, TensorType]] = ...,
        return_attention_mask: Optional[bool] = ...,
        return_overflowing_tokens: bool = ...,
        return_special_tokens_mask: bool = ...,
        return_length: bool = ...,
        verbose: bool = ...,
        **kwargs,
    ) -> BatchEncoding: ...
    @classmethod
    def from_pretrained(
        cls,
        pretrained_model_name_or_path: Union[str, os.PathLike],
        *init_inputs,
        mode: ValidationMode = ...,
        cache_dir: Optional[Union[str, os.PathLike]] = ...,
        force_download: bool = ...,
        local_files_only: bool = ...,
        token: Optional[Union[str, bool]] = ...,
        revision: str = ...,
        model_max_length: int = ...,
        padding_side: str = ...,
        truncation_side: str = ...,
        model_input_names: Optional[list[str]] = ...,
        clean_up_tokenization_spaces: bool = ...,
        **kwargs,
    ): ...
    def save_pretrained(
        self,
        save_directory: Union[str, os.PathLike, Path],
        push_to_hub: bool = ...,
        token: Optional[Union[str, bool]] = ...,
        commit_message: Optional[str] = ...,
        repo_id: Optional[str] = ...,
        private: Optional[bool] = ...,
        repo_url: Optional[str] = ...,
        organization: Optional[str] = ...,
        **kwargs,
    ) -> tuple[str]: ...
