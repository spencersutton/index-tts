from typing import Any, overload

from .tokenization_utils_base import AddedToken, PreTrainedTokenizerBase, TextInput

"""
Tokenization classes for python tokenizers. For fast tokenizers (provided by HuggingFace's tokenizers library) see
tokenization_utils_fast.py
"""
logger = ...
SPECIAL_TOKENS_MAP_FILE = ...
ADDED_TOKENS_FILE = ...
TOKENIZER_CONFIG_FILE = ...

class Trie:
    def __init__(self, *args) -> None: ...
    def update(self, *args):  # -> None:

        ...
    def add(self, word: str):  # -> None:

        ...
    def split(self, text: str) -> list[str]: ...
    def cut_text(self, text, offsets):  # -> list[Any]:
        ...

class ExtensionsTrie(Trie):
    def __init__(self, *args) -> None: ...
    def extensions(self, prefix: str):  # -> list[Any]:

        ...

class PreTrainedTokenizer(PreTrainedTokenizerBase):
    def __init__(self, **kwargs) -> None: ...
    @property
    def is_fast(self) -> bool: ...
    @property
    def vocab_size(self) -> int: ...
    @property
    def added_tokens_encoder(self) -> dict[str, int]: ...
    @property
    def added_tokens_decoder(self) -> dict[int, AddedToken]: ...
    @added_tokens_decoder.setter
    def added_tokens_decoder(self, value: dict[int, AddedToken | str]) -> dict[int, AddedToken]: ...
    def get_added_vocab(self) -> dict[str, int]: ...
    def __len__(self) -> int:  # -> int:

        ...
    def num_special_tokens_to_add(self, pair: bool = ...) -> int: ...
    def tokenize(self, text: TextInput, **kwargs) -> list[str]: ...
    def convert_tokens_to_ids(self, tokens: str | list[str]) -> int | list[int]: ...
    def prepare_for_tokenization(
        self, text: str, is_split_into_words: bool = ..., **kwargs
    ) -> tuple[str, dict[str, Any]]: ...
    def get_special_tokens_mask(
        self, token_ids_0: list, token_ids_1: list | None = ..., already_has_special_tokens: bool = ...
    ) -> list[int]: ...
    @overload
    def convert_ids_to_tokens(self, ids: int, skip_special_tokens: bool = ...) -> str: ...
    @overload
    def convert_ids_to_tokens(self, ids: list[int], skip_special_tokens: bool = ...) -> list[str]: ...
    def convert_ids_to_tokens(self, ids: int | list[int], skip_special_tokens: bool = ...) -> str | list[str]: ...
    def convert_tokens_to_string(self, tokens: list[str]) -> str: ...

    eos_token_id: str | None = ...
