"""
This type stub file was generated by pyright.
"""

from typing import Any, Optional, Union, overload
from .tokenization_utils_base import AddedToken, INIT_TOKENIZER_DOCSTRING, PreTrainedTokenizerBase, TextInput
from .utils import add_end_docstrings

logger = ...
SPECIAL_TOKENS_MAP_FILE = ...
ADDED_TOKENS_FILE = ...
TOKENIZER_CONFIG_FILE = ...

class Trie:
    def __init__(self, *args) -> None: ...
    def update(self, *args): ...
    def add(self, word: str): ...
    def split(self, text: str) -> list[str]: ...
    def cut_text(self, text, offsets): ...

class ExtensionsTrie(Trie):
    def __init__(self, *args) -> None: ...
    def extensions(self, prefix: str): ...

@add_end_docstrings(INIT_TOKENIZER_DOCSTRING)
class PreTrainedTokenizer(PreTrainedTokenizerBase):
    def __init__(self, **kwargs) -> None: ...
    @property
    def is_fast(self) -> bool: ...
    @property
    def vocab_size(self) -> int: ...
    @property
    def added_tokens_encoder(self) -> dict[str, int]: ...
    @property
    def added_tokens_decoder(self) -> dict[int, AddedToken]: ...
    @added_tokens_decoder.setter
    def added_tokens_decoder(self, value: dict[int, Union[AddedToken, str]]) -> dict[int, AddedToken]: ...
    def get_added_vocab(self) -> dict[str, int]: ...
    def __len__(self): ...
    def num_special_tokens_to_add(self, pair: bool = ...) -> int: ...
    def tokenize(self, text: TextInput, **kwargs) -> list[str]: ...
    def convert_tokens_to_ids(self, tokens: Union[str, list[str]]) -> Union[int, list[int]]: ...
    def prepare_for_tokenization(
        self, text: str, is_split_into_words: bool = ..., **kwargs
    ) -> tuple[str, dict[str, Any]]: ...
    def get_special_tokens_mask(
        self, token_ids_0: list, token_ids_1: Optional[list] = ..., already_has_special_tokens: bool = ...
    ) -> list[int]: ...
    @overload
    def convert_ids_to_tokens(self, ids: int, skip_special_tokens: bool = ...) -> str: ...
    @overload
    def convert_ids_to_tokens(self, ids: list[int], skip_special_tokens: bool = ...) -> list[str]: ...
    def convert_ids_to_tokens(
        self, ids: Union[int, list[int]], skip_special_tokens: bool = ...
    ) -> Union[str, list[str]]: ...
    def convert_tokens_to_string(self, tokens: list[str]) -> str: ...
