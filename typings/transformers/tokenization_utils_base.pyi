"""
This type stub file was generated by pyright.
"""

import os
import numpy as np
from collections import UserDict
from collections.abc import Sequence
from contextlib import contextmanager
from dataclasses import dataclass
from typing import Any, Callable, NamedTuple, Optional, TYPE_CHECKING, TypeAlias, Union
from .utils import (
    ExplicitEnum,
    PaddingStrategy,
    PushToHubMixin,
    TensorType,
    add_end_docstrings,
    is_tokenizers_available,
)
from tokenizers import AddedToken, Encoding as EncodingFast

if TYPE_CHECKING: ...

def import_protobuf_decode_error(error_message=...): ...

if is_tokenizers_available(): ...
else:
    @dataclass(frozen=False, eq=True)
    class AddedToken:
        def __init__(
            self, content: str, single_word=..., lstrip=..., rstrip=..., special=..., normalized=...
        ) -> None: ...
        def __getstate__(self): ...

    @dataclass
    class EncodingFast: ...

logger = ...
VERY_LARGE_INTEGER = ...
LARGE_INTEGER = ...
TextInput = str
PreTokenizedInput: TypeAlias = list[str]
EncodedInput: TypeAlias = list[int]
TextInputPair: TypeAlias = tuple[str, str]
PreTokenizedInputPair: TypeAlias = tuple[list[str], list[str]]
EncodedInputPair: TypeAlias = tuple[list[int], list[int]]
AudioInput: TypeAlias = Union[np.ndarray, torch.Tensor, list[np.ndarray], list[torch.Tensor]]
SPECIAL_TOKENS_MAP_FILE = ...
ADDED_TOKENS_FILE = ...
TOKENIZER_CONFIG_FILE = ...
FULL_TOKENIZER_FILE = ...
_re_tokenizer_file = ...

class TruncationStrategy(ExplicitEnum):
    ONLY_FIRST = ...
    ONLY_SECOND = ...
    LONGEST_FIRST = ...
    DO_NOT_TRUNCATE = ...

class CharSpan(NamedTuple):
    start: int
    end: int
    ...

class TokenSpan(NamedTuple):
    start: int
    end: int
    ...

class BatchEncoding(UserDict):
    def __init__(
        self,
        data: Optional[dict[str, Any]] = ...,
        encoding: Optional[Union[EncodingFast, Sequence[EncodingFast]]] = ...,
        tensor_type: Union[None, str, TensorType] = ...,
        prepend_batch_axis: bool = ...,
        n_sequences: Optional[int] = ...,
    ) -> None: ...
    @property
    def n_sequences(self) -> Optional[int]: ...
    @property
    def is_fast(self) -> bool: ...
    def __getitem__(self, item: Union[int, str]) -> Union[Any, EncodingFast]: ...
    def __getattr__(self, item: str): ...
    def __getstate__(self): ...
    def __setstate__(self, state): ...
    @property
    def encodings(self) -> Optional[list[EncodingFast]]: ...
    def tokens(self, batch_index: int = ...) -> list[str]: ...
    def sequence_ids(self, batch_index: int = ...) -> list[Optional[int]]: ...
    def words(self, batch_index: int = ...) -> list[Optional[int]]: ...
    def word_ids(self, batch_index: int = ...) -> list[Optional[int]]: ...
    def token_to_sequence(self, batch_or_token_index: int, token_index: Optional[int] = ...) -> int: ...
    def token_to_word(self, batch_or_token_index: int, token_index: Optional[int] = ...) -> int: ...
    def word_to_tokens(
        self, batch_or_word_index: int, word_index: Optional[int] = ..., sequence_index: int = ...
    ) -> Optional[TokenSpan]: ...
    def token_to_chars(self, batch_or_token_index: int, token_index: Optional[int] = ...) -> Optional[CharSpan]: ...
    def char_to_token(
        self, batch_or_char_index: int, char_index: Optional[int] = ..., sequence_index: int = ...
    ) -> int: ...
    def word_to_chars(
        self, batch_or_word_index: int, word_index: Optional[int] = ..., sequence_index: int = ...
    ) -> CharSpan: ...
    def char_to_word(
        self, batch_or_char_index: int, char_index: Optional[int] = ..., sequence_index: int = ...
    ) -> int: ...
    def convert_to_tensors(
        self, tensor_type: Optional[Union[str, TensorType]] = ..., prepend_batch_axis: bool = ...
    ): ...
    def to(self, device: Union[str, torch.device], *, non_blocking: bool = ...) -> BatchEncoding: ...

class SpecialTokensMixin:
    SPECIAL_TOKENS_ATTRIBUTES = ...
    def __init__(self, verbose=..., **kwargs) -> None: ...
    def sanitize_special_tokens(self) -> int: ...
    def add_special_tokens(
        self,
        special_tokens_dict: dict[str, Union[str, AddedToken, Sequence[Union[str, AddedToken]]]],
        replace_additional_special_tokens=...,
    ) -> int: ...
    def add_tokens(
        self, new_tokens: Union[str, AddedToken, Sequence[Union[str, AddedToken]]], special_tokens: bool = ...
    ) -> int: ...
    @property
    def pad_token_type_id(self) -> int: ...
    def __setattr__(self, key, value): ...
    def __getattr__(self, key): ...
    @property
    def special_tokens_map(self) -> dict[str, Union[str, list[str]]]: ...
    @property
    def special_tokens_map_extended(self) -> dict[str, Union[str, AddedToken, list[Union[str, AddedToken]]]]: ...
    @property
    def all_special_tokens_extended(self) -> list[Union[str, AddedToken]]: ...
    @property
    def all_special_tokens(self) -> list[str]: ...
    @property
    def all_special_ids(self) -> list[int]: ...

ENCODE_KWARGS_DOCSTRING = ...
ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING = ...
INIT_TOKENIZER_DOCSTRING = ...

@add_end_docstrings(INIT_TOKENIZER_DOCSTRING)
class PreTrainedTokenizerBase(SpecialTokensMixin, PushToHubMixin):
    vocab_files_names: dict[str, str] = ...
    pretrained_vocab_files_map: dict[str, dict[str, str]] = ...
    _auto_class: Optional[str] = ...
    model_input_names: list[str] = ...
    padding_side: str = ...
    truncation_side: str = ...
    slow_tokenizer_class = ...
    def __init__(self, **kwargs) -> None: ...
    @property
    def max_len_single_sentence(self) -> int: ...
    @property
    def max_len_sentences_pair(self) -> int: ...
    @max_len_single_sentence.setter
    def max_len_single_sentence(self, value) -> int: ...
    @max_len_sentences_pair.setter
    def max_len_sentences_pair(self, value) -> int: ...
    @property
    def added_tokens_decoder(self) -> dict[int, AddedToken]: ...
    def __len__(self) -> int: ...
    def get_vocab(self) -> dict[str, int]: ...
    def apply_chat_template(
        self,
        conversation: Union[list[dict[str, str]], list[list[dict[str, str]]]],
        tools: Optional[list[Union[dict, Callable]]] = ...,
        documents: Optional[list[dict[str, str]]] = ...,
        chat_template: Optional[str] = ...,
        add_generation_prompt: bool = ...,
        continue_final_message: bool = ...,
        tokenize: bool = ...,
        padding: Union[bool, str, PaddingStrategy] = ...,
        truncation: bool = ...,
        max_length: Optional[int] = ...,
        return_tensors: Optional[Union[str, TensorType]] = ...,
        return_dict: bool = ...,
        return_assistant_tokens_mask: bool = ...,
        tokenizer_kwargs: Optional[dict[str, Any]] = ...,
        **kwargs,
    ) -> Union[str, list[int], list[str], list[list[int]], BatchEncoding]: ...
    def encode_message_with_chat_template(
        self, message: dict[str, str], conversation_history: Optional[list[dict[str, str]]] = ..., **kwargs
    ) -> list[int]: ...
    def get_chat_template(self, chat_template: Optional[str] = ..., tools: Optional[list[dict]] = ...) -> str: ...
    @classmethod
    def from_pretrained(
        cls,
        pretrained_model_name_or_path: Union[str, os.PathLike],
        *init_inputs,
        cache_dir: Optional[Union[str, os.PathLike]] = ...,
        force_download: bool = ...,
        local_files_only: bool = ...,
        token: Optional[Union[str, bool]] = ...,
        revision: str = ...,
        trust_remote_code=...,
        **kwargs,
    ): ...
    @classmethod
    def convert_added_tokens(cls, obj: Union[AddedToken, Any], save=..., add_type_field=...): ...
    def save_chat_templates(
        self,
        save_directory: Union[str, os.PathLike],
        tokenizer_config: dict,
        filename_prefix: Optional[str],
        save_jinja_files: bool,
    ): ...
    def save_pretrained(
        self,
        save_directory: Union[str, os.PathLike],
        legacy_format: Optional[bool] = ...,
        filename_prefix: Optional[str] = ...,
        push_to_hub: bool = ...,
        **kwargs,
    ) -> tuple[str]: ...
    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = ...) -> tuple[str]: ...
    def tokenize(self, text: str, pair: Optional[str] = ..., add_special_tokens: bool = ..., **kwargs) -> list[str]: ...
    @add_end_docstrings(ENCODE_KWARGS_DOCSTRING, ..., ...)
    def encode(
        self,
        text: Union[TextInput, PreTokenizedInput, EncodedInput],
        text_pair: Optional[Union[TextInput, PreTokenizedInput, EncodedInput]] = ...,
        add_special_tokens: bool = ...,
        padding: Union[bool, str, PaddingStrategy] = ...,
        truncation: Union[bool, str, TruncationStrategy, None] = ...,
        max_length: Optional[int] = ...,
        stride: int = ...,
        padding_side: Optional[str] = ...,
        return_tensors: Optional[Union[str, TensorType]] = ...,
        **kwargs,
    ) -> list[int]: ...
    def num_special_tokens_to_add(self, pair: bool = ...) -> int: ...
    @add_end_docstrings(ENCODE_KWARGS_DOCSTRING, ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING)
    def __call__(
        self,
        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput], None] = ...,
        text_pair: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = ...,
        text_target: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput], None] = ...,
        text_pair_target: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = ...,
        add_special_tokens: bool = ...,
        padding: Union[bool, str, PaddingStrategy] = ...,
        truncation: Union[bool, str, TruncationStrategy, None] = ...,
        max_length: Optional[int] = ...,
        stride: int = ...,
        is_split_into_words: bool = ...,
        pad_to_multiple_of: Optional[int] = ...,
        padding_side: Optional[str] = ...,
        return_tensors: Optional[Union[str, TensorType]] = ...,
        return_token_type_ids: Optional[bool] = ...,
        return_attention_mask: Optional[bool] = ...,
        return_overflowing_tokens: bool = ...,
        return_special_tokens_mask: bool = ...,
        return_offsets_mapping: bool = ...,
        return_length: bool = ...,
        verbose: bool = ...,
        **kwargs,
    ) -> BatchEncoding: ...
    @add_end_docstrings(ENCODE_KWARGS_DOCSTRING, ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING)
    def encode_plus(
        self,
        text: Union[TextInput, PreTokenizedInput, EncodedInput],
        text_pair: Optional[Union[TextInput, PreTokenizedInput, EncodedInput]] = ...,
        add_special_tokens: bool = ...,
        padding: Union[bool, str, PaddingStrategy] = ...,
        truncation: Union[bool, str, TruncationStrategy, None] = ...,
        max_length: Optional[int] = ...,
        stride: int = ...,
        is_split_into_words: bool = ...,
        pad_to_multiple_of: Optional[int] = ...,
        padding_side: Optional[str] = ...,
        return_tensors: Optional[Union[str, TensorType]] = ...,
        return_token_type_ids: Optional[bool] = ...,
        return_attention_mask: Optional[bool] = ...,
        return_overflowing_tokens: bool = ...,
        return_special_tokens_mask: bool = ...,
        return_offsets_mapping: bool = ...,
        return_length: bool = ...,
        verbose: bool = ...,
        **kwargs,
    ) -> BatchEncoding: ...
    @add_end_docstrings(ENCODE_KWARGS_DOCSTRING, ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING)
    def batch_encode_plus(
        self,
        batch_text_or_text_pairs: Union[
            list[TextInput],
            list[TextInputPair],
            list[PreTokenizedInput],
            list[PreTokenizedInputPair],
            list[EncodedInput],
            list[EncodedInputPair],
        ],
        add_special_tokens: bool = ...,
        padding: Union[bool, str, PaddingStrategy] = ...,
        truncation: Union[bool, str, TruncationStrategy, None] = ...,
        max_length: Optional[int] = ...,
        stride: int = ...,
        is_split_into_words: bool = ...,
        pad_to_multiple_of: Optional[int] = ...,
        padding_side: Optional[str] = ...,
        return_tensors: Optional[Union[str, TensorType]] = ...,
        return_token_type_ids: Optional[bool] = ...,
        return_attention_mask: Optional[bool] = ...,
        return_overflowing_tokens: bool = ...,
        return_special_tokens_mask: bool = ...,
        return_offsets_mapping: bool = ...,
        return_length: bool = ...,
        verbose: bool = ...,
        split_special_tokens: bool = ...,
        **kwargs,
    ) -> BatchEncoding: ...
    def pad(
        self,
        encoded_inputs: Union[
            BatchEncoding,
            list[BatchEncoding],
            dict[str, EncodedInput],
            dict[str, list[EncodedInput]],
            list[dict[str, EncodedInput]],
        ],
        padding: Union[bool, str, PaddingStrategy] = ...,
        max_length: Optional[int] = ...,
        pad_to_multiple_of: Optional[int] = ...,
        padding_side: Optional[str] = ...,
        return_attention_mask: Optional[bool] = ...,
        return_tensors: Optional[Union[str, TensorType]] = ...,
        verbose: bool = ...,
    ) -> BatchEncoding: ...
    def create_token_type_ids_from_sequences(
        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = ...
    ) -> list[int]: ...
    def build_inputs_with_special_tokens(
        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = ...
    ) -> list[int]: ...
    @add_end_docstrings(ENCODE_KWARGS_DOCSTRING, ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING)
    def prepare_for_model(
        self,
        ids: list[int],
        pair_ids: Optional[list[int]] = ...,
        add_special_tokens: bool = ...,
        padding: Union[bool, str, PaddingStrategy] = ...,
        truncation: Union[bool, str, TruncationStrategy, None] = ...,
        max_length: Optional[int] = ...,
        stride: int = ...,
        pad_to_multiple_of: Optional[int] = ...,
        padding_side: Optional[str] = ...,
        return_tensors: Optional[Union[str, TensorType]] = ...,
        return_token_type_ids: Optional[bool] = ...,
        return_attention_mask: Optional[bool] = ...,
        return_overflowing_tokens: bool = ...,
        return_special_tokens_mask: bool = ...,
        return_offsets_mapping: bool = ...,
        return_length: bool = ...,
        verbose: bool = ...,
        prepend_batch_axis: bool = ...,
        **kwargs,
    ) -> BatchEncoding: ...
    def truncate_sequences(
        self,
        ids: list[int],
        pair_ids: Optional[list[int]] = ...,
        num_tokens_to_remove: int = ...,
        truncation_strategy: Union[str, TruncationStrategy] = ...,
        stride: int = ...,
    ) -> tuple[list[int], list[int], list[int]]: ...
    def convert_tokens_to_string(self, tokens: list[str]) -> str: ...
    def batch_decode(
        self,
        sequences: Union[list[int], list[list[int]], np.ndarray, torch.Tensor, tf.Tensor],
        skip_special_tokens: bool = ...,
        clean_up_tokenization_spaces: Optional[bool] = ...,
        **kwargs,
    ) -> list[str]: ...
    def decode(
        self,
        token_ids: Union[int, list[int], np.ndarray, torch.Tensor, tf.Tensor],
        skip_special_tokens: bool = ...,
        clean_up_tokenization_spaces: Optional[bool] = ...,
        **kwargs,
    ) -> str: ...
    def get_special_tokens_mask(
        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = ..., already_has_special_tokens: bool = ...
    ) -> list[int]: ...
    @staticmethod
    def clean_up_tokenization(out_string: str) -> str: ...
    @contextmanager
    def as_target_tokenizer(self): ...
    @classmethod
    def register_for_auto_class(cls, auto_class=...): ...
    def prepare_seq2seq_batch(
        self,
        src_texts: list[str],
        tgt_texts: Optional[list[str]] = ...,
        max_length: Optional[int] = ...,
        max_target_length: Optional[int] = ...,
        padding: str = ...,
        return_tensors: Optional[str] = ...,
        truncation: bool = ...,
        **kwargs,
    ) -> BatchEncoding: ...

def get_fast_tokenizer_file(tokenization_files: list[str]) -> str: ...

if PreTrainedTokenizerBase.push_to_hub.__doc__ is not None: ...
