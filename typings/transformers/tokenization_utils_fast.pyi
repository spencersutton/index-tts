from collections.abc import Iterable

from tokenizers import Tokenizer as TokenizerFast
from tokenizers.decoders import Decoder as DecoderFast

from .tokenization_utils import PreTrainedTokenizer
from .tokenization_utils_base import INIT_TOKENIZER_DOCSTRING, AddedToken, PreTrainedTokenizerBase, TruncationStrategy
from .utils import PaddingStrategy, add_end_docstrings

"""
Tokenization classes for fast tokenizers (provided by HuggingFace's tokenizers library). For slow (python) tokenizers
see tokenization_utils.py
"""
logger = ...
TOKENIZER_FILE = ...
SPECIAL_TOKENS_MAP_FILE = ...
TOKENIZER_CONFIG_FILE = ...
TIKTOKEN_VOCAB_FILE = ...
ADDED_TOKENS_FILE = ...
MODEL_TO_TRAINER_MAPPING = ...
VOCAB_FILES_NAMES = ...

@add_end_docstrings(INIT_TOKENIZER_DOCSTRING)
class PreTrainedTokenizerFast(PreTrainedTokenizerBase):
    vocab_files_names = ...
    slow_tokenizer_class: PreTrainedTokenizer = ...
    def __init__(self, *args, **kwargs) -> None: ...
    @property
    def is_fast(self) -> bool: ...
    @property
    def can_save_slow_tokenizer(self) -> bool: ...
    @property
    def vocab_size(self) -> int: ...
    def get_vocab(self) -> dict[str, int]: ...
    @property
    def vocab(self) -> dict[str, int]: ...
    @property
    def added_tokens_encoder(self) -> dict[str, int]: ...
    @property
    def added_tokens_decoder(self) -> dict[int, AddedToken]: ...
    def get_added_vocab(self) -> dict[str, int]: ...
    def __bool__(self) -> bool: ...
    def __len__(self) -> int: ...
    @property
    def backend_tokenizer(self) -> TokenizerFast: ...
    @property
    def decoder(self) -> DecoderFast: ...
    def convert_tokens_to_ids(self, tokens: str | Iterable[str]) -> int | list[int]: ...
    def num_special_tokens_to_add(self, pair: bool = ...) -> int: ...
    def convert_ids_to_tokens(self, ids: int | list[int], skip_special_tokens: bool = ...) -> str | list[str]: ...
    def tokenize(self, text: str, pair: str | None = ..., add_special_tokens: bool = ..., **kwargs) -> list[str]: ...
    def set_truncation_and_padding(
        self,
        padding_strategy: PaddingStrategy,
        truncation_strategy: TruncationStrategy,
        max_length: int,
        stride: int,
        pad_to_multiple_of: int | None,
        padding_side: str | None,
    ):  # -> None:

        ...
    def convert_tokens_to_string(self, tokens: list[str]) -> str: ...
    def train_new_from_iterator(
        self, text_iterator, vocab_size, length=..., new_special_tokens=..., special_tokens_map=..., **kwargs
    ): ...
