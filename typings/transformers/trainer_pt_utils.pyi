"""
This type stub file was generated by pyright.
"""

import numpy as np
import torch
import smdistributed.modelparallel.torch as smp
from collections.abc import Iterator
from contextlib import contextmanager
from dataclasses import dataclass
from typing import Any, Optional, Union
from torch.utils.data import Dataset, IterableDataset, Sampler
from torch.utils.data.distributed import DistributedSampler
from .utils import is_sagemaker_mp_enabled, is_torch_available, is_torch_xla_available, is_training_run_on_sagemaker
from torch.optim.lr_scheduler import LRScheduler

if is_training_run_on_sagemaker(): ...
if is_torch_xla_available(): ...
if is_torch_available(): ...
logger = ...

def get_dataloader_sampler(dataloader): ...
def atleast_1d(tensor_or_array: Union[torch.Tensor, np.ndarray]): ...
def torch_pad_and_concatenate(tensor1, tensor2, padding_index=...): ...
def numpy_pad_and_concatenate(array1, array2, padding_index=...): ...
def nested_concat(tensors, new_tensors, padding_index=...): ...
def find_batch_size(tensors): ...
def nested_numpify(tensors): ...
def nested_detach(tensors): ...
def nested_xla_mesh_reduce(tensors, name): ...
def distributed_concat(tensor: Any, num_total_examples: Optional[int] = ...) -> Any: ...
def distributed_broadcast_scalars(
    scalars: list[Union[int, float]], num_total_examples: Optional[int] = ..., device: Optional[torch.device] = ...
) -> torch.Tensor: ...
def reissue_pt_warnings(caught_warnings): ...
@contextmanager
def torch_distributed_zero_first(local_rank: int): ...

class DistributedSamplerWithLoop(DistributedSampler):
    def __init__(self, dataset, batch_size, **kwargs) -> None: ...
    def __iter__(self): ...

class EvalLoopContainer:
    def __init__(self, do_nested_concat: bool = ..., padding_index: int = ...) -> None: ...
    def add(self, tensors) -> None: ...
    def to_cpu_and_numpy(self) -> None: ...
    def get_arrays(self): ...

class SequentialDistributedSampler(Sampler):
    def __init__(self, dataset, num_replicas=..., rank=..., batch_size=...) -> None: ...
    def __iter__(self): ...
    def __len__(self): ...

def get_tpu_sampler(dataset: torch.utils.data.Dataset, batch_size: int): ...
def nested_new_like(arrays, num_samples, padding_index=...): ...
def expand_like(arrays, new_seq_length, padding_index=...): ...
def nested_truncate(tensors, limit): ...

class DistributedTensorGatherer:
    def __init__(self, world_size, num_samples, make_multiple_of=..., padding_index=...) -> None: ...
    def add_arrays(self, arrays): ...
    def finalize(self): ...

@dataclass
class LabelSmoother:
    epsilon: float = ...
    ignore_index: int = ...
    def __call__(self, model_output, labels, shift_labels=...): ...

def get_length_grouped_indices(lengths, batch_size, mega_batch_mult=..., generator=...): ...

class LengthGroupedSampler(Sampler):
    def __init__(
        self,
        batch_size: int,
        dataset: Optional[Dataset] = ...,
        lengths: Optional[list[int]] = ...,
        model_input_name: Optional[str] = ...,
        generator=...,
    ) -> None: ...
    def __len__(self): ...
    def __iter__(self): ...

class DistributedLengthGroupedSampler(DistributedSampler):
    def __init__(
        self,
        batch_size: int,
        dataset: Optional[Dataset] = ...,
        num_replicas: Optional[int] = ...,
        rank: Optional[int] = ...,
        seed: int = ...,
        drop_last: bool = ...,
        lengths: Optional[list[int]] = ...,
        model_input_name: Optional[str] = ...,
    ) -> None: ...
    def __iter__(self) -> Iterator: ...

class ShardSampler(Sampler):
    def __init__(
        self,
        dataset: Dataset,
        batch_size: int = ...,
        drop_last: bool = ...,
        num_processes: int = ...,
        process_index: int = ...,
    ) -> None: ...
    def __iter__(self): ...
    def __len__(self): ...

class IterableDatasetShard(IterableDataset):
    def __init__(
        self,
        dataset: IterableDataset,
        batch_size: int = ...,
        drop_last: bool = ...,
        num_processes: int = ...,
        process_index: int = ...,
        seed: int = ...,
    ) -> None: ...
    def set_epoch(self, epoch): ...
    def __iter__(self): ...
    def __len__(self): ...

def metrics_format(self, metrics: dict[str, float]) -> dict[str, float]: ...
def log_metrics(self, split, metrics): ...
def save_metrics(self, split, metrics, combined=...): ...
def save_state(self): ...
def get_model_param_count(model, trainable_only=...): ...
def get_parameter_names(model, forbidden_layer_types, forbidden_layer_names=...): ...
def get_module_class_from_name(module, name): ...
def remove_dummy_checkpoint(is_main_process, output_dir, filenames): ...

if is_sagemaker_mp_enabled():
    @smp.step()
    def smp_forward_backward(model, inputs, gradient_accumulation_steps=...): ...
    @smp.step()
    def smp_forward_only(model, inputs): ...
    def smp_gather(tensor): ...
    def smp_nested_concat(tensor): ...

@dataclass
class AcceleratorConfig:
    split_batches: bool = ...
    dispatch_batches: Optional[bool] = ...
    even_batches: bool = ...
    use_seedable_sampler: bool = ...
    non_blocking: Optional[bool] = ...
    gradient_accumulation_kwargs: Optional[dict] = ...
    use_configured_state: bool = ...
    @classmethod
    def from_json_file(cls, json_file): ...
    def to_dict(self): ...
    def pop(self, key, default=...): ...

class LayerWiseDummyOptimizer(torch.optim.Optimizer):
    def __init__(self, optimizer_dict=..., *args, **kwargs) -> None: ...
    def zero_grad(self, set_to_none: bool = ...) -> None: ...
    def step(self, closure=...) -> Optional[float]: ...

class LayerWiseDummyScheduler(LRScheduler):
    def __init__(self, *args, **kwargs) -> None: ...
    def get_lr(self): ...

def set_rng_state_for_device(device_name, device_module, checkpoint_rng_state, is_distributed): ...
