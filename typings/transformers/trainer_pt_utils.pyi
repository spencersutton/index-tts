from collections.abc import Iterator
from contextlib import contextmanager
from dataclasses import dataclass
from typing import Any

import numpy as np
import smdistributed.modelparallel.torch as smp
import torch
from torch.optim.lr_scheduler import LRScheduler
from torch.utils.data import Dataset, IterableDataset, Sampler
from torch.utils.data.distributed import DistributedSampler

from .utils import is_sagemaker_mp_enabled, is_torch_available, is_torch_xla_available, is_training_run_on_sagemaker

"""
Torch utilities for the Trainer class.
"""
if is_training_run_on_sagemaker(): ...
if is_torch_xla_available(): ...
if is_torch_available(): ...
logger = ...

def get_dataloader_sampler(dataloader):  # -> None:
    ...
def atleast_1d(tensor_or_array: torch.Tensor | np.ndarray):  # -> Tensor | ndarray[_AnyShape, dtype[Any]]:
    ...
def torch_pad_and_concatenate(tensor1, tensor2, padding_index=...):  # -> Tensor:

    ...
def numpy_pad_and_concatenate(array1, array2, padding_index=...):  # -> NDArray[Any]:

    ...
def nested_concat(
    tensors, new_tensors, padding_index=...
):  # -> list[Any] | tuple[Any, ...] | Tensor | Mapping[Any, object] | NDArray[Any]:

    ...
def find_batch_size(tensors):  # -> int | None:

    ...
def nested_numpify(tensors):  # -> list[Any] | tuple[Any, ...] | Mapping[Any, object]:

    ...
def nested_detach(tensors):  # -> list[Any] | tuple[Any, ...] | Mapping[Any, object] | Tensor:

    ...
def nested_xla_mesh_reduce(tensors, name):  # -> list[Any] | tuple[Any, ...] | Mapping[Any, object]:
    ...
def distributed_concat(tensor: Any, num_total_examples: int | None = ...) -> Any: ...
def distributed_broadcast_scalars(
    scalars: list[int | float], num_total_examples: int | None = ..., device: torch.device | None = ...
) -> torch.Tensor: ...
def reissue_pt_warnings(caught_warnings):  # -> None:
    ...
@contextmanager
def torch_distributed_zero_first(local_rank: int):  # -> Generator[None, Any, None]:

    ...

class DistributedSamplerWithLoop(DistributedSampler):
    def __init__(self, dataset, batch_size, **kwargs) -> None: ...
    def __iter__(self):  # -> Iterator[Any]:
        ...

class EvalLoopContainer:
    def __init__(self, do_nested_concat: bool = ..., padding_index: int = ...) -> None: ...
    def add(self, tensors) -> None: ...
    def to_cpu_and_numpy(self) -> None: ...
    def get_arrays(
        self,
    ):  # -> list[Any] | tuple[Any, ...] | Mapping[Any, object] | Mapping[Any, Any] | ndarray[_AnyShape, dtype[Any]] | Tensor | None:

        ...

class SequentialDistributedSampler(Sampler):
    def __init__(self, dataset, num_replicas=..., rank=..., batch_size=...) -> None: ...
    def __iter__(self):  # -> Iterator[int]:
        ...
    def __len__(self) -> int:  # -> int:
        ...

def get_tpu_sampler(dataset: torch.utils.data.Dataset, batch_size: int):  # -> RandomSampler | DistributedSampler[Any]:
    ...
def nested_new_like(arrays, num_samples, padding_index=...):  # -> list[Any] | tuple[Any, ...] | NDArray[Any]:

    ...
def expand_like(arrays, new_seq_length, padding_index=...): ...
def nested_truncate(tensors, limit):  # -> list[Any] | tuple[Any, ...] | Mapping[Any, object]:

    ...

class DistributedTensorGatherer:
    def __init__(self, world_size, num_samples, make_multiple_of=..., padding_index=...) -> None: ...
    def add_arrays(self, arrays):  # -> None:

        ...
    def finalize(self):  # -> list[Any] | tuple[Any, ...] | Mapping[Any, Any] | ndarray[_AnyShape, dtype[Any]] | None:

        ...

@dataclass
class LabelSmoother:
    epsilon: float = ...
    ignore_index: int = ...
    def __call__(self, model_output, labels, shift_labels=...): ...

def get_length_grouped_indices(lengths, batch_size, mega_batch_mult=..., generator=...):  # -> list[Any]:

    ...

class LengthGroupedSampler(Sampler):
    def __init__(
        self,
        batch_size: int,
        dataset: Dataset | None = ...,
        lengths: list[int] | None = ...,
        model_input_name: str | None = ...,
        generator=...,
    ) -> None: ...
    def __len__(self) -> int:  # -> int:
        ...
    def __iter__(self):  # -> Iterator[Any]:
        ...

class DistributedLengthGroupedSampler(DistributedSampler):
    def __init__(
        self,
        batch_size: int,
        dataset: Dataset | None = ...,
        num_replicas: int | None = ...,
        rank: int | None = ...,
        seed: int = ...,
        drop_last: bool = ...,
        lengths: list[int] | None = ...,
        model_input_name: str | None = ...,
    ) -> None: ...
    def __iter__(self) -> Iterator: ...

class ShardSampler(Sampler):
    def __init__(
        self,
        dataset: Dataset,
        batch_size: int = ...,
        drop_last: bool = ...,
        num_processes: int = ...,
        process_index: int = ...,
    ) -> None: ...
    def __iter__(self):  # -> Iterator[Any]:
        ...
    def __len__(self) -> int:  # -> int:
        ...

class IterableDatasetShard(IterableDataset):
    def __init__(
        self,
        dataset: IterableDataset,
        batch_size: int = ...,
        drop_last: bool = ...,
        num_processes: int = ...,
        process_index: int = ...,
        seed: int = ...,
    ) -> None: ...
    def set_epoch(self, epoch):  # -> None:
        ...
    def __iter__(self):  # -> Generator[Any, Any, None]:
        ...
    def __len__(self) -> int:  # -> int:
        ...

def metrics_format(self, metrics: dict[str, float]) -> dict[str, float]: ...
def log_metrics(self, split, metrics):  # -> None:

    ...
def save_metrics(self, split, metrics, combined=...):  # -> None:

    ...
def save_state(self):  # -> None:

    ...
def get_model_param_count(model, trainable_only=...):  # -> int:

    ...
def get_parameter_names(model, forbidden_layer_types, forbidden_layer_names=...):  # -> list[Any]:

    ...
def get_module_class_from_name(module, name):  # -> None:

    ...
def remove_dummy_checkpoint(is_main_process, output_dir, filenames):  # -> None:
    ...

if is_sagemaker_mp_enabled():
    @smp.step()
    def smp_forward_backward(model, inputs, gradient_accumulation_steps=...): ...
    @smp.step()
    def smp_forward_only(model, inputs): ...
    def smp_gather(tensor):  # -> list[Any] | tuple[Any, ...] | dict[Any, Any] | Tensor:
        ...
    def smp_nested_concat(tensor):  # -> list[Any] | tuple[Any, ...] | dict[Any, Any]:
        ...

@dataclass
class AcceleratorConfig:
    split_batches: bool = ...
    dispatch_batches: bool | None = ...
    even_batches: bool = ...
    use_seedable_sampler: bool = ...
    non_blocking: bool | None = ...
    gradient_accumulation_kwargs: dict | None = ...
    use_configured_state: bool = ...
    @classmethod
    def from_json_file(cls, json_file):  # -> Self:
        ...
    def to_dict(self):  # -> dict[str, Any]:
        ...
    def pop(self, key, default=...):  # -> Any:
        ...

class LayerWiseDummyOptimizer(torch.optim.Optimizer):
    def __init__(self, optimizer_dict=..., *args, **kwargs) -> None: ...
    def zero_grad(self, set_to_none: bool = ...) -> None: ...
    def step(self, closure=...) -> float | None: ...

class LayerWiseDummyScheduler(LRScheduler):
    def __init__(self, *args, **kwargs) -> None: ...
    def get_lr(self):  # -> list[Any]:
        ...

def set_rng_state_for_device(device_name, device_module, checkpoint_rng_state, is_distributed):  # -> None:

    ...
