"""
This type stub file was generated by pyright.
"""

import numpy as np
from typing import Any, NamedTuple, Optional, Union
from .utils import ExplicitEnum, is_torch_available

if is_torch_available(): ...

def seed_worker(worker_id: int, num_workers: int, rank: int): ...
def enable_full_determinism(seed: int, warn_only: bool = ...): ...
def set_seed(seed: int, deterministic: bool = ...): ...
def neftune_post_forward_hook(module, input, output): ...

class EvalPrediction:
    def __init__(
        self,
        predictions: Union[np.ndarray, tuple[np.ndarray]],
        label_ids: Union[np.ndarray, tuple[np.ndarray]],
        inputs: Optional[Union[np.ndarray, tuple[np.ndarray]]] = ...,
        losses: Optional[Union[np.ndarray, tuple[np.ndarray]]] = ...,
    ) -> None: ...
    def __iter__(self): ...
    def __getitem__(self, idx): ...

class EvalLoopOutput(NamedTuple):
    predictions: Union[np.ndarray, tuple[np.ndarray]]
    label_ids: Optional[Union[np.ndarray, tuple[np.ndarray]]]
    metrics: Optional[dict[str, float]]
    num_samples: Optional[int]
    ...

class PredictionOutput(NamedTuple):
    predictions: Union[np.ndarray, tuple[np.ndarray]]
    label_ids: Optional[Union[np.ndarray, tuple[np.ndarray]]]
    metrics: Optional[dict[str, float]]
    ...

class TrainOutput(NamedTuple):
    global_step: int
    training_loss: float
    metrics: dict[str, float]
    ...

PREFIX_CHECKPOINT_DIR = ...
_re_checkpoint = ...

def get_last_checkpoint(folder): ...

class IntervalStrategy(ExplicitEnum):
    NO = ...
    STEPS = ...
    EPOCH = ...

class SaveStrategy(ExplicitEnum):
    NO = ...
    STEPS = ...
    EPOCH = ...
    BEST = ...

class EvaluationStrategy(ExplicitEnum):
    NO = ...
    STEPS = ...
    EPOCH = ...

class HubStrategy(ExplicitEnum):
    END = ...
    EVERY_SAVE = ...
    CHECKPOINT = ...
    ALL_CHECKPOINTS = ...

class BestRun(NamedTuple):
    run_id: str
    objective: Union[float, list[float]]
    hyperparameters: dict[str, Any]
    run_summary: Optional[Any] = ...

def default_compute_objective(metrics: dict[str, float]) -> float: ...
def default_hp_space_optuna(trial) -> dict[str, float]: ...
def default_hp_space_ray(trial) -> dict[str, float]: ...
def default_hp_space_sigopt(trial): ...
def default_hp_space_wandb(trial) -> dict[str, float]: ...

class HPSearchBackend(ExplicitEnum):
    OPTUNA = ...
    RAY = ...
    SIGOPT = ...
    WANDB = ...

def is_main_process(local_rank): ...
def total_processes_number(local_rank): ...
def speed_metrics(split, start_time, num_samples=..., num_steps=..., num_tokens=...): ...

class SchedulerType(ExplicitEnum):
    LINEAR = ...
    COSINE = ...
    COSINE_WITH_RESTARTS = ...
    POLYNOMIAL = ...
    CONSTANT = ...
    CONSTANT_WITH_WARMUP = ...
    INVERSE_SQRT = ...
    REDUCE_ON_PLATEAU = ...
    COSINE_WITH_MIN_LR = ...
    COSINE_WARMUP_WITH_MIN_LR = ...
    WARMUP_STABLE_DECAY = ...

class TrainerMemoryTracker:
    stages = ...
    def __init__(self, skip_memory_metrics=...) -> None: ...
    def derive_stage(self): ...
    def cpu_mem_used(self): ...
    def peak_monitor_func(self): ...
    def start(self): ...
    def stop(self, stage): ...
    def update_metrics(self, stage, metrics): ...
    def stop_and_update_metrics(self, metrics=...): ...

def has_length(dataset): ...
def denumpify_detensorize(metrics): ...
def number_of_arguments(func): ...
def find_executable_batch_size(
    function: Optional[callable] = ..., starting_batch_size: int = ..., auto_find_batch_size: bool = ...
): ...

class FSDPOption(ExplicitEnum):
    FULL_SHARD = ...
    SHARD_GRAD_OP = ...
    NO_SHARD = ...
    HYBRID_SHARD = ...
    HYBRID_SHARD_ZERO2 = ...
    OFFLOAD = ...
    AUTO_WRAP = ...

class RemoveColumnsCollator:
    def __init__(
        self,
        data_collator,
        signature_columns,
        logger=...,
        model_name: Optional[str] = ...,
        description: Optional[str] = ...,
    ) -> None: ...
    def __call__(self, features: list[dict]): ...

def check_target_module_exists(optim_target_modules, key: str, return_is_regex: bool = ...): ...
