from typing import Any, NamedTuple

import numpy as np

from .utils import ExplicitEnum, is_torch_available

"""
PyTorch-independent utilities for the Trainer class.
"""
if is_torch_available(): ...

def seed_worker(worker_id: int, num_workers: int, rank: int):  # -> None:

    ...
def enable_full_determinism(seed: int, warn_only: bool = ...):  # -> None:

    ...
def set_seed(seed: int, deterministic: bool = ...):  # -> None:

    ...
def neftune_post_forward_hook(module, input, output): ...

class EvalPrediction:
    def __init__(
        self,
        predictions: np.ndarray | tuple[np.ndarray],
        label_ids: np.ndarray | tuple[np.ndarray],
        inputs: np.ndarray | tuple[np.ndarray] | None = ...,
        losses: np.ndarray | tuple[np.ndarray] | None = ...,
    ) -> None: ...
    def __iter__(self):  # -> Iterator[ndarray[_AnyShape, dtype[Any]] | tuple[ndarray[_AnyShape, dtype[Any]]]]:
        ...
    def __getitem__(self, idx): ...

class EvalLoopOutput(NamedTuple):
    predictions: np.ndarray | tuple[np.ndarray]
    label_ids: np.ndarray | tuple[np.ndarray] | None
    metrics: dict[str, float] | None
    num_samples: int | None

class PredictionOutput(NamedTuple):
    predictions: np.ndarray | tuple[np.ndarray]
    label_ids: np.ndarray | tuple[np.ndarray] | None
    metrics: dict[str, float] | None

class TrainOutput(NamedTuple):
    global_step: int
    training_loss: float
    metrics: dict[str, float]

PREFIX_CHECKPOINT_DIR = ...
_re_checkpoint = ...

def get_last_checkpoint(folder):  # -> None:
    ...

class IntervalStrategy(ExplicitEnum):
    NO = ...
    STEPS = ...
    EPOCH = ...

class SaveStrategy(ExplicitEnum):
    NO = ...
    STEPS = ...
    EPOCH = ...
    BEST = ...

class EvaluationStrategy(ExplicitEnum):
    NO = ...
    STEPS = ...
    EPOCH = ...

class HubStrategy(ExplicitEnum):
    END = ...
    EVERY_SAVE = ...
    CHECKPOINT = ...
    ALL_CHECKPOINTS = ...

class BestRun(NamedTuple):
    run_id: str
    objective: float | list[float]
    hyperparameters: dict[str, Any]
    run_summary: Any | None = ...

def default_compute_objective(metrics: dict[str, float]) -> float: ...
def default_hp_space_optuna(trial) -> dict[str, float]: ...
def default_hp_space_ray(trial) -> dict[str, float]: ...
def default_hp_space_sigopt(
    trial,
):  # -> list[dict[str, dict[str, float] | str] | dict[str, dict[str, int] | str] | dict[str, list[str] | str]]:
    ...
def default_hp_space_wandb(trial) -> dict[str, float]: ...

class HPSearchBackend(ExplicitEnum):
    OPTUNA = ...
    RAY = ...
    SIGOPT = ...
    WANDB = ...

def is_main_process(local_rank):  # -> bool:

    ...
def total_processes_number(local_rank):  # -> int:

    ...
def speed_metrics(split, start_time, num_samples=..., num_steps=..., num_tokens=...):  # -> dict[str, Any]:

    ...

class SchedulerType(ExplicitEnum):
    LINEAR = ...
    COSINE = ...
    COSINE_WITH_RESTARTS = ...
    POLYNOMIAL = ...
    CONSTANT = ...
    CONSTANT_WITH_WARMUP = ...
    INVERSE_SQRT = ...
    REDUCE_ON_PLATEAU = ...
    COSINE_WITH_MIN_LR = ...
    COSINE_WARMUP_WITH_MIN_LR = ...
    WARMUP_STABLE_DECAY = ...

class TrainerMemoryTracker:
    stages = ...
    def __init__(self, skip_memory_metrics=...) -> None: ...
    def derive_stage(self):  # -> str:

        ...
    def cpu_mem_used(self):  # -> Any:

        ...
    def peak_monitor_func(self):  # -> None:
        ...
    def start(self):  # -> None:

        ...
    def stop(self, stage):  # -> None:

        ...
    def update_metrics(self, stage, metrics):  # -> None:

        ...
    def stop_and_update_metrics(self, metrics=...):  # -> None:

        ...

def has_length(dataset):  # -> bool:

    ...
def denumpify_detensorize(metrics):  # -> Number | list[Any] | tuple[Any, ...] | dict[Any, Any] | object | Tensor:

    ...
def number_of_arguments(func):  # -> int:

    ...
def find_executable_batch_size(
    function: callable | None = ..., starting_batch_size: int = ..., auto_find_batch_size: bool = ...
):  # -> partial[Any]:

    ...

class FSDPOption(ExplicitEnum):
    FULL_SHARD = ...
    SHARD_GRAD_OP = ...
    NO_SHARD = ...
    HYBRID_SHARD = ...
    HYBRID_SHARD_ZERO2 = ...
    OFFLOAD = ...
    AUTO_WRAP = ...

class RemoveColumnsCollator:
    def __init__(
        self,
        data_collator,
        signature_columns,
        logger=...,
        model_name: str | None = ...,
        description: str | None = ...,
    ) -> None: ...
    def __call__(self, features: list[dict]): ...

def check_target_module_exists(
    optim_target_modules, key: str, return_is_regex: bool = ...
):  # -> tuple[bool, bool] | bool:

    ...
