"""
This type stub file was generated by pyright.
"""

import contextlib
import torch
from dataclasses import dataclass
from datetime import timedelta
from enum import Enum
from typing import Any, Optional, Union
from .debug_utils import DebugOption
from .trainer_utils import FSDPOption, HubStrategy, IntervalStrategy, SaveStrategy, SchedulerType
from .utils import (
    ExplicitEnum,
    is_accelerate_available,
    is_sagemaker_mp_enabled,
    is_torch_available,
    is_torch_neuroncore_available,
    is_torch_xla_available,
)

logger = ...
log_levels = ...
trainer_log_levels = ...
if is_torch_available(): ...
if is_accelerate_available(): ...
if is_torch_xla_available(): ...
if is_torch_neuroncore_available(check_device=False): ...
if is_sagemaker_mp_enabled(): ...

def default_logdir() -> str: ...
def get_int_from_env(env_keys, default): ...
def get_xla_device_type(device: torch.device) -> Optional[str]: ...

class OptimizerNames(ExplicitEnum):
    ADAMW_TORCH = ...
    ADAMW_TORCH_FUSED = ...
    ADAMW_TORCH_XLA = ...
    ADAMW_TORCH_NPU_FUSED = ...
    ADAMW_APEX_FUSED = ...
    ADAFACTOR = ...
    ADAMW_ANYPRECISION = ...
    ADAMW_TORCH_4BIT = ...
    ADAMW_TORCH_8BIT = ...
    ADEMAMIX = ...
    SGD = ...
    ADAGRAD = ...
    ADAMW_BNB = ...
    ADAMW_8BIT = ...
    ADEMAMIX_8BIT = ...
    LION_8BIT = ...
    LION = ...
    PAGED_ADAMW = ...
    PAGED_ADAMW_8BIT = ...
    PAGED_ADEMAMIX = ...
    PAGED_ADEMAMIX_8BIT = ...
    PAGED_LION = ...
    PAGED_LION_8BIT = ...
    RMSPROP = ...
    RMSPROP_BNB = ...
    RMSPROP_8BIT = ...
    RMSPROP_32BIT = ...
    GALORE_ADAMW = ...
    GALORE_ADAMW_8BIT = ...
    GALORE_ADAFACTOR = ...
    GALORE_ADAMW_LAYERWISE = ...
    GALORE_ADAMW_8BIT_LAYERWISE = ...
    GALORE_ADAFACTOR_LAYERWISE = ...
    LOMO = ...
    ADALOMO = ...
    GROKADAMW = ...
    SCHEDULE_FREE_RADAM = ...
    SCHEDULE_FREE_ADAMW = ...
    SCHEDULE_FREE_SGD = ...
    APOLLO_ADAMW = ...
    APOLLO_ADAMW_LAYERWISE = ...
    STABLE_ADAMW = ...

@dataclass
class TrainingArguments:
    _VALID_DICT_FIELDS = ...
    framework = ...
    output_dir: Optional[str] = ...
    overwrite_output_dir: bool = ...
    do_train: bool = ...
    do_eval: bool = ...
    do_predict: bool = ...
    eval_strategy: Union[IntervalStrategy, str] = ...
    prediction_loss_only: bool = ...
    per_device_train_batch_size: int = ...
    per_device_eval_batch_size: int = ...
    per_gpu_train_batch_size: Optional[int] = ...
    per_gpu_eval_batch_size: Optional[int] = ...
    gradient_accumulation_steps: int = ...
    eval_accumulation_steps: Optional[int] = ...
    eval_delay: Optional[float] = ...
    torch_empty_cache_steps: Optional[int] = ...
    learning_rate: float = ...
    weight_decay: float = ...
    adam_beta1: float = ...
    adam_beta2: float = ...
    adam_epsilon: float = ...
    max_grad_norm: float = ...
    num_train_epochs: float = ...
    max_steps: int = ...
    lr_scheduler_type: Union[SchedulerType, str] = ...
    lr_scheduler_kwargs: Optional[Union[dict[str, Any], str]] = ...
    warmup_ratio: float = ...
    warmup_steps: int = ...
    log_level: str = ...
    log_level_replica: str = ...
    log_on_each_node: bool = ...
    logging_dir: Optional[str] = ...
    logging_strategy: Union[IntervalStrategy, str] = ...
    logging_first_step: bool = ...
    logging_steps: float = ...
    logging_nan_inf_filter: bool = ...
    save_strategy: Union[SaveStrategy, str] = ...
    save_steps: float = ...
    save_total_limit: Optional[int] = ...
    save_safetensors: Optional[bool] = ...
    save_on_each_node: bool = ...
    save_only_model: bool = ...
    restore_callback_states_from_checkpoint: bool = ...
    no_cuda: bool = ...
    use_cpu: bool = ...
    use_mps_device: bool = ...
    seed: int = ...
    data_seed: Optional[int] = ...
    jit_mode_eval: bool = ...
    use_ipex: bool = ...
    bf16: bool = ...
    fp16: bool = ...
    fp16_opt_level: str = ...
    half_precision_backend: str = ...
    bf16_full_eval: bool = ...
    fp16_full_eval: bool = ...
    tf32: Optional[bool] = ...
    local_rank: int = ...
    ddp_backend: Optional[str] = ...
    tpu_num_cores: Optional[int] = ...
    tpu_metrics_debug: bool = ...
    debug: Union[str, list[DebugOption]] = ...
    dataloader_drop_last: bool = ...
    eval_steps: Optional[float] = ...
    dataloader_num_workers: int = ...
    dataloader_prefetch_factor: Optional[int] = ...
    past_index: int = ...
    run_name: Optional[str] = ...
    disable_tqdm: Optional[bool] = ...
    remove_unused_columns: Optional[bool] = ...
    label_names: Optional[list[str]] = ...
    load_best_model_at_end: Optional[bool] = ...
    metric_for_best_model: Optional[str] = ...
    greater_is_better: Optional[bool] = ...
    ignore_data_skip: bool = ...
    fsdp: Optional[Union[list[FSDPOption], str]] = ...
    fsdp_min_num_params: int = ...
    fsdp_config: Optional[Union[dict[str, Any], str]] = ...
    fsdp_transformer_layer_cls_to_wrap: Optional[str] = ...
    accelerator_config: Optional[Union[dict, str]] = ...
    deepspeed: Optional[Union[dict, str]] = ...
    label_smoothing_factor: float = ...
    default_optim = ...
    if is_torch_available(): ...
    optim: Union[OptimizerNames, str] = ...
    optim_args: Optional[str] = ...
    adafactor: bool = ...
    group_by_length: bool = ...
    length_column_name: Optional[str] = ...
    report_to: Union[None, str, list[str]] = ...
    ddp_find_unused_parameters: Optional[bool] = ...
    ddp_bucket_cap_mb: Optional[int] = ...
    ddp_broadcast_buffers: Optional[bool] = ...
    dataloader_pin_memory: bool = ...
    dataloader_persistent_workers: bool = ...
    skip_memory_metrics: bool = ...
    use_legacy_prediction_loop: bool = ...
    push_to_hub: bool = ...
    resume_from_checkpoint: Optional[str] = ...
    hub_model_id: Optional[str] = ...
    hub_strategy: Union[HubStrategy, str] = ...
    hub_token: Optional[str] = ...
    hub_private_repo: Optional[bool] = ...
    hub_always_push: bool = ...
    hub_revision: Optional[str] = ...
    gradient_checkpointing: bool = ...
    gradient_checkpointing_kwargs: Optional[Union[dict[str, Any], str]] = ...
    include_inputs_for_metrics: bool = ...
    include_for_metrics: list[str] = ...
    eval_do_concat_batches: bool = ...
    fp16_backend: str = ...
    push_to_hub_model_id: Optional[str] = ...
    push_to_hub_organization: Optional[str] = ...
    push_to_hub_token: Optional[str] = ...
    _n_gpu: int = ...
    mp_parameters: str = ...
    auto_find_batch_size: bool = ...
    full_determinism: bool = ...
    torchdynamo: Optional[str] = ...
    ray_scope: Optional[str] = ...
    ddp_timeout: int = ...
    torch_compile: bool = ...
    torch_compile_backend: Optional[str] = ...
    torch_compile_mode: Optional[str] = ...
    include_tokens_per_second: Optional[bool] = ...
    include_num_input_tokens_seen: Optional[bool] = ...
    neftune_noise_alpha: Optional[float] = ...
    optim_target_modules: Union[None, str, list[str]] = ...
    batch_eval_metrics: bool = ...
    eval_on_start: bool = ...
    use_liger_kernel: Optional[bool] = ...
    liger_kernel_config: Optional[dict[str, bool]] = ...
    eval_use_gather_object: Optional[bool] = ...
    average_tokens_across_devices: Optional[bool] = ...
    def __post_init__(self): ...

    __repr__ = ...
    @property
    def train_batch_size(self) -> int: ...
    @property
    def eval_batch_size(self) -> int: ...
    @property
    def ddp_timeout_delta(self) -> timedelta: ...
    @property
    def device(self) -> torch.device: ...
    @property
    def n_gpu(self): ...
    @property
    def parallel_mode(self): ...
    @property
    def world_size(self): ...
    @property
    def process_index(self): ...
    @property
    def local_process_index(self): ...
    @property
    def should_log(self): ...
    @property
    def should_save(self): ...
    def get_process_log_level(self): ...
    @property
    def place_model_on_device(self): ...
    @contextlib.contextmanager
    def main_process_first(self, local=..., desc=...): ...
    def get_warmup_steps(self, num_training_steps: int): ...
    def to_dict(self): ...
    def to_json_string(self): ...
    def to_sanitized_dict(self) -> dict[str, Any]: ...
    def set_training(
        self,
        learning_rate: float = ...,
        batch_size: int = ...,
        weight_decay: float = ...,
        num_epochs: float = ...,
        max_steps: int = ...,
        gradient_accumulation_steps: int = ...,
        seed: int = ...,
        gradient_checkpointing: bool = ...,
    ): ...
    def set_evaluate(
        self,
        strategy: Union[str, IntervalStrategy] = ...,
        steps: int = ...,
        batch_size: int = ...,
        accumulation_steps: Optional[int] = ...,
        delay: Optional[float] = ...,
        loss_only: bool = ...,
        jit_mode: bool = ...,
    ): ...
    def set_testing(self, batch_size: int = ..., loss_only: bool = ..., jit_mode: bool = ...): ...
    def set_save(
        self,
        strategy: Union[str, IntervalStrategy] = ...,
        steps: int = ...,
        total_limit: Optional[int] = ...,
        on_each_node: bool = ...,
    ): ...
    def set_logging(
        self,
        strategy: Union[str, IntervalStrategy] = ...,
        steps: int = ...,
        report_to: Union[str, list[str]] = ...,
        level: str = ...,
        first_step: bool = ...,
        nan_inf_filter: bool = ...,
        on_each_node: bool = ...,
        replica_level: str = ...,
    ): ...
    def set_push_to_hub(
        self,
        model_id: str,
        strategy: Union[str, HubStrategy] = ...,
        token: Optional[str] = ...,
        private_repo: Optional[bool] = ...,
        always_push: bool = ...,
        revision: Optional[str] = ...,
    ): ...
    def set_optimizer(
        self,
        name: Union[str, OptimizerNames] = ...,
        learning_rate: float = ...,
        weight_decay: float = ...,
        beta1: float = ...,
        beta2: float = ...,
        epsilon: float = ...,
        args: Optional[str] = ...,
    ): ...
    def set_lr_scheduler(
        self,
        name: Union[str, SchedulerType] = ...,
        num_epochs: float = ...,
        max_steps: int = ...,
        warmup_ratio: float = ...,
        warmup_steps: int = ...,
    ): ...
    def set_dataloader(
        self,
        train_batch_size: int = ...,
        eval_batch_size: int = ...,
        drop_last: bool = ...,
        num_workers: int = ...,
        pin_memory: bool = ...,
        persistent_workers: bool = ...,
        prefetch_factor: Optional[int] = ...,
        auto_find_batch_size: bool = ...,
        ignore_data_skip: bool = ...,
        sampler_seed: Optional[int] = ...,
    ): ...

class ParallelMode(Enum):
    NOT_PARALLEL = ...
    NOT_DISTRIBUTED = ...
    DISTRIBUTED = ...
    SAGEMAKER_MODEL_PARALLEL = ...
    SAGEMAKER_DATA_PARALLEL = ...
    TPU = ...
