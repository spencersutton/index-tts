import contextlib
from dataclasses import dataclass
from datetime import timedelta
from enum import Enum
from typing import Any

import torch

from .debug_utils import DebugOption
from .trainer_utils import FSDPOption, HubStrategy, IntervalStrategy, SaveStrategy, SchedulerType
from .utils import (
    ExplicitEnum,
    is_accelerate_available,
    is_sagemaker_mp_enabled,
    is_torch_available,
    is_torch_neuroncore_available,
    is_torch_xla_available,
)

logger = ...
log_levels = ...
trainer_log_levels = ...
if is_torch_available(): ...
if is_accelerate_available(): ...
if is_torch_xla_available(): ...
if is_torch_neuroncore_available(check_device=False): ...
if is_sagemaker_mp_enabled(): ...

def default_logdir() -> str: ...
def get_int_from_env(env_keys, default):  # -> int:

    ...
def get_xla_device_type(device: torch.device) -> str | None: ...

class OptimizerNames(ExplicitEnum):
    ADAMW_TORCH = ...
    ADAMW_TORCH_FUSED = ...
    ADAMW_TORCH_XLA = ...
    ADAMW_TORCH_NPU_FUSED = ...
    ADAMW_APEX_FUSED = ...
    ADAFACTOR = ...
    ADAMW_ANYPRECISION = ...
    ADAMW_TORCH_4BIT = ...
    ADAMW_TORCH_8BIT = ...
    ADEMAMIX = ...
    SGD = ...
    ADAGRAD = ...
    ADAMW_BNB = ...
    ADAMW_8BIT = ...
    ADEMAMIX_8BIT = ...
    LION_8BIT = ...
    LION = ...
    PAGED_ADAMW = ...
    PAGED_ADAMW_8BIT = ...
    PAGED_ADEMAMIX = ...
    PAGED_ADEMAMIX_8BIT = ...
    PAGED_LION = ...
    PAGED_LION_8BIT = ...
    RMSPROP = ...
    RMSPROP_BNB = ...
    RMSPROP_8BIT = ...
    RMSPROP_32BIT = ...
    GALORE_ADAMW = ...
    GALORE_ADAMW_8BIT = ...
    GALORE_ADAFACTOR = ...
    GALORE_ADAMW_LAYERWISE = ...
    GALORE_ADAMW_8BIT_LAYERWISE = ...
    GALORE_ADAFACTOR_LAYERWISE = ...
    LOMO = ...
    ADALOMO = ...
    GROKADAMW = ...
    SCHEDULE_FREE_RADAM = ...
    SCHEDULE_FREE_ADAMW = ...
    SCHEDULE_FREE_SGD = ...
    APOLLO_ADAMW = ...
    APOLLO_ADAMW_LAYERWISE = ...
    STABLE_ADAMW = ...

@dataclass
class TrainingArguments:
    _VALID_DICT_FIELDS = ...
    framework = ...
    output_dir: str | None = ...
    overwrite_output_dir: bool = ...
    do_train: bool = ...
    do_eval: bool = ...
    do_predict: bool = ...
    eval_strategy: IntervalStrategy | str = ...
    prediction_loss_only: bool = ...
    per_device_train_batch_size: int = ...
    per_device_eval_batch_size: int = ...
    per_gpu_train_batch_size: int | None = ...
    per_gpu_eval_batch_size: int | None = ...
    gradient_accumulation_steps: int = ...
    eval_accumulation_steps: int | None = ...
    eval_delay: float | None = ...
    torch_empty_cache_steps: int | None = ...
    learning_rate: float = ...
    weight_decay: float = ...
    adam_beta1: float = ...
    adam_beta2: float = ...
    adam_epsilon: float = ...
    max_grad_norm: float = ...
    num_train_epochs: float = ...
    max_steps: int = ...
    lr_scheduler_type: SchedulerType | str = ...
    lr_scheduler_kwargs: dict[str, Any] | str | None = ...
    warmup_ratio: float = ...
    warmup_steps: int = ...
    log_level: str = ...
    log_level_replica: str = ...
    log_on_each_node: bool = ...
    logging_dir: str | None = ...
    logging_strategy: IntervalStrategy | str = ...
    logging_first_step: bool = ...
    logging_steps: float = ...
    logging_nan_inf_filter: bool = ...
    save_strategy: SaveStrategy | str = ...
    save_steps: float = ...
    save_total_limit: int | None = ...
    save_safetensors: bool | None = ...
    save_on_each_node: bool = ...
    save_only_model: bool = ...
    restore_callback_states_from_checkpoint: bool = ...
    no_cuda: bool = ...
    use_cpu: bool = ...
    use_mps_device: bool = ...
    seed: int = ...
    data_seed: int | None = ...
    jit_mode_eval: bool = ...
    use_ipex: bool = ...
    bf16: bool = ...
    fp16: bool = ...
    fp16_opt_level: str = ...
    half_precision_backend: str = ...
    bf16_full_eval: bool = ...
    fp16_full_eval: bool = ...
    tf32: bool | None = ...
    local_rank: int = ...
    ddp_backend: str | None = ...
    tpu_num_cores: int | None = ...
    tpu_metrics_debug: bool = ...
    debug: str | list[DebugOption] = ...
    dataloader_drop_last: bool = ...
    eval_steps: float | None = ...
    dataloader_num_workers: int = ...
    dataloader_prefetch_factor: int | None = ...
    past_index: int = ...
    run_name: str | None = ...
    disable_tqdm: bool | None = ...
    remove_unused_columns: bool | None = ...
    label_names: list[str] | None = ...
    load_best_model_at_end: bool | None = ...
    metric_for_best_model: str | None = ...
    greater_is_better: bool | None = ...
    ignore_data_skip: bool = ...
    fsdp: list[FSDPOption] | str | None = ...
    fsdp_min_num_params: int = ...
    fsdp_config: dict[str, Any] | str | None = ...
    fsdp_transformer_layer_cls_to_wrap: str | None = ...
    accelerator_config: dict | str | None = ...
    deepspeed: dict | str | None = ...
    label_smoothing_factor: float = ...
    default_optim = ...
    if is_torch_available(): ...
    optim: OptimizerNames | str = ...
    optim_args: str | None = ...
    adafactor: bool = ...
    group_by_length: bool = ...
    length_column_name: str | None = ...
    report_to: None | str | list[str] = ...
    ddp_find_unused_parameters: bool | None = ...
    ddp_bucket_cap_mb: int | None = ...
    ddp_broadcast_buffers: bool | None = ...
    dataloader_pin_memory: bool = ...
    dataloader_persistent_workers: bool = ...
    skip_memory_metrics: bool = ...
    use_legacy_prediction_loop: bool = ...
    push_to_hub: bool = ...
    resume_from_checkpoint: str | None = ...
    hub_model_id: str | None = ...
    hub_strategy: HubStrategy | str = ...
    hub_token: str | None = ...
    hub_private_repo: bool | None = ...
    hub_always_push: bool = ...
    hub_revision: str | None = ...
    gradient_checkpointing: bool = ...
    gradient_checkpointing_kwargs: dict[str, Any] | str | None = ...
    include_inputs_for_metrics: bool = ...
    include_for_metrics: list[str] = ...
    eval_do_concat_batches: bool = ...
    fp16_backend: str = ...
    push_to_hub_model_id: str | None = ...
    push_to_hub_organization: str | None = ...
    push_to_hub_token: str | None = ...
    _n_gpu: int = ...
    mp_parameters: str = ...
    auto_find_batch_size: bool = ...
    full_determinism: bool = ...
    torchdynamo: str | None = ...
    ray_scope: str | None = ...
    ddp_timeout: int = ...
    torch_compile: bool = ...
    torch_compile_backend: str | None = ...
    torch_compile_mode: str | None = ...
    include_tokens_per_second: bool | None = ...
    include_num_input_tokens_seen: bool | None = ...
    neftune_noise_alpha: float | None = ...
    optim_target_modules: None | str | list[str] = ...
    batch_eval_metrics: bool = ...
    eval_on_start: bool = ...
    use_liger_kernel: bool | None = ...
    liger_kernel_config: dict[str, bool] | None = ...
    eval_use_gather_object: bool | None = ...
    average_tokens_across_devices: bool | None = ...
    def __post_init__(self):  # -> None:
        ...

    __repr__ = ...
    @property
    def train_batch_size(self) -> int: ...
    @property
    def eval_batch_size(self) -> int: ...
    @property
    def ddp_timeout_delta(self) -> timedelta: ...
    @property
    def device(self) -> torch.device: ...
    @property
    def n_gpu(self):  # -> int:

        ...
    @property
    def parallel_mode(
        self,
    ):  # -> Literal[ParallelMode.TPU, ParallelMode.SAGEMAKER_MODEL_PARALLEL, ParallelMode.SAGEMAKER_DATA_PARALLEL, ParallelMode.DISTRIBUTED, ParallelMode.NOT_DISTRIBUTED, ParallelMode.NOT_PARALLEL]:

        ...
    @property
    def world_size(self):  # -> Literal[1]:

        ...
    @property
    def process_index(self):  # -> Literal[0]:

        ...
    @property
    def local_process_index(self):  # -> Literal[0]:

        ...
    @property
    def should_log(self):  # -> bool:

        ...
    @property
    def should_save(self):  # -> bool:

        ...
    def get_process_log_level(self):  # -> int:

        ...
    @property
    def place_model_on_device(self):  # -> bool:

        ...
    @contextlib.contextmanager
    def main_process_first(self, local=..., desc=...):  # -> Generator[None, Any, None]:

        ...
    def get_warmup_steps(self, num_training_steps: int):  # -> int:

        ...
    def to_dict(self):  # -> dict[str, Any]:

        ...
    def to_json_string(self):  # -> str:

        ...
    def to_sanitized_dict(self) -> dict[str, Any]: ...
    def set_training(
        self,
        learning_rate: float = ...,
        batch_size: int = ...,
        weight_decay: float = ...,
        num_epochs: float = ...,
        max_steps: int = ...,
        gradient_accumulation_steps: int = ...,
        seed: int = ...,
        gradient_checkpointing: bool = ...,
    ):  # -> Self:

        ...
    def set_evaluate(
        self,
        strategy: str | IntervalStrategy = ...,
        steps: int = ...,
        batch_size: int = ...,
        accumulation_steps: int | None = ...,
        delay: float | None = ...,
        loss_only: bool = ...,
        jit_mode: bool = ...,
    ):  # -> Self:

        ...
    def set_testing(self, batch_size: int = ..., loss_only: bool = ..., jit_mode: bool = ...):  # -> Self:

        ...
    def set_save(
        self,
        strategy: str | IntervalStrategy = ...,
        steps: int = ...,
        total_limit: int | None = ...,
        on_each_node: bool = ...,
    ):  # -> Self:

        ...
    def set_logging(
        self,
        strategy: str | IntervalStrategy = ...,
        steps: int = ...,
        report_to: str | list[str] = ...,
        level: str = ...,
        first_step: bool = ...,
        nan_inf_filter: bool = ...,
        on_each_node: bool = ...,
        replica_level: str = ...,
    ):  # -> Self:

        ...
    def set_push_to_hub(
        self,
        model_id: str,
        strategy: str | HubStrategy = ...,
        token: str | None = ...,
        private_repo: bool | None = ...,
        always_push: bool = ...,
        revision: str | None = ...,
    ):  # -> Self:

        ...
    def set_optimizer(
        self,
        name: str | OptimizerNames = ...,
        learning_rate: float = ...,
        weight_decay: float = ...,
        beta1: float = ...,
        beta2: float = ...,
        epsilon: float = ...,
        args: str | None = ...,
    ):  # -> Self:

        ...
    def set_lr_scheduler(
        self,
        name: str | SchedulerType = ...,
        num_epochs: float = ...,
        max_steps: int = ...,
        warmup_ratio: float = ...,
        warmup_steps: int = ...,
    ):  # -> Self:

        ...
    def set_dataloader(
        self,
        train_batch_size: int = ...,
        eval_batch_size: int = ...,
        drop_last: bool = ...,
        num_workers: int = ...,
        pin_memory: bool = ...,
        persistent_workers: bool = ...,
        prefetch_factor: int | None = ...,
        auto_find_batch_size: bool = ...,
        ignore_data_skip: bool = ...,
        sampler_seed: int | None = ...,
    ):  # -> Self:

        ...

class ParallelMode(Enum):
    NOT_PARALLEL = ...
    NOT_DISTRIBUTED = ...
    DISTRIBUTED = ...
    SAGEMAKER_MODEL_PARALLEL = ...
    SAGEMAKER_DATA_PARALLEL = ...
    TPU = ...
