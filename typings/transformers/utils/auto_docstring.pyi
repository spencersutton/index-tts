"""
This type stub file was generated by pyright.
"""

from typing import Optional, Union

PATH_TO_TRANSFORMERS = ...
AUTODOC_FILES = ...
PLACEHOLDER_TO_AUTO_MODULE = ...
UNROLL_KWARGS_METHODS = ...
UNROLL_KWARGS_CLASSES = ...
HARDCODED_CONFIG_FOR_MODELS = ...
_re_checkpoint = ...

class ImageProcessorArgs:
    images = ...
    videos = ...
    do_resize = ...
    size = ...
    default_to_square = ...
    resample = ...
    do_center_crop = ...
    crop_size = ...
    do_rescale = ...
    rescale_factor = ...
    do_normalize = ...
    image_mean = ...
    image_std = ...
    do_convert_rgb = ...
    return_tensors = ...
    data_format = ...
    input_data_format = ...
    device = ...
    disable_grouping = ...

class ModelArgs:
    labels = ...
    num_logits_to_keep = ...
    input_ids = ...
    input_values = ...
    attention_mask = ...
    head_mask = ...
    cross_attn_head_mask = ...
    decoder_attention_mask = ...
    decoder_head_mask = ...
    encoder_hidden_states = ...
    encoder_attention_mask = ...
    token_type_ids = ...
    position_ids = ...
    past_key_values = ...
    past_key_value = ...
    inputs_embeds = ...
    decoder_input_ids = ...
    decoder_inputs_embeds = ...
    use_cache = ...
    output_attentions = ...
    output_hidden_states = ...
    return_dict = ...
    cache_position = ...
    hidden_states = ...
    interpolate_pos_encoding = ...
    position_embeddings = ...
    config = ...
    start_positions = ...
    end_positions = ...
    encoder_outputs = ...
    output_router_logits = ...
    logits_to_keep = ...
    pixel_values = ...
    pixel_values_videos = ...
    vision_feature_layer = ...
    vision_feature_select_strategy = ...
    image_sizes = ...
    pixel_mask = ...
    input_features = ...

class ModelOutputArgs:
    last_hidden_state = ...
    past_key_values = ...
    hidden_states = ...
    attentions = ...
    pooler_output = ...
    cross_attentions = ...
    decoder_hidden_states = ...
    decoder_attentions = ...
    encoder_last_hidden_state = ...
    encoder_hidden_states = ...
    encoder_attentions = ...
    router_logits = ...
    router_probs = ...
    z_loss = ...
    aux_loss = ...
    start_logits = ...
    end_logits = ...
    feature_maps = ...
    reconstruction = ...
    spectrogram = ...
    predicted_depth = ...
    sequences = ...
    params = ...
    loc = ...
    scale = ...
    static_features = ...
    embeddings = ...
    extract_features = ...
    projection_state = ...
    image_hidden_states = ...
    video_hidden_states = ...

class ClassDocstring:
    PreTrainedModel = ...
    Model = ...
    ForPreTraining = ...
    Decoder = ...
    TextModel = ...
    ForSequenceClassification = ...
    ForQuestionAnswering = ...
    ForMultipleChoice = ...
    ForMaskedLM = ...
    ForTokenClassification = ...
    ForConditionalGeneration = ...
    ForCausalLM = ...
    ImageProcessorFast = ...
    Backbone = ...
    ForImageClassification = ...
    ForSemanticSegmentation = ...
    ForAudioClassification = ...
    ForAudioFrameClassification = ...
    ForPrediction = ...
    WithProjection = ...

class ClassAttrs:
    base_model_prefix = ...
    supports_gradient_checkpointing = ...
    _no_split_modules = ...
    _skip_keys_device_placement = ...
    _supports_flash_attn = ...
    _supports_sdpa = ...
    _supports_flex_attn = ...
    _can_compile_fullgraph = ...
    _supports_attention_backend = ...
    _tied_weights_keys = ...

ARGS_TO_IGNORE = ...

def get_indent_level(func):  # -> int:
    ...
def equalize_indent(docstring, indent_level):  # -> str:
    """
    Adjust the indentation of a docstring to match the specified indent level.
    """
    ...

def set_min_indent(docstring, indent_level):  # -> str:
    """
    Adjust the indentation of a docstring to match the specified indent level.
    """
    ...

def parse_shape(docstring):  # -> str | Any | None:
    ...
def parse_default(docstring):  # -> str | Any | None:
    ...
def parse_docstring(
    docstring, max_indent_level=..., return_intro=...
):  # -> tuple[dict[Any, Any], str, str | Any | None] | tuple[dict[Any, Any], str]:
    """
    Parse the docstring to extract the Args section and return it as a dictionary.
    The docstring is expected to be in the format:
    Args:
        arg1 (type):
            Description of arg1.
        arg2 (type):
            Description of arg2.

    # This function will also return the remaining part of the docstring after the Args section.
    Returns:/Example:
    ...
    """
    ...

def contains_type(type_hint, target_type) -> tuple[bool, Optional[object]]:
    """
    Check if a "nested" type hint contains a specific target type,
    return the first-level type containing the target_type if found.
    """
    ...

def get_model_name(obj):  # -> str | None:
    """
    Get the model name from the file path of the object.
    """
    ...

def get_placeholders_dict(placeholders: list, model_name: str) -> dict:
    """
    Get the dictionary of placeholders for the given model name.
    """
    ...

def format_args_docstring(docstring, model_name):
    """
    Replaces placeholders such as {image_processor_class} in the docstring with the actual values,
    deducted from the model name and the auto modules.
    """
    ...

def get_args_doc_from_source(args_classes: Union[object, list[object]]) -> dict: ...
def get_checkpoint_from_config_class(config_class):  # -> Any | None:
    ...
def add_intro_docstring(func, class_name, parent_class=..., indent_level=...):  # -> str:
    ...
def find_sig_line(lines, line_end): ...
def auto_method_docstring(
    func, parent_class=..., custom_intro=..., custom_args=..., checkpoint=..., source_args_dict=...
):
    """
    Wrapper that automatically generates docstring.
    """
    ...

def auto_class_docstring(cls, custom_intro=..., custom_args=..., checkpoint=...):
    """
    Wrapper that automatically generates a docstring for classes based on their attributes and methods.
    """
    ...

def auto_docstring(obj=..., *, custom_intro=..., custom_args=..., checkpoint=...):  # -> Callable[..., Any]:
    r"""
    Automatically generates comprehensive docstrings for model classes and methods in the Transformers library.

    This decorator reduces boilerplate by automatically including standard argument descriptions while allowing
    overrides to add new or custom arguments. It inspects function signatures, retrieves predefined docstrings
    for common arguments (like `input_ids`, `attention_mask`, etc.), and generates complete documentation
    including examples and return value descriptions.

    For complete documentation and examples, read this [guide](https://huggingface.co/docs/transformers/auto_docstring).

    Examples of usage:

        Basic usage (no parameters):
        ```python
        @auto_docstring
        class MyAwesomeModel(PreTrainedModel):
            def __init__(self, config, custom_parameter: int = 10):
                r'''
                custom_parameter (`int`, *optional*, defaults to 10):
                    Description of the custom parameter for MyAwesomeModel.
                '''
                super().__init__(config)
                self.custom_parameter = custom_parameter
        ```

        Using `custom_intro` with a class:
        ```python
        @auto_docstring(
            custom_intro="This model implements a novel attention mechanism for improved performance."
        )
        class MySpecialModel(PreTrainedModel):
            def __init__(self, config, attention_type: str = "standard"):
                r'''
                attention_type (`str`, *optional*, defaults to "standard"):
                    Type of attention mechanism to use.
                '''
                super().__init__(config)
        ```

        Using `custom_intro` with a method, and specify custom arguments and example directly in the docstring:
        ```python
        @auto_docstring(
            custom_intro="Performs forward pass with enhanced attention computation."
        )
        def forward(
            self,
            input_ids: Optional[torch.Tensor] = None,
            attention_mask: Optional[torch.Tensor] = None,
        ):
            r'''
            custom_parameter (`int`, *optional*, defaults to 10):
                Description of the custom parameter for MyAwesomeModel.

            Example:

            ```python
            >>> model = MyAwesomeModel(config)
            >>> model.forward(input_ids=torch.tensor([1, 2, 3]), attention_mask=torch.tensor([1, 1, 1]))
            ```
            '''
        ```

        Using `custom_args` to define reusable arguments:
        ```python
        VISION_ARGS = r'''
        pixel_values (`torch.FloatTensor`, *optional*):
            Pixel values of the input images.
        image_features (`torch.FloatTensor`, *optional*):
            Pre-computed image features for efficient processing.
        '''

        @auto_docstring(custom_args=VISION_ARGS)
        def encode_images(self, pixel_values=None, image_features=None):
            # ... method implementation
        ```

        Combining `custom_intro` and `custom_args`:
        ```python
        MULTIMODAL_ARGS = r'''
        vision_features (`torch.FloatTensor`, *optional*):
            Pre-extracted vision features from the vision encoder.
        fusion_strategy (`str`, *optional*, defaults to "concat"):
            Strategy for fusing text and vision modalities.
        '''

        @auto_docstring(
            custom_intro="Processes multimodal inputs combining text and vision.",
            custom_args=MULTIMODAL_ARGS
        )
        def forward(
            self,
            input_ids,
            attention_mask=None,
            vision_features=None,
            fusion_strategy="concat"
        ):
            # ... multimodal processing
        ```

        Using with ModelOutput classes:
        ```python
        @dataclass
        @auto_docstring(
            custom_intro="Custom model outputs with additional fields."
        )
        class MyModelOutput(ImageClassifierOutput):
            r'''
            loss (`torch.FloatTensor`, *optional*):
                The loss of the model.
            custom_field (`torch.FloatTensor` of shape `(batch_size, hidden_size)`, *optional*):
                A custom output field specific to this model.
            '''

            # Standard fields like hidden_states, logits, attentions etc. can be automatically documented
            # However, given that the loss docstring is often different per model, you should document it above
            loss: Optional[torch.FloatTensor] = None
            logits: Optional[torch.FloatTensor] = None
            hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None
            attentions: Optional[tuple[torch.FloatTensor, ...]] = None
            custom_field: Optional[torch.FloatTensor] = None
        ```

    Args:
        custom_intro (`str`, *optional*):
            Custom introduction text to add to the docstring. This replaces the default
            introduction text generated by the decorator before the Args section. Use this to describe what
            makes your model or method special.
        custom_args (`str`, *optional*):
            Custom argument documentation in docstring format. This allows you to define
            argument descriptions once and reuse them across multiple methods. The format should follow the
            standard docstring convention: `arg_name (`type`, *optional*, defaults to `value`): Description.`
        checkpoint (`str`, *optional*):
            Checkpoint name to use in examples within the docstring. This is typically
            automatically inferred from the model configuration class, but can be overridden if needed for
            custom examples.

    Note:
        - Standard arguments (`input_ids`, `attention_mask`, `pixel_values`, etc.) are automatically documented
          from predefined descriptions and should not be redefined unless their behavior differs in your model.
        - New or custom arguments should be documented in the method's docstring using the `r''' '''` block
          or passed via the `custom_args` parameter.
        - For model classes, the decorator derives parameter descriptions from the `__init__` method's signature
          and docstring.
        - Return value documentation is automatically generated for methods that return ModelOutput subclasses.
    """
    ...
