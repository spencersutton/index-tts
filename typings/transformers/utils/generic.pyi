"""
This type stub file was generated by pyright.
"""

import torch
from collections import OrderedDict
from collections.abc import MutableMapping
from contextlib import contextmanager
from dataclasses import dataclass
from enum import Enum
from typing import Any, Callable, ContextManager, Optional, TypedDict
from .import_utils import is_torch_available, requires

_CAN_RECORD_REGISTRY = ...
logger = ...
if is_torch_available(): ...

class cached_property(property):
    def __get__(self, obj, objtype=...): ...

def strtobool(val): ...
def infer_framework_from_repr(x): ...
def is_tensor(x): ...
def is_numpy_array(x): ...
def is_torch_tensor(x): ...
def is_torch_device(x): ...
def is_torch_dtype(x): ...
def is_tf_tensor(x): ...
def is_tf_symbolic_tensor(x): ...
def is_jax_tensor(x): ...
def is_mlx_array(x): ...
def to_py_obj(obj): ...
def to_numpy(obj): ...

class ModelOutput(OrderedDict):
    def __init_subclass__(cls) -> None: ...
    def __init__(self, *args, **kwargs) -> None: ...
    def __post_init__(self): ...
    def __delitem__(self, *args, **kwargs): ...
    def setdefault(self, *args, **kwargs): ...
    def pop(self, *args, **kwargs): ...
    def update(self, *args, **kwargs): ...
    def __getitem__(self, k): ...
    def __setattr__(self, name, value): ...
    def __setitem__(self, key, value): ...
    def __reduce__(self): ...
    def to_tuple(self) -> tuple[Any]: ...

if is_torch_available(): ...

class ExplicitEnum(str, Enum): ...

class PaddingStrategy(ExplicitEnum):
    LONGEST = ...
    MAX_LENGTH = ...
    DO_NOT_PAD = ...

class TensorType(ExplicitEnum):
    PYTORCH = ...
    TENSORFLOW = ...
    NUMPY = ...
    JAX = ...
    MLX = ...

class ContextManagers:
    def __init__(self, context_managers: list[ContextManager]) -> None: ...
    def __enter__(self): ...
    def __exit__(self, *args, **kwargs): ...

def can_return_loss(model_class): ...
def find_labels(model_class): ...
def flatten_dict(d: MutableMapping, parent_key: str = ..., delimiter: str = ...): ...
@contextmanager
def working_or_temp_dir(working_dir, use_temp_dir: bool = ...): ...
def transpose(array, axes=...): ...
def reshape(array, newshape): ...
def squeeze(array, axis=...): ...
def expand_dims(array, axis): ...
def tensor_size(array): ...
def infer_framework(model_class): ...
def torch_int(x): ...
def torch_float(x): ...
def filter_out_non_signature_kwargs(extra: Optional[list] = ...): ...

class TransformersKwargs(TypedDict, total=False):
    num_items_in_batch: Optional[torch.Tensor]
    output_hidden_states: Optional[bool]
    output_attentions: Optional[bool]
    output_router_logits: Optional[bool]
    cumulative_seqlens_q: Optional[torch.LongTensor]
    cumulative_seqlens_k: Optional[torch.LongTensor]
    max_length_q: Optional[int]
    max_length_k: Optional[int]
    ...

def is_timm_config_dict(config_dict: dict[str, Any]) -> bool: ...
def is_timm_local_checkpoint(pretrained_model_path: str) -> bool: ...
def set_attribute_for_modules(module: torch.nn.Module, key: str, value: Any): ...
def del_attribute_from_modules(module: torch.nn.Module, key: str): ...
def can_return_tuple(func): ...

@dataclass
@requires(backends=("torch",))
class OutputRecorder:
    target_class: type[torch.nn.Module]
    index: Optional[int] = ...
    layer_name: Optional[str] = ...
    class_name: Optional[str] = ...

def check_model_inputs(func): ...

class GeneralInterface(MutableMapping):
    _global_mapping = ...
    def __init__(self) -> None: ...
    def __getitem__(self, key): ...
    def __setitem__(self, key, value): ...
    def __delitem__(self, key): ...
    def __iter__(self): ...
    def __len__(self): ...
    @classmethod
    def register(cls, key: str, value: Callable): ...
    def valid_keys(self) -> list[str]: ...
