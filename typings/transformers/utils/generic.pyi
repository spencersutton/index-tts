"""
This type stub file was generated by pyright.
"""

from collections import OrderedDict
from collections.abc import Callable, MutableMapping
from contextlib import contextmanager
from dataclasses import dataclass
from enum import Enum, StrEnum
from typing import Any, ContextManager, Optional, TypedDict

import torch

from .import_utils import is_torch_available, requires

"""
Generic utilities
"""
_CAN_RECORD_REGISTRY = ...
logger = ...
if is_torch_available(): ...

class cached_property(property):
    """
    Descriptor that mimics @property but caches output in member variable.

    From tensorflow_datasets

    Built-in in functools from Python 3.8.
    """
    def __get__(self, obj, objtype=...):  # -> Self | Any:
        ...

def strtobool(val):  # -> Literal[1, 0]:
    """Convert a string representation of truth to true (1) or false (0).

    True values are 'y', 'yes', 't', 'true', 'on', and '1'; false values are 'n', 'no', 'f', 'false', 'off', and '0'.
    Raises ValueError if 'val' is anything else.
    """
    ...

def infer_framework_from_repr(x):  # -> Literal['pt', 'tf', 'jax', 'np', 'mlx'] | None:
    """
    Tries to guess the framework of an object `x` from its repr (brittle but will help in `is_tensor` to try the
    frameworks in a smart order, without the need to import the frameworks).
    """
    ...

def is_tensor(x):  # -> bool:
    """
    Tests if `x` is a `torch.Tensor`, `tf.Tensor`, `jaxlib.xla_extension.DeviceArray`, `np.ndarray` or `mlx.array`
    in the order defined by `infer_framework_from_repr`
    """
    ...

def is_numpy_array(x):  # -> bool:
    """
    Tests if `x` is a numpy array or not.
    """
    ...

def is_torch_tensor(x):  # -> bool:
    """
    Tests if `x` is a torch tensor or not. Safe to call even if torch is not installed.
    """
    ...

def is_torch_device(x):  # -> bool:
    """
    Tests if `x` is a torch device or not. Safe to call even if torch is not installed.
    """
    ...

def is_torch_dtype(x):  # -> bool:
    """
    Tests if `x` is a torch dtype or not. Safe to call even if torch is not installed.
    """
    ...

def is_tf_tensor(x):  # -> bool:
    """
    Tests if `x` is a tensorflow tensor or not. Safe to call even if tensorflow is not installed.
    """
    ...

def is_tf_symbolic_tensor(x):  # -> bool:
    """
    Tests if `x` is a tensorflow symbolic tensor or not (ie. not eager). Safe to call even if tensorflow is not
    installed.
    """
    ...

def is_jax_tensor(x):  # -> bool:
    """
    Tests if `x` is a Jax tensor or not. Safe to call even if jax is not installed.
    """
    ...

def is_mlx_array(x):  # -> bool:
    """
    Tests if `x` is a mlx array or not. Safe to call even when mlx is not installed.
    """
    ...

def to_py_obj(
    obj,
):  # -> int | float | dict[Any, int | float | dict[Any, int | float | dict[Any, Any] | Any | list[int | float | dict[Any, Any] | Any | list[Any] | complex] | complex] | Any | list[int | float | dict[Any, Any] | Any | list[Any] | complex] | complex] | Any | list[int | float | dict[Any, Any] | Any | list[Any] | complex] | complex:
    """
    Convert a TensorFlow tensor, PyTorch tensor, Numpy array or python list to a python list.
    """
    ...

def to_numpy(obj):  # -> dict[Any, dict[Any, Any] | NDArray[Any] | Any] | NDArray[Any]:
    """
    Convert a TensorFlow tensor, PyTorch tensor, Numpy array or python list to a Numpy array.
    """
    ...

class ModelOutput(OrderedDict):
    """
    Base class for all model outputs as dataclass. Has a `__getitem__` that allows indexing by integer or slice (like a
    tuple) or strings (like a dictionary) that will ignore the `None` attributes. Otherwise behaves like a regular
    python dictionary.

    <Tip warning={true}>

    You can't unpack a `ModelOutput` directly. Use the [`~utils.ModelOutput.to_tuple`] method to convert it to a tuple
    before.

    </Tip>
    """
    def __init_subclass__(cls) -> None:
        """Register subclasses as pytree nodes.

        This is necessary to synchronize gradients when using `torch.nn.parallel.DistributedDataParallel` with
        `static_graph=True` with modules that output `ModelOutput` subclasses.
        """
        ...

    def __init__(self, *args, **kwargs) -> None: ...
    def __post_init__(self):  # -> None:
        """Check the ModelOutput dataclass.

        Only occurs if @dataclass decorator has been used.
        """
        ...

    def __delitem__(self, *args, **kwargs): ...
    def setdefault(self, *args, **kwargs): ...
    def pop(self, *args, **kwargs): ...
    def update(self, *args, **kwargs): ...
    def __getitem__(self, k):  # -> Any:
        ...
    def __setattr__(self, name, value):  # -> None:
        ...
    def __setitem__(self, key, value):  # -> None:
        ...
    def __reduce__(self):  # -> str | tuple[Any, ...] | tuple[str | Any, tuple[Any, ...], *tuple[str | Any, ...]]:
        ...
    def to_tuple(self) -> tuple[Any]:
        """
        Convert self to a tuple containing all the attributes/keys that are not `None`.
        """
        ...

if is_torch_available(): ...

class ExplicitEnum(StrEnum):
    """
    Enum with more explicit error message for missing values.
    """

    ...

class PaddingStrategy(ExplicitEnum):
    """
    Possible values for the `padding` argument in [`PreTrainedTokenizerBase.__call__`]. Useful for tab-completion in an
    IDE.
    """

    LONGEST = ...
    MAX_LENGTH = ...
    DO_NOT_PAD = ...

class TensorType(ExplicitEnum):
    """
    Possible values for the `return_tensors` argument in [`PreTrainedTokenizerBase.__call__`]. Useful for
    tab-completion in an IDE.
    """

    PYTORCH = ...
    TENSORFLOW = ...
    NUMPY = ...
    JAX = ...
    MLX = ...

class ContextManagers:
    """
    Wrapper for `contextlib.ExitStack` which enters a collection of context managers. Adaptation of `ContextManagers`
    in the `fastcore` library.
    """
    def __init__(self, context_managers: list[ContextManager]) -> None: ...
    def __enter__(self):  # -> None:
        ...
    def __exit__(self, *args, **kwargs):  # -> None:
        ...

def can_return_loss(model_class):  # -> bool:
    """
    Check if a given model can return loss.

    Args:
        model_class (`type`): The class of the model.
    """
    ...

def find_labels(model_class):  # -> list[str]:
    """
    Find the labels used by a given model.

    Args:
        model_class (`type`): The class of the model.
    """
    ...

def flatten_dict(d: MutableMapping, parent_key: str = ..., delimiter: str = ...):  # -> dict[str | Any, Any]:
    """Flatten a nested dict into a single level dict."""
    ...

@contextmanager
def working_or_temp_dir(working_dir, use_temp_dir: bool = ...):  # -> Generator[str | Any, Any, None]:
    ...
def transpose(array, axes=...):  # -> NDArray[Any]:
    """
    Framework-agnostic version of `numpy.transpose` that will work on torch/TensorFlow/Jax tensors as well as NumPy
    arrays.
    """
    ...

def reshape(array, newshape):  # -> ndarray[tuple[int], dtype[Any]]:
    """
    Framework-agnostic version of `numpy.reshape` that will work on torch/TensorFlow/Jax tensors as well as NumPy
    arrays.
    """
    ...

def squeeze(array, axis=...):
    """
    Framework-agnostic version of `numpy.squeeze` that will work on torch/TensorFlow/Jax tensors as well as NumPy
    arrays.
    """
    ...

def expand_dims(array, axis):  # -> NDArray[Any]:
    """
    Framework-agnostic version of `numpy.expand_dims` that will work on torch/TensorFlow/Jax tensors as well as NumPy
    arrays.
    """
    ...

def tensor_size(array):  # -> int:
    """
    Framework-agnostic version of `numpy.size` that will work on torch/TensorFlow/Jax tensors as well as NumPy arrays.
    """
    ...

def infer_framework(model_class):  # -> Literal['tf', 'pt', 'flax']:
    """
    Infers the framework of a given model without using isinstance(), because we cannot guarantee that the relevant
    classes are imported or available.
    """
    ...

def torch_int(x):  # -> int | Tensor:
    """
    Casts an input to a torch int64 tensor if we are in a tracing context, otherwise to a Python int.
    """
    ...

def torch_float(x):  # -> int | Tensor:
    """
    Casts an input to a torch float32 tensor if we are in a tracing context, otherwise to a Python float.
    """
    ...

def filter_out_non_signature_kwargs(extra: list | None = ...):  # -> Callable[..., _Wrapped[..., Any, ..., Any]]:
    """
    Decorator to filter out named arguments that are not in the function signature.

    This decorator ensures that only the keyword arguments that match the function's signature, or are specified in the
    `extra` list, are passed to the function. Any additional keyword arguments are filtered out and a warning is issued.

    Parameters:
        extra (`Optional[list]`, *optional*):
            A list of extra keyword argument names that are allowed even if they are not in the function's signature.

    Returns:
        Callable:
            A decorator that wraps the function and filters out invalid keyword arguments.

    Example usage:

        ```python
        @filter_out_non_signature_kwargs(extra=["allowed_extra_arg"])
        def my_function(arg1, arg2, **kwargs):
            print(arg1, arg2, kwargs)

        my_function(arg1=1, arg2=2, allowed_extra_arg=3, invalid_arg=4)
        # This will print: 1 2 {"allowed_extra_arg": 3}
        # And issue a warning: "The following named arguments are not valid for `my_function` and were ignored: 'invalid_arg'"
        ```
    """
    ...

class TransformersKwargs(TypedDict, total=False):
    """
    Keyword arguments to be passed to the loss function

    Attributes:
        num_items_in_batch (`Optional[torch.Tensor]`, *optional*):
            Number of items in the batch. It is recommended to pass it when
            you are doing gradient accumulation.
        output_hidden_states (`Optional[bool]`, *optional*):
            Most of the models support outputing all hidden states computed during the forward pass.
        output_attentions (`Optional[bool]`, *optional*):
            Turn this on to return the intermediary attention scores.
        output_router_logits (`Optional[bool]`, *optional*):
            For MoE models, this allows returning the router logits to compute the loss.
        cumulative_seqlens_q (`torch.LongTensor`, *optional*)
            Gets cumulative sequence length for query state.
        cumulative_seqlens_k (`torch.LongTensor`, *optional*)
            Gets cumulative sequence length for key state.
        max_length_q (`int`, *optional*):
            Maximum sequence length for query state.
        max_length_k (`int`, *optional*):
            Maximum sequence length for key state.
    """

    num_items_in_batch: torch.Tensor | None
    output_hidden_states: bool | None
    output_attentions: bool | None
    output_router_logits: bool | None
    cumulative_seqlens_q: torch.LongTensor | None
    cumulative_seqlens_k: torch.LongTensor | None
    max_length_q: int | None
    max_length_k: int | None
    ...

def is_timm_config_dict(config_dict: dict[str, Any]) -> bool:
    """Checks whether a config dict is a timm config dict."""
    ...

def is_timm_local_checkpoint(pretrained_model_path: str) -> bool:
    """
    Checks whether a checkpoint is a timm model checkpoint.
    """
    ...

def set_attribute_for_modules(module: torch.nn.Module, key: str, value: Any):  # -> None:
    """
    Set a value to a module and all submodules.
    """
    ...

def del_attribute_from_modules(module: torch.nn.Module, key: str):  # -> None:
    """
    Delete a value from a module and all submodules.
    """
    ...

def can_return_tuple(func):  # -> _Wrapped[..., Any, ..., Any | tuple[Any, ...]]:
    """
    Decorator to wrap model method, to call output.to_tuple() if return_dict=False passed as a kwarg or
    use_return_dict=False is set in the config.

    Note:
        output.to_tuple() convert output to tuple skipping all `None` values.
    """
    ...

@dataclass
@requires(backends=("torch",))
class OutputRecorder:
    """
    Configuration for recording outputs from a model via hooks.

    Attributes:
        target_class (Type): The class (e.g., nn.Module) to which the hook will be attached.
        index (Optional[int]): If the output is a tuple/list, optionally record only at a specific index.
        layer_name (Optional[str]): Name of the submodule to target (if needed), e.g., "transformer.layer.3.attn".
        class_name (Optional[str]): Name of the class to which the hook will be attached. Could be the suffix of class name in some cases.
    """

    target_class: type[torch.nn.Module]
    index: int | None = ...
    layer_name: str | None = ...
    class_name: str | None = ...

def check_model_inputs(func):  # -> _Wrapped[..., Any, ..., Any]:
    """
    Decorator to intercept specific layer outputs without using hooks.
    Compatible with torch.compile (Dynamo tracing).
    """
    ...

class GeneralInterface(MutableMapping):
    """
    Dict-like object keeping track of a class-wide mapping, as well as a local one. Allows to have library-wide
    modifications though the class mapping, as well as local modifications in a single file with the local mapping.
    """

    _global_mapping = ...
    def __init__(self) -> None: ...
    def __getitem__(self, key): ...
    def __setitem__(self, key, value):  # -> None:
        ...
    def __delitem__(self, key):  # -> None:
        ...
    def __iter__(self):  # -> Iterator[Any]:
        ...
    def __len__(self):  # -> int:
        ...
    @classmethod
    def register(cls, key: str, value: Callable):  # -> None:
        ...
    def valid_keys(self) -> list[str]: ...
