"""
This type stub file was generated by pyright.
"""

import importlib.machinery
from enum import Enum
from functools import lru_cache
from types import ModuleType
from typing import Any, Optional

"""
Import utilities: Utilities related to imports and our lazy inits.
"""
logger = ...
ENV_VARS_TRUE_VALUES = ...
ENV_VARS_TRUE_AND_AUTO_VALUES = ...
USE_TF = ...
USE_TORCH = ...
USE_JAX = ...
USE_TORCH_XLA = ...
FORCE_TF_AVAILABLE = ...
TORCH_FX_REQUIRED_VERSION = ...
ACCELERATE_MIN_VERSION = ...
SCHEDULEFREE_MIN_VERSION = ...
FSDP_MIN_VERSION = ...
GGUF_MIN_VERSION = ...
XLA_FSDPV2_MIN_VERSION = ...
HQQ_MIN_VERSION = ...
VPTQ_MIN_VERSION = ...
TORCHAO_MIN_VERSION = ...
AUTOROUND_MIN_VERSION = ...
TRITON_MIN_VERSION = ...
_apex_available = ...
_apollo_torch_available = ...
_aqlm_available = ...
_av_available = ...
_decord_available = ...
_torchcodec_available = ...
_libcst_available = ...
_bitsandbytes_available = ...
_eetq_available = ...
_fbgemm_gpu_available = ...
_galore_torch_available = ...
_lomo_available = ...
_grokadamw_available = ...
_torch_optimi_available = ...
_bs4_available = ...
_coloredlogs_available = ...
_cv2_available = ...
_yt_dlp_available = ...
_datasets_available = ...
_detectron2_available = ...
_faiss_available = ...
_faiss_version = ...
_ftfy_available = ...
_g2p_en_available = ...
_hadamard_available = ...
_jieba_available = ...
_jinja_available = ...
_kenlm_available = ...
_keras_nlp_available = ...
_levenshtein_available = ...
_librosa_available = ...
_natten_available = ...
_nltk_available = ...
_onnx_available = ...
_openai_available = ...
_optimum_available = ...
_auto_gptq_available = ...
_gptqmodel_available = ...
_auto_awq_available = ...
_quark_available = ...
_qutlass_available = ...
_is_optimum_quanto_available = ...
_is_optimum_quanto_available = ...
_compressed_tensors_available = ...
_pandas_available = ...
_peft_available = ...
_phonemizer_available = ...
_uroman_available = ...
_psutil_available = ...
_py3nvml_available = ...
_pyctcdecode_available = ...
_pygments_available = ...
_pytesseract_available = ...
_pytest_available = ...
_pytorch_quantization_available = ...
_rjieba_available = ...
_sacremoses_available = ...
_safetensors_available = ...
_scipy_available = ...
_sentencepiece_available = ...
_is_seqio_available = ...
_sklearn_available = ...
if _sklearn_available: ...
_smdistributed_available = ...
_soundfile_available = ...
_spacy_available = ...
_tensorflow_probability_available = ...
_tensorflow_text_available = ...
_tf2onnx_available = ...
_timm_available = ...
_tokenizers_available = ...
_torchaudio_available = ...
_torchdistx_available = ...
_mlx_available = ...
_num2words_available = ...
_tiktoken_available = ...
_blobfile_available = ...
_liger_kernel_available = ...
_spqr_available = ...
_rich_available = ...
_kernels_available = ...
_matplotlib_available = ...
_mistral_common_available = ...
_torch_version = ...
_torch_available = ...
if USE_TORCH in ENV_VARS_TRUE_AND_AUTO_VALUES and USE_TF not in ENV_VARS_TRUE_VALUES: ...
else:
    _torch_available = ...
_tf_version = ...
_tf_available = ...
if FORCE_TF_AVAILABLE in ENV_VARS_TRUE_VALUES:
    _tf_available = ...
else: ...
_essentia_available = ...
_essentia_version = ...
_pydantic_available = ...
_pydantic_version = ...
_fastapi_available = ...
_fastapi_version = ...
_uvicorn_available = ...
_uvicorn_version = ...
_pretty_midi_available = ...
_pretty_midi_version = ...
ccl_version = ...
_is_ccl_available = ...
ccl_version = ...
_flax_available = ...
if USE_JAX in ENV_VARS_TRUE_AND_AUTO_VALUES: ...
_torch_xla_available = ...
if USE_TORCH_XLA in ENV_VARS_TRUE_VALUES: ...

def is_kenlm_available():  # -> tuple[bool, str] | bool:
    ...
def is_kernels_available():  # -> tuple[bool, str] | bool:
    ...
def is_cv2_available():  # -> bool:
    ...
def is_yt_dlp_available():  # -> bool:
    ...
def is_torch_available():  # -> bool:
    ...
def is_libcst_available():  # -> tuple[bool, str] | bool:
    ...
def is_accelerate_available(min_version: str = ...):  # -> bool:
    ...
def is_torch_accelerator_available():  # -> bool:
    ...
def is_torch_deterministic():  # -> bool:
    """
    Check whether pytorch uses deterministic algorithms by looking if torch.set_deterministic_debug_mode() is set to 1 or 2"
    """
    ...

def is_triton_available(min_version: str = ...):  # -> bool:
    ...
def is_hadamard_available():  # -> tuple[bool, str] | bool:
    ...
def is_hqq_available(min_version: str = ...):  # -> bool:
    ...
def is_pygments_available():  # -> tuple[bool, str] | bool:
    ...
def get_torch_version():  # -> str:
    ...
def get_torch_major_and_minor_version() -> str: ...
def is_torch_sdpa_available():  # -> bool:
    ...
def is_torch_flex_attn_available():  # -> bool:
    ...
def is_torchvision_available():  # -> bool:
    ...
def is_torchvision_v2_available():  # -> bool:
    ...
def is_galore_torch_available():  # -> tuple[bool, str] | bool:
    ...
def is_apollo_torch_available():  # -> tuple[bool, str] | bool:
    ...
def is_torch_optimi_available():  # -> bool:
    ...
def is_lomo_available():  # -> tuple[bool, str] | bool:
    ...
def is_grokadamw_available():  # -> tuple[bool, str] | bool:
    ...
def is_schedulefree_available(min_version: str = ...):  # -> bool:
    ...
def is_pyctcdecode_available():  # -> tuple[bool, str] | bool:
    ...
def is_librosa_available():  # -> tuple[bool, str] | bool:
    ...
def is_essentia_available():  # -> bool:
    ...
def is_pydantic_available():  # -> bool:
    ...
def is_fastapi_available():  # -> bool:
    ...
def is_uvicorn_available():  # -> bool:
    ...
def is_openai_available():  # -> tuple[bool, str] | bool:
    ...
def is_pretty_midi_available():  # -> bool:
    ...
def is_torch_cuda_available():  # -> bool:
    ...
def is_cuda_platform():  # -> bool:
    ...
def is_rocm_platform():  # -> bool:
    ...
def is_mamba_ssm_available():  # -> tuple[bool, str] | bool:
    ...
def is_mamba_2_ssm_available():  # -> bool:
    ...
def is_causal_conv1d_available():  # -> tuple[bool, str] | bool:
    ...
def is_xlstm_available():  # -> tuple[bool, str] | bool:
    ...
def is_mambapy_available():  # -> tuple[bool, str] | bool:
    ...
def is_torch_mps_available(min_version: str | None = ...):  # -> bool:
    ...
def is_torch_bf16_gpu_available() -> bool: ...
def is_torch_bf16_cpu_available() -> bool: ...
def is_torch_bf16_available():  # -> bool:
    ...
@lru_cache
def is_torch_fp16_available_on_device(device):  # -> bool:
    ...
@lru_cache
def is_torch_bf16_available_on_device(device):  # -> bool:
    ...
def is_torch_tf32_available():  # -> bool:
    ...
def is_torch_fx_available():  # -> bool:
    ...
def is_peft_available():  # -> tuple[bool, str] | bool:
    ...
def is_bs4_available():  # -> bool:
    ...
def is_tf_available():  # -> bool:
    ...
def is_coloredlogs_available():  # -> tuple[bool, str] | bool:
    ...
def is_tf2onnx_available():  # -> tuple[bool, str] | bool:
    ...
def is_onnx_available():  # -> tuple[bool, str] | bool:
    ...
def is_flax_available():  # -> bool:
    ...
def is_flute_available():  # -> bool:
    ...
def is_ftfy_available():  # -> tuple[bool, str] | bool:
    ...
def is_g2p_en_available():  # -> tuple[bool, str] | bool:
    ...
@lru_cache
def is_torch_xla_available(check_is_tpu=..., check_is_gpu=...):  # -> bool:
    """
    Check if `torch_xla` is available. To train a native pytorch job in an environment with torch xla installed, set
    the USE_TORCH_XLA to false.
    """
    ...

@lru_cache
def is_torch_neuroncore_available(check_device=...):  # -> bool:
    ...
@lru_cache
def is_torch_npu_available(check_device=...):  # -> Literal[False]:
    "Checks if `torch_npu` is installed and potentially if a NPU is in the environment"
    ...

@lru_cache
def is_torch_mlu_available(check_device=...):  # -> Literal[False]:
    """
    Checks if `mlu` is available via an `cndev-based` check which won't trigger the drivers and leave mlu
    uninitialized.
    """
    ...

@lru_cache
def is_torch_musa_available(check_device=...):  # -> Literal[False]:
    "Checks if `torch_musa` is installed and potentially if a MUSA is in the environment"
    ...

@lru_cache
def is_torch_hpu_available():  # -> bool:
    "Checks if `torch.hpu` is available and potentially if a HPU is in the environment"
    ...

@lru_cache
def is_habana_gaudi1():  # -> Literal[False]:
    ...
def is_torchdynamo_available():  # -> bool:
    ...
def is_torch_compile_available():  # -> bool:
    ...
def is_torchdynamo_compiling():  # -> bool:
    ...
def is_torchdynamo_exporting():  # -> bool:
    ...
def is_torch_tensorrt_fx_available():  # -> bool:
    ...
def is_datasets_available():  # -> tuple[bool, str] | bool:
    ...
def is_detectron2_available():  # -> tuple[bool, str] | bool:
    ...
def is_rjieba_available():  # -> tuple[bool, str] | bool:
    ...
def is_psutil_available():  # -> tuple[bool, str] | bool:
    ...
def is_py3nvml_available():  # -> tuple[bool, str] | bool:
    ...
def is_sacremoses_available():  # -> tuple[bool, str] | bool:
    ...
def is_apex_available():  # -> tuple[bool, str] | bool:
    ...
def is_aqlm_available():  # -> tuple[bool, str] | bool:
    ...
def is_vptq_available(min_version: str = ...):  # -> bool:
    ...
def is_av_available():  # -> bool:
    ...
def is_decord_available():  # -> bool:
    ...
def is_torchcodec_available():  # -> bool:
    ...
def is_ninja_available():  # -> bool:
    r"""
    Code comes from *torch.utils.cpp_extension.is_ninja_available()*. Returns `True` if the
    [ninja](https://ninja-build.org/) build system is available on the system, `False` otherwise.
    """
    ...

def is_ipex_available(min_version: str = ...):  # -> bool:
    ...
@lru_cache
def is_torch_xpu_available(check_device=...):  # -> bool:
    """
    Checks if XPU acceleration is available either via native PyTorch (>=2.6),
    `intel_extension_for_pytorch` or via stock PyTorch (>=2.4) and potentially
    if a XPU is in the environment.
    """
    ...

@lru_cache
def is_bitsandbytes_available(check_library_only=...) -> bool: ...
def is_bitsandbytes_multi_backend_available() -> bool: ...
def is_flash_attn_2_available():  # -> bool:
    ...
@lru_cache
def is_flash_attn_3_available():  # -> bool:
    ...
@lru_cache
def is_flash_attn_greater_or_equal_2_10():  # -> bool:
    ...
@lru_cache
def is_flash_attn_greater_or_equal(library_version: str):  # -> bool:
    ...
@lru_cache
def is_torch_greater_or_equal(library_version: str, accept_dev: bool = ...):  # -> bool:
    """
    Accepts a library version and returns True if the current version of the library is greater than or equal to the
    given version. If `accept_dev` is True, it will also accept development versions (e.g. 2.7.0.dev20250320 matches
    2.7.0).
    """
    ...

@lru_cache
def is_torch_less_or_equal(library_version: str, accept_dev: bool = ...):  # -> bool:
    """
    Accepts a library version and returns True if the current version of the library is less than or equal to the
    given version. If `accept_dev` is True, it will also accept development versions (e.g. 2.7.0.dev20250320 matches
    2.7.0).
    """
    ...

@lru_cache
def is_huggingface_hub_greater_or_equal(library_version: str, accept_dev: bool = ...):  # -> bool:
    ...
def is_torchdistx_available():  # -> tuple[bool, str] | bool:
    ...
def is_faiss_available():  # -> bool:
    ...
def is_scipy_available():  # -> tuple[bool, str] | bool:
    ...
def is_sklearn_available():  # -> bool:
    ...
def is_sentencepiece_available():  # -> tuple[bool, str] | bool:
    ...
def is_seqio_available():  # -> tuple[bool, str] | bool:
    ...
def is_gguf_available(min_version: str = ...):  # -> bool:
    ...
def is_protobuf_available():  # -> bool:
    ...
def is_fsdp_available(min_version: str = ...):  # -> bool:
    ...
def is_optimum_available():  # -> tuple[bool, str] | bool:
    ...
def is_auto_awq_available():  # -> bool:
    ...
def is_auto_round_available(min_version: str = ...):  # -> bool:
    ...
def is_optimum_quanto_available():  # -> bool:
    ...
def is_quark_available():  # -> tuple[bool, str] | bool:
    ...
def is_fp_quant_available():  # -> bool:
    ...
def is_qutlass_available():  # -> tuple[bool, str] | bool:
    ...
def is_compressed_tensors_available():  # -> bool:
    ...
def is_auto_gptq_available():  # -> tuple[bool, str] | bool:
    ...
def is_gptqmodel_available():  # -> tuple[bool, str] | bool:
    ...
def is_eetq_available():  # -> tuple[bool, str] | bool:
    ...
def is_fbgemm_gpu_available():  # -> tuple[bool, str] | bool:
    ...
def is_levenshtein_available():  # -> tuple[bool, str] | bool:
    ...
def is_optimum_neuron_available():  # -> tuple[bool, str] | bool:
    ...
def is_safetensors_available():  # -> tuple[bool, str] | bool:
    ...
def is_tokenizers_available():  # -> tuple[bool, str] | bool:
    ...
@lru_cache
def is_vision_available():  # -> bool:
    ...
def is_pytesseract_available():  # -> tuple[bool, str] | bool:
    ...
def is_pytest_available():  # -> tuple[bool, str] | bool:
    ...
def is_spacy_available():  # -> tuple[bool, str] | bool:
    ...
def is_tensorflow_text_available():  # -> tuple[bool, str] | bool:
    ...
def is_keras_nlp_available():  # -> tuple[bool, str] | bool:
    ...
def is_in_notebook():  # -> bool:
    ...
def is_pytorch_quantization_available():  # -> tuple[bool, str] | bool:
    ...
def is_tensorflow_probability_available():  # -> tuple[bool, str] | bool:
    ...
def is_pandas_available():  # -> tuple[bool, str] | bool:
    ...
def is_sagemaker_dp_enabled():  # -> bool:
    ...
def is_sagemaker_mp_enabled():  # -> bool:
    ...
def is_training_run_on_sagemaker():  # -> bool:
    ...
def is_soundfile_available():  # -> tuple[bool, str] | bool:
    ...
def is_timm_available():  # -> tuple[bool, str] | bool:
    ...
def is_natten_available():  # -> tuple[bool, str] | bool:
    ...
def is_nltk_available():  # -> tuple[bool, str] | bool:
    ...
def is_torchaudio_available():  # -> tuple[bool, str] | bool:
    ...
def is_torchao_available(min_version: str = ...):  # -> bool:
    ...
def is_speech_available():  # -> tuple[bool, str] | bool:
    ...
def is_spqr_available():  # -> tuple[bool, str] | bool:
    ...
def is_phonemizer_available():  # -> tuple[bool, str] | bool:
    ...
def is_uroman_available():  # -> tuple[bool, str] | bool:
    ...
def torch_only_method(fn):  # -> Callable[..., Any]:
    ...
def is_ccl_available():  # -> bool:
    ...
def is_sudachi_available():  # -> bool:
    ...
def get_sudachi_version():  # -> str:
    ...
def is_sudachi_projection_available():  # -> bool:
    ...
def is_jumanpp_available():  # -> bool:
    ...
def is_cython_available():  # -> bool:
    ...
def is_jieba_available():  # -> tuple[bool, str] | bool:
    ...
def is_jinja_available():  # -> tuple[bool, str] | bool:
    ...
def is_mlx_available():  # -> tuple[bool, str] | bool:
    ...
def is_num2words_available():  # -> tuple[bool, str] | bool:
    ...
def is_tiktoken_available():  # -> tuple[bool, str] | bool:
    ...
def is_liger_kernel_available():  # -> bool:
    ...
def is_rich_available():  # -> tuple[bool, str] | bool:
    ...
def is_matplotlib_available():  # -> tuple[bool, str] | bool:
    ...
def is_mistral_common_available():  # -> tuple[bool, str] | bool:
    ...
def check_torch_load_is_safe():  # -> None:
    ...

AV_IMPORT_ERROR = ...
YT_DLP_IMPORT_ERROR = ...
DECORD_IMPORT_ERROR = ...
TORCHCODEC_IMPORT_ERROR = ...
CV2_IMPORT_ERROR = ...
DATASETS_IMPORT_ERROR = ...
TOKENIZERS_IMPORT_ERROR = ...
SENTENCEPIECE_IMPORT_ERROR = ...
PROTOBUF_IMPORT_ERROR = ...
FAISS_IMPORT_ERROR = ...
PYTORCH_IMPORT_ERROR = ...
TORCHVISION_IMPORT_ERROR = ...
PYTORCH_IMPORT_ERROR_WITH_TF = ...
TF_IMPORT_ERROR_WITH_PYTORCH = ...
BS4_IMPORT_ERROR = ...
SKLEARN_IMPORT_ERROR = ...
TENSORFLOW_IMPORT_ERROR = ...
DETECTRON2_IMPORT_ERROR = ...
FLAX_IMPORT_ERROR = ...
FTFY_IMPORT_ERROR = ...
LEVENSHTEIN_IMPORT_ERROR = ...
G2P_EN_IMPORT_ERROR = ...
PYTORCH_QUANTIZATION_IMPORT_ERROR = ...
TENSORFLOW_PROBABILITY_IMPORT_ERROR = ...
TENSORFLOW_TEXT_IMPORT_ERROR = ...
TORCHAUDIO_IMPORT_ERROR = ...
PANDAS_IMPORT_ERROR = ...
PHONEMIZER_IMPORT_ERROR = ...
UROMAN_IMPORT_ERROR = ...
SACREMOSES_IMPORT_ERROR = ...
SCIPY_IMPORT_ERROR = ...
KERAS_NLP_IMPORT_ERROR = ...
SPEECH_IMPORT_ERROR = ...
TIMM_IMPORT_ERROR = ...
NATTEN_IMPORT_ERROR = ...
NUMEXPR_IMPORT_ERROR = ...
NLTK_IMPORT_ERROR = ...
VISION_IMPORT_ERROR = ...
PYDANTIC_IMPORT_ERROR = ...
FASTAPI_IMPORT_ERROR = ...
UVICORN_IMPORT_ERROR = ...
OPENAI_IMPORT_ERROR = ...
PYTESSERACT_IMPORT_ERROR = ...
PYCTCDECODE_IMPORT_ERROR = ...
ACCELERATE_IMPORT_ERROR = ...
CCL_IMPORT_ERROR = ...
ESSENTIA_IMPORT_ERROR = ...
LIBROSA_IMPORT_ERROR = ...
PRETTY_MIDI_IMPORT_ERROR = ...
CYTHON_IMPORT_ERROR = ...
JIEBA_IMPORT_ERROR = ...
PEFT_IMPORT_ERROR = ...
JINJA_IMPORT_ERROR = ...
RICH_IMPORT_ERROR = ...
MISTRAL_COMMON_IMPORT_ERROR = ...
BACKENDS_MAPPING = ...

def requires_backends(obj, backends):  # -> None:
    ...

class DummyObject(type):
    """
    Metaclass for the dummy objects. Any class inheriting from it will return the ImportError generated by
    `requires_backend` each time a user tries to access any method of that class.
    """

    is_dummy = ...
    def __getattribute__(cls, key):  # -> Any | None:
        ...

def is_torch_fx_proxy(x):  # -> bool:
    ...

BACKENDS_T = frozenset[str]
IMPORT_STRUCTURE_T = dict[BACKENDS_T, dict[str, set[str]]]

class _LazyModule(ModuleType):
    """
    Module class that surfaces all objects but only performs associated imports when the objects are requested.
    """
    def __init__(
        self,
        name: str,
        module_file: str,
        import_structure: IMPORT_STRUCTURE_T,
        module_spec: importlib.machinery.ModuleSpec | None = ...,
        extra_objects: dict[str, object] | None = ...,
        explicit_import_shortcut: dict[str, list[str]] | None = ...,
    ) -> None: ...
    def __dir__(self):  # -> Iterable[str]:
        ...
    def __getattr__(self, name: str) -> Any: ...
    def __reduce__(self):  # -> tuple[type[Self], tuple[str, str | None, IMPORT_STRUCTURE_T]]:
        ...

class OptionalDependencyNotAvailable(BaseException):
    """Internally used error class for signalling an optional dependency was not found."""

    ...

def direct_transformers_import(path: str, file=...) -> ModuleType:
    """Imports transformers directly

    Args:
        path (`str`): The path to the source file
        file (`str`, *optional*): The file to join with the path. Defaults to "__init__.py".

    Returns:
        `ModuleType`: The resulting imported module
    """
    ...

class VersionComparison(Enum):
    EQUAL = ...
    NOT_EQUAL = ...
    GREATER_THAN = ...
    LESS_THAN = ...
    GREATER_THAN_OR_EQUAL = ...
    LESS_THAN_OR_EQUAL = ...
    @staticmethod
    def from_string(version_string: str) -> VersionComparison: ...

@lru_cache
def split_package_version(package_version_str) -> tuple[str, str, str]: ...

class Backend:
    def __init__(self, backend_requirement: str) -> None: ...
    def is_satisfied(self) -> bool: ...
    def __repr__(self) -> str: ...
    @property
    def error_message(self):  # -> str:
        ...

def requires(*, backends=...):  # -> Callable[..., Any]:
    """
    This decorator enables two things:
    - Attaching a `__backends` tuple to an object to see what are the necessary backends for it
      to execute correctly without instantiating it
    - The '@requires' string is used to dynamically import objects
    """
    ...

BASE_FILE_REQUIREMENTS = ...

def fetch__all__(file_content):  # -> list[Any]:
    """
    Returns the content of the __all__ variable in the file content.
    Returns None if not defined, otherwise returns a list of strings.
    """
    ...

@lru_cache
def create_import_structure_from_path(module_path):  # -> dict[Any, Any]:
    """
    This method takes the path to a file/a folder and returns the import structure.
    If a file is given, it will return the import structure of the parent folder.

    Import structures are designed to be digestible by `_LazyModule` objects. They are
    created from the __all__ definitions in each files as well as the `@require` decorators
    above methods and objects.

    The import structure allows explicit display of the required backends for a given object.
    These backends are specified in two ways:

    1. Through their `@require`, if they are exported with that decorator. This `@require` decorator
       accepts a `backend` tuple kwarg mentioning which backends are required to run this object.

    2. If an object is defined in a file with "default" backends, it will have, at a minimum, this
       backend specified. The default backends are defined according to the filename:

       - If a file is named like `modeling_*.py`, it will have a `torch` backend
       - If a file is named like `modeling_tf_*.py`, it will have a `tf` backend
       - If a file is named like `modeling_flax_*.py`, it will have a `flax` backend
       - If a file is named like `tokenization_*_fast.py`, it will have a `tokenizers` backend
       - If a file is named like `image_processing*_fast.py`, it will have a `torchvision` + `torch` backend

    Backends serve the purpose of displaying a clear error message to the user in case the backends are not installed.
    Should an object be imported without its required backends being in the environment, any attempt to use the
    object will raise an error mentioning which backend(s) should be added to the environment in order to use
    that object.

    Here's an example of an input import structure at the src.transformers.models level:

    {
        'albert': {
            frozenset(): {
                'configuration_albert': {'AlbertConfig', 'AlbertOnnxConfig'}
            },
            frozenset({'tokenizers'}): {
                'tokenization_albert_fast': {'AlbertTokenizerFast'}
            },
        },
        'align': {
            frozenset(): {
                'configuration_align': {'AlignConfig', 'AlignTextConfig', 'AlignVisionConfig'},
                'processing_align': {'AlignProcessor'}
            },
        },
        'altclip': {
            frozenset(): {
                'configuration_altclip': {'AltCLIPConfig', 'AltCLIPTextConfig', 'AltCLIPVisionConfig'},
                'processing_altclip': {'AltCLIPProcessor'},
            }
        }
    }
    """
    ...

def spread_import_structure(nested_import_structure):  # -> dict[Any, Any]:
    """
    This method takes as input an unordered import structure and brings the required backends at the top-level,
    aggregating modules and objects under their required backends.

    Here's an example of an input import structure at the src.transformers.models level:

    {
        'albert': {
            frozenset(): {
                'configuration_albert': {'AlbertConfig', 'AlbertOnnxConfig'}
            },
            frozenset({'tokenizers'}): {
                'tokenization_albert_fast': {'AlbertTokenizerFast'}
            },
        },
        'align': {
            frozenset(): {
                'configuration_align': {'AlignConfig', 'AlignTextConfig', 'AlignVisionConfig'},
                'processing_align': {'AlignProcessor'}
            },
        },
        'altclip': {
            frozenset(): {
                'configuration_altclip': {'AltCLIPConfig', 'AltCLIPTextConfig', 'AltCLIPVisionConfig'},
                'processing_altclip': {'AltCLIPProcessor'},
            }
        }
    }

    Here's an example of an output import structure at the src.transformers.models level:

    {
        frozenset({'tokenizers'}): {
            'albert.tokenization_albert_fast': {'AlbertTokenizerFast'}
        },
        frozenset(): {
            'albert.configuration_albert': {'AlbertConfig', 'AlbertOnnxConfig'},
            'align.processing_align': {'AlignProcessor'},
            'align.configuration_align': {'AlignConfig', 'AlignTextConfig', 'AlignVisionConfig'},
            'altclip.configuration_altclip': {'AltCLIPConfig', 'AltCLIPTextConfig', 'AltCLIPVisionConfig'},
            'altclip.processing_altclip': {'AltCLIPProcessor'}
        }
    }

    """
    ...

@lru_cache
def define_import_structure(module_path: str, prefix: str | None = ...) -> IMPORT_STRUCTURE_T:
    """
    This method takes a module_path as input and creates an import structure digestible by a _LazyModule.

    Here's an example of an output import structure at the src.transformers.models level:

    {
        frozenset({'tokenizers'}): {
            'albert.tokenization_albert_fast': {'AlbertTokenizerFast'}
        },
        frozenset(): {
            'albert.configuration_albert': {'AlbertConfig', 'AlbertOnnxConfig'},
            'align.processing_align': {'AlignProcessor'},
            'align.configuration_align': {'AlignConfig', 'AlignTextConfig', 'AlignVisionConfig'},
            'altclip.configuration_altclip': {'AltCLIPConfig', 'AltCLIPTextConfig', 'AltCLIPVisionConfig'},
            'altclip.processing_altclip': {'AltCLIPProcessor'}
        }
    }

    The import structure is a dict defined with frozensets as keys, and dicts of strings to sets of objects.

    If `prefix` is not None, it will add that prefix to all keys in the returned dict.
    """
    ...

def clear_import_cache():  # -> None:
    """
    Clear cached Transformers modules to allow reloading modified code.

    This is useful when actively developing/modifying Transformers code.
    """
    ...
