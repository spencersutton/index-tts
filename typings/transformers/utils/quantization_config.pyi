"""
This type stub file was generated by pyright.
"""

import os
from dataclasses import dataclass
from enum import Enum
from typing import Any, Optional, Union
from ..utils import is_torch_available

if is_torch_available(): ...
logger = ...

class QuantizationMethod(str, Enum):
    BITS_AND_BYTES = ...
    GPTQ = ...
    AWQ = ...
    AQLM = ...
    VPTQ = ...
    QUANTO = ...
    EETQ = ...
    HIGGS = ...
    HQQ = ...
    COMPRESSED_TENSORS = ...
    FBGEMM_FP8 = ...
    TORCHAO = ...
    BITNET = ...
    SPQR = ...
    FP8 = ...
    QUARK = ...
    FPQUANT = ...
    AUTOROUND = ...
    MXFP4 = ...

class AWQLinearVersion(str, Enum):
    GEMM = ...
    GEMV = ...
    EXLLAMA = ...
    IPEX = ...
    @staticmethod
    def from_str(version: str): ...

class AwqBackendPackingMethod(str, Enum):
    AUTOAWQ = ...
    LLMAWQ = ...

@dataclass
class QuantizationConfigMixin:
    quant_method: QuantizationMethod
    @classmethod
    def from_dict(cls, config_dict, return_unused_kwargs=..., **kwargs): ...
    def to_json_file(self, json_file_path: Union[str, os.PathLike]): ...
    def to_dict(self) -> dict[str, Any]: ...
    def __iter__(self): ...
    def __repr__(self): ...
    def to_json_string(self, use_diff: bool = ...) -> str: ...
    def update(self, **kwargs): ...

@dataclass
class AutoRoundConfig(QuantizationConfigMixin):
    def __init__(
        self, bits: int = ..., group_size: int = ..., sym: bool = ..., backend: str = ..., **kwargs
    ) -> None: ...
    def post_init(self): ...
    def get_loading_attributes(self): ...
    def to_dict(self): ...
    @classmethod
    def from_dict(cls, config_dict, return_unused_kwargs=..., **kwargs): ...

@dataclass
class HqqConfig(QuantizationConfigMixin):
    def __init__(
        self,
        nbits: int = ...,
        group_size: int = ...,
        view_as_float: bool = ...,
        axis: Optional[int] = ...,
        dynamic_config: Optional[dict] = ...,
        skip_modules: list[str] = ...,
        **kwargs,
    ) -> None: ...
    def post_init(self): ...
    @classmethod
    def from_dict(cls, config: dict[str, Any]): ...
    def to_dict(self) -> dict[str, Any]: ...
    def __repr__(self): ...
    def to_diff_dict(self) -> dict[str, Any]: ...

@dataclass
class BitsAndBytesConfig(QuantizationConfigMixin):
    def __init__(
        self,
        load_in_8bit=...,
        load_in_4bit=...,
        llm_int8_threshold=...,
        llm_int8_skip_modules=...,
        llm_int8_enable_fp32_cpu_offload=...,
        llm_int8_has_fp16_weight=...,
        bnb_4bit_compute_dtype=...,
        bnb_4bit_quant_type=...,
        bnb_4bit_use_double_quant=...,
        bnb_4bit_quant_storage=...,
        **kwargs,
    ) -> None: ...
    @property
    def load_in_4bit(self): ...
    @load_in_4bit.setter
    def load_in_4bit(self, value: bool): ...
    @property
    def load_in_8bit(self): ...
    @load_in_8bit.setter
    def load_in_8bit(self, value: bool): ...
    def post_init(self): ...
    def is_quantizable(self): ...
    def quantization_method(self): ...
    def to_dict(self) -> dict[str, Any]: ...
    def __repr__(self): ...
    def to_diff_dict(self) -> dict[str, Any]: ...

class ExllamaVersion(int, Enum):
    ONE = ...
    TWO = ...

@dataclass
class GPTQConfig(QuantizationConfigMixin):
    def __init__(
        self,
        bits: int,
        tokenizer: Any = ...,
        dataset: Optional[Union[list[str], str]] = ...,
        group_size: int = ...,
        damp_percent: float = ...,
        desc_act: bool = ...,
        sym: bool = ...,
        true_sequential: bool = ...,
        checkpoint_format: str = ...,
        meta: Optional[dict[str, Any]] = ...,
        backend: Optional[str] = ...,
        use_cuda_fp16: bool = ...,
        model_seqlen: Optional[int] = ...,
        block_name_to_quantize: Optional[str] = ...,
        module_name_preceding_first_block: Optional[list[str]] = ...,
        batch_size: int = ...,
        pad_token_id: Optional[int] = ...,
        use_exllama: Optional[bool] = ...,
        max_input_length: Optional[int] = ...,
        exllama_config: Optional[dict[str, Any]] = ...,
        cache_block_outputs: bool = ...,
        modules_in_block_to_quantize: Optional[list[list[str]]] = ...,
        **kwargs,
    ) -> None: ...
    def get_loading_attributes(self): ...
    def post_init(self): ...
    def to_dict(self): ...
    def to_dict_optimum(self): ...
    @classmethod
    def from_dict_optimum(cls, config_dict): ...

@dataclass
class AwqConfig(QuantizationConfigMixin):
    def __init__(
        self,
        bits: int = ...,
        group_size: int = ...,
        zero_point: bool = ...,
        version: AWQLinearVersion = ...,
        backend: AwqBackendPackingMethod = ...,
        do_fuse: Optional[bool] = ...,
        fuse_max_seq_len: Optional[int] = ...,
        modules_to_fuse: Optional[dict] = ...,
        modules_to_not_convert: Optional[list] = ...,
        exllama_config: Optional[dict[str, int]] = ...,
        **kwargs,
    ) -> None: ...
    def post_init(self): ...
    def get_loading_attributes(self): ...

@dataclass
class AqlmConfig(QuantizationConfigMixin):
    def __init__(
        self,
        in_group_size: int = ...,
        out_group_size: int = ...,
        num_codebooks: int = ...,
        nbits_per_codebook: int = ...,
        linear_weights_not_to_quantize: Optional[list[str]] = ...,
        **kwargs,
    ) -> None: ...
    def post_init(self): ...

@dataclass
class VptqLayerConfig(QuantizationConfigMixin):
    def __init__(
        self,
        enable_norm: bool = ...,
        enable_perm: bool = ...,
        group_num: int = ...,
        group_size: int = ...,
        in_features: int = ...,
        indices_as_float: bool = ...,
        is_indice_packed: bool = ...,
        num_centroids: tuple = ...,
        num_res_centroids: tuple = ...,
        out_features: int = ...,
        outlier_size: int = ...,
        vector_lens: tuple = ...,
        **kwargs,
    ) -> None: ...
    def post_init(self): ...

@dataclass
class VptqConfig(QuantizationConfigMixin):
    def __init__(
        self,
        enable_proxy_error: bool = ...,
        config_for_layers: dict[str, Any] = ...,
        shared_layer_config: dict[str, Any] = ...,
        modules_to_not_convert: Optional[list] = ...,
        **kwargs,
    ) -> None: ...
    def post_init(self): ...

@dataclass
class QuantoConfig(QuantizationConfigMixin):
    def __init__(
        self, weights=..., activations=..., modules_to_not_convert: Optional[list] = ..., **kwargs
    ) -> None: ...
    def post_init(self): ...

@dataclass
class EetqConfig(QuantizationConfigMixin):
    def __init__(self, weights: str = ..., modules_to_not_convert: Optional[list] = ..., **kwargs) -> None: ...
    def post_init(self): ...

class CompressedTensorsConfig(QuantizationConfigMixin):
    def __init__(
        self,
        config_groups: Optional[dict[str, Union[QuantizationScheme, list[str]]]] = ...,
        format: str = ...,
        quantization_status: QuantizationStatus = ...,
        kv_cache_scheme: Optional[QuantizationArgs] = ...,
        global_compression_ratio: Optional[float] = ...,
        ignore: Optional[list[str]] = ...,
        sparsity_config: Optional[dict[str, Any]] = ...,
        quant_method: str = ...,
        run_compressed: bool = ...,
        **kwargs,
    ) -> None: ...
    def post_init(self): ...
    @classmethod
    def from_dict(cls, config_dict, return_unused_kwargs=..., **kwargs): ...
    def to_dict(self) -> dict[str, Any]: ...
    def to_diff_dict(self) -> dict[str, Any]: ...
    def get_loading_attributes(self): ...
    @property
    def is_quantized(self): ...
    @property
    def is_quantization_compressed(self): ...
    @property
    def is_sparsification_compressed(self): ...

@dataclass
class FbgemmFp8Config(QuantizationConfigMixin):
    def __init__(
        self, activation_scale_ub: float = ..., modules_to_not_convert: Optional[list] = ..., **kwargs
    ) -> None: ...
    def get_loading_attributes(self): ...

@dataclass
class HiggsConfig(QuantizationConfigMixin):
    def __init__(
        self,
        bits: int = ...,
        p: int = ...,
        modules_to_not_convert: Optional[list[str]] = ...,
        hadamard_size: int = ...,
        group_size: int = ...,
        tune_metadata: Optional[dict[str, Any]] = ...,
        **kwargs,
    ) -> None: ...
    def post_init(self): ...

@dataclass
class FPQuantConfig(QuantizationConfigMixin):
    def __init__(
        self,
        forward_dtype: str = ...,
        forward_method: str = ...,
        backward_dtype: str = ...,
        store_master_weights: bool = ...,
        hadamard_group_size: int = ...,
        pseudoquantization: bool = ...,
        modules_to_not_convert: Optional[list[str]] = ...,
        **kwargs,
    ) -> None: ...
    def post_init(self): ...

@dataclass
class TorchAoConfig(QuantizationConfigMixin):
    quant_method: QuantizationMethod
    quant_type: Union[str, AOBaseConfig]
    modules_to_not_convert: Optional[list]
    quant_type_kwargs: dict[str, Any]
    include_input_output_embeddings: bool
    untie_embedding_weights: bool
    def __init__(
        self,
        quant_type: Union[str, AOBaseConfig],
        modules_to_not_convert: Optional[list] = ...,
        include_input_output_embeddings: bool = ...,
        untie_embedding_weights: bool = ...,
        **kwargs,
    ) -> None: ...
    def post_init(self): ...
    def get_apply_tensor_subclass(self): ...
    def to_dict(self): ...
    @classmethod
    def from_dict(cls, config_dict, return_unused_kwargs=..., **kwargs): ...

@dataclass
class BitNetQuantConfig(QuantizationConfigMixin):
    def __init__(
        self,
        modules_to_not_convert: Optional[list] = ...,
        linear_class: Optional[str] = ...,
        quantization_mode: Optional[str] = ...,
        use_rms_norm: Optional[bool] = ...,
        rms_norm_eps: Optional[float] = ...,
        **kwargs,
    ) -> None: ...
    def post_init(self): ...

@dataclass
class SpQRConfig(QuantizationConfigMixin):
    def __init__(
        self,
        bits: int = ...,
        beta1: int = ...,
        beta2: int = ...,
        shapes: Optional[dict[str, int]] = ...,
        modules_to_not_convert: Optional[list[str]] = ...,
        **kwargs,
    ) -> None: ...
    def post_init(self): ...

@dataclass
class FineGrainedFP8Config(QuantizationConfigMixin):
    def __init__(
        self,
        activation_scheme: str = ...,
        weight_block_size: tuple[int, int] = ...,
        modules_to_not_convert: Optional[list] = ...,
        **kwargs,
    ) -> None: ...
    def post_init(self): ...

class QuarkConfig(QuantizationConfigMixin):
    def __init__(self, **kwargs) -> None: ...

@dataclass
class Mxfp4Config(QuantizationConfigMixin):
    def __init__(self, modules_to_not_convert: Optional[list] = ..., dequantize: bool = ..., **kwargs) -> None: ...
    def get_loading_attributes(self): ...
