"""
This type stub file was generated by pyright.
"""

import torch
from typing import Callable
from torch.nn import Module
from torch import Tensor
from dataclasses import dataclass

"""
This type stub file was generated by pyright.
"""

@dataclass
class Intermediates:
    qk_similarities: Tensor | None = ...
    pre_softmax_attn: Tensor | None = ...
    post_softmax_attn: Tensor | None = ...
    values: Tensor | None = ...
    cached_kv: tuple[Tensor, Tensor] | None = ...
    layer_type: str | None = ...
    hybrid_hidden: Tensor | None = ...
    def to_tuple(self): ...

def exists(val): ...
def default(val, d): ...
def at_most_one_of(*bools): ...
def compact(arr): ...
@torch.jit.script
def softclamp(t: Tensor, value: float): ...
def pack_one(t, pattern): ...
def unpack_one(t, ps, pattern): ...
def once(fn): ...

print_once = ...

def log_prob_from_hard_attend(intermeds: Intermediates): ...
def selective_attn(sim, sim_head_gate=..., no_mask_sos=...): ...
def qk_l2_dist_squared(q, k): ...
def one_hot_straight_through(logits, temperature=...): ...
def sparse_topk_attn(logits, sparse_topk, temperature=..., straight_through=...): ...
def create_causal_mask(i, j, device): ...
def onnx_create_causal_mask(i, j, device): ...

class Attend(Module):
    def __init__(
        self,
        *,
        dropout=...,
        causal=...,
        heads=...,
        pre_talking_heads=...,
        post_talking_heads=...,
        pre_scale_post_talking_heads=...,
        sparse_topk=...,
        sparse_topk_straight_through=...,
        scale=...,
        qk_norm=...,
        l2_distance=...,
        sigmoid=...,
        gumbel_softmax=...,
        gumbel_softmax_temp=...,
        gumbel_softmax_hard=...,
        cog_signed=...,
        custom_attn_fn: Callable | None = ...,
        flash=...,
        softclamp_logits=...,
        logit_softclamp_value=...,
        add_zero_kv=...,
        head_learned_sink=...,
        selective=...,
        hard=...,
        cope=...,
        onnxable=...,
        sdp_kwargs: dict = ...,
    ) -> None: ...
    def flash_attn(self, q, k, v, mask=..., attn_bias=...): ...
    def forward(self, q, k, v, mask=..., attn_bias=..., prev_attn=...):
        """
        einstein notation
        b - batch
        h - heads
        n, i, j - sequence length (base sequence length, source, target)
        d - feature dimension
        """
        ...
