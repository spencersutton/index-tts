"""
This type stub file was generated by pyright.
"""

import torch
from torch.nn import Module
from x_transformers.x_transformers import AttentionLayers

"""
This type stub file was generated by pyright.
"""

def exists(val): ...
def default(val, d): ...
def sample_from_mean_variance(mean, variance, eps=..., temperature=...): ...
def masked_mean(t, mask): ...

class GaussianNLL(Module):
    def forward(self, pred, target): ...

class ContinuousTransformerWrapper(Module):
    def __init__(
        self,
        *,
        max_seq_len,
        attn_layers: AttentionLayers,
        dim_in=...,
        dim_out=...,
        emb_dim=...,
        max_mem_len=...,
        num_memory_tokens=...,
        post_emb_norm=...,
        emb_dropout=...,
        use_abs_pos_emb=...,
        scaled_sinu_pos_emb=...,
        average_pool_embed=...,
        probabilistic=...,
    ) -> None: ...
    def forward(
        self,
        x,
        return_embeddings=...,
        return_intermediates=...,
        return_mems=...,
        mask=...,
        lens=...,
        return_attn=...,
        mems=...,
        mem_masks=...,
        pos=...,
        sum_embeds=...,
        prepend_embeds=...,
        prepend_mask=...,
        cache: LayerIntermediates | None = ...,
        input_not_include_cache=...,
        seq_start_pos=...,
        **kwargs,
    ): ...

class ContinuousAutoregressiveWrapper(Module):
    def __init__(
        self,
        net: ContinuousTransformerWrapper,
        loss_fn: Module | None = ...,
        use_l1_loss=...,
        equal_loss_weight_batch=...,
    ) -> None: ...
    @torch.no_grad()
    def generate(self, start_tokens, seq_len, temperature=..., cache_kv=..., **kwargs): ...
    def forward_rollout(self, x, rollout_steps=..., **kwargs): ...
    def forward(self, x, rollout_steps=..., **kwargs): ...
