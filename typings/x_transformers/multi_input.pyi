from torch import Tensor
from torch.nn import Module
from x_transformers.x_transformers import AttentionLayers, LayerIntermediates

"""
This type stub file was generated by pyright.
"""

def exists(val): ...
def default(val, d): ...

class MultiInputTransformerWrapper(Module):
    def __init__(
        self,
        *,
        num_tokens: dict[str, int] = ...,
        max_seq_len,
        attn_layers: AttentionLayers,
        emb_dim=...,
        max_mem_len=...,
        shift_mem_down=...,
        emb_dropout=...,
        post_emb_norm=...,
        num_memory_tokens=...,
        memory_tokens_interspersed_every=...,
        return_only_embed=...,
        use_abs_pos_emb=...,
        scaled_sinu_pos_emb=...,
        emb_frac_gradient=...,
        attn_z_loss_weight=...,
    ) -> None: ...
    def forward(
        self,
        x: dict[str, Tensor],
        return_embeddings=...,
        return_logits_and_embeddings=...,
        return_intermediates=...,
        mask=...,
        return_mems=...,
        return_attn=...,
        mems=...,
        mem_masks=...,
        pos=...,
        prepend_embeds=...,
        prepend_mask=...,
        sum_embeds=...,
        return_attn_z_loss=...,
        attn_z_loss_weight=...,
        seq_start_pos=...,
        cache: LayerIntermediates | None = ...,
        **kwargs,
    ): ...
