import torch
from torch.nn import Module
from x_transformers.x_transformers import TransformerWrapper

"""
This type stub file was generated by pyright.
"""
Losses = ...

def exists(val): ...
def default(val, d): ...
def top_k(logits, thres=...): ...
def log(t, eps=...): ...
def gumbel_noise(t): ...
def gumbel_sample(t, temperature=..., dim=...): ...
def sample_prob(prob): ...
def coin_flip(): ...
def get_mask_subset_prob(mask, prob, min_mask=...): ...
def linear_schedule(t): ...
def cosine_schedule(t): ...

class SelfCritic(Module):
    def __init__(self, net) -> None: ...
    def forward(self, x): ...

class NonAutoregressiveWrapper(Module):
    def __init__(
        self,
        net,
        *,
        mask_id,
        steps=...,
        self_cond=...,
        self_cond_train_prob=...,
        no_replace_prob=...,
        random_token_prob=...,
        schedule=...,
        can_mask_prev_unmasked=...,
        token_critic: TransformerWrapper | None = ...,
        self_token_critic=...,
        critic_loss_weight=...,
        use_simple_mdlm_loss_weight=...,
    ) -> None: ...
    @torch.no_grad()
    def generate(
        self,
        batch_size=...,
        start_temperature=...,
        filter_thres=...,
        noise_level_scale=...,
        **kwargs,
    ): ...
    def forward(
        self,
        x,
        only_train_generator=...,
        only_train_critic=...,
        generator_sample_temperature=...,
        **kwargs,
    ): ...
