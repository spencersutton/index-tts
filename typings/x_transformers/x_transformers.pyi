from collections.abc import Callable
from dataclasses import dataclass

import torch
from torch import Tensor, nn
from torch.amp import autocast
from torch.nn import Module
from x_transformers.attend import Intermediates

"""
This type stub file was generated by pyright.
"""
DEFAULT_DIM_HEAD = ...

@dataclass
class LayerIntermediates:
    hiddens: list[Tensor] | None = ...
    last_hidden: Tensor | None = ...
    attn_intermediates: list[Intermediates] | None = ...
    layer_hiddens: list[Tensor] | None = ...
    attn_z_loss: Tensor | None = ...
    mems: Tensor | None = ...
    last_layer_hiddens: Tensor | None = ...
    initial_embeds: Tensor | None = ...
    attn_pooled_tokens: Tensor | None = ...
    memory_tokens: Tensor | None = ...
    logit_entropies: Tensor | None = ...
    logits: Tensor | None = ...
    cache_length: int = ...

LinearNoBias = ...

def exists(val): ...
def default(val, d): ...
def identity(t, *args, **kwargs): ...
def first(it, default=...): ...
def is_empty(x): ...
def cast_tuple(val, depth=...): ...
def divisible_by(num, den): ...
def detach_all(obj): ...
def maybe(fn=...): ...
def at_most_one_of(*bools): ...

class always:
    def __init__(self, val) -> None: ...
    def __call__(self, *args, **kwargs): ...

class not_equals:
    def __init__(self, val) -> None: ...
    def __call__(self, x, *args, **kwargs): ...

class equals:
    def __init__(self, val) -> None: ...
    def __call__(self, x, *args, **kwargs): ...

def Sequential(*modules): ...
def log(t, eps=...): ...
def max_neg_value(tensor): ...
def l2norm(t, groups=...): ...
def softclamp(t, value): ...
def masked_mean(t, mask=..., dim=...): ...
def pad_at_dim(t, pad: tuple[int, int], dim=..., value=...): ...
def or_reduce(masks): ...
def orthog_project(x, y): ...
def get_cached_kvs(
    cache: LayerIntermediates,
) -> list[tuple[Tensor, Tensor]]: ...
def calc_entropy(t: Tensor, is_prob=...): ...
def calc_z_loss(pre_softmax_attns: list[Tensor], mask=..., weight=...): ...
def init_zero_(layer): ...
def pick_and_pop(keys, d): ...
def group_dict_by_key(cond, d): ...
def string_begins_with(prefix, str): ...
def group_by_key_prefix(prefix, d): ...
def groupby_prefix_and_trim(prefix, d): ...
def dropout_seq(seq, mask, dropout): ...

class ReluSquared(Module):
    def forward(self, x): ...

class SoLU(Module):
    def __init__(self, dim) -> None: ...
    def forward(self, x): ...

class TokenEmbedding(Module):
    def __init__(self, dim, num_tokens, l2norm_embed=...) -> None: ...
    def forward(self, x): ...
    def init_(self): ...

class AbsolutePositionalEmbedding(Module):
    def __init__(self, dim, max_seq_len, l2norm_embed=...) -> None: ...
    def forward(self, x, pos=..., seq_start_pos=..., offset=...): ...

class ScaledSinusoidalEmbedding(Module):
    def __init__(self, dim, theta=...) -> None: ...
    def forward(self, x, pos=..., seq_start_pos=..., offset=...): ...

class RelativePositionBias(Module):
    def __init__(self, scale, causal=..., num_buckets=..., max_distance=..., heads=...) -> None: ...
    @property
    def device(self): ...
    def forward(self, i, j): ...

class CoPE(Module):
    def __init__(
        self,
        dim,
        heads,
        max_pos,
        soft_onehot=...,
        talking_heads=...,
        soft_onehot_temp=...,
    ) -> None: ...
    def forward(self, query, attn_logits): ...

class DynamicPositionBias(Module):
    def __init__(self, dim, *, heads, depth, log_distance=..., norm=...) -> None: ...
    @property
    def device(self): ...
    def forward(self, i, j): ...

class AlibiPositionalBias(Module):
    def __init__(self, heads, total_heads=..., slopes: list[int] | None = ..., **kwargs) -> None: ...
    @property
    def device(self): ...
    def forward_custom_pos(self, pos_i: Tensor, pos_j: Tensor | None = ...): ...
    def forward(self, i, j): ...

class DataDependentAlibi(Module):
    def __init__(self, dim, heads, causal=..., bias_init=..., post_log_scale=...) -> None: ...
    def forward(self, x): ...

class PerRowDataDependentAlibi(Module):
    def __init__(self, dim, heads, causal=..., dim_head=..., post_log_scale=...) -> None: ...
    def forward(self, x): ...

class RotaryEmbedding(Module):
    def __init__(
        self,
        dim,
        use_xpos=...,
        scale_base=...,
        interpolation_factor=...,
        base=...,
        base_rescale_factor=...,
    ) -> None: ...
    def forward_from_seq_len(self, seq_len): ...
    @autocast("cuda", enabled=False)
    def forward(self, t, offset=...): ...

def rotate_half(x): ...
@autocast("cuda", enabled=False)
def apply_rotary_pos_emb(t, freqs, scale=...): ...

class Scale(Module):
    def __init__(self, value, fn) -> None: ...
    def forward(self, x, **kwargs): ...

class LayerNorm(Module):
    def __init__(self, dim, unit_offset=...) -> None: ...
    def forward(self, x): ...

class AdaptiveLayerNorm(Module):
    def __init__(self, dim, dim_condition=...) -> None: ...
    def forward(self, x, *, condition): ...

class ScaleNorm(Module):
    def __init__(self, dim, unit_offset=...) -> None: ...
    def forward(self, x): ...

class RMSNorm(Module):
    def __init__(self, dim, unit_offset=...) -> None: ...
    def forward(self, x): ...

class AdaptiveRMSNorm(Module):
    def __init__(self, dim, dim_condition=...) -> None: ...
    def forward(self, x, *, condition): ...

class SimpleRMSNorm(Module):
    def __init__(self, dim, **kwargs) -> None: ...
    def forward(self, x): ...

class MultiheadRMSNorm(Module):
    def __init__(self, dim, heads) -> None: ...
    def forward(self, x): ...

class DynamicTanh(Module):
    def __init__(self, dim, init_alpha=..., gamma=..., beta=..., unit_offset=...) -> None: ...
    def forward(self, x): ...

class Residual(Module):
    def __init__(self, dim, scale_residual=..., scale_residual_constant=..., **kwargs) -> None: ...
    def prepare(self, residual): ...
    def forward(self, x, residual, **kwargs): ...

class GRUGating(Module):
    def __init__(self, dim, scale_residual=..., **kwargs) -> None: ...
    def prepare(self, residual): ...
    def forward(self, x, residual, **kwargs): ...

class HyperConnection(Module):
    def __init__(
        self,
        dim,
        *,
        layer_index,
        num_residual_streams,
        num_input_views=...,
        tanh=...,
        **kwargs,
    ) -> None: ...
    def prepare(self, residuals): ...
    def forward(self, x, residuals, *, beta): ...

class DynamicLIMe(Module):
    def __init__(self, dim, num_layers, num_views=..., norm=..., use_softmax=...) -> None: ...
    def forward(self, x, hiddens): ...

def shift(t, amount, mask=...): ...

class ShiftTokens(Module):
    def __init__(self, shifts, fn) -> None: ...
    def forward(self, x, **kwargs): ...

class FoldAxially(Module):
    def __init__(self, axial_dim, fn: Module) -> None: ...
    def forward(self, x, *args, **kwargs): ...

class LayerScale(Module):
    def __init__(self, fn: Module, dim, init_value=..., unit_offset=...) -> None: ...
    def forward(self, x, **kwargs): ...

class AdaptiveLayerScale(Module):
    def __init__(self, fn: Module, dim, dim_condition=..., init_bias_value=...) -> None: ...
    def forward(self, x, *, condition, **kwargs): ...

class ConcatCombine(Module):
    def __init__(self, dim, prev_layer_ind) -> None: ...
    def forward(self, x, prev_layers: list[Tensor]): ...

class GLU(Module):
    def __init__(self, dim_in, dim_out, activation: Callable, mult_bias=...) -> None: ...
    def forward(self, x): ...

class FeedForward(Module):
    def __init__(
        self,
        dim,
        dim_out=...,
        mult=...,
        glu=...,
        glu_mult_bias=...,
        swish=...,
        relu_squared=...,
        solu=...,
        custom_activation=...,
        post_act_ln=...,
        dropout=...,
        sublayer_dropout=...,
        no_bias=...,
        zero_init_output=...,
    ) -> None: ...
    def muon_parameters(self): ...
    def forward(self, x, deep_embed=...): ...

class Attention(Module):
    def __init__(
        self,
        dim,
        dim_head=...,
        dim_context=...,
        heads=...,
        causal=...,
        flash=...,
        pre_talking_heads=...,
        post_talking_heads=...,
        pre_scale_post_talking_heads=...,
        head_scale=...,
        sparse_topk=...,
        sparse_topk_straight_through=...,
        num_mem_kv=...,
        dropout=...,
        sublayer_dropout=...,
        on_attn=...,
        gate_value_heads=...,
        swiglu_values=...,
        gate_values=...,
        zero_init_output=...,
        hard=...,
        max_attend_past=...,
        qk_norm=...,
        qk_norm_groups=...,
        qk_norm_scale=...,
        qk_norm_dim_scale=...,
        value_rmsnorm=...,
        l2_distance=...,
        sigmoid=...,
        gumbel_softmax=...,
        gumbel_softmax_temp=...,
        gumbel_softmax_hard=...,
        selective=...,
        cog_signed=...,
        custom_attn_fn: Callable | None = ...,
        hybrid_module: Module | None = ...,
        hybrid_mask_kwarg: str | None = ...,
        hybrid_fold_axial_dim: int | None = ...,
        hybrid_learned_mix=...,
        one_kv_head=...,
        kv_heads=...,
        value_dim_head=...,
        dim_out=...,
        add_zero_kv=...,
        head_learned_sink=...,
        rotate_num_heads=...,
        data_dependent_alibi=...,
        data_dependent_alibi_per_row=...,
        data_dependent_alibi_per_row_dim_head=...,
        data_dependent_alibi_kwargs: dict = ...,
        use_cope=...,
        cope_max_pos=...,
        cope_soft_onehot_pos=...,
        cope_talking_heads=...,
        softclamp_logits=...,
        logit_softclamp_value=...,
        learned_value_residual_mix=...,
        orthog_projected_values=...,
        orthog_projected_values_per_head=...,
        laser=...,
        laser_softclamp_value=...,
        qkv_receive_diff_residuals=...,
        use_latent_q=...,
        dim_latent_q=...,
        use_latent_kv=...,
        dim_latent_kv=...,
        latent_rope_subheads=...,
        onnxable=...,
        attend_sdp_kwargs: dict = ...,
    ) -> None: ...
    @torch.no_grad()
    def qk_clip_(self, pre_softmax_attn: Tensor | Intermediates, tau=...): ...
    def muon_parameters(self): ...
    def forward(
        self,
        x,
        context=...,
        mask=...,
        context_mask=...,
        attn_mask=...,
        rel_pos=...,
        attn_bias=...,
        rotary_pos_emb=...,
        context_rotary_pos_emb=...,
        pos=...,
        prev_attn=...,
        mem=...,
        mem_mask=...,
        return_intermediates=...,
        cache: Intermediates | None = ...,
        value_residual=...,
        additional_key_values: tuple[Tensor, Tensor] | None = ...,
        additional_key_value_mask=...,
        kv_input_residual=...,
    ): ...

class AttentionLayers(Module):
    def __init__(
        self,
        dim,
        depth=...,
        heads=...,
        causal=...,
        cross_attend=...,
        only_cross=...,
        use_scalenorm=...,
        use_rmsnorm=...,
        use_dynamic_tanh=...,
        dynamic_tanh_init_alpha=...,
        use_simple_rmsnorm=...,
        use_adaptive_layernorm=...,
        use_adaptive_rmsnorm=...,
        use_adaptive_layerscale=...,
        norm_add_unit_offset=...,
        dim_condition=...,
        adaptive_condition_mlp=...,
        adaptive_condition_mlp_expansion=...,
        alibi_pos_bias=...,
        alibi_num_heads=...,
        rel_pos_bias=...,
        rel_pos_num_buckets=...,
        rel_pos_max_distance=...,
        dynamic_pos_bias=...,
        dynamic_pos_bias_log_distance=...,
        dynamic_pos_bias_mlp_depth=...,
        dynamic_pos_bias_norm=...,
        rotary_pos_emb=...,
        rotary_emb_dim=...,
        rotary_xpos=...,
        rotary_interpolation_factor=...,
        rotary_xpos_scale_base=...,
        rotary_base_rescale_factor=...,
        rotate_num_heads=...,
        weight_tie_layers=...,
        custom_layers: tuple[str, ...] | None = ...,
        layers_execute_order: tuple[int, ...] | None = ...,
        sandwich_coef=...,
        par_ratio=...,
        residual_attn=...,
        cross_residual_attn=...,
        macaron=...,
        pre_norm=...,
        pre_norm_has_final_norm=...,
        gate_residual=...,
        scale_residual=...,
        scale_residual_constant=...,
        shift_tokens=...,
        sandwich_norm=...,
        softclamp_output=...,
        softclamp_output_value=...,
        zero_init_branch_output=...,
        layer_dropout=...,
        cross_attn_tokens_dropout=...,
        disable_abs_pos_emb=...,
        use_layerscale=...,
        layerscale_init_value=...,
        unet_skips=...,
        integrate_layers=...,
        layer_integrate_use_softmax=...,
        num_residual_streams=...,
        qkv_receive_diff_residuals=...,
        reinject_input=...,
        learned_reinject_input_gate=...,
        add_value_residual=...,
        learned_value_residual_mix=...,
        rel_pos_kwargs: dict = ...,
        residual_fn_kwargs: dict = ...,
        verbose=...,
        **kwargs,
    ) -> None: ...
    def attn_qk_clip_(self, intermediates: LayerIntermediates, tau=...): ...
    def muon_parameters(self): ...
    def forward(
        self,
        x,
        context=...,
        mask=...,
        context_mask=...,
        attn_mask=...,
        self_attn_kv_mask=...,
        mems=...,
        mem_masks=...,
        seq_start_pos: Tensor | None = ...,
        seq_pos_offset: int = ...,
        cache: LayerIntermediates | None = ...,
        input_not_include_cache=...,
        cache_age=...,
        return_hiddens=...,
        rotary_pos_emb=...,
        pos=...,
        context_pos=...,
        attn_bias=...,
        deep_embeds_and_ids: tuple[nn.Parameter, Tensor] | None = ...,
        self_attn_additional_kv: (LayerIntermediates | list[tuple[Tensor, Tensor]] | None) = ...,
        additional_kv_mask=...,
        detach_additional_kv=...,
        route_additional_kv_to_top=...,
        condition=...,
        in_attn_cond=...,
        layers_execute_order: tuple[int, ...] | None = ...,
        self_attn_kv_residuals: Tensor | None = ...,
        cross_attn_kv_residuals: Tensor | None = ...,
    ): ...

class Encoder(AttentionLayers):
    def __init__(self, **kwargs) -> None: ...

class Decoder(AttentionLayers):
    def __init__(self, **kwargs) -> None: ...

class PrefixDecoder(AttentionLayers):
    def __init__(self, **kwargs) -> None: ...
    def forward(self, x, *args, attn_mask=..., prefix_attn_len=..., **kwargs): ...

class CrossAttender(AttentionLayers):
    def __init__(self, **kwargs) -> None: ...

class AttentionPool(Module):
    def __init__(
        self,
        dim,
        num_pooled_tokens=...,
        dim_context=...,
        add_residual=...,
        depth=...,
        heads=...,
        dim_head=...,
        use_transformer_blocks=...,
        squeeze_output=...,
        attn_kwargs: dict = ...,
    ) -> None: ...
    def forward(self, context, mask=...): ...

class ViTransformerWrapper(Module):
    def __init__(
        self,
        *,
        image_size,
        patch_size,
        attn_layers: Encoder,
        channels=...,
        num_classes=...,
        post_emb_norm=...,
        num_register_tokens=...,
        emb_dropout=...,
    ) -> None: ...
    def forward(self, img, return_embeddings=..., return_logits_and_embeddings=...): ...

class TransformerWrapper(Module):
    def __init__(
        self,
        *,
        num_tokens,
        max_seq_len,
        attn_layers: AttentionLayers,
        embed_num_tokens: dict[str, int] = ...,
        emb_dim=...,
        max_mem_len=...,
        shift_mem_down=...,
        emb_dropout=...,
        post_emb_norm=...,
        num_memory_tokens=...,
        memory_tokens_interspersed_every=...,
        tie_embedding=...,
        logits_dim=...,
        return_only_embed=...,
        num_output_heads=...,
        use_abs_pos_emb=...,
        scaled_sinu_pos_emb=...,
        l2norm_embed=...,
        recycling=...,
        train_max_recycle_steps=...,
        emb_frac_gradient=...,
        attn_z_loss_weight=...,
        average_pool_embed=...,
        use_cls_token=...,
        num_cls_tokens=...,
        attn_pool=...,
        num_pooled_tokens=...,
        attn_pool_depth=...,
        dim_pooled_tokens=...,
        squeeze_out_last_dim=...,
        token_emb: TokenEmbedding | None = ...,
        mixture_of_softmax=...,
        mixture_of_softmax_k=...,
        sigsoftmax_logits=...,
        ff_deep_embed=...,
        to_logits: Module | None = ...,
        add_continuous_pred_head=...,
    ) -> None: ...
    def init_(self): ...
    def attn_qk_clip_(self, intermediates: LayerIntermediates, tau=...): ...
    def muon_parameters(self): ...
    def forward(
        self,
        x,
        return_embeddings=...,
        return_logits_and_embeddings=...,
        return_intermediates=...,
        return_embeddings_and_intermediates=...,
        return_logit_entropies=...,
        return_next_embed_pred=...,
        mask=...,
        return_mems=...,
        return_attn=...,
        mems=...,
        mem_masks=...,
        recycle_steps=...,
        pos=...,
        prepend_embeds=...,
        prepend_mask=...,
        embed_ids: dict[str, Tensor] = ...,
        sum_embeds=...,
        return_attn_z_loss=...,
        attn_z_loss_weight=...,
        seq_start_pos=...,
        cache: LayerIntermediates | None = ...,
        input_not_include_cache=...,
        token_emb_kwargs=...,
        to_logits_kwargs=...,
        **kwargs,
    ): ...

class XTransformer(Module):
    def __init__(
        self,
        *,
        dim,
        tie_token_emb=...,
        ignore_index=...,
        pad_value=...,
        cross_attn_tokens_dropout=...,
        **kwargs,
    ) -> None: ...
    @torch.no_grad()
    def generate(self, seq_in, seq_out_start, seq_len, mask=..., attn_mask=..., **kwargs): ...
    def forward(self, src, tgt, mask=..., attn_mask=..., src_prepend_embeds=...): ...
