"""
This type stub file was generated by pyright.
"""

import torch
from torch import Tensor, nn
from typing import Callable
from x_transformers.x_transformers import AttentionLayers

"""
This type stub file was generated by pyright.
"""
LossBreakdown = ...
GenerateReturn = ...

def exists(val): ...
def default(val, d): ...

class XValTransformerWrapper(nn.Module):
    def __init__(
        self,
        *,
        num_tokens,
        max_seq_len,
        numerical_token_id,
        attn_layers: AttentionLayers,
        emb_dim=...,
        logits_dim=...,
        tie_embedding=...,
        max_mem_len=...,
        num_memory_tokens=...,
        emb_dropout=...,
        use_abs_pos_emb=...,
        scaled_sinu_pos_emb=...,
    ) -> None: ...
    def forward(
        self,
        x: Tensor,
        x_num: Tensor,
        return_embeddings=...,
        return_intermediates=...,
        return_mems=...,
        mask=...,
        return_attn=...,
        mems=...,
        pos=...,
        prepend_embeds=...,
        **kwargs,
    ): ...

class XValAutoregressiveWrapper(nn.Module):
    def __init__(
        self, net: XValTransformerWrapper, ignore_index=..., pad_value=..., numerical_loss_weight=...
    ) -> None: ...
    @torch.no_grad()
    def generate(
        self,
        start_tokens: Tensor,
        start_numbers: Tensor,
        seq_len,
        filter_logits_fn: Callable = ...,
        filter_kwargs: dict = ...,
        temperature=...,
        **kwargs,
    ): ...
    def forward(self, x: Tensor, x_num: Tensor, return_loss_breakdown=..., **kwargs): ...
